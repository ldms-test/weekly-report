2023-07-25 12:08:47 INFO: WORK_DIR: /mnt/300G/data/2023-07-25-120847
2023-07-25 12:08:47 INFO: LOG: /mnt/300G/data/2023-07-25-120847/cygnus-weekly.log
2023-07-25 12:08:47 INFO: OVIS_NEW_GIT_SHA: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:08:47 INFO: OVIS_OLD_GIT_SHA: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:08:47 INFO: CONT_GIT_SHA: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:08:47 INFO: -----------------------------------------------
2023-07-25 12:08:47 INFO: LDMS_TEST_REPO: https://github.com/ovis-hpc/ldms-test
2023-07-25 12:08:47 INFO: LDMS_TEST_BRANCH: master
2023-07-25 12:08:47 INFO: LDMS_TEST_NEW_GIT_SHA: bce2a42312f219de61a1c269097a1504cbc72407
2023-07-25 12:08:47 INFO: LDMS_TEST_OLD_GIT_SHA: fb5e44882f90ae264f47566353d7fc4b1897caac
~/cron/ldms-test ~/cron/ldms-test
/mnt/300G/data/2023-07-25-120847 ~/cron/ldms-test ~/cron/ldms-test
2023-07-25 12:08:48 INFO: Skip building on host because GIT SHA has not changed: 
841f538cd5dc1b6336459d711ef8cc723e733f0a
OVIS_LDMS_OVIS_GIT_LONG "841f538cd5dc1b6336459d711ef8cc723e733f0a"
2023-07-25 12:08:48 INFO: Skip building containerized binary because GIT SHA has not changed: 
2023-07-25 12:08:48 INFO: -- Installation process succeeded --
2023-07-25 12:08:48 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-07-25-120847
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-07-25-120847
HEAD is now at 639ea94 2023-07-21-112227
[master 9c654c5] 2023-07-25-120847
 2 files changed, 24 insertions(+), 15648 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   639ea94..9c654c5  master -> master
~/cron/ldms-test /mnt/300G/data/2023-07-25-120847
2023-07-25 12:08:50 INFO: ==== OVIS+SOS Installation Completed ====
2023-07-25 12:08:50 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-07-25-120847 ~/cron/ldms-test ~/cron/ldms-test
2023-07-25 12:08:50 INFO: ======== direct_ldms_ls_conn_test ========
2023-07-25 12:08:50 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/direct_ldms_ls_conn_test
2023-07-25 12:08:51,411 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-07-25 12:08:51,411 TADA INFO   test-id: 702aac4ac197c9797fff7f69f03b9a87b46768593e0853acd4573d0195af1434
2023-07-25 12:08:51,411 TADA INFO   test-suite: LDMSD
2023-07-25 12:08:51,411 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-07-25 12:08:51,411 TADA INFO   test-user: narate
2023-07-25 12:08:51,411 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:08:51,865 __main__ INFO starting munged on cygnus-01-iw
2023-07-25 12:08:52,170 __main__ INFO starting munged on localhost
2023-07-25 12:08:52,403 __main__ INFO starting ldmsd on cygnus-01-iw
2023-07-25 12:08:52,705 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-07-25 12:08:57,905 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-07-25 12:08:57,905 __main__ INFO Stopping sampler daemon ...
2023-07-25 12:09:03,328 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-07-25 12:09:03,374 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-07-25 12:09:03,408 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-07-25 12:09:03,409 TADA INFO test direct_ldms_ls_conn_test ended
2023-07-25 12:09:03,617 __main__ INFO stopping munged on cygnus-01-iw
2023-07-25 12:09:04,028 __main__ INFO stopping munged on localhost
2023-07-25 12:09:04 INFO: ----------------------------------------------
2023-07-25 12:09:04 INFO: ======== direct_prdcr_subscribe_test ========
2023-07-25 12:09:04 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/direct_prdcr_subscribe_test
2023-07-25 12:09:04,868 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-07-25 12:09:04,868 TADA INFO   test-id: d53c9d5e55587d29be3709d6384948761f53b6bafd1cfdc60dd792e65650cdd5
2023-07-25 12:09:04,868 TADA INFO   test-suite: LDMSD
2023-07-25 12:09:04,868 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-07-25 12:09:04,868 TADA INFO   test-user: narate
2023-07-25 12:09:04,868 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:09:06,771 __main__ INFO starting munged on cygnus-01-iw
2023-07-25 12:09:07,596 __main__ INFO starting munged on cygnus-05-iw
2023-07-25 12:09:08,371 __main__ INFO starting munged on cygnus-03-iw
2023-07-25 12:09:09,189 __main__ INFO starting munged on cygnus-04-iw
2023-07-25 12:09:09,501 __main__ INFO starting munged on localhost
2023-07-25 12:09:09,735 __main__ INFO starting ldmsd on cygnus-01-iw
2023-07-25 12:09:10,271 __main__ INFO starting ldmsd on cygnus-05-iw
2023-07-25 12:09:10,793 __main__ INFO starting ldmsd on cygnus-03-iw
2023-07-25 12:09:11,287 __main__ INFO starting ldmsd on cygnus-04-iw
2023-07-25 12:09:18,258 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-07-25 12:09:18,259 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-07-25 12:09:18,260 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-07-25 12:09:18,260 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-07-25 12:09:18,261 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
2023-07-25 12:09:18,311 TADA INFO assertion 5, Stopping the producers succeeds: agg-1 producers stopped, passed
2023-07-25 12:09:19,313 TADA INFO assertion 6, Restarting the producers succeeds: agg-1 producers started, passed
2023-07-25 12:09:26,071 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-07-25 12:09:26,072 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-07-25 12:09:26,072 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-07-25 12:09:26,073 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-07-25 12:09:26,074 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verified, passed
2023-07-25 12:09:26,074 __main__ INFO stopping sampler-1
2023-07-25 12:09:27,492 TADA INFO assertion 12, stream-sampler-1 is not running: sampler-1 stopped, passed
2023-07-25 12:09:27,492 __main__ INFO starting sampler-1
2023-07-25 12:09:28,742 TADA INFO assertion 13, stream-sampler-1 has restarted: sampler-1 running, passed
2023-07-25 12:09:28,742 __main__ INFO allow some time for prdcr to reconnect ...
2023-07-25 12:09:34,670 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-07-25 12:09:34,670 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-07-25 12:09:34,671 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verified, passed
2023-07-25 12:09:34,672 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: unsubscribed, passed
2023-07-25 12:09:37,083 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-07-25 12:09:37,084 TADA INFO test direct_prdcr_subscribe_test ended
2023-07-25 12:09:37,289 __main__ INFO stopping munged on cygnus-01-iw
2023-07-25 12:09:37,697 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-07-25 12:09:38,141 __main__ INFO stopping munged on cygnus-05-iw
2023-07-25 12:09:38,607 __main__ INFO stopping ldmsd on cygnus-05-iw
2023-07-25 12:09:39,071 __main__ INFO stopping munged on cygnus-03-iw
2023-07-25 12:09:39,494 __main__ INFO stopping ldmsd on cygnus-03-iw
2023-07-25 12:09:39,925 __main__ INFO stopping munged on cygnus-04-iw
2023-07-25 12:09:40,340 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-07-25 12:09:40,550 __main__ INFO stopping munged on localhost
2023-07-25 12:09:40 INFO: ----------------------------------------------
2023-07-25 12:09:40 INFO: ======== agg_slurm_test ========
2023-07-25 12:09:40 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/agg_slurm_test
2023-07-25 12:09:41,447 TADA INFO starting test `agg_slurm_test`
2023-07-25 12:09:41,448 TADA INFO   test-id: c446003c4044aa98ad689022125fb5d5493380b0a2f903bc11dcb769838f4a4a
2023-07-25 12:09:41,448 TADA INFO   test-suite: LDMSD
2023-07-25 12:09:41,448 TADA INFO   test-name: agg_slurm_test
2023-07-25 12:09:41,448 TADA INFO   test-user: narate
2023-07-25 12:09:41,448 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:09:41,449 __main__ INFO -- Get or create the cluster --
2023-07-25 12:09:55,097 __main__ INFO -- Preparing syspapi JSON file --
2023-07-25 12:09:55,196 __main__ INFO -- Preparing jobpapi JSON file --
2023-07-25 12:09:55,292 __main__ INFO -- Preparing job script & programs --
2023-07-25 12:09:56,614 __main__ INFO -- Start daemons --
2023-07-25 12:10:28,238 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:10:33,240 __main__ INFO -- ldms_ls to agg-2 --
2023-07-25 12:10:33,370 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-07-25 12:10:33,493 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-07-25 12:10:38,496 __main__ INFO -- Submitting jobs --
2023-07-25 12:10:38,626 __main__ INFO job_one: 1
2023-07-25 12:10:38,745 __main__ INFO job_two: 2
2023-07-25 12:10:48,755 __main__ INFO -- Cancelling jobs --
2023-07-25 12:10:48,756 __main__ INFO job_one: 1
2023-07-25 12:10:48,876 __main__ INFO job_two: 2
2023-07-25 12:12:01,011 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-07-25 12:12:01,012 TADA INFO assertion 3, meminfo data verification: No data missing, passed
2023-07-25 12:12:01,012 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: No data missing, passed
2023-07-25 12:12:01,013 TADA INFO test agg_slurm_test ended
2023-07-25 12:12:15 INFO: ----------------------------------------------
2023-07-25 12:12:16 INFO: ======== papi_sampler_test ========
2023-07-25 12:12:16 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/papi_sampler_test
2023-07-25 12:12:17,165 TADA INFO starting test `papi_sampler_test`
2023-07-25 12:12:17,166 TADA INFO   test-id: 17ce0f98786826e732b47a83c04eed4713159bfe1927f8a15df82efafbca0466
2023-07-25 12:12:17,166 TADA INFO   test-suite: LDMSD
2023-07-25 12:12:17,166 TADA INFO   test-name: papi_sampler_test
2023-07-25 12:12:17,166 TADA INFO   test-user: narate
2023-07-25 12:12:17,166 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:12:17,166 __main__ INFO -- Get or create the cluster --
2023-07-25 12:12:22,421 __main__ INFO -- Start daemons --
2023-07-25 12:12:36,108 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-07-25 12:12:36,316 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-07-25 12:12:41,445 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-07-25 12:12:41,608 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-07-25 12:12:41,608 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-07-25 12:12:55,446 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-07-25 12:12:55,446 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-07-25 12:12:55,447 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-07-25 12:12:55,447 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-07-25 12:12:55,679 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-07-25 12:13:01,470 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-07-25 12:13:01,470 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-07-25 12:13:01,471 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-07-25 12:13:01,471 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-07-25 12:13:01,682 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-07-25 12:13:01,683 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/meminfo', 'node-1/papi0/2.0', 'node-1/papi1/3.0'}), passed
2023-07-25 12:13:12,203 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-07-25 12:13:52,524 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-07-25 12:13:54,857 TADA INFO assertion 8, Missing config file attribute is logged: : sampler.papi_sampler: papi_sampler[515]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-07-25 12:14:00,305 TADA INFO assertion 9, Bad config file is logged: : sampler.papi_sampler: configuration file syntax error., passed
2023-07-25 12:14:00,305 __main__ INFO -- Finishing Test --
2023-07-25 12:14:00,305 TADA INFO test papi_sampler_test ended
2023-07-25 12:14:00,306 __main__ INFO -- Cleaning up files --
2023-07-25 12:14:00,306 __main__ INFO -- Removing the virtual cluster --
2023-07-25 12:14:11 INFO: ----------------------------------------------
2023-07-25 12:14:12 INFO: ======== papi_store_test ========
2023-07-25 12:14:12 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/papi_store_test
2023-07-25 12:14:13,330 TADA INFO starting test `papi_store_test`
2023-07-25 12:14:13,331 TADA INFO   test-id: 5a0ba7e09ed90175ea95d5eb52dee380ba2b046f029c866f092f1b95e7241d8f
2023-07-25 12:14:13,331 TADA INFO   test-suite: LDMSD
2023-07-25 12:14:13,331 TADA INFO   test-name: papi_store_test
2023-07-25 12:14:13,331 TADA INFO   test-user: narate
2023-07-25 12:14:13,331 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:14:13,333 __main__ INFO -- Get or create the cluster --
2023-07-25 12:14:20,852 __main__ INFO -- Start daemons --
2023-07-25 12:15:02,882 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-07-25 12:15:02,883 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-07-25 12:15:02,883 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-07-25 12:15:02,883 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-07-25 12:15:02,883 TADA INFO test papi_store_test ended
2023-07-25 12:15:15 INFO: ----------------------------------------------
2023-07-25 12:15:16 INFO: ======== store_app_test ========
2023-07-25 12:15:16 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/store_app_test
2023-07-25 12:15:16,795 TADA INFO starting test `store_app_test`
2023-07-25 12:15:16,796 TADA INFO   test-id: b12956d343f6545147845c7d7c42bd6dcb621476b80b81650cc96b5074a1ea94
2023-07-25 12:15:16,796 TADA INFO   test-suite: LDMSD
2023-07-25 12:15:16,796 TADA INFO   test-name: store_app_test
2023-07-25 12:15:16,796 TADA INFO   test-user: narate
2023-07-25 12:15:16,796 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:15:16,797 __main__ INFO -- Get or create the cluster --
2023-07-25 12:15:31,107 __main__ INFO -- Preparing job script & programs --
2023-07-25 12:15:31,529 __main__ INFO -- Start daemons --
2023-07-25 12:16:03,068 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:16:08,073 __main__ INFO -- Submitting jobs --
2023-07-25 12:16:08,284 __main__ INFO job_one: 1
2023-07-25 12:16:13,434 __main__ INFO job_two: 2
2023-07-25 12:16:22,720 __main__ INFO Verifying data ...
2023-07-25 12:18:26,370 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-07-25 12:18:26,370 TADA INFO test store_app_test ended
2023-07-25 12:18:40 INFO: ----------------------------------------------
2023-07-25 12:18:41 INFO: ======== syspapi_test ========
2023-07-25 12:18:41 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/syspapi_test
2023-07-25 12:18:42,018 TADA INFO starting test `syspapi_test`
2023-07-25 12:18:42,019 TADA INFO   test-id: 539ab98070e8f27c8c400b2f145a38c8b57fb75fd39772b8ba6e61bf26aa14d9
2023-07-25 12:18:42,019 TADA INFO   test-suite: LDMSD
2023-07-25 12:18:42,019 TADA INFO   test-name: syspapi_test
2023-07-25 12:18:42,019 TADA INFO   test-user: narate
2023-07-25 12:18:42,019 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:18:42,019 __main__ INFO -- Get or create the cluster --
2023-07-25 12:18:53,654 __main__ INFO -- Write syspapi JSON config files --
2023-07-25 12:18:53,655 __main__ INFO    - db/syspapi-1.json
2023-07-25 12:18:53,655 __main__ INFO    - db/syspapi-bad.json
2023-07-25 12:18:53,656 __main__ INFO -- Start daemons --
2023-07-25 12:19:13,862 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:19:18,868 __main__ INFO -- Verifying --
2023-07-25 12:19:18,985 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-07-25 12:19:18,985 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-07-25 12:19:19,093 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-07-25 12:19:21,216 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-07-25 12:19:21,332 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-07-25 12:19:21,445 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-07-25 12:19:42,738 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-07-25 12:19:42,738 __main__ INFO  events succeeded: 77
2023-07-25 12:19:42,738 __main__ INFO  events failed: 114
2023-07-25 12:19:42,738 TADA INFO test syspapi_test ended
2023-07-25 12:19:56 INFO: ----------------------------------------------
2023-07-25 12:19:57 INFO: ======== agg_test ========
2023-07-25 12:19:57 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/agg_test
2023-07-25 12:19:57,789 TADA INFO starting test `agg_test`
2023-07-25 12:19:57,789 TADA INFO   test-id: 7ed154268b69b2b88d1e3da5e610f5c38b5bdc989da3318846ae079eb4e0324b
2023-07-25 12:19:57,789 TADA INFO   test-suite: LDMSD
2023-07-25 12:19:57,789 TADA INFO   test-name: agg_test
2023-07-25 12:19:57,789 TADA INFO   test-user: narate
2023-07-25 12:19:57,789 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:19:57,790 __main__ INFO -- Get or create the cluster --
2023-07-25 12:20:15,054 __main__ INFO -- Start daemons --
2023-07-25 12:20:51,531 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:20:56,535 __main__ INFO -- ldms_ls to agg-2 --
2023-07-25 12:20:56,651 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-07-25 12:20:57,442 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-07-25 12:20:57,442 __main__ INFO -- Terminating ldmsd on node-1 --
2023-07-25 12:20:59,773 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-25 12:21:00,016 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-07-25 12:21:00,016 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-07-25 12:21:09,602 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-3/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:21:09,718 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:21:09,718 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-07-25 12:21:12,054 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-4/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-07-25 12:21:12,164 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-07-25 12:21:12,281 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-25 12:21:12,281 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-07-25 12:21:21,835 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:21:21,835 TADA INFO test agg_test ended
2023-07-25 12:21:37 INFO: ----------------------------------------------
2023-07-25 12:21:38 INFO: ======== failover_test ========
2023-07-25 12:21:38 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/failover_test
2023-07-25 12:21:39,238 TADA INFO starting test `failover_test`
2023-07-25 12:21:39,238 TADA INFO   test-id: 8cda6e929efe94ff0c2f49a27ddfab4b258689a18078cbcef928570c00648bdb
2023-07-25 12:21:39,238 TADA INFO   test-suite: LDMSD
2023-07-25 12:21:39,238 TADA INFO   test-name: failover_test
2023-07-25 12:21:39,238 TADA INFO   test-user: narate
2023-07-25 12:21:39,239 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:21:39,239 __main__ INFO -- Get or create the cluster --
2023-07-25 12:21:56,928 __main__ INFO -- Start daemons --
2023-07-25 12:22:33,396 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:22:48,408 __main__ INFO -- ldms_ls to agg-2 --
2023-07-25 12:22:48,525 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-07-25 12:22:49,273 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-07-25 12:22:49,273 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-07-25 12:22:54,621 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:22:54,738 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:22:54,856 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-07-25 12:22:54,980 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-25 12:22:54,980 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-07-25 12:23:19,579 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-4/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-07-25 12:23:19,695 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:23:19,695 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-07-25 12:23:25,002 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:23:25,128 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:23:25,245 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-07-25 12:23:25,359 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-07-25 12:23:25,359 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-07-25 12:23:49,969 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-3/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:23:50,092 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-07-25 12:23:50,092 TADA INFO test failover_test ended
2023-07-25 12:24:05 INFO: ----------------------------------------------
2023-07-25 12:24:06 INFO: ======== ldmsd_auth_ovis_test ========
2023-07-25 12:24:06 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_auth_ovis_test
2023-07-25 12:24:07,087 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-07-25 12:24:07,087 TADA INFO   test-id: 89b0e1acb691452342a760644cde7b0e512b8462903468d6c6d53940316f9e75
2023-07-25 12:24:07,088 TADA INFO   test-suite: LDMSD
2023-07-25 12:24:07,088 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-07-25 12:24:07,088 TADA INFO   test-user: narate
2023-07-25 12:24:07,088 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:24:07,088 __main__ INFO -- Get or create the cluster --
2023-07-25 12:24:12,228 __main__ INFO -- Start daemons --
2023-07-25 12:24:18,097 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:24:23,218 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-07-25 12:24:23,341 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-07-25 12:24:23,456 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-07-25 12:24:23,739 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-07-25 12:24:23,739 TADA INFO test ldmsd_auth_ovis_test ended
2023-07-25 12:24:35 INFO: ----------------------------------------------
2023-07-25 12:24:36 INFO: ======== ldmsd_auth_test ========
2023-07-25 12:24:36 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_auth_test
2023-07-25 12:24:36,819 TADA INFO starting test `ldmsd_auth_test`
2023-07-25 12:24:36,819 TADA INFO   test-id: aa0eb47f0373960886760cbe93fda878f1b27af3e4c7f76f3462b82312f33a95
2023-07-25 12:24:36,819 TADA INFO   test-suite: LDMSD
2023-07-25 12:24:36,819 TADA INFO   test-name: ldmsd_auth_test
2023-07-25 12:24:36,819 TADA INFO   test-user: narate
2023-07-25 12:24:36,819 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:24:36,820 __main__ INFO -- Get or create the cluster --
2023-07-25 12:24:54,241 __main__ INFO -- Start daemons --
2023-07-25 12:25:40,429 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:25:45,553 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-07-25 12:25:45,668 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-07-25 12:25:45,781 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-07-25 12:25:45,892 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-07-25 12:25:46,009 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-07-25 12:25:46,010 TADA INFO test ldmsd_auth_test ended
2023-07-25 12:26:01 INFO: ----------------------------------------------
2023-07-25 12:26:02 INFO: ======== ldmsd_ctrl_test ========
2023-07-25 12:26:02 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_ctrl_test
2023-07-25 12:26:03,225 TADA INFO starting test `ldmsd_ctrl_test`
2023-07-25 12:26:03,225 TADA INFO   test-id: cad8313e6537c1c12c16ac2bc3af10238757840860cd4f12a280e1bd1f889243
2023-07-25 12:26:03,226 TADA INFO   test-suite: LDMSD
2023-07-25 12:26:03,226 TADA INFO   test-name: ldmsd_ctrl_test
2023-07-25 12:26:03,226 TADA INFO   test-user: narate
2023-07-25 12:26:03,226 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:26:03,226 __main__ INFO -- Get or create the cluster --
2023-07-25 12:26:12,240 __main__ INFO -- Start daemons --
2023-07-25 12:26:28,304 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:26:34,426 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-07-25 12:26:35,542 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-07-25 12:26:36,143 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-07-25 12:26:36,745 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-07-25 12:26:37,346 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-07-25 12:26:37,948 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-07-25 12:26:38,549 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-07-25 12:26:39,151 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-07-25 12:26:56,347 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-07-25 12:27:13,547 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-07-25 12:27:13,547 TADA INFO test ldmsd_ctrl_test ended
2023-07-25 12:27:26 INFO: ----------------------------------------------
2023-07-25 12:27:27 INFO: ======== ldmsd_stream_test2 ========
2023-07-25 12:27:27 INFO: CMD: python3 ldmsd_stream_test2 --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_stream_test2
2023-07-25 12:27:27,855 TADA INFO starting test `ldmsd_stream_test`
2023-07-25 12:27:27,855 TADA INFO   test-id: cfaa6d4e8588ac1c8232c07bc05849f91a51a7c707f1f9cd6452ef4398f0c87e
2023-07-25 12:27:27,855 TADA INFO   test-suite: LDMSD
2023-07-25 12:27:27,855 TADA INFO   test-name: ldmsd_stream_test
2023-07-25 12:27:27,855 TADA INFO   test-user: narate
2023-07-25 12:27:27,855 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:27:27,856 __main__ INFO -- Get or create the cluster --
2023-07-25 12:27:37,245 __main__ INFO -- Start daemons --
2023-07-25 12:27:54,715 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:27:56,717 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-841f538-new 
2023-07-25 12:27:59,735 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-841f538-agg-2 
2023-07-25 12:28:10,115 TADA INFO assertion 1, Check data from old ldmsd_stream at agg-1: , passed
2023-07-25 12:28:10,115 TADA INFO assertion 2, Check data from old ldmsd_stream at agg-2: , passed
2023-07-25 12:28:10,115 TADA INFO assertion 3, Check data from old ldmsd_stream at the last subscriber: , passed
2023-07-25 12:28:10,116 TADA INFO assertion 4, Check data from the matching new ldms stream at agg-1: , passed
2023-07-25 12:28:10,116 TADA INFO assertion 5, Check data from the matching new ldms stream at agg-2: , passed
2023-07-25 12:28:10,116 TADA INFO assertion 6, Check data from the matching new ldms stream at the last subscriber: , passed
2023-07-25 12:28:10,117 TADA INFO assertion 7, Check data from the non-matching new ldms stream at agg-1: , passed
2023-07-25 12:28:10,117 TADA INFO assertion 8, Check data from the non-matching new ldms stream at agg-2: , passed
2023-07-25 12:28:10,117 TADA INFO assertion 9, Check data from the non-matching new ldms stream at last subscriber: , passed
2023-07-25 12:28:10,662 TADA INFO assertion 10, Check stream_stats before stream data transfer: , passed
2023-07-25 12:28:10,662 TADA INFO assertion 11, Check stream_client_stats before stream data transfer: , passed
2023-07-25 12:28:10,663 TADA INFO assertion 12, Check stream_stats after stream data transfer: , passed
2023-07-25 12:28:10,663 TADA INFO assertion 13, Check stream_client_stats after stream data transfer: , passed
2023-07-25 12:28:10,663 TADA INFO test ldmsd_stream_test ended
2023-07-25 12:28:23 INFO: ----------------------------------------------
2023-07-25 12:28:24 INFO: ======== maestro_cfg_test ========
2023-07-25 12:28:24 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/maestro_cfg_test
2023-07-25 12:28:25,138 TADA INFO starting test `maestro_cfg_test`
2023-07-25 12:28:25,139 TADA INFO   test-id: 11d8bf52861949ecd235fc0a2a69f0d7dbeffd863820fdbcf543a5e7ac4bd930
2023-07-25 12:28:25,139 TADA INFO   test-suite: LDMSD
2023-07-25 12:28:25,139 TADA INFO   test-name: maestro_cfg_test
2023-07-25 12:28:25,139 TADA INFO   test-user: narate
2023-07-25 12:28:25,139 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:28:35,151 __main__ INFO -- Get or create cluster --
2023-07-25 12:29:00,662 __main__ INFO -- Start daemons --
2023-07-25 12:30:02,424 __main__ INFO ... make sure ldmsd's are up
2023-07-25 12:30:10,140 TADA INFO assertion 1, load maestro etcd cluster: etcd cluster loaded successfully, passed
2023-07-25 12:30:50,160 TADA INFO assertion 2, config ldmsd cluster with maestro: Maestro ldmsd configuration successful, passed
2023-07-25 12:30:51,753 TADA INFO assertion 3, verify sampler daemons: OK, passed
2023-07-25 12:30:52,360 TADA INFO assertion 4, verify L1 aggregator daemons: OK, passed
2023-07-25 12:30:52,617 TADA INFO assertion 5, verify L2 aggregator daemon: OK, passed
2023-07-25 12:30:52,936 TADA INFO assertion 6, verify data storage: OK, passed
---Wait for config to write to file---
2023-07-25 12:30:52,937 TADA INFO test maestro_cfg_test ended
2023-07-25 12:31:11 INFO: ----------------------------------------------
2023-07-25 12:31:12 INFO: ======== mt-slurm-test ========
2023-07-25 12:31:12 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1690306328.960112', '1,1690306328.960112', '2,1690306329.962724', '3,1690306329.962724', '4,1690306329.962724', '5,1690306329.962724', '6,1690306329.962724', '7,1690306329.962724', '8,1690306329.962724', '9,1690306331.918261', '10,1690306331.918261', '11,1690306331.918261', '12,1690306331.918261', '13,1690306331.918261', '14,1690306331.918261', '15,1690306331.918261', '16,1690306331.918261', '17,1690306332.906625', '18,1690306333.895946', '19,1690306333.895946', '20,1690306333.895946', '21,1690306333.895946', '22,1690306333.895946', '23,1690306333.895946', '24,1690306334.998684', '25,1690306334.998684', '26,1690306334.998684', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-07-25 12:32:48 INFO: ----------------------------------------------
2023-07-25 12:32:49 INFO: ======== ovis_ev_test ========
2023-07-25 12:32:49 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ovis_ev_test
2023-07-25 12:32:49,941 __main__ INFO -- Create the cluster -- 
2023-07-25 12:32:59,127 TADA INFO starting test `ovis_ev_test`
2023-07-25 12:32:59,127 TADA INFO   test-id: 282ef03aaf1431ca8d5fe9eebd303a13cad97fe3e00a15b0d73c8cd316e6c778
2023-07-25 12:32:59,127 TADA INFO   test-suite: test_ovis_ev
2023-07-25 12:32:59,127 TADA INFO   test-name: ovis_ev_test
2023-07-25 12:32:59,127 TADA INFO   test-user: narate
2023-07-25 12:32:59,127 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:32:59,128 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-07-25 12:32:59,129 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-07-25 12:32:59,129 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-07-25 12:32:59,129 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-07-25 12:32:59,129 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-07-25 12:32:59,130 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-07-25 12:32:59,130 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-07-25 12:32:59,130 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-07-25 12:32:59,130 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-07-25 12:32:59,130 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-07-25 12:32:59,131 TADA INFO test ovis_ev_test ended
2023-07-25 12:33:09 INFO: ----------------------------------------------
2023-07-25 12:33:10 INFO: ======== prdcr_subscribe_test ========
2023-07-25 12:33:10 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/prdcr_subscribe_test
2023-07-25 12:33:11,489 TADA INFO starting test `prdcr_subscribe_test`
2023-07-25 12:33:11,489 TADA INFO   test-id: 5d6053f3df8fdca3cade6cbaae9bfe64ff44095a1418362e142655ca3e128081
2023-07-25 12:33:11,489 TADA INFO   test-suite: LDMSD
2023-07-25 12:33:11,489 TADA INFO   test-name: prdcr_subscribe_test
2023-07-25 12:33:11,489 TADA INFO   test-user: narate
2023-07-25 12:33:11,490 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:33:58,676 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-07-25 12:33:58,677 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-07-25 12:33:58,677 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-07-25 12:33:58,677 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-07-25 12:33:58,678 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-07-25 12:33:59,043 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-07-25 12:33:59,408 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-07-25 12:34:07,399 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-07-25 12:34:07,399 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-07-25 12:34:07,400 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-07-25 12:34:07,400 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-07-25 12:34:07,400 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-07-25 12:34:08,635 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-07-25 12:34:14,077 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-07-25 12:34:21,637 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-07-25 12:34:21,638 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-07-25 12:34:21,638 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-07-25 12:34:22,016 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-07-25 12:34:25,278 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-07-25 12:34:25,278 TADA INFO test prdcr_subscribe_test ended
2023-07-25 12:34:37 INFO: ----------------------------------------------
2023-07-25 12:34:38 INFO: ======== set_array_test ========
2023-07-25 12:34:38 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/set_array_test
2023-07-25 12:34:39,528 TADA INFO starting test `set_array_test`
2023-07-25 12:34:39,528 TADA INFO   test-id: 169cb0b89df1ca07eb7fd840e31eec051cb5a5571556719dd9a31e9d2652e248
2023-07-25 12:34:39,528 TADA INFO   test-suite: LDMSD
2023-07-25 12:34:39,528 TADA INFO   test-name: set_array_test
2023-07-25 12:34:39,528 TADA INFO   test-user: narate
2023-07-25 12:34:39,528 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:34:39,529 __main__ INFO -- Get or create the cluster --
2023-07-25 12:34:44,868 __main__ INFO -- Start daemons --
2023-07-25 12:34:50,732 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:35:20,536 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 5 snapshots, passed
2023-07-25 12:35:20,536 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-07-25 12:35:20,536 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-07-25 12:35:20,536 TADA INFO test set_array_test ended
2023-07-25 12:35:31 INFO: ----------------------------------------------
2023-07-25 12:35:32 INFO: ======== setgroup_test ========
2023-07-25 12:35:32 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/setgroup_test
2023-07-25 12:35:33,505 TADA INFO starting test `setgroup_test`
2023-07-25 12:35:33,506 TADA INFO   test-id: 5bc0e60c98b581a5bfe801b5860e8afe75d3b3b7fca9d62c19f5efb544b0ea77
2023-07-25 12:35:33,506 TADA INFO   test-suite: LDMSD
2023-07-25 12:35:33,506 TADA INFO   test-name: setgroup_test
2023-07-25 12:35:33,506 TADA INFO   test-user: narate
2023-07-25 12:35:33,506 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:35:33,507 __main__ INFO -- Get or create the cluster --
2023-07-25 12:35:42,669 __main__ INFO -- Start daemons --
2023-07-25 12:35:58,781 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:36:03,786 __main__ INFO -- ldms_ls to agg-2 --
2023-07-25 12:36:03,906 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-07-25 12:36:06,160 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-07-25 12:36:06,160 __main__ INFO -- Removing test_2 from grp --
2023-07-25 12:36:06,645 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-07-25 12:36:10,779 TADA INFO assertion 4, test_2 is removed from grp on agg-1: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-07-25 12:36:14,906 TADA INFO assertion 5, test_2 is removed from grp on agg-2: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-07-25 12:36:18,911 __main__ INFO -- Adding test_2 back into grp --
2023-07-25 12:36:19,391 TADA INFO assertion 6, test_2 is added back to grp on sampler: expect {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-07-25 12:36:23,525 TADA INFO assertion 7, test_2 is added back to grp on agg-1: expect {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-07-25 12:36:25,659 TADA INFO assertion 8, test_2 is added back to grp on agg-2: expect {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-07-25 12:36:27,662 TADA INFO test setgroup_test ended
2023-07-25 12:36:40 INFO: ----------------------------------------------
2023-07-25 12:36:41 INFO: ======== slurm_stream_test ========
2023-07-25 12:36:41 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/slurm_stream_test
2023-07-25 12:36:42,047 TADA INFO starting test `slurm_stream_test`
2023-07-25 12:36:42,047 TADA INFO   test-id: 6a2d5137a19498572a6db8b957a49333490f2ac47ac6db60d6ed7ad31da68b93
2023-07-25 12:36:42,047 TADA INFO   test-suite: LDMSD
2023-07-25 12:36:42,047 TADA INFO   test-name: slurm_stream_test
2023-07-25 12:36:42,047 TADA INFO   test-user: narate
2023-07-25 12:36:42,047 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:36:42,048 __main__ INFO -- Get or create the cluster --
2023-07-25 12:36:48,956 __main__ INFO -- Start daemons --
2023-07-25 12:36:59,452 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:37:29,427 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:29,427 __main__ INFO 12345
2023-07-25 12:37:29,427 __main__ INFO 12345
2023-07-25 12:37:29,427 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,427 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-07-25 12:37:29,427 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,428 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,428 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,428 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,542 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:29,542 __main__ INFO 12345
2023-07-25 12:37:29,542 __main__ INFO 12345
2023-07-25 12:37:29,542 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,542 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-07-25 12:37:29,543 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,543 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,543 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,543 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-25 12:37:29,651 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:29,652 __main__ INFO 12346
2023-07-25 12:37:29,652 __main__ INFO 12346
2023-07-25 12:37:29,652 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,652 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-07-25 12:37:29,652 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,652 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,653 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,653 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,776 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:29,776 __main__ INFO 12346
2023-07-25 12:37:29,777 __main__ INFO 12346
2023-07-25 12:37:29,777 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,777 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-07-25 12:37:29,777 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,777 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,777 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,778 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-25 12:37:29,889 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:29,890 __main__ INFO 12347
2023-07-25 12:37:29,890 __main__ INFO 12347
2023-07-25 12:37:29,890 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-07-25 12:37:29,890 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-07-25 12:37:29,890 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:29,890 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:29,890 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:29,891 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,000 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,001 __main__ INFO 12347
2023-07-25 12:37:30,001 __main__ INFO 12347
2023-07-25 12:37:30,001 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,001 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-07-25 12:37:30,001 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,001 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,002 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,002 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-25 12:37:30,110 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,110 __main__ INFO 12348
2023-07-25 12:37:30,110 __main__ INFO 12348
2023-07-25 12:37:30,110 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,110 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-07-25 12:37:30,110 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,111 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,111 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,111 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,211 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,211 __main__ INFO 12348
2023-07-25 12:37:30,211 __main__ INFO 12348
2023-07-25 12:37:30,211 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,211 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-07-25 12:37:30,211 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,211 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,212 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,212 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-25 12:37:30,324 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,324 __main__ INFO 12355
2023-07-25 12:37:30,324 __main__ INFO 12355
2023-07-25 12:37:30,324 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,324 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-07-25 12:37:30,324 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,325 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,326 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,429 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,429 __main__ INFO 12355
2023-07-25 12:37:30,429 __main__ INFO 12355
2023-07-25 12:37:30,429 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,429 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-07-25 12:37:30,429 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,430 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,431 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-25 12:37:30,536 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,536 __main__ INFO 12356
2023-07-25 12:37:30,536 __main__ INFO 12356
2023-07-25 12:37:30,536 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,536 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,537 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,538 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,661 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,661 __main__ INFO 12356
2023-07-25 12:37:30,661 __main__ INFO 12356
2023-07-25 12:37:30,662 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,662 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-07-25 12:37:30,662 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,662 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,662 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,662 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,663 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,663 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,663 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,663 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-25 12:37:30,774 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,774 __main__ INFO 12357
2023-07-25 12:37:30,774 __main__ INFO 12357
2023-07-25 12:37:30,774 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,774 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-07-25 12:37:30,774 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,775 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,776 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,882 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,883 __main__ INFO 12357
2023-07-25 12:37:30,883 __main__ INFO 12357
2023-07-25 12:37:30,883 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,883 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-07-25 12:37:30,883 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,883 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,883 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,884 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,884 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,884 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,884 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,884 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-25 12:37:30,994 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:30,994 __main__ INFO 12358
2023-07-25 12:37:30,994 __main__ INFO 12358
2023-07-25 12:37:30,994 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,994 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-07-25 12:37:30,994 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:30,995 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,114 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-25 12:37:31,115 __main__ INFO 12358
2023-07-25 12:37:31,115 __main__ INFO 12358
2023-07-25 12:37:31,115 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,115 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-07-25 12:37:31,115 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,115 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:31,116 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-25 12:37:33,243 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-07-25 12:37:33,243 __main__ INFO 12353
2023-07-25 12:37:33,243 __main__ INFO 12353
2023-07-25 12:37:33,243 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,243 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,244 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,245 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,245 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-25 12:37:33,245 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-07-25 12:37:33,245 TADA INFO test slurm_stream_test ended
2023-07-25 12:37:44 INFO: ----------------------------------------------
2023-07-25 12:37:45 INFO: ======== spank_notifier_test ========
2023-07-25 12:37:45 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/spank_notifier_test
2023-07-25 12:37:46,267 TADA INFO starting test `spank_notifier_test`
2023-07-25 12:37:46,267 TADA INFO   test-id: 8a7993f12ff16fdec342d545cb89dc99a0e66b7e3e41ba257e6ffba9e37a4beb
2023-07-25 12:37:46,267 TADA INFO   test-suite: Slurm_Plugins
2023-07-25 12:37:46,267 TADA INFO   test-name: spank_notifier_test
2023-07-25 12:37:46,267 TADA INFO   test-user: narate
2023-07-25 12:37:46,267 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:37:46,268 __main__ INFO -- Create the cluster --
2023-07-25 12:38:11,729 __main__ INFO -- Cleanup output --
2023-07-25 12:38:12,032 __main__ INFO -- Test bad plugstack config --
2023-07-25 12:38:12,032 __main__ INFO Starting slurm ...
2023-07-25 12:38:25,902 __main__ INFO Starting slurm ... OK
2023-07-25 12:38:46,356 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-25 12:38:46,484 __main__ INFO   jobid = 1
2023-07-25 12:38:46,698 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-25 12:38:46,813 __main__ INFO   jobid = 2
2023-07-25 12:38:47,022 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-25 12:38:47,144 __main__ INFO   jobid = 3
2023-07-25 12:38:47,344 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-25 12:38:47,456 __main__ INFO   jobid = 4
2023-07-25 12:38:57,069 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-07-25 12:38:57,070 __main__ INFO Killin slurm ...
2023-07-25 12:38:59,988 __main__ INFO Killin slurm ... OK
2023-07-25 12:39:19,989 __main__ INFO -- Start daemons --
2023-07-25 12:39:30,286 __main__ INFO Starting slurm ... OK
2023-07-25 12:39:50,543 __main__ INFO -- Submitting job with no stream listener --
2023-07-25 12:39:50,737 __main__ INFO -- Submitting job with num_tasks 8 --
2023-07-25 12:39:50,851 __main__ INFO   jobid = 5
2023-07-25 12:40:06,808 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-07-25 12:40:06,808 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-07-25 12:40:12,696 __main__ INFO -- Submitting job with listener --
2023-07-25 12:40:12,911 __main__ INFO -- Submitting job with num_tasks 1 --
2023-07-25 12:40:13,017 __main__ INFO   jobid = 6
2023-07-25 12:40:13,221 __main__ INFO -- Submitting job with num_tasks 2 --
2023-07-25 12:40:13,342 __main__ INFO   jobid = 7
2023-07-25 12:40:13,543 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-25 12:40:13,665 __main__ INFO   jobid = 8
2023-07-25 12:40:13,880 __main__ INFO -- Submitting job with num_tasks 8 --
2023-07-25 12:40:13,992 __main__ INFO   jobid = 9
2023-07-25 12:40:14,207 __main__ INFO -- Submitting job with num_tasks 27 --
2023-07-25 12:40:14,327 __main__ INFO   jobid = 10
2023-07-25 12:40:36,063 __main__ INFO -- Verifying Events --
2023-07-25 12:40:36,064 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-07-25 12:40:36,064 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-25 12:40:36,064 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-25 12:40:36,064 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-25 12:40:36,064 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-07-25 12:40:36,065 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-07-25 12:40:36,065 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-25 12:40:36,065 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-25 12:40:36,065 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-25 12:40:36,065 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-07-25 12:40:36,066 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-07-25 12:40:36,067 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-25 12:40:36,067 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-25 12:40:36,067 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-25 12:40:36,067 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-07-25 12:40:36,068 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-07-25 12:40:36,068 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-25 12:40:36,068 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-25 12:40:36,068 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-25 12:40:36,068 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-07-25 12:40:36,068 __main__ INFO job 7 multi-tenant with dict_keys([6])
2023-07-25 12:40:36,068 __main__ INFO job 10 multi-tenant with dict_keys([6, 7])
2023-07-25 12:40:36,069 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-07-25 12:40:36,069 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-07-25 12:40:36,069 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-07-25 12:40:36,069 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-07-25 12:40:36,276 __main__ INFO -- Submitting job that crashes listener --
2023-07-25 12:40:36,398 __main__ INFO   jobid = 11
2023-07-25 12:40:46,628 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-07-25 12:40:46,739 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-07-25 12:40:46,739 TADA INFO test spank_notifier_test ended
2023-07-25 12:41:03 INFO: ----------------------------------------------
2023-07-25 12:41:04 INFO: ======== ldms_list_test ========
2023-07-25 12:41:04 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_list_test
2023-07-25 12:41:05,235 TADA INFO starting test `ldms_list_test`
2023-07-25 12:41:05,235 TADA INFO   test-id: b7ebd2d8a3c5c7bde691b6d999c92ded32fe14fcbe1be191a821eaaf008ff02b
2023-07-25 12:41:05,235 TADA INFO   test-suite: LDMSD
2023-07-25 12:41:05,235 TADA INFO   test-name: ldms_list_test
2023-07-25 12:41:05,235 TADA INFO   test-user: narate
2023-07-25 12:41:05,235 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:41:05,236 __main__ INFO -- Get or create the cluster --
2023-07-25 12:41:08,224 __main__ INFO -- Start daemons --
2023-07-25 12:41:16,595 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:41:18,598 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-07-25 12:41:24,634 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-07-25 12:41:24,634 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-07-25 12:41:24,635 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-07-25 12:41:24,635 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-07-25 12:41:24,635 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-07-25 12:41:24,635 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-07-25 12:41:24,636 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-07-25 12:41:24,636 __main__ INFO 2nd sampling on the sampler...
2023-07-25 12:41:31,845 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-07-25 12:41:31,846 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-07-25 12:41:31,846 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-07-25 12:41:31,846 __main__ INFO 2nd update on the aggregator...
2023-07-25 12:41:39,055 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-07-25 12:41:39,056 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-07-25 12:41:39,056 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-07-25 12:41:39,056 __main__ INFO 3rd sampling on the sampler...
2023-07-25 12:41:46,265 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-07-25 12:41:46,266 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-07-25 12:41:46,266 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-07-25 12:41:46,266 __main__ INFO 3rd update on the aggregator...
2023-07-25 12:41:53,475 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-07-25 12:41:53,476 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-07-25 12:41:53,476 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-07-25 12:41:53,476 __main__ INFO 4th sampling on the sampler...
2023-07-25 12:42:00,685 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-07-25 12:42:00,685 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-07-25 12:42:00,686 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-07-25 12:42:00,686 __main__ INFO 4th update on the aggregator...
2023-07-25 12:42:07,895 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-07-25 12:42:07,895 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-07-25 12:42:07,896 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-07-25 12:42:07,896 __main__ INFO 5th sampling on the sampler...
2023-07-25 12:42:15,105 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-07-25 12:42:15,106 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-07-25 12:42:15,106 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-07-25 12:42:15,106 __main__ INFO 5th update on the aggregator...
2023-07-25 12:42:22,315 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-07-25 12:42:22,316 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-07-25 12:42:22,316 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-07-25 12:42:22,316 __main__ INFO 6th sampling on the sampler...
2023-07-25 12:42:29,525 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-07-25 12:42:29,526 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-07-25 12:42:29,526 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-07-25 12:42:29,526 __main__ INFO 6th update on the updator...
2023-07-25 12:42:36,735 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-07-25 12:42:36,736 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-07-25 12:42:36,736 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-07-25 12:42:36,736 TADA INFO test ldms_list_test ended
2023-07-25 12:42:47 INFO: ----------------------------------------------
2023-07-25 12:42:48 INFO: ======== quick_set_add_rm_test ========
2023-07-25 12:42:48 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/quick_set_add_rm_test
2023-07-25 12:42:48,993 TADA INFO starting test `quick_set_add_rm_test`
2023-07-25 12:42:48,993 TADA INFO   test-id: 7f7b71f6c7aae9ebd14060e47bf2f38f2bd4bf4c79dc8aee669b12655a4991f6
2023-07-25 12:42:48,993 TADA INFO   test-suite: LDMSD
2023-07-25 12:42:48,993 TADA INFO   test-name: quick_set_add_rm_test
2023-07-25 12:42:48,993 TADA INFO   test-user: narate
2023-07-25 12:42:48,993 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:42:48,994 __main__ INFO -- Get or create the cluster --
2023-07-25 12:42:56,240 __main__ INFO -- Start samp.py --
2023-07-25 12:43:01,356 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-07-25 12:43:01,357 __main__ INFO -- Start daemons --
2023-07-25 12:43:10,995 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:43:16,593 TADA INFO assertion 2, verify data: verified, passed
2023-07-25 12:43:21,209 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-07-25 12:43:25,811 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-07-25 12:43:30,384 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-07-25 12:43:35,510 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-07-25 12:43:35,510 TADA INFO test quick_set_add_rm_test ended
2023-07-25 12:43:47 INFO: ----------------------------------------------
2023-07-25 12:43:48 INFO: ======== set_array_hang_test ========
2023-07-25 12:43:48 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/set_array_hang_test
2023-07-25 12:43:49,176 TADA INFO starting test `set_array_hang_test`
2023-07-25 12:43:49,176 TADA INFO   test-id: 66e234c59e5cf2402a63d75f4f6759ee16d164bacb0ed650d9db44050adc47d8
2023-07-25 12:43:49,176 TADA INFO   test-suite: LDMSD
2023-07-25 12:43:49,176 TADA INFO   test-name: set_array_hang_test
2023-07-25 12:43:49,176 TADA INFO   test-user: narate
2023-07-25 12:43:49,176 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:43:49,177 __main__ INFO -- Get or create the cluster --
2023-07-25 12:43:52,188 __main__ INFO -- Start processes --
2023-07-25 12:43:52,188 __main__ INFO starting interactive set_array_samp.py
2023-07-25 12:43:55,203 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-07-25 12:43:58,221 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-07-25 12:44:05,429 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-07-25 12:44:12,638 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-07-25 12:44:16,243 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-07-25 12:44:23,452 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-07-25 12:44:23,453 TADA INFO test set_array_hang_test ended
2023-07-25 12:44:34 INFO: ----------------------------------------------
2023-07-25 12:44:35 INFO: ======== ldmsd_autointerval_test ========
2023-07-25 12:44:35 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_autointerval_test
2023-07-25 12:44:35,711 TADA INFO starting test `ldmsd_autointerval_test`
2023-07-25 12:44:35,711 TADA INFO   test-id: 2e32f57c99240cd96a5192c83a683e48b6a9fffcd6ffbca5a20b84b709067ff7
2023-07-25 12:44:35,711 TADA INFO   test-suite: LDMSD
2023-07-25 12:44:35,711 TADA INFO   test-name: ldmsd_autointerval_test
2023-07-25 12:44:35,711 TADA INFO   test-user: narate
2023-07-25 12:44:35,711 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:44:35,712 __main__ INFO -- Get or create the cluster --
2023-07-25 12:44:42,699 __main__ INFO -- Start daemons --
2023-07-25 12:44:58,150 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:45:04,671 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-07-25 12:45:06,892 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-07-25 12:45:06,892 __main__ INFO Let them run for a while to collect data ...
2023-07-25 12:45:16,893 __main__ INFO Setting sample interval to 1000000 ...
2023-07-25 12:45:25,130 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-07-25 12:45:25,130 __main__ INFO Let them run for a while to collect data ...
2023-07-25 12:45:35,140 __main__ INFO Setting sample interval to 2000000 ...
2023-07-25 12:45:43,397 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-07-25 12:45:43,397 __main__ INFO Let them run for a while to collect data ...
2023-07-25 12:45:53,647 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-07-25 12:45:53,770 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-07-25 12:45:53,771 TADA INFO test ldmsd_autointerval_test ended
2023-07-25 12:46:05 INFO: ----------------------------------------------
2023-07-25 12:46:06 INFO: ======== ldms_record_test ========
2023-07-25 12:46:06 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_record_test
2023-07-25 12:46:07,317 TADA INFO starting test `ldms_record_test`
2023-07-25 12:46:07,317 TADA INFO   test-id: d7ff39779bd75024af70bbe5e937a6fe53c2c35707a573ae8351869919b95fd0
2023-07-25 12:46:07,317 TADA INFO   test-suite: LDMSD
2023-07-25 12:46:07,318 TADA INFO   test-name: ldms_record_test
2023-07-25 12:46:07,318 TADA INFO   test-user: narate
2023-07-25 12:46:07,318 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:46:07,318 __main__ INFO -- Get or create the cluster --
2023-07-25 12:46:10,392 __main__ INFO -- Start daemons --
2023-07-25 12:46:18,767 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:46:20,769 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-07-25 12:46:26,805 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-07-25 12:46:26,805 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-07-25 12:46:26,805 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-07-25 12:46:26,806 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-07-25 12:46:26,806 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-07-25 12:46:26,806 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-07-25 12:46:26,807 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-07-25 12:46:26,807 __main__ INFO 2nd sampling on the sampler...
2023-07-25 12:46:34,016 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-07-25 12:46:34,017 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-07-25 12:46:34,017 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-07-25 12:46:34,017 __main__ INFO 2nd update on the aggregator...
2023-07-25 12:46:41,226 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-07-25 12:46:41,226 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-07-25 12:46:41,226 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-07-25 12:46:41,227 __main__ INFO 3rd sampling on the sampler...
2023-07-25 12:46:48,436 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-07-25 12:46:48,436 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-07-25 12:46:48,437 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-07-25 12:46:48,437 __main__ INFO 3rd update on the aggregator...
2023-07-25 12:46:55,646 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-07-25 12:46:55,646 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-07-25 12:46:55,647 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-07-25 12:46:55,647 __main__ INFO 4th sampling on the sampler...
2023-07-25 12:47:02,856 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-07-25 12:47:02,857 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-07-25 12:47:02,857 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-07-25 12:47:02,857 __main__ INFO 4th update on the aggregator...
2023-07-25 12:47:10,066 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-07-25 12:47:10,067 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-07-25 12:47:10,067 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-07-25 12:47:10,067 __main__ INFO 5th sampling on the sampler...
2023-07-25 12:47:17,276 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-07-25 12:47:17,277 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-07-25 12:47:17,277 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-07-25 12:47:17,277 __main__ INFO 5th update on the aggregator...
2023-07-25 12:47:24,486 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-07-25 12:47:24,487 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-07-25 12:47:24,487 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-07-25 12:47:24,487 __main__ INFO 6th sampling on the sampler...
2023-07-25 12:47:31,696 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-07-25 12:47:31,697 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-07-25 12:47:31,697 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-07-25 12:47:31,697 __main__ INFO 6th update on the updator...
2023-07-25 12:47:38,906 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-07-25 12:47:38,907 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-07-25 12:47:38,907 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-07-25 12:47:38,908 TADA INFO test ldms_record_test ended
2023-07-25 12:47:49 INFO: ----------------------------------------------
2023-07-25 12:47:50 INFO: ======== ldms_schema_digest_test ========
2023-07-25 12:47:50 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_schema_digest_test
2023-07-25 12:47:51,194 TADA INFO starting test `ldms_schema_digest_test`
2023-07-25 12:47:51,194 TADA INFO   test-id: df4ae970869d6e781fa32a5ef9085a9829fbfce99991379d75a30b937ef0d7a2
2023-07-25 12:47:51,195 TADA INFO   test-suite: LDMSD
2023-07-25 12:47:51,195 TADA INFO   test-name: ldms_schema_digest_test
2023-07-25 12:47:51,195 TADA INFO   test-user: narate
2023-07-25 12:47:51,195 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:47:51,195 __main__ INFO -- Get or create the cluster --
2023-07-25 12:47:58,243 __main__ INFO -- Start daemons --
2023-07-25 12:48:09,224 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:48:14,346 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-07-25 12:48:14,456 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-07-25 12:48:14,575 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-07-25 12:48:14,775 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-07-25 12:48:14,775 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-07-25 12:48:14,775 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-07-25 12:48:17,098 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-07-25 12:48:17,098 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-07-25 12:48:17,099 TADA INFO test ldms_schema_digest_test ended
2023-07-25 12:48:29 INFO: ----------------------------------------------
2023-07-25 12:48:30 INFO: ======== ldmsd_decomp_test ========
2023-07-25 12:48:30 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_decomp_test
2023-07-25 12:48:30,811 TADA INFO starting test `ldmsd_decomp_test`
2023-07-25 12:48:30,811 TADA INFO   test-id: 21715ce1ad023e70393a106e52efdda8b4b9956cc0b025457069e5682538a91a
2023-07-25 12:48:30,811 TADA INFO   test-suite: LDMSD
2023-07-25 12:48:30,811 TADA INFO   test-name: ldmsd_decomp_test
2023-07-25 12:48:30,811 TADA INFO   test-user: narate
2023-07-25 12:48:30,812 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:48:30,812 __main__ INFO -- Get or create the cluster --
2023-07-25 12:48:46,248 __main__ INFO -- Start daemons --
2023-07-25 12:49:15,691 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:50:10,219 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-07-25 12:50:10,219 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-07-25 12:50:10,219 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-07-25 12:50:10,219 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-07-25 12:50:10,219 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-07-25 12:50:10,220 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-07-25 12:50:10,221 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-07-25 12:50:10,222 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-07-25 12:50:10,223 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-07-25 12:50:10,225 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-07-25 12:50:10,306 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-07-25 12:50:10,310 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-07-25 12:50:10,313 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-07-25 12:50:10,323 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-07-25 12:50:10,325 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-07-25 12:50:10,327 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-07-25 12:50:10,403 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-07-25 12:50:10,408 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-07-25 12:50:10,411 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-07-25 12:50:10,422 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-07-25 12:50:10,423 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-07-25 12:50:10,424 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-07-25 12:50:10,453 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-07-25 12:50:10,455 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-07-25 12:50:10,457 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-07-25 12:50:10,462 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-07-25 12:50:10,462 TADA INFO test ldmsd_decomp_test ended
2023-07-25 12:50:10,462 TADA INFO test ldmsd_decomp_test ended
2023-07-25 12:50:25 INFO: ----------------------------------------------
2023-07-25 12:50:26 INFO: ======== store_list_record_test ========
2023-07-25 12:50:26 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/store_list_record_test
2023-07-25 12:50:27,252 __main__ INFO -- Get or create the cluster --
2023-07-25 12:50:27,252 TADA INFO starting test `store_sos_lists_test`
2023-07-25 12:50:27,252 TADA INFO   test-id: b82d0be0dbe16fe3ef5f59497accf0e8c8e075a0f7bf5da708248782cd84554e
2023-07-25 12:50:27,252 TADA INFO   test-suite: LDMSD
2023-07-25 12:50:27,252 TADA INFO   test-name: store_sos_lists_test
2023-07-25 12:50:27,252 TADA INFO   test-user: narate
2023-07-25 12:50:27,252 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:50:34,674 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:50:50,227 __main__ INFO All sampler daemons are up.
2023-07-25 12:50:50,329 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-07-25 12:50:50,441 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-07-25 12:50:51,765 TADA INFO assertion 3, store_sos is storing data.: file_exists(a) for a in supported_schema, passed
2023-07-25 12:50:52,533 TADA INFO assertion 4, store_sos stores data correctly.: verify_data(db) for db in all_db, passed
2023-07-25 12:51:01,402 TADA INFO assertion 5, store_sos stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-07-25 12:51:02,053 TADA INFO assertion 6, store_sos reports multiple list errror messages resulted by the config file.: store_sos reported the multiple list error messages., passed
2023-07-25 12:51:06,898 TADA INFO assertion 7, store_sos reports multiple list errror messages resulted by ldmsd_controller.: store_sos reported the multiple list error messages., passed
2023-07-25 12:51:07,209 TADA INFO assertion 8, store_csv is storing data.: file_exists(a) for a in supported_schema, passed
2023-07-25 12:51:13,698 TADA INFO assertion 9, store_csv stores data correctly.: verify_data(db) for db in all_db, passed
2023-07-25 12:51:23,355 TADA INFO assertion 10, store_csv stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-07-25 12:51:24,003 TADA INFO assertion 11, store_csv reports multiple list errror messages resulted by the config file.: store_csv reported the multiple list error messages., passed
2023-07-25 12:51:28,798 TADA INFO assertion 12, store_csv reports multiple list errror messages resulted by ldmsd_controller.: store_csv reported the multiple list error messages., passed
2023-07-25 12:51:28,798 TADA INFO test store_sos_lists_test ended
2023-07-25 12:51:41 INFO: ----------------------------------------------
2023-07-25 12:51:41 INFO: ======== ovis_json_test ========
2023-07-25 12:51:41 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ovis_json_test
2023-07-25 12:51:42,673 __main__ INFO -- Create the cluster -- 
2023-07-25 12:51:47,872 TADA INFO starting test `ovis_json_test`
2023-07-25 12:51:47,872 TADA INFO   test-id: 804bb759848295e51239787464d419f8456261a67d06ac971cf3b171c5866763
2023-07-25 12:51:47,872 TADA INFO   test-suite: OVIS-LIB
2023-07-25 12:51:47,872 TADA INFO   test-name: ovis_json_test
2023-07-25 12:51:47,872 TADA INFO   test-user: narate
2023-07-25 12:51:47,872 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:51:47,873 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-07-25 12:51:47,873 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-07-25 12:51:47,873 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-07-25 12:51:47,873 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-07-25 12:51:47,873 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-07-25 12:51:47,873 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-07-25 12:51:47,874 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-07-25 12:51:47,874 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-07-25 12:51:47,874 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,874 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,874 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,874 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,875 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,875 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,875 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,875 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-25 12:51:47,875 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-07-25 12:51:47,875 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-07-25 12:51:47,875 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-07-25 12:51:47,876 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-07-25 12:51:47,876 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-07-25 12:51:47,876 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-07-25 12:51:47,876 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-07-25 12:51:47,876 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-07-25 12:51:47,876 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-07-25 12:51:47,877 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-07-25 12:51:47,877 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-07-25 12:51:47,877 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,877 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,877 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,877 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,877 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,878 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,878 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,878 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,878 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-25 12:51:47,878 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-07-25 12:51:47,878 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-07-25 12:51:47,878 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-07-25 12:51:47,879 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-07-25 12:51:47,879 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-07-25 12:51:47,879 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-07-25 12:51:47,879 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-07-25 12:51:47,879 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-07-25 12:51:47,879 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-07-25 12:51:47,879 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-07-25 12:51:47,880 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-07-25 12:51:47,880 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-07-25 12:51:47,880 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-07-25 12:51:47,880 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-07-25 12:51:47,880 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-07-25 12:51:47,880 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-07-25 12:51:47,881 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-07-25 12:51:47,881 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-07-25 12:51:47,881 TADA INFO test ovis_json_test ended
2023-07-25 12:51:58 INFO: ----------------------------------------------
2023-07-25 12:51:59 INFO: ======== updtr_add_test ========
2023-07-25 12:51:59 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_add_test
2023-07-25 12:52:00,284 __main__ INFO -- Get or create the cluster --
2023-07-25 12:52:00,284 TADA INFO starting test `updtr_add test`
2023-07-25 12:52:00,284 TADA INFO   test-id: 5886a5a2276e05d1651e6ad5ed7f45ab80e699749cd700670f0e56aa01c2c0e8
2023-07-25 12:52:00,284 TADA INFO   test-suite: LDMSD
2023-07-25 12:52:00,284 TADA INFO   test-name: updtr_add test
2023-07-25 12:52:00,284 TADA INFO   test-user: narate
2023-07-25 12:52:00,284 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:52:07,931 __main__ INFO -- Start daemons --
2023-07-25 12:52:23,354 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:52:23,663 __main__ INFO All LDMSDs are up.
2023-07-25 12:52:24,871 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:52:26,085 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:52:27,291 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:52:28,503 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:52:29,727 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:52:32,153 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:52:34,573 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:52:35,785 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-07-25 12:52:35,785 __main__ INFO --- done ---
2023-07-25 12:52:35,786 TADA INFO test updtr_add test ended
2023-07-25 12:52:47 INFO: ----------------------------------------------
2023-07-25 12:52:48 INFO: ======== updtr_del_test ========
2023-07-25 12:52:48 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_del_test
2023-07-25 12:52:49,522 __main__ INFO -- Get or create the cluster --
2023-07-25 12:52:49,522 TADA INFO starting test `updtr_add test`
2023-07-25 12:52:49,522 TADA INFO   test-id: f4f6e198494bb34f2898e72e3c9fab43cb0d6666b0308533ffae1e64b5cc116a
2023-07-25 12:52:49,522 TADA INFO   test-suite: LDMSD
2023-07-25 12:52:49,523 TADA INFO   test-name: updtr_add test
2023-07-25 12:52:49,523 TADA INFO   test-user: narate
2023-07-25 12:52:49,523 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:52:57,231 __main__ INFO -- Start daemons --
2023-07-25 12:53:12,670 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:53:13,011 __main__ INFO All LDMSDs are up.
2023-07-25 12:53:14,231 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:53:15,434 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-25 12:53:16,653 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:53:17,859 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:53:17,859 __main__ INFO --- done ---
2023-07-25 12:53:17,859 TADA INFO test updtr_add test ended
2023-07-25 12:53:30 INFO: ----------------------------------------------
2023-07-25 12:53:30 INFO: ======== updtr_match_add_test ========
2023-07-25 12:53:30 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_match_add_test
2023-07-25 12:53:31,633 __main__ INFO -- Get or create the cluster --
2023-07-25 12:53:31,634 TADA INFO starting test `updtr_add test`
2023-07-25 12:53:31,634 TADA INFO   test-id: b63ce846ee3d6f4288c9d4e5aeb98aff280a7220cda5466a6a804a520129319f
2023-07-25 12:53:31,634 TADA INFO   test-suite: LDMSD
2023-07-25 12:53:31,634 TADA INFO   test-name: updtr_add test
2023-07-25 12:53:31,634 TADA INFO   test-user: narate
2023-07-25 12:53:31,634 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:53:39,319 __main__ INFO -- Start daemons --
2023-07-25 12:53:54,728 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:53:55,030 __main__ INFO All LDMSDs are up.
2023-07-25 12:53:56,241 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:53:57,449 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:53:58,651 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:53:59,875 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:54:01,092 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-25 12:54:01,092 __main__ INFO --- done ---
2023-07-25 12:54:01,092 TADA INFO test updtr_add test ended
2023-07-25 12:54:13 INFO: ----------------------------------------------
2023-07-25 12:54:14 INFO: ======== updtr_match_del_test ========
2023-07-25 12:54:14 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_match_del_test
2023-07-25 12:54:14,855 __main__ INFO -- Get or create the cluster --
2023-07-25 12:54:14,855 TADA INFO starting test `updtr_add test`
2023-07-25 12:54:14,855 TADA INFO   test-id: f41412edf06db6d236eb4a6feec076c2d849d014d391a70ac433ba8b51137b01
2023-07-25 12:54:14,855 TADA INFO   test-suite: LDMSD
2023-07-25 12:54:14,855 TADA INFO   test-name: updtr_add test
2023-07-25 12:54:14,855 TADA INFO   test-user: narate
2023-07-25 12:54:14,855 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:54:22,534 __main__ INFO -- Start daemons --
2023-07-25 12:54:37,930 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:54:38,233 __main__ INFO All LDMSDs are up.
2023-07-25 12:54:39,453 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-07-25 12:54:40,672 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:54:41,878 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:54:43,100 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:54:44,304 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:54:45,533 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:54:46,745 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:54:46,745 __main__ INFO --- done ---
2023-07-25 12:54:46,745 TADA INFO test updtr_add test ended
2023-07-25 12:54:58 INFO: ----------------------------------------------
2023-07-25 12:54:59 INFO: ======== updtr_prdcr_add_test ========
2023-07-25 12:54:59 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_prdcr_add_test
2023-07-25 12:55:00,512 __main__ INFO -- Get or create the cluster --
2023-07-25 12:55:00,513 TADA INFO starting test `updtr_add test`
2023-07-25 12:55:00,513 TADA INFO   test-id: a714d0b332a29b2654e8fff1aa7b6eb873a0256df6a1fa2f7706afd5dd9c677c
2023-07-25 12:55:00,513 TADA INFO   test-suite: LDMSD
2023-07-25 12:55:00,513 TADA INFO   test-name: updtr_add test
2023-07-25 12:55:00,513 TADA INFO   test-user: narate
2023-07-25 12:55:00,513 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:55:08,162 __main__ INFO -- Start daemons --
2023-07-25 12:55:23,547 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:55:23,864 __main__ INFO All LDMSDs are up.
2023-07-25 12:55:25,083 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:55:27,495 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:55:29,933 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:55:31,136 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-07-25 12:55:32,362 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:55:32,362 __main__ INFO --- done ---
2023-07-25 12:55:32,362 TADA INFO test updtr_add test ended
2023-07-25 12:55:44 INFO: ----------------------------------------------
2023-07-25 12:55:45 INFO: ======== updtr_prdcr_del_test ========
2023-07-25 12:55:45 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_prdcr_del_test
2023-07-25 12:55:46,094 __main__ INFO -- Get or create the cluster --
2023-07-25 12:55:46,094 TADA INFO starting test `updtr_add test`
2023-07-25 12:55:46,094 TADA INFO   test-id: 516742b352b14b7f417b3549c6bce5ee8ea573ca292e969b140ecfb318dac5e8
2023-07-25 12:55:46,094 TADA INFO   test-suite: LDMSD
2023-07-25 12:55:46,094 TADA INFO   test-name: updtr_add test
2023-07-25 12:55:46,094 TADA INFO   test-user: narate
2023-07-25 12:55:46,094 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:55:53,858 __main__ INFO -- Start daemons --
2023-07-25 12:56:09,272 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:56:09,574 __main__ INFO All LDMSDs are up.
2023-07-25 12:56:10,776 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:56:11,983 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-25 12:56:13,187 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:56:15,616 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-25 12:56:15,616 __main__ INFO --- done ---
2023-07-25 12:56:15,616 TADA INFO test updtr_add test ended
2023-07-25 12:56:27 INFO: ----------------------------------------------
2023-07-25 12:56:28 INFO: ======== updtr_start_test ========
2023-07-25 12:56:28 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_start_test
2023-07-25 12:56:29,367 __main__ INFO -- Get or create the cluster --
2023-07-25 12:56:29,367 TADA INFO starting test `updtr_add test`
2023-07-25 12:56:29,367 TADA INFO   test-id: 681b36cdb6e4c197d4179ca2c850b6f628616a88019e282314a36f9c2cba5fcc
2023-07-25 12:56:29,367 TADA INFO   test-suite: LDMSD
2023-07-25 12:56:29,368 TADA INFO   test-name: updtr_add test
2023-07-25 12:56:29,368 TADA INFO   test-user: narate
2023-07-25 12:56:29,368 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:56:36,974 __main__ INFO -- Start daemons --
2023-07-25 12:56:52,429 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:56:52,750 __main__ INFO All LDMSDs are up.
2023-07-25 12:56:53,969 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:56:55,181 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:56:56,383 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-07-25 12:56:57,604 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:56:58,817 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-07-25 12:57:01,243 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-25 12:57:02,448 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-25 12:57:04,880 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-25 12:57:07,307 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-25 12:57:09,740 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-25 12:57:10,960 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-25 12:57:10,960 __main__ INFO --- done ---
2023-07-25 12:57:10,961 TADA INFO test updtr_add test ended
2023-07-25 12:57:23 INFO: ----------------------------------------------
2023-07-25 12:57:24 INFO: ======== updtr_status_test ========
2023-07-25 12:57:24 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/updtr_status_test
2023-07-25 12:57:24,770 __main__ INFO -- Get or create the cluster --
2023-07-25 12:57:24,770 TADA INFO starting test `updtr_status test`
2023-07-25 12:57:24,770 TADA INFO   test-id: 12ac9326a2c9bc4e7fa8cfdf79c24aaabb722aab529996b65943dd8e93588bd5
2023-07-25 12:57:24,770 TADA INFO   test-suite: LDMSD
2023-07-25 12:57:24,770 TADA INFO   test-name: updtr_status test
2023-07-25 12:57:24,770 TADA INFO   test-user: narate
2023-07-25 12:57:24,771 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:57:34,686 __main__ INFO -- Start daemons --
2023-07-25 12:57:55,121 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-25 12:57:55,531 __main__ INFO All LDMSDs are up.
2023-07-25 12:57:56,757 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-07-25 12:57:57,971 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-07-25 12:57:59,178 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:58:00,393 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:58:01,608 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-25 12:58:01,609 __main__ INFO --- done ---
2023-07-25 12:58:01,609 TADA INFO test updtr_status test ended
2023-07-25 12:58:14 INFO: ----------------------------------------------
2023-07-25 12:58:15 INFO: ======== ldmsd_flex_decomp_test ========
2023-07-25 12:58:15 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_flex_decomp_test
2023-07-25 12:58:16,149 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-07-25 12:58:16,149 TADA INFO   test-id: 66590f1831a17d0d3531d0f3cdcf25c019c34ea965bbc67bc802e387f5bdbba2
2023-07-25 12:58:16,150 TADA INFO   test-suite: LDMSD
2023-07-25 12:58:16,150 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-07-25 12:58:16,150 TADA INFO   test-user: narate
2023-07-25 12:58:16,150 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 12:58:16,151 __main__ INFO -- Get or create the cluster --
2023-07-25 12:58:31,719 __main__ INFO -- Start daemons --
2023-07-25 12:59:01,222 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 12:59:50,323 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-07-25 12:59:50,323 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-07-25 12:59:50,323 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-07-25 12:59:50,323 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-07-25 12:59:50,324 TADA INFO assertion 5, record sos schema check: OK, passed
2023-07-25 12:59:50,324 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-07-25 12:59:50,324 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-07-25 12:59:50,324 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-07-25 12:59:50,324 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 10, record csv schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-07-25 12:59:50,325 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-07-25 12:59:50,327 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-07-25 12:59:50,403 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-07-25 12:59:50,406 TADA INFO assertion 18, fill sos data check: OK, passed
2023-07-25 12:59:50,408 TADA INFO assertion 19, filter sos data check: OK, passed
2023-07-25 12:59:50,417 TADA INFO assertion 20, record sos data check: OK, passed
2023-07-25 12:59:50,419 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-07-25 12:59:50,495 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-07-25 12:59:50,498 TADA INFO assertion 23, fill csv data check: OK, passed
2023-07-25 12:59:50,500 TADA INFO assertion 24, filter csv data check: OK, passed
2023-07-25 12:59:50,508 TADA INFO assertion 25, record csv data check: OK, passed
2023-07-25 12:59:50,509 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-07-25 12:59:50,532 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-07-25 12:59:50,533 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-07-25 12:59:50,534 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-07-25 12:59:50,538 TADA INFO assertion 30, record kafka data check: OK, passed
2023-07-25 12:59:50,538 TADA INFO test ldmsd_flex_decomp_test ended
2023-07-25 12:59:50,539 TADA INFO test ldmsd_flex_decomp_test ended
2023-07-25 13:00:05 INFO: ----------------------------------------------
2023-07-25 13:00:06 INFO: ======== ldms_set_info_test ========
2023-07-25 13:00:06 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_set_info_test
2023-07-25 13:00:17,195 TADA INFO starting test `ldms_set_info_test`
2023-07-25 13:00:17,195 TADA INFO   test-id: c4dad1c2155a65174f823f09d8d5b055d044ad4b83f15ed98a570be09f6c5997
2023-07-25 13:00:17,196 TADA INFO   test-suite: LDMSD
2023-07-25 13:00:17,196 TADA INFO   test-name: ldms_set_info_test
2023-07-25 13:00:17,196 TADA INFO   test-user: narate
2023-07-25 13:00:17,196 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:00:17,196 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 3, Get a value : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 4, Unset a pair : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-07-25 13:00:17,197 TADA INFO assertion 7, Server resetting a key : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 8, Server unset a key : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 9, Server add a key : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 10, Adding a key : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-07-25 13:00:17,198 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-07-25 13:00:17,199 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-07-25 13:00:17,199 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-07-25 13:00:17,199 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-07-25 13:00:17,199 TADA INFO test ldms_set_info_test ended
2023-07-25 13:00:27 INFO: ----------------------------------------------
2023-07-25 13:00:28 INFO: ======== slurm_sampler2_test ========
2023-07-25 13:00:28 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/slurm_sampler2_test
2023-07-25 13:00:29,487 TADA INFO starting test `slurm_sampler2_test`
2023-07-25 13:00:29,487 TADA INFO   test-id: e48b42e616537c07cd94787e84b5662112b8634dc2b107c5b6b0eb649821f9b4
2023-07-25 13:00:29,487 TADA INFO   test-suite: LDMSD
2023-07-25 13:00:29,487 TADA INFO   test-name: slurm_sampler2_test
2023-07-25 13:00:29,487 TADA INFO   test-user: narate
2023-07-25 13:00:29,487 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:00:29,488 __main__ INFO -- Get or create the cluster --
2023-07-25 13:00:42,590 __main__ INFO -- Add users --
2023-07-25 13:00:47,739 __main__ INFO -- Preparing job script & programs --
2023-07-25 13:00:48,429 __main__ INFO -- Start daemons --
2023-07-25 13:01:29,510 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-07-25 13:01:34,183 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-07-25 13:01:36,907 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-07-25 13:01:39,547 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-07-25 13:01:42,191 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:01:44,828 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:01:49,511 TADA INFO assertion 3.1, Expanding the set heap -- job_init: The metric values are as expected on all nodes., passed
2023-07-25 13:01:52,245 TADA INFO assertion 3.2, Expanding the set heap -- step_init: The metric values are as expected on all nodes., passed
2023-07-25 13:01:56,210 TADA INFO assertion 3.3, Expanding the set heap -- task_init: The metric values are as expected on all nodes., passed
2023-07-25 13:02:00,149 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:02:02,795 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:02:09,049 TADA INFO assertion 4.1, Multi-tenant -- job_init: The metric values are as expected on all nodes., passed
2023-07-25 13:02:10,723 TADA INFO assertion 4.2, Multi-tenant -- step_init: The metric values are as expected on all nodes., passed
2023-07-25 13:02:13,191 TADA INFO assertion 4.3, Multi-tenant -- task_init: The metric values are as expected on all nodes., passed
2023-07-25 13:02:15,753 TADA INFO assertion 4.4, Multi-tenant -- task_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:02:17,387 TADA INFO assertion 4.5, Multi-tenant -- job_exit: The metric values are as expected on all nodes., passed
2023-07-25 13:02:17,387 TADA INFO test slurm_sampler2_test ended
2023-07-25 13:02:31 INFO: ----------------------------------------------
2023-07-25 13:02:32 INFO: ======== libovis_log_test ========
2023-07-25 13:02:32 INFO: CMD: python3 libovis_log_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/libovis_log_test
2023-07-25 13:02:33,247 TADA INFO starting test `libovis_log_test`
2023-07-25 13:02:33,247 TADA INFO   test-id: 645bcca661ff1705908d1df8f4709d04cb8b45ee7a3df0dd1dcdd4351a953279
2023-07-25 13:02:33,247 TADA INFO   test-suite: LDMSD
2023-07-25 13:02:33,247 TADA INFO   test-name: libovis_log_test
2023-07-25 13:02:33,247 TADA INFO   test-user: narate
2023-07-25 13:02:33,247 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:02:33,248 __main__ INFO -- Create the cluster -- 
2023-07-25 13:02:38,045 __main__ INFO -- Start daemons --
2023-07-25 13:02:40,221 TADA INFO assertion 1, Call ovis_log_init() with valid arguments: 'return_code=0' and 'liovis_log_test' in 'Tue Jul 25 13:02:39 2023:         : libovis_log_test: return_code=0
', passed
2023-07-25 13:02:41,330 TADA INFO assertion 2, Call ovis_log_init() with name = NULL: ('return_code=0' and ': :') in 'Tue Jul 25 13:02:40 2023:         : : return_code=0
', passed
2023-07-25 13:02:42,435 TADA INFO assertion 3, Call ovis_log_init() with an invalid level: 'return_code=22' in 'Tue Jul 25 13:02:41 2023:         : : return_code=22
', passed
2023-07-25 13:02:43,531 TADA INFO assertion 4, Call ovis_log_init() with an invalid mode: 'return_code=22' in 'Tue Jul 25 13:02:42 2023:         : : return_code=22
', passed
2023-07-25 13:02:44,149 TADA INFO assertion 6, Log messages to a file: 0 == ovis_log_open(/var/log/6.log) # (0), passed
2023-07-25 13:02:45,265 TADA INFO assertion 5, Log messages to stdout: 'return_code=0' in 'Tue Jul 25 13:02:44 2023:         : : return_code=0
', passed
2023-07-25 13:02:45,363 TADA INFO assertion 7, Open the log file at a non-existing path: 'Could not open the log file' in 'Tue Jul 25 13:02:45 2023:         : test: result=0
Tue Jul 25 13:02:45 2023:    ERROR: test: Could not open the log file named '/data/log/foo/7.log'
Tue Jul 25 13:02:45 2023:    ERROR: test: Failed to open the log file at /data/log/foo/7.log. Error 22
', passed
2023-07-25 13:02:45,897 TADA INFO assertion 8, Reopen the log file at another path: ovis_log_open() closes and opens the second path successfully, passed
2023-07-25 13:02:46,365 TADA INFO assertion 9, Convert 'DEBUG,INFO' integer to a string: DEBUG,INFO == DEBUG,INFO (expected), passed
2023-07-25 13:02:46,473 TADA INFO assertion 10, Convert 'DEBUG,WARNING' integer to a string: DEBUG,WARNING == DEBUG,WARNING (expected), passed
2023-07-25 13:02:46,582 TADA INFO assertion 11, Convert 'DEBUG,ERROR' integer to a string: DEBUG,ERROR == DEBUG,ERROR (expected), passed
2023-07-25 13:02:46,695 TADA INFO assertion 12, Convert 'DEBUG,CRITICAL' integer to a string: DEBUG,CRITICAL == DEBUG,CRITICAL (expected), passed
2023-07-25 13:02:46,810 TADA INFO assertion 13, Convert 'INFO,WARNING' integer to a string: INFO,WARNING == INFO,WARNING (expected), passed
2023-07-25 13:02:46,917 TADA INFO assertion 14, Convert 'INFO,ERROR' integer to a string: INFO,ERROR == INFO,ERROR (expected), passed
2023-07-25 13:02:47,041 TADA INFO assertion 15, Convert 'INFO,CRITICAL' integer to a string: INFO,CRITICAL == INFO,CRITICAL (expected), passed
2023-07-25 13:02:47,150 TADA INFO assertion 16, Convert 'WARNING,ERROR' integer to a string: WARNING,ERROR == WARNING,ERROR (expected), passed
2023-07-25 13:02:47,254 TADA INFO assertion 17, Convert 'WARNING,CRITICAL' integer to a string: WARNING,CRITICAL == WARNING,CRITICAL (expected), passed
2023-07-25 13:02:47,371 TADA INFO assertion 18, Convert 'ERROR,CRITICAL' integer to a string: ERROR,CRITICAL == ERROR,CRITICAL (expected), passed
2023-07-25 13:02:47,475 TADA INFO assertion 19, Convert 'DEBUG,INFO,WARNING' integer to a string: DEBUG,INFO,WARNING == DEBUG,INFO,WARNING (expected), passed
2023-07-25 13:02:47,583 TADA INFO assertion 20, Convert 'DEBUG,INFO,ERROR' integer to a string: DEBUG,INFO,ERROR == DEBUG,INFO,ERROR (expected), passed
2023-07-25 13:02:47,702 TADA INFO assertion 21, Convert 'DEBUG,INFO,CRITICAL' integer to a string: DEBUG,INFO,CRITICAL == DEBUG,INFO,CRITICAL (expected), passed
2023-07-25 13:02:47,798 TADA INFO assertion 22, Convert 'DEBUG,WARNING,ERROR' integer to a string: DEBUG,WARNING,ERROR == DEBUG,WARNING,ERROR (expected), passed
2023-07-25 13:02:47,904 TADA INFO assertion 23, Convert 'DEBUG,WARNING,CRITICAL' integer to a string: DEBUG,WARNING,CRITICAL == DEBUG,WARNING,CRITICAL (expected), passed
2023-07-25 13:02:48,001 TADA INFO assertion 24, Convert 'DEBUG,ERROR,CRITICAL' integer to a string: DEBUG,ERROR,CRITICAL == DEBUG,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:48,107 TADA INFO assertion 25, Convert 'INFO,WARNING,ERROR' integer to a string: INFO,WARNING,ERROR == INFO,WARNING,ERROR (expected), passed
2023-07-25 13:02:48,220 TADA INFO assertion 26, Convert 'INFO,WARNING,CRITICAL' integer to a string: INFO,WARNING,CRITICAL == INFO,WARNING,CRITICAL (expected), passed
2023-07-25 13:02:48,329 TADA INFO assertion 27, Convert 'INFO,ERROR,CRITICAL' integer to a string: INFO,ERROR,CRITICAL == INFO,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:48,438 TADA INFO assertion 28, Convert 'WARNING,ERROR,CRITICAL' integer to a string: WARNING,ERROR,CRITICAL == WARNING,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:48,542 TADA INFO assertion 29, Convert 'DEBUG,INFO,WARNING,ERROR' integer to a string: DEBUG,INFO,WARNING,ERROR == DEBUG,INFO,WARNING,ERROR (expected), passed
2023-07-25 13:02:48,656 TADA INFO assertion 30, Convert 'DEBUG,INFO,WARNING,CRITICAL' integer to a string: DEBUG,INFO,WARNING,CRITICAL == DEBUG,INFO,WARNING,CRITICAL (expected), passed
2023-07-25 13:02:48,765 TADA INFO assertion 31, Convert 'DEBUG,INFO,ERROR,CRITICAL' integer to a string: DEBUG,INFO,ERROR,CRITICAL == DEBUG,INFO,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:48,861 TADA INFO assertion 32, Convert 'DEBUG,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,WARNING,ERROR,CRITICAL == DEBUG,WARNING,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:48,970 TADA INFO assertion 33, Convert 'INFO,WARNING,ERROR,CRITICAL' integer to a string: INFO,WARNING,ERROR,CRITICAL == INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:49,082 TADA INFO assertion 34, Convert 'DEBUG,INFO,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,INFO,WARNING,ERROR,CRITICAL == DEBUG,INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-07-25 13:02:49,201 TADA INFO assertion 35, Convert 'DEBUG,' integer to a string: DEBUG, == DEBUG, (expected), passed
2023-07-25 13:02:49,323 TADA INFO assertion 36, Convert 'INFO,' integer to a string: INFO, == INFO, (expected), passed
2023-07-25 13:02:49,432 TADA INFO assertion 37, Convert 'WARNING,' integer to a string: WARNING, == WARNING, (expected), passed
2023-07-25 13:02:49,534 TADA INFO assertion 38, Convert 'ERROR,' integer to a string: ERROR, == ERROR, (expected), passed
2023-07-25 13:02:49,646 TADA INFO assertion 39, Convert 'CRITICAL,' integer to a string: CRITICAL, == CRITICAL, (expected), passed
2023-07-25 13:02:49,754 TADA INFO assertion 40, Convert an invalid integer to a level string: (null) == (null) (expected), passed
2023-07-25 13:02:49,853 TADA INFO assertion 41, Convert the 'DEBUG,INFO' to an integer: 3 == 3 (expected), passed
2023-07-25 13:02:49,961 TADA INFO assertion 42, Convert the 'DEBUG,WARNING' to an integer: 5 == 5 (expected), passed
2023-07-25 13:02:50,071 TADA INFO assertion 43, Convert the 'DEBUG,ERROR' to an integer: 9 == 9 (expected), passed
2023-07-25 13:02:50,177 TADA INFO assertion 44, Convert the 'DEBUG,CRITICAL' to an integer: 17 == 17 (expected), passed
2023-07-25 13:02:50,287 TADA INFO assertion 45, Convert the 'INFO,WARNING' to an integer: 6 == 6 (expected), passed
2023-07-25 13:02:50,400 TADA INFO assertion 46, Convert the 'INFO,ERROR' to an integer: 10 == 10 (expected), passed
2023-07-25 13:02:50,502 TADA INFO assertion 47, Convert the 'INFO,CRITICAL' to an integer: 18 == 18 (expected), passed
2023-07-25 13:02:50,622 TADA INFO assertion 48, Convert the 'WARNING,ERROR' to an integer: 12 == 12 (expected), passed
2023-07-25 13:02:50,720 TADA INFO assertion 49, Convert the 'WARNING,CRITICAL' to an integer: 20 == 20 (expected), passed
2023-07-25 13:02:50,835 TADA INFO assertion 50, Convert the 'ERROR,CRITICAL' to an integer: 24 == 24 (expected), passed
2023-07-25 13:02:50,930 TADA INFO assertion 51, Convert the 'DEBUG,INFO,WARNING' to an integer: 7 == 7 (expected), passed
2023-07-25 13:02:51,035 TADA INFO assertion 52, Convert the 'DEBUG,INFO,ERROR' to an integer: 11 == 11 (expected), passed
2023-07-25 13:02:51,139 TADA INFO assertion 53, Convert the 'DEBUG,INFO,CRITICAL' to an integer: 19 == 19 (expected), passed
2023-07-25 13:02:51,249 TADA INFO assertion 54, Convert the 'DEBUG,WARNING,ERROR' to an integer: 13 == 13 (expected), passed
2023-07-25 13:02:51,358 TADA INFO assertion 55, Convert the 'DEBUG,WARNING,CRITICAL' to an integer: 21 == 21 (expected), passed
2023-07-25 13:02:51,462 TADA INFO assertion 56, Convert the 'DEBUG,ERROR,CRITICAL' to an integer: 25 == 25 (expected), passed
2023-07-25 13:02:51,567 TADA INFO assertion 57, Convert the 'INFO,WARNING,ERROR' to an integer: 14 == 14 (expected), passed
2023-07-25 13:02:51,667 TADA INFO assertion 58, Convert the 'INFO,WARNING,CRITICAL' to an integer: 22 == 22 (expected), passed
2023-07-25 13:02:51,780 TADA INFO assertion 59, Convert the 'INFO,ERROR,CRITICAL' to an integer: 26 == 26 (expected), passed
2023-07-25 13:02:51,883 TADA INFO assertion 60, Convert the 'WARNING,ERROR,CRITICAL' to an integer: 28 == 28 (expected), passed
2023-07-25 13:02:51,988 TADA INFO assertion 61, Convert the 'DEBUG,INFO,WARNING,ERROR' to an integer: 15 == 15 (expected), passed
2023-07-25 13:02:52,112 TADA INFO assertion 62, Convert the 'DEBUG,INFO,WARNING,CRITICAL' to an integer: 23 == 23 (expected), passed
2023-07-25 13:02:52,217 TADA INFO assertion 63, Convert the 'DEBUG,INFO,ERROR,CRITICAL' to an integer: 27 == 27 (expected), passed
2023-07-25 13:02:52,320 TADA INFO assertion 64, Convert the 'DEBUG,WARNING,ERROR,CRITICAL' to an integer: 29 == 29 (expected), passed
2023-07-25 13:02:52,435 TADA INFO assertion 65, Convert the 'INFO,WARNING,ERROR,CRITICAL' to an integer: 30 == 30 (expected), passed
2023-07-25 13:02:52,537 TADA INFO assertion 66, Convert the 'DEBUG,INFO,WARNING,ERROR,CRITICAL' to an integer: 31 == 31 (expected), passed
2023-07-25 13:02:52,640 TADA INFO assertion 67, Convert the 'DEBUG,' to an integer: 1 == 1 (expected), passed
2023-07-25 13:02:52,752 TADA INFO assertion 68, Convert the 'INFO,' to an integer: 2 == 2 (expected), passed
2023-07-25 13:02:52,874 TADA INFO assertion 69, Convert the 'WARNING,' to an integer: 4 == 4 (expected), passed
2023-07-25 13:02:52,981 TADA INFO assertion 70, Convert the 'ERROR,' to an integer: 8 == 8 (expected), passed
2023-07-25 13:02:53,098 TADA INFO assertion 71, Convert the 'CRITICAL,' to an integer: 16 == 16 (expected), passed
2023-07-25 13:02:53,211 TADA INFO assertion 72, Convert the 'DEBUG' to an integer: 31 == 31 (expected), passed
2023-07-25 13:02:53,326 TADA INFO assertion 73, Convert the 'INFO' to an integer: 30 == 30 (expected), passed
2023-07-25 13:02:53,441 TADA INFO assertion 74, Convert the 'WARNING' to an integer: 28 == 28 (expected), passed
2023-07-25 13:02:53,552 TADA INFO assertion 75, Convert the 'ERROR' to an integer: 24 == 24 (expected), passed
2023-07-25 13:02:53,658 TADA INFO assertion 76, Convert the 'CRITICAL' to an integer: 16 == 16 (expected), passed
2023-07-25 13:02:53,777 TADA INFO assertion 77, Convert an invalid level string to an integer: -22 == -22 (expected), passed
2023-07-25 13:02:54,411 TADA INFO assertion 78, Verify that no messages were printed when the level is QUIET.: No messages were printed., passed
2023-07-25 13:02:54,727 TADA INFO assertion 79, Verify that messages of DEBUG,INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:55,044 TADA INFO assertion 80, Verify that messages of DEBUG,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:55,363 TADA INFO assertion 81, Verify that messages of DEBUG,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:55,696 TADA INFO assertion 82, Verify that messages of DEBUG,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:56,022 TADA INFO assertion 83, Verify that messages of INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:56,360 TADA INFO assertion 84, Verify that messages of INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:56,665 TADA INFO assertion 85, Verify that messages of INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:57,019 TADA INFO assertion 86, Verify that messages of WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:57,345 TADA INFO assertion 87, Verify that messages of WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:57,692 TADA INFO assertion 88, Verify that messages of ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:58,028 TADA INFO assertion 89, Verify that messages of DEBUG,INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:58,357 TADA INFO assertion 90, Verify that messages of DEBUG,INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:58,681 TADA INFO assertion 91, Verify that messages of DEBUG,INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:59,019 TADA INFO assertion 92, Verify that messages of DEBUG,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:59,333 TADA INFO assertion 93, Verify that messages of DEBUG,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:59,642 TADA INFO assertion 94, Verify that messages of DEBUG,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:02:59,978 TADA INFO assertion 95, Verify that messages of INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:00,324 TADA INFO assertion 96, Verify that messages of INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:00,646 TADA INFO assertion 97, Verify that messages of INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:00,998 TADA INFO assertion 98, Verify that messages of WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:01,313 TADA INFO assertion 99, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:01,644 TADA INFO assertion 100, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:01,981 TADA INFO assertion 101, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:02,323 TADA INFO assertion 102, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:02,647 TADA INFO assertion 103, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:02,953 TADA INFO assertion 104, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:03,287 TADA INFO assertion 105, Verify that messages of DEBUG, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:03,610 TADA INFO assertion 106, Verify that messages of INFO, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:03,930 TADA INFO assertion 107, Verify that messages of WARNING, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:04,271 TADA INFO assertion 108, Verify that messages of ERROR, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:04,594 TADA INFO assertion 109, Verify that messages of CRITICAL, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:04,940 TADA INFO assertion 110, Verify that messages of DEBUG were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:05,263 TADA INFO assertion 111, Verify that messages of INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:05,562 TADA INFO assertion 112, Verify that messages of WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:05,859 TADA INFO assertion 113, Verify that messages of ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:06,190 TADA INFO assertion 114, Verify that messages of CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:06,814 TADA INFO assertion 116, Verify that ovis_log_close() works properly: ovis_log_close() works properly., passed
2023-07-25 13:03:08,643 TADA INFO assertion 115, Verify that applications can open, rename, and reopen log files to perform log rotation.: ovis_log supports open, rename (external), and reopen., passed
2023-07-25 13:03:09,053 TADA INFO assertion 117, Test a ovis_log_register() call with valid arguments: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}], passed
2023-07-25 13:03:09,152 TADA INFO assertion 118, Test a ovis_log_register() call with NULL name: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-07-25 13:03:09,267 TADA INFO assertion 119, Test a ovis_log_register() call with NULL desc: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-07-25 13:03:09,370 TADA INFO assertion 120, Test a ovis_log_register() call with an existing subsystem: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}], passed
2023-07-25 13:03:09,987 TADA INFO assertion 122, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:10,283 TADA INFO assertion 123, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:10,591 TADA INFO assertion 124, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:10,915 TADA INFO assertion 125, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:11,240 TADA INFO assertion 126, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:11,559 TADA INFO assertion 127, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:11,885 TADA INFO assertion 128, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:12,201 TADA INFO assertion 129, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:12,524 TADA INFO assertion 130, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:12,838 TADA INFO assertion 131, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:13,135 TADA INFO assertion 132, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:13,443 TADA INFO assertion 133, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:13,747 TADA INFO assertion 134, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:14,077 TADA INFO assertion 135, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:14,404 TADA INFO assertion 136, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:14,740 TADA INFO assertion 137, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:15,068 TADA INFO assertion 138, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:15,376 TADA INFO assertion 139, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:15,727 TADA INFO assertion 140, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:16,058 TADA INFO assertion 141, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:16,387 TADA INFO assertion 142, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:16,690 TADA INFO assertion 143, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:17,012 TADA INFO assertion 144, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:17,343 TADA INFO assertion 145, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:17,679 TADA INFO assertion 146, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:18,008 TADA INFO assertion 147, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:18,341 TADA INFO assertion 148, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:18,668 TADA INFO assertion 149, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:18,991 TADA INFO assertion 150, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:19,308 TADA INFO assertion 151, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:19,625 TADA INFO assertion 152, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:19,950 TADA INFO assertion 153, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:20,270 TADA INFO assertion 154, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:20,603 TADA INFO assertion 155, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:20,900 TADA INFO assertion 156, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:21,238 TADA INFO assertion 157, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:21,552 TADA INFO assertion 158, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:21,872 TADA INFO assertion 159, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:22,194 TADA INFO assertion 160, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:22,538 TADA INFO assertion 161, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:22,879 TADA INFO assertion 162, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:23,197 TADA INFO assertion 163, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:23,527 TADA INFO assertion 164, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:23,849 TADA INFO assertion 165, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:24,169 TADA INFO assertion 166, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:24,503 TADA INFO assertion 167, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:24,811 TADA INFO assertion 168, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:25,135 TADA INFO assertion 169, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:25,459 TADA INFO assertion 170, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:25,775 TADA INFO assertion 171, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:26,112 TADA INFO assertion 172, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:26,435 TADA INFO assertion 173, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:26,762 TADA INFO assertion 174, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:27,082 TADA INFO assertion 175, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:27,406 TADA INFO assertion 176, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:27,726 TADA INFO assertion 177, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:28,047 TADA INFO assertion 178, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:28,356 TADA INFO assertion 179, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:28,678 TADA INFO assertion 180, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:28,999 TADA INFO assertion 181, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:29,325 TADA INFO assertion 182, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:29,634 TADA INFO assertion 183, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:29,962 TADA INFO assertion 184, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:30,271 TADA INFO assertion 185, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:30,598 TADA INFO assertion 186, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:30,934 TADA INFO assertion 187, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:31,248 TADA INFO assertion 188, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:31,560 TADA INFO assertion 189, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:31,885 TADA INFO assertion 190, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:32,181 TADA INFO assertion 191, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:32,491 TADA INFO assertion 192, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:32,804 TADA INFO assertion 193, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-25 13:03:33,449 TADA INFO assertion 195, Verify that ovis_log_set_level_by_regex() returns an error when the given regular expression string is invalid.: 'result=22' in 'Tue Jul 25 13:03:33 2023:         : test: result=22
Tue Jul 25 13:03:33 2023:         : test: result=22
', passed
2023-07-25 13:03:33,792 TADA INFO assertion 194, Verify that ovis_log_set_level_by_regex() returns ENOENT when the given regular expression string doesn't match any logs.: 'result=2' in 'Tue Jul 25 13:03:33 2023:         : test: result=2
Tue Jul 25 13:03:33 2023:         : test: result=2
', passed
2023-07-25 13:03:34,117 TADA INFO assertion 196, Verify that ovis_log_set_level_by_regex() sets the level of the matched log subsystems to the given value.: ('config:' in 'Tue Jul 25 13:03:33 2023:         : config: ALWAYS' and (('CRITICAL' in 'Tue Jul 25 13:03:33 2023:         : config: ALWAYS') or ('ALWAYS' in Tue Jul 25 13:03:33 2023:         : config: ALWAYS)), passed
2023-07-25 13:03:34,765 TADA INFO assertion 197, Verify that ovis_log_list() works correctly.: '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]' == '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]', passed
2023-07-25 13:03:34,765 TADA INFO test libovis_log_test ended
2023-07-25 13:03:45 INFO: ----------------------------------------------
2023-07-25 13:03:46 INFO: ======== ldmsd_long_config_test ========
2023-07-25 13:03:46 INFO: CMD: python3 ldmsd_long_config_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldmsd_long_config_test
2023-07-25 13:03:47,265 TADA INFO starting test `ldmsd_long_config_line`
2023-07-25 13:03:47,265 TADA INFO   test-id: 79580dbcdd5d1794d8aa73821ee5d419a4d5a1611df0424480cd35e5d9cd13c9
2023-07-25 13:03:47,265 TADA INFO   test-suite: LDMSD
2023-07-25 13:03:47,265 TADA INFO   test-name: ldmsd_long_config_line
2023-07-25 13:03:47,265 TADA INFO   test-user: narate
2023-07-25 13:03:47,265 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:03:47,266 __main__ INFO ---Get or create the cluster --
2023-07-25 13:03:54,617 __main__ INFO --- Start daemons ---
2023-07-25 13:04:11,160 TADA INFO assertion 1, LDMSD correctly processes a config line in a config file: LDMSD processed the long config line in the config file correctly., passed
2023-07-25 13:04:11,664 TADA INFO assertion 2, LDMSD correctly handle a config line from ldmsd_controller: LDMSD receives the correct message from ldmsd_controller., passed
2023-07-25 13:04:12,335 TADA INFO assertion 3, LDMSD correctly handle a config line from ldmsctl: LDMSD receives the correct message from ldmsctl., passed
2023-07-25 13:04:12,336 TADA INFO test ldmsd_long_config_line ended
2023-07-25 13:04:24 INFO: ----------------------------------------------
2023-07-25 13:04:25 INFO: ======== ldms_rail_test ========
2023-07-25 13:04:25 INFO: CMD: python3 ldms_rail_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_rail_test
2023-07-25 13:04:26,026 TADA INFO starting test `ldms_rail_test`
2023-07-25 13:04:26,026 TADA INFO   test-id: 3f66d0c764133a04142f9efa2fae9855a7e33eb7c6ccdf696e2be6d65c8a1197
2023-07-25 13:04:26,026 TADA INFO   test-suite: LDMSD
2023-07-25 13:04:26,026 TADA INFO   test-name: ldms_rail_test
2023-07-25 13:04:26,026 TADA INFO   test-user: narate
2023-07-25 13:04:26,026 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:04:26,027 __main__ INFO -- Get or create the cluster --
2023-07-25 13:04:33,118 __main__ INFO -- Start daemons --
2023-07-25 13:04:37,866 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 13:04:39,868 __main__ INFO start ldms_rail_server.py and ldms_rail_client.py interactive sessions
2023-07-25 13:04:42,888 TADA INFO assertion 1, Start interactive LDMS server: OK, passed
2023-07-25 13:04:45,904 TADA INFO assertion 2, Start interactive LDMS client: OK, passed
2023-07-25 13:04:49,509 TADA INFO assertion 3, Client rail has 8 endpoints on 8 thread pools: OK, passed
2023-07-25 13:04:53,114 TADA INFO assertion 4, Server rail has 8 endpoints on 8 thread pools: OK, passed
2023-07-25 13:04:56,718 TADA INFO assertion 5, Sets on client are processed by different threads: OK, passed
2023-07-25 13:05:00,323 TADA INFO assertion 6, Verify sets on the client: OK, passed
2023-07-25 13:05:03,340 TADA INFO assertion 7, Start interactive LDMS client2: OK, passed
2023-07-25 13:05:06,945 TADA INFO assertion 8, Client2 rail has 8 endpoints on 4 thread pools: OK, passed
2023-07-25 13:05:09,963 TADA INFO assertion 9, Client3 (wrong auth) cannot connect: OK, passed
2023-07-25 13:05:12,983 TADA INFO assertion 10, Start interactive client4 (for push mode): OK, passed
2023-07-25 13:05:12,984 __main__ INFO waiting push ...
2023-07-25 13:05:14,986 __main__ INFO server: sampling new data (2)
2023-07-25 13:05:19,592 __main__ INFO client4: set pushes received
2023-07-25 13:05:19,593 __main__ INFO client4: verifying data in sets
2023-07-25 13:05:23,197 __main__ INFO client4: verifying threads-sets-endpoints spread
2023-07-25 13:05:34,011 TADA INFO assertion 11, Client4 got push callback from the corresponding thread: OK, passed
2023-07-25 13:05:37,027 TADA INFO assertion 12, Client5 started (for clean-up path test): OK, passed
2023-07-25 13:05:37,028 __main__ INFO xprt close by client1
2023-07-25 13:05:49,843 TADA INFO assertion 13, Active-side close: client1 clean up: OK, passed
2023-07-25 13:05:53,448 TADA INFO assertion 14, Active-side close: server-side clean up: OK, passed
2023-07-25 13:06:09,869 TADA INFO assertion 15, Passive-side close: client2 clean up: OK, passed
2023-07-25 13:06:09,869 TADA INFO assertion 16, Passive-side close: server-side clean up: OK, passed
2023-07-25 13:06:15,476 TADA INFO assertion 17, Active-side term: server-side clean up: OK, passed
2023-07-25 13:06:22,686 TADA INFO assertion 18, Passive-side term: client5 clean up: OK, passed
2023-07-25 13:06:43,139 TADA INFO assertion 19, server -> client overspending send: error message verified, passed
2023-07-25 13:06:53,953 TADA INFO assertion 20, client -> server overspending send: error message verified, passed
2023-07-25 13:06:57,558 TADA INFO assertion 21, verify send credits on the server: OK, passed
2023-07-25 13:07:01,164 TADA INFO assertion 22, verify send credits on the client: OK, passed
2023-07-25 13:07:09,275 TADA INFO assertion 23, server unblock, verify recv data: recv data verified, passed
2023-07-25 13:07:17,386 TADA INFO assertion 24, client unblock, verify recv data: recv data verified, passed
2023-07-25 13:07:20,991 TADA INFO assertion 25, verify send credits on the server: OK, passed
2023-07-25 13:07:24,596 TADA INFO assertion 26, verify send credits on the client: OK, passed
2023-07-25 13:07:28,202 TADA INFO assertion 27, server -> client send after credited back: OK, passed
2023-07-25 13:07:31,807 TADA INFO assertion 28, client -> server send after credited back: OK, passed
2023-07-25 13:07:35,412 TADA INFO assertion 29, verify send credits on the server: OK, passed
2023-07-25 13:07:39,017 TADA INFO assertion 30, verify send credits on the client: OK, passed
2023-07-25 13:07:42,621 TADA INFO assertion 31, server unblock, verify recv data: OK, passed
2023-07-25 13:07:46,226 TADA INFO assertion 32, client unblock, verify recv data: OK, passed
2023-07-25 13:07:49,831 TADA INFO assertion 33, verify send credits on the server: OK, passed
2023-07-25 13:07:53,435 TADA INFO assertion 34, verify send credits on the client: OK, passed
2023-07-25 13:07:57,040 TADA INFO assertion 35, verify send-credit deposits on the server: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-07-25 13:08:00,645 TADA INFO assertion 36, verify send-credit deposits on the client: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-07-25 13:08:00,646 TADA INFO test ldms_rail_test ended
2023-07-25 13:08:12 INFO: ----------------------------------------------
2023-07-25 13:08:13 INFO: ======== ldms_stream_test ========
2023-07-25 13:08:13 INFO: CMD: python3 ldms_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-25-120847/data/ldms_stream_test
2023-07-25 13:08:14,384 TADA INFO starting test `ldms_stream_test`
2023-07-25 13:08:14,384 TADA INFO   test-id: ecb39f0875bc11883abc1fb6ee2a96de58594c6491b9fa1f7099471f42e78288
2023-07-25 13:08:14,384 TADA INFO   test-suite: LDMSD
2023-07-25 13:08:14,384 TADA INFO   test-name: ldms_stream_test
2023-07-25 13:08:14,384 TADA INFO   test-user: narate
2023-07-25 13:08:14,385 TADA INFO   commit-id: 841f538cd5dc1b6336459d711ef8cc723e733f0a
2023-07-25 13:08:14,385 __main__ INFO -- Get or create the cluster --
2023-07-25 13:08:31,362 __main__ INFO -- Adding 'foo' and 'bar' users --
2023-07-25 13:08:41,763 __main__ INFO -- Start daemons --
2023-07-25 13:08:54,159 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-25 13:08:56,161 __main__ INFO start interactive stream servers
2023-07-25 13:08:56,162 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-7 
2023-07-25 13:08:59,180 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-6 
2023-07-25 13:09:02,201 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-5 
2023-07-25 13:09:05,221 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-4 
2023-07-25 13:09:08,242 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-3 
2023-07-25 13:09:11,264 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-2 
2023-07-25 13:09:14,284 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-1 
2023-07-25 13:09:17,302 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-4 
2023-07-25 13:09:20,822 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-5 
2023-07-25 13:09:24,340 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-6 
2023-07-25 13:09:27,860 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-7 
2023-07-25 13:09:31,381 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-4 as foo
2023-07-25 13:09:34,901 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-841f538-node-4 as bar
2023-07-25 13:09:38,420 __main__ INFO starting /tada-src/python/ldms_stream_client.py on narate-ldms_stream_test-841f538-node-8 as foo
2023-07-25 13:09:41,940 TADA INFO assertion 1, Publishing oversize data results in an error: checking..., passed
2023-07-25 13:09:42,450 __main__ INFO getting data from srv1
2023-07-25 13:09:44,957 __main__ INFO getting data from srv2
2023-07-25 13:09:47,463 __main__ INFO getting data from srv3
2023-07-25 13:09:49,969 __main__ INFO getting data from srv4
2023-07-25 13:09:52,475 __main__ INFO getting data from srv5
2023-07-25 13:09:54,981 __main__ INFO getting data from srv6
2023-07-25 13:09:57,487 __main__ INFO getting data from srv7
2023-07-25 13:09:59,992 __main__ INFO getting data from cli8foo
2023-07-25 13:10:02,499 TADA INFO assertion 2, JSON support (l3-stream): client data verified, passed
2023-07-25 13:10:02,499 __main__ INFO publishing 'four' on l3-stream by pub4
2023-07-25 13:10:03,002 __main__ INFO publishing 'five' on l3-stream by pub5
2023-07-25 13:10:03,504 __main__ INFO publishing 'six' on l3-stream by pub6
2023-07-25 13:10:04,007 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-07-25 13:10:04,510 TADA INFO assertion 301, send-credit taken: credits: [114, 128, 128, 128], passed
2023-07-25 13:10:04,510 __main__ INFO obtaining all client data (0)
2023-07-25 13:10:04,510 __main__ INFO getting data from srv1
2023-07-25 13:10:07,017 __main__ INFO getting data from srv2
2023-07-25 13:10:09,523 __main__ INFO getting data from srv3
2023-07-25 13:10:12,030 __main__ INFO getting data from srv4
2023-07-25 13:10:14,536 __main__ INFO getting data from srv5
2023-07-25 13:10:17,042 __main__ INFO getting data from srv6
2023-07-25 13:10:19,549 __main__ INFO getting data from srv7
2023-07-25 13:10:22,055 __main__ INFO getting data from cli8foo
2023-07-25 13:10:24,562 __main__ INFO obtaining all client data (1)
2023-07-25 13:10:24,562 __main__ INFO getting data from srv1
2023-07-25 13:10:27,069 __main__ INFO getting data from srv2
2023-07-25 13:10:29,575 __main__ INFO getting data from srv3
2023-07-25 13:10:32,081 __main__ INFO getting data from srv4
2023-07-25 13:10:34,587 __main__ INFO getting data from srv5
2023-07-25 13:10:37,093 __main__ INFO getting data from srv6
2023-07-25 13:10:39,598 __main__ INFO getting data from srv7
2023-07-25 13:10:42,104 __main__ INFO getting data from cli8foo
2023-07-25 13:10:44,610 __main__ INFO obtaining all client data (2)
2023-07-25 13:10:44,611 __main__ INFO getting data from srv1
2023-07-25 13:10:47,117 __main__ INFO getting data from srv2
2023-07-25 13:10:49,623 __main__ INFO getting data from srv3
2023-07-25 13:10:52,128 __main__ INFO getting data from srv4
2023-07-25 13:10:54,634 __main__ INFO getting data from srv5
2023-07-25 13:10:57,140 __main__ INFO getting data from srv6
2023-07-25 13:10:59,646 __main__ INFO getting data from srv7
2023-07-25 13:11:02,151 __main__ INFO getting data from cli8foo
2023-07-25 13:11:04,658 __main__ INFO obtaining all client data (3)
2023-07-25 13:11:04,658 __main__ INFO getting data from srv1
2023-07-25 13:11:07,165 __main__ INFO getting data from srv2
2023-07-25 13:11:09,670 __main__ INFO getting data from srv3
2023-07-25 13:11:12,176 __main__ INFO getting data from srv4
2023-07-25 13:11:14,681 __main__ INFO getting data from srv5
2023-07-25 13:11:17,187 __main__ INFO getting data from srv6
2023-07-25 13:11:19,693 __main__ INFO getting data from srv7
2023-07-25 13:11:22,199 __main__ INFO getting data from cli8foo
2023-07-25 13:11:25,207 TADA INFO assertion 302, send-credit returned: credits: [128, 128, 128, 128], passed
2023-07-25 13:11:25,207 TADA INFO assertion 303, stream delivery spread among rails: tids: {208, 205, 206, 207}, passed
2023-07-25 13:11:25,209 TADA INFO assertion 3, l3-stream delivery: client data verified, passed
2023-07-25 13:11:25,209 __main__ INFO publishing 'four' on l2-stream by pub4
2023-07-25 13:11:25,711 __main__ INFO publishing 'five' on l2-stream by pub5
2023-07-25 13:11:26,213 __main__ INFO publishing 'six' on l2-stream by pub6
2023-07-25 13:11:26,715 __main__ INFO publishing 'seven' on l2-stream by pub7
2023-07-25 13:11:27,218 __main__ INFO obtaining all client data (0)
2023-07-25 13:11:27,218 __main__ INFO getting data from srv1
2023-07-25 13:11:29,725 __main__ INFO getting data from srv2
2023-07-25 13:11:32,231 __main__ INFO getting data from srv3
2023-07-25 13:11:34,737 __main__ INFO getting data from srv4
2023-07-25 13:11:37,244 __main__ INFO getting data from srv5
2023-07-25 13:11:39,750 __main__ INFO getting data from srv6
2023-07-25 13:11:42,256 __main__ INFO getting data from srv7
2023-07-25 13:11:44,763 __main__ INFO getting data from cli8foo
2023-07-25 13:11:47,269 __main__ INFO obtaining all client data (1)
2023-07-25 13:11:47,270 __main__ INFO getting data from srv1
2023-07-25 13:11:49,776 __main__ INFO getting data from srv2
2023-07-25 13:11:52,283 __main__ INFO getting data from srv3
2023-07-25 13:11:54,789 __main__ INFO getting data from srv4
2023-07-25 13:11:57,295 __main__ INFO getting data from srv5
2023-07-25 13:11:59,800 __main__ INFO getting data from srv6
2023-07-25 13:12:02,306 __main__ INFO getting data from srv7
2023-07-25 13:12:04,812 __main__ INFO getting data from cli8foo
2023-07-25 13:12:07,319 TADA INFO assertion 4, l2-stream delivery: client data verified, passed
2023-07-25 13:12:07,319 __main__ INFO publishing 'four' on l1-stream by pub4
2023-07-25 13:12:07,821 __main__ INFO publishing 'five' on l1-stream by pub5
2023-07-25 13:12:08,322 __main__ INFO publishing 'six' on l1-stream by pub6
2023-07-25 13:12:08,825 __main__ INFO publishing 'seven' on l1-stream by pub7
2023-07-25 13:12:09,327 __main__ INFO obtaining all client data (0)
2023-07-25 13:12:09,328 __main__ INFO getting data from srv1
2023-07-25 13:12:11,834 __main__ INFO getting data from srv2
2023-07-25 13:12:14,340 __main__ INFO getting data from srv3
2023-07-25 13:12:16,846 __main__ INFO getting data from srv4
2023-07-25 13:12:19,353 __main__ INFO getting data from srv5
2023-07-25 13:12:21,859 __main__ INFO getting data from srv6
2023-07-25 13:12:24,365 __main__ INFO getting data from srv7
2023-07-25 13:12:26,872 __main__ INFO getting data from cli8foo
2023-07-25 13:12:29,378 __main__ INFO obtaining all client data (1)
2023-07-25 13:12:29,378 __main__ INFO getting data from srv1
2023-07-25 13:12:31,884 __main__ INFO getting data from srv2
2023-07-25 13:12:34,390 __main__ INFO getting data from srv3
2023-07-25 13:12:36,896 __main__ INFO getting data from srv4
2023-07-25 13:12:39,402 __main__ INFO getting data from srv5
2023-07-25 13:12:41,907 __main__ INFO getting data from srv6
2023-07-25 13:12:44,413 __main__ INFO getting data from srv7
2023-07-25 13:12:46,918 __main__ INFO getting data from cli8foo
2023-07-25 13:12:49,425 TADA INFO assertion 5, l1-stream delivery: client data verified, passed
2023-07-25 13:12:49,426 __main__ INFO publishing 'four' on x-stream by pub4
2023-07-25 13:12:49,927 __main__ INFO publishing 'five' on x-stream by pub5
2023-07-25 13:12:50,429 __main__ INFO publishing 'six' on x-stream by pub6
2023-07-25 13:12:50,931 __main__ INFO publishing 'seven' on x-stream by pub7
2023-07-25 13:12:51,434 __main__ INFO obtaining all client data (0)
2023-07-25 13:12:51,434 __main__ INFO getting data from srv1
2023-07-25 13:12:53,940 __main__ INFO getting data from srv2
2023-07-25 13:12:56,446 __main__ INFO getting data from srv3
2023-07-25 13:12:58,952 __main__ INFO getting data from srv4
2023-07-25 13:13:01,458 __main__ INFO getting data from srv5
2023-07-25 13:13:03,964 __main__ INFO getting data from srv6
2023-07-25 13:13:06,470 __main__ INFO getting data from srv7
2023-07-25 13:13:08,976 __main__ INFO getting data from cli8foo
2023-07-25 13:13:11,482 __main__ INFO obtaining all client data (1)
2023-07-25 13:13:11,482 __main__ INFO getting data from srv1
2023-07-25 13:13:13,988 __main__ INFO getting data from srv2
2023-07-25 13:13:16,494 __main__ INFO getting data from srv3
2023-07-25 13:13:19,000 __main__ INFO getting data from srv4
2023-07-25 13:13:21,506 __main__ INFO getting data from srv5
2023-07-25 13:13:24,011 __main__ INFO getting data from srv6
2023-07-25 13:13:26,517 __main__ INFO getting data from srv7
2023-07-25 13:13:29,023 __main__ INFO getting data from cli8foo
2023-07-25 13:13:31,530 TADA INFO assertion 6, x-stream delivery: client data verified, passed
2023-07-25 13:13:31,530 __main__ INFO publishing 'four' on nada by pub4
2023-07-25 13:13:32,032 __main__ INFO publishing 'five' on nada by pub5
2023-07-25 13:13:32,533 __main__ INFO publishing 'six' on nada by pub6
2023-07-25 13:13:33,035 __main__ INFO publishing 'seven' on nada by pub7
2023-07-25 13:13:33,536 __main__ INFO obtaining all client data (0)
2023-07-25 13:13:33,537 __main__ INFO getting data from srv1
2023-07-25 13:13:36,042 __main__ INFO getting data from srv2
2023-07-25 13:13:38,548 __main__ INFO getting data from srv3
2023-07-25 13:13:41,053 __main__ INFO getting data from srv4
2023-07-25 13:13:43,559 __main__ INFO getting data from srv5
2023-07-25 13:13:46,065 __main__ INFO getting data from srv6
2023-07-25 13:13:48,571 __main__ INFO getting data from srv7
2023-07-25 13:13:51,077 __main__ INFO getting data from cli8foo
2023-07-25 13:13:53,582 __main__ INFO obtaining all client data (1)
2023-07-25 13:13:53,583 __main__ INFO getting data from srv1
2023-07-25 13:13:56,088 __main__ INFO getting data from srv2
2023-07-25 13:13:58,594 __main__ INFO getting data from srv3
2023-07-25 13:14:01,100 __main__ INFO getting data from srv4
2023-07-25 13:14:03,605 __main__ INFO getting data from srv5
2023-07-25 13:14:06,111 __main__ INFO getting data from srv6
2023-07-25 13:14:08,617 __main__ INFO getting data from srv7
2023-07-25 13:14:11,122 __main__ INFO getting data from cli8foo
2023-07-25 13:14:13,629 TADA INFO assertion 7, nada delivery: client data verified, passed
2023-07-25 13:14:13,629 __main__ INFO publishing 'four' on l3-stream by pub4 (0400)
2023-07-25 13:14:14,131 __main__ INFO publishing 'five' on l3-stream by pub5 (0400)
2023-07-25 13:14:14,634 __main__ INFO publishing 'six' on l3-stream by pub6 (0400)
2023-07-25 13:14:15,136 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400)
2023-07-25 13:14:15,639 __main__ INFO obtaining all client data (0)
2023-07-25 13:14:15,639 __main__ INFO getting data from srv1
2023-07-25 13:14:18,145 __main__ INFO getting data from srv2
2023-07-25 13:14:20,652 __main__ INFO getting data from srv3
2023-07-25 13:14:23,158 __main__ INFO getting data from srv4
2023-07-25 13:14:25,664 __main__ INFO getting data from srv5
2023-07-25 13:14:28,171 __main__ INFO getting data from srv6
2023-07-25 13:14:30,678 __main__ INFO getting data from srv7
2023-07-25 13:14:33,184 __main__ INFO getting data from cli8foo
2023-07-25 13:14:35,690 __main__ INFO obtaining all client data (1)
2023-07-25 13:14:35,690 __main__ INFO getting data from srv1
2023-07-25 13:14:38,196 __main__ INFO getting data from srv2
2023-07-25 13:14:40,702 __main__ INFO getting data from srv3
2023-07-25 13:14:43,208 __main__ INFO getting data from srv4
2023-07-25 13:14:45,714 __main__ INFO getting data from srv5
2023-07-25 13:14:48,220 __main__ INFO getting data from srv6
2023-07-25 13:14:50,727 __main__ INFO getting data from srv7
2023-07-25 13:14:53,232 __main__ INFO getting data from cli8foo
2023-07-25 13:14:55,738 __main__ INFO obtaining all client data (2)
2023-07-25 13:14:55,738 __main__ INFO getting data from srv1
2023-07-25 13:14:58,245 __main__ INFO getting data from srv2
2023-07-25 13:15:00,750 __main__ INFO getting data from srv3
2023-07-25 13:15:03,256 __main__ INFO getting data from srv4
2023-07-25 13:15:05,762 __main__ INFO getting data from srv5
2023-07-25 13:15:08,267 __main__ INFO getting data from srv6
2023-07-25 13:15:10,773 __main__ INFO getting data from srv7
2023-07-25 13:15:13,279 __main__ INFO getting data from cli8foo
2023-07-25 13:15:15,784 __main__ INFO obtaining all client data (3)
2023-07-25 13:15:15,784 __main__ INFO getting data from srv1
2023-07-25 13:15:18,291 __main__ INFO getting data from srv2
2023-07-25 13:15:20,796 __main__ INFO getting data from srv3
2023-07-25 13:15:23,302 __main__ INFO getting data from srv4
2023-07-25 13:15:25,808 __main__ INFO getting data from srv5
2023-07-25 13:15:28,314 __main__ INFO getting data from srv6
2023-07-25 13:15:30,819 __main__ INFO getting data from srv7
2023-07-25 13:15:33,325 __main__ INFO getting data from cli8foo
2023-07-25 13:15:35,832 TADA INFO assertion 8, l3-stream by 'root' with 0400 permission: client data verified, passed
2023-07-25 13:15:35,832 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as foo
2023-07-25 13:15:36,334 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as foo
2023-07-25 13:15:36,837 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as foo
2023-07-25 13:15:37,339 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as foo
2023-07-25 13:15:37,842 __main__ INFO obtaining all client data (0)
2023-07-25 13:15:37,842 __main__ INFO getting data from srv1
2023-07-25 13:15:40,349 __main__ INFO getting data from srv2
2023-07-25 13:15:42,855 __main__ INFO getting data from srv3
2023-07-25 13:15:45,361 __main__ INFO getting data from srv4
2023-07-25 13:15:47,867 __main__ INFO getting data from srv5
2023-07-25 13:15:50,374 __main__ INFO getting data from srv6
2023-07-25 13:15:52,880 __main__ INFO getting data from srv7
2023-07-25 13:15:55,386 __main__ INFO getting data from cli8foo
2023-07-25 13:15:57,893 __main__ INFO obtaining all client data (1)
2023-07-25 13:15:57,893 __main__ INFO getting data from srv1
2023-07-25 13:16:00,399 __main__ INFO getting data from srv2
2023-07-25 13:16:02,906 __main__ INFO getting data from srv3
2023-07-25 13:16:05,412 __main__ INFO getting data from srv4
2023-07-25 13:16:07,918 __main__ INFO getting data from srv5
2023-07-25 13:16:10,423 __main__ INFO getting data from srv6
2023-07-25 13:16:12,929 __main__ INFO getting data from srv7
2023-07-25 13:16:15,435 __main__ INFO getting data from cli8foo
2023-07-25 13:16:17,941 __main__ INFO obtaining all client data (2)
2023-07-25 13:16:17,942 __main__ INFO getting data from srv1
2023-07-25 13:16:20,448 __main__ INFO getting data from srv2
2023-07-25 13:16:22,954 __main__ INFO getting data from srv3
2023-07-25 13:16:25,459 __main__ INFO getting data from srv4
2023-07-25 13:16:27,965 __main__ INFO getting data from srv5
2023-07-25 13:16:30,471 __main__ INFO getting data from srv6
2023-07-25 13:16:32,977 __main__ INFO getting data from srv7
2023-07-25 13:16:35,482 __main__ INFO getting data from cli8foo
2023-07-25 13:16:37,989 __main__ INFO obtaining all client data (3)
2023-07-25 13:16:37,989 __main__ INFO getting data from srv1
2023-07-25 13:16:40,495 __main__ INFO getting data from srv2
2023-07-25 13:16:43,001 __main__ INFO getting data from srv3
2023-07-25 13:16:45,507 __main__ INFO getting data from srv4
2023-07-25 13:16:48,012 __main__ INFO getting data from srv5
2023-07-25 13:16:50,518 __main__ INFO getting data from srv6
2023-07-25 13:16:53,024 __main__ INFO getting data from srv7
2023-07-25 13:16:55,530 __main__ INFO getting data from cli8foo
2023-07-25 13:16:58,037 TADA INFO assertion 9, l3-stream by 'root' as 'foo' with 0400 permission: client data verified, passed
2023-07-25 13:16:58,038 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as bar
2023-07-25 13:16:58,540 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as bar
2023-07-25 13:16:59,043 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as bar
2023-07-25 13:16:59,545 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as bar
2023-07-25 13:17:00,048 __main__ INFO obtaining all client data (0)
2023-07-25 13:17:00,048 __main__ INFO getting data from srv1
2023-07-25 13:17:02,554 __main__ INFO getting data from srv2
2023-07-25 13:17:05,060 __main__ INFO getting data from srv3
2023-07-25 13:17:07,567 __main__ INFO getting data from srv4
2023-07-25 13:17:10,073 __main__ INFO getting data from srv5
2023-07-25 13:17:12,579 __main__ INFO getting data from srv6
2023-07-25 13:17:15,086 __main__ INFO getting data from srv7
2023-07-25 13:17:17,592 __main__ INFO getting data from cli8foo
2023-07-25 13:17:20,098 __main__ INFO obtaining all client data (1)
2023-07-25 13:17:20,098 __main__ INFO getting data from srv1
2023-07-25 13:17:22,604 __main__ INFO getting data from srv2
2023-07-25 13:17:25,111 __main__ INFO getting data from srv3
2023-07-25 13:17:27,617 __main__ INFO getting data from srv4
2023-07-25 13:17:30,123 __main__ INFO getting data from srv5
2023-07-25 13:17:32,628 __main__ INFO getting data from srv6
2023-07-25 13:17:35,134 __main__ INFO getting data from srv7
2023-07-25 13:17:37,640 __main__ INFO getting data from cli8foo
2023-07-25 13:17:40,145 __main__ INFO obtaining all client data (2)
2023-07-25 13:17:40,146 __main__ INFO getting data from srv1
2023-07-25 13:17:42,652 __main__ INFO getting data from srv2
2023-07-25 13:17:45,157 __main__ INFO getting data from srv3
2023-07-25 13:17:47,663 __main__ INFO getting data from srv4
2023-07-25 13:17:50,169 __main__ INFO getting data from srv5
2023-07-25 13:17:52,675 __main__ INFO getting data from srv6
2023-07-25 13:17:55,180 __main__ INFO getting data from srv7
2023-07-25 13:17:57,686 __main__ INFO getting data from cli8foo
2023-07-25 13:18:00,192 __main__ INFO obtaining all client data (3)
2023-07-25 13:18:00,192 __main__ INFO getting data from srv1
2023-07-25 13:18:02,698 __main__ INFO getting data from srv2
2023-07-25 13:18:05,204 __main__ INFO getting data from srv3
2023-07-25 13:18:07,709 __main__ INFO getting data from srv4
2023-07-25 13:18:10,215 __main__ INFO getting data from srv5
2023-07-25 13:18:12,720 __main__ INFO getting data from srv6
2023-07-25 13:18:15,226 __main__ INFO getting data from srv7
2023-07-25 13:18:17,731 __main__ INFO getting data from cli8foo
2023-07-25 13:18:20,238 TADA INFO assertion 10, l3-stream by 'root' as 'bar' with 0400 permission: client data verified, passed
2023-07-25 13:18:20,239 __main__ INFO publishing 'four' on l3-stream by pub4 (0440) root as bar
2023-07-25 13:18:20,741 __main__ INFO publishing 'five' on l3-stream by pub5 (0440) root as bar
2023-07-25 13:18:21,244 __main__ INFO publishing 'six' on l3-stream by pub6 (0440) root as bar
2023-07-25 13:18:21,746 __main__ INFO publishing 'seven' on l3-stream by pub7 (0440) root as bar
2023-07-25 13:18:22,249 __main__ INFO obtaining all client data (0)
2023-07-25 13:18:22,249 __main__ INFO getting data from srv1
2023-07-25 13:18:24,756 __main__ INFO getting data from srv2
2023-07-25 13:18:27,262 __main__ INFO getting data from srv3
2023-07-25 13:18:29,769 __main__ INFO getting data from srv4
2023-07-25 13:18:32,275 __main__ INFO getting data from srv5
2023-07-25 13:18:34,781 __main__ INFO getting data from srv6
2023-07-25 13:18:37,287 __main__ INFO getting data from srv7
2023-07-25 13:18:39,794 __main__ INFO getting data from cli8foo
2023-07-25 13:18:42,299 __main__ INFO obtaining all client data (1)
2023-07-25 13:18:42,300 __main__ INFO getting data from srv1
2023-07-25 13:18:44,806 __main__ INFO getting data from srv2
2023-07-25 13:18:47,312 __main__ INFO getting data from srv3
2023-07-25 13:18:49,819 __main__ INFO getting data from srv4
2023-07-25 13:18:52,325 __main__ INFO getting data from srv5
2023-07-25 13:18:54,831 __main__ INFO getting data from srv6
2023-07-25 13:18:57,336 __main__ INFO getting data from srv7
2023-07-25 13:18:59,842 __main__ INFO getting data from cli8foo
2023-07-25 13:19:02,347 __main__ INFO obtaining all client data (2)
2023-07-25 13:19:02,348 __main__ INFO getting data from srv1
2023-07-25 13:19:04,854 __main__ INFO getting data from srv2
2023-07-25 13:19:07,359 __main__ INFO getting data from srv3
2023-07-25 13:19:09,865 __main__ INFO getting data from srv4
2023-07-25 13:19:12,371 __main__ INFO getting data from srv5
2023-07-25 13:19:14,877 __main__ INFO getting data from srv6
2023-07-25 13:19:17,383 __main__ INFO getting data from srv7
2023-07-25 13:19:19,888 __main__ INFO getting data from cli8foo
2023-07-25 13:19:22,394 __main__ INFO obtaining all client data (3)
2023-07-25 13:19:22,394 __main__ INFO getting data from srv1
2023-07-25 13:19:24,901 __main__ INFO getting data from srv2
2023-07-25 13:19:27,407 __main__ INFO getting data from srv3
2023-07-25 13:19:29,913 __main__ INFO getting data from srv4
2023-07-25 13:19:32,418 __main__ INFO getting data from srv5
2023-07-25 13:19:34,924 __main__ INFO getting data from srv6
2023-07-25 13:19:37,430 __main__ INFO getting data from srv7
2023-07-25 13:19:39,935 __main__ INFO getting data from cli8foo
2023-07-25 13:19:42,442 TADA INFO assertion 11, l3-stream by 'root' as 'bar' with 0440 permission: client data verified, passed
2023-07-25 13:19:42,943 TADA INFO assertion 12, l3-stream by 'foo' as 'bar' results in an error: checking..., passed
2023-07-25 13:19:42,944 __main__ INFO publishing 'four' on l3-stream by pub4foo (0440)
2023-07-25 13:19:43,446 __main__ INFO obtaining all client data (0)
2023-07-25 13:19:43,446 __main__ INFO getting data from srv1
2023-07-25 13:19:45,953 __main__ INFO getting data from srv2
2023-07-25 13:19:48,459 __main__ INFO getting data from srv3
2023-07-25 13:19:50,965 __main__ INFO getting data from srv4
2023-07-25 13:19:53,472 __main__ INFO getting data from srv5
2023-07-25 13:19:55,977 __main__ INFO getting data from srv6
2023-07-25 13:19:58,483 __main__ INFO getting data from srv7
2023-07-25 13:20:00,989 __main__ INFO getting data from cli8foo
2023-07-25 13:20:03,496 TADA INFO assertion 13, l3-stream by 'foo' with 0440 permission: client data verified, passed
2023-07-25 13:20:03,496 __main__ INFO publishing 'four' on l3-stream by pub4bar (0440)
2023-07-25 13:20:03,998 __main__ INFO obtaining all client data (0)
2023-07-25 13:20:03,998 __main__ INFO getting data from srv1
2023-07-25 13:20:06,505 __main__ INFO getting data from srv2
2023-07-25 13:20:09,011 __main__ INFO getting data from srv3
2023-07-25 13:20:11,517 __main__ INFO getting data from srv4
2023-07-25 13:20:14,023 __main__ INFO getting data from srv5
2023-07-25 13:20:16,529 __main__ INFO getting data from srv6
2023-07-25 13:20:19,035 __main__ INFO getting data from srv7
2023-07-25 13:20:21,541 __main__ INFO getting data from cli8foo
2023-07-25 13:20:24,047 TADA INFO assertion 14, l3-stream by 'bar' with 0440 permission: client data verified, passed
2023-07-25 13:20:27,556 TADA INFO assertion 15, Blocking client and asynchronous client have the same data: verified, passed
2023-07-25 13:20:29,059 __main__ INFO publishing 'four' on l3-stream by srv4
2023-07-25 13:20:29,562 __main__ INFO obtaining all client data (0)
2023-07-25 13:20:29,562 __main__ INFO getting data from srv1
2023-07-25 13:20:32,068 __main__ INFO getting data from srv2
2023-07-25 13:20:34,574 __main__ INFO getting data from srv3
2023-07-25 13:20:37,080 __main__ INFO getting data from srv4
2023-07-25 13:20:39,587 __main__ INFO getting data from srv5
2023-07-25 13:20:42,092 __main__ INFO getting data from srv6
2023-07-25 13:20:44,598 __main__ INFO getting data from srv7
2023-07-25 13:20:47,103 __main__ INFO getting data from cli8foo
2023-07-25 13:20:49,610 TADA INFO assertion 20, l3-stream publish from L1 (srv4): client data verified, passed
2023-07-25 13:20:49,611 __main__ INFO publishing 'four' on nada by srv4
2023-07-25 13:20:50,112 __main__ INFO obtaining all client data (0)
2023-07-25 13:20:50,113 __main__ INFO getting data from srv1
2023-07-25 13:20:52,618 __main__ INFO getting data from srv2
2023-07-25 13:20:55,124 __main__ INFO getting data from srv3
2023-07-25 13:20:57,629 __main__ INFO getting data from srv4
2023-07-25 13:21:00,136 __main__ INFO getting data from srv5
2023-07-25 13:21:02,641 __main__ INFO getting data from srv6
2023-07-25 13:21:05,147 __main__ INFO getting data from srv7
2023-07-25 13:21:07,652 __main__ INFO getting data from cli8foo
2023-07-25 13:21:10,159 TADA INFO assertion 21, nada publish from L1 (srv4): client data verified, passed
2023-07-25 13:21:13,757 TADA INFO assertion 22, Check stream stats in each process: verified, passed
2023-07-25 13:21:17,360 TADA INFO assertion 23, Check stream client stats in each process: verified, passed
2023-07-25 13:21:19,865 TADA INFO assertion 16, srv-6 clean up properly after srv-3 exited: checking..., passed
2023-07-25 13:21:19,865 TADA INFO assertion 17, srv-7 clean up properly after srv-3 exited: checking..., passed
2023-07-25 13:21:19,865 TADA INFO assertion 18, srv-1 clean up properly after srv-3 exited: checking..., passed
2023-07-25 13:21:19,865 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-841f538-node-3 
2023-07-25 13:21:23,385 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-07-25 13:21:23,887 __main__ INFO obtaining all client data (0)
2023-07-25 13:21:23,888 __main__ INFO getting data from srv1
2023-07-25 13:21:26,394 __main__ INFO getting data from srv2
2023-07-25 13:21:28,900 __main__ INFO getting data from srv3
2023-07-25 13:21:31,406 __main__ INFO getting data from srv4
2023-07-25 13:21:33,912 __main__ INFO getting data from srv5
2023-07-25 13:21:36,418 __main__ INFO getting data from srv6
2023-07-25 13:21:38,924 __main__ INFO getting data from srv7
2023-07-25 13:21:41,430 __main__ INFO getting data from cli8foo
2023-07-25 13:21:43,937 TADA INFO assertion 19, l3-stream successfully delivered after srv-3 restarted: client data verified, passed
2023-07-25 13:21:43,937 TADA INFO test ldms_stream_test ended
2023-07-25 13:21:59 INFO: ----------------------------------------------
2023-07-25 13:22:00 INFO: ======== test-ldms ========
2023-07-25 13:22:00 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-07-25T13:22:00-05:00 INFO: starting test-samp-1
af98ccf16ce6f0f22f93c31abbef1e33d11d0bc1125d55115f8ecc519a7072d4
2023-07-25T13:22:02-05:00 INFO: starting test-samp-2
18fdf72426077dbaa148e0c9c9169fba3d52f01b8448d8bd9664d3a9a51776cf
2023-07-25T13:22:04-05:00 INFO: starting test-samp-3
7bfcb72df01c090fa8a4a5a19549af4da5fb996ebbe75adc9db329ce22a01979
2023-07-25T13:22:06-05:00 INFO: starting test-samp-4
30ec055b45811c6a729d31657f48206c790d6103177a983a370bcae398c2ea03
2023-07-25T13:22:07-05:00 INFO: test-samp-1 is running
2023-07-25T13:22:07-05:00 INFO: test-samp-2 is running
2023-07-25T13:22:07-05:00 INFO: test-samp-3 is running
2023-07-25T13:22:07-05:00 INFO: test-samp-4 is running
2023-07-25T13:22:07-05:00 INFO: starting test-agg-11
ec4c6fe8e18f0e57b195eb91a5f2171229329ca25bf146667148cdc942ad8013
2023-07-25T13:22:09-05:00 INFO: starting test-agg-12
8cdd9e44294fcd13999cb669085f7cc5c49c82f81bb5826ba3043aca4abace26
2023-07-25T13:22:11-05:00 INFO: test-agg-11 is running
2023-07-25T13:22:11-05:00 INFO: test-agg-12 is running
2023-07-25T13:22:11-05:00 INFO: starting test-agg-2
56982f12dedcdf96cd0aece6f9723316b0e5469beba354136330b74131b0db59
2023-07-25T13:22:12-05:00 INFO: test-agg-2 is running
2023-07-25T13:22:12-05:00 INFO: Collecting data (into SOS)
2023-07-25T13:22:22-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-25T13:22:25-05:00 INFO: check rc: 0
2023-07-25T13:22:25-05:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-07-25T13:22:29-05:00 INFO: DONE
2023-07-25 13:22:39 INFO: ----------------------------------------------
2023-07-25 13:22:39 INFO: ======== test-maestro ========
2023-07-25 13:22:39 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-07-25T13:22:39-05:00 INFO: starting mtest-maestro
e6a71455b192e263449b065072efe118a528e687ecc1e1600f93ccf0f9922655
2023-07-25T13:22:41-05:00 INFO: starting mtest-samp-1
b278034a7cbf07df762004a84820b0828123e5376c400562105a7c9ef27e0b5c
2023-07-25T13:22:43-05:00 INFO: starting mtest-samp-2
c5953a005d3be719dbb696834d09e7a7b3dbfd95cf69178ae79ee70dc3562ac7
2023-07-25T13:22:44-05:00 INFO: starting mtest-samp-3
5e39b77da662fce3ee02983d752c88d9620916f28be6f389df80d9b7b015ff56
2023-07-25T13:22:46-05:00 INFO: starting mtest-samp-4
9e32e1b34e0d43816d5625382a249ecd25a07aa0638dac8335e0d3c7615bb3a0
2023-07-25T13:22:47-05:00 INFO: mtest-samp-1 is running
2023-07-25T13:22:47-05:00 INFO: mtest-samp-2 is running
2023-07-25T13:22:47-05:00 INFO: mtest-samp-3 is running
2023-07-25T13:22:47-05:00 INFO: mtest-samp-4 is running
2023-07-25T13:22:47-05:00 INFO: starting mtest-agg-11
2e1c7b631f88b3dc2fcc2a883e6ceddefbccc29beafaf81dd6b87323155c776c
2023-07-25T13:22:49-05:00 INFO: starting mtest-agg-12
2594032803c0fa7c89535637666b7bf99f808d834f12617c2d3d6b3e2f09d538
2023-07-25T13:22:50-05:00 INFO: mtest-agg-11 is running
2023-07-25T13:22:50-05:00 INFO: mtest-agg-12 is running
2023-07-25T13:22:50-05:00 INFO: starting mtest-agg-2
eb29fafbc2b8929c0844abafaf7902b3409353057769c655cb09502a9a541f9a
2023-07-25T13:22:52-05:00 INFO: mtest-agg-2 is running
2023-07-25T13:22:52-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-25T13:24:53-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-25T13:24:55-05:00 INFO: sos check rc: 0
2023-07-25T13:24:56-05:00 INFO: starting mtest-ui
774d605c682aab42e49619fabf8074ab4e667c79ff0b6a03011815e8466e2976
2023-07-25T13:25:03-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2181056, 1690309392002.022], [2181056, 1690309392002.029], [2181056, 1690309392002.034], [2181056, 1690309392002.036], [2181428, 1690309393000.749], [2181428, 1690309393001.149], [2181428, 1690309393001.1892], [2181428, 1690309393001.191], [2181428, 1690309394001.292], [2181428, 1690309394001.293], [2181428, 1690309394001.301], [2181428, 1690309394001.304], [2181428, 1690309395001.438], [2181428, 1690309395001.44], [2181428, 1690309395001.4421], [2181428, 1690309395001.444], [2181428, 1690309396001.586], [2181428, 1690309396001.59], [2181428, 1690309396001.593], [2181428, 1690309396001.594], [2181428, 1690309397001.723], [2181428, 1690309397001.7258], [2181428, 1690309397001.729], [2181428, 1690309397001.73], [2181428, 1690309398001.043], [2181428, 1690309398001.176], [2181428, 1690309398001.395], [2181428, 1690309398001.814], [2181428, 1690309399001.205], [2181428, 1690309399001.209], [2181428, 1690309399001.517], [2181428, 1690309399001.958], [2181428, 1690309400001.331], [2181428, 1690309400001.332], [2181428, 1690309400001.3381], [2181428, 1690309400002.081], [2181428, 1690309401001.219], [2181428, 1690309401001.2239], [2181428, 1690309401001.445], [2181428, 1690309401001.459], [2181428, 1690309402001.375], [2181428, 1690309402001.3801], [2181428, 1690309402001.381], [2181428, 1690309402001.581], [2181428, 1690309403001.529], [2181428, 1690309403001.5308], [2181428, 1690309403001.533], [2181428, 1690309403001.536], [2181428, 1690309404001.709], [2181428, 1690309404001.71], [2181428, 1690309404001.7122], [2181428, 1690309404001.7139], [2181428, 1690309405001.836], [2181428, 1690309405001.8408], [2181428, 1690309405001.8408], [2181428, 1690309405001.844], [2181428, 1690309406001.974], [2181428, 1690309406001.977], [2181428, 1690309406001.981], [2181428, 1690309406001.984], [2181428, 1690309407002.1162], [2181428, 1690309407002.1199], [2181428, 1690309407002.1199], [2181428, 1690309407002.1208], [2181428, 1690309408001.2422], [2181428, 1690309408001.2458], [2181428, 1690309408001.247], [2181428, 1690309408001.25], [2181428, 1690309409001.4011], [2181428, 1690309409001.408], [2181428, 1690309409001.408], [2181428, 1690309409001.409], [2181428, 1690309410001.54], [2181428, 1690309410001.549], [2181428, 1690309410001.55], [2181428, 1690309410001.551], [2181428, 1690309411001.664], [2181428, 1690309411001.667], [2181428, 1690309411001.6682], [2181428, 1690309411001.674], [2181428, 1690309412001.8171], [2181428, 1690309412001.8171], [2181428, 1690309412001.818], [2181428, 1690309412001.821], [2181428, 1690309413001.957], [2181428, 1690309413001.969], [2181428, 1690309413001.9788], [2181428, 1690309413001.9788], [2181428, 1690309414002.1172], [2181428, 1690309414002.1199], [2181428, 1690309414002.129], [2181428, 1690309414002.1309], [2181428, 1690309415001.255], [2181428, 1690309415001.257], [2181428, 1690309415001.259], [2181428, 1690309415001.26], [2181428, 1690309416001.388], [2181428, 1690309416001.391], [2181428, 1690309416001.3938], [2181428, 1690309416001.397], [2181428, 1690309417000.974], [2181428, 1690309417001.501], [2181428, 1690309417001.5051], [2181428, 1690309417001.509], [2181428, 1690309418001.633], [2181428, 1690309418001.636], [2181428, 1690309418001.64], [2181428, 1690309418001.6482], [2181428, 1690309419001.7878], [2181428, 1690309419001.794], [2181428, 1690309419001.7961], [2181428, 1690309419001.7961], [2181428, 1690309420001.925], [2181428, 1690309420001.9258], [2181428, 1690309420001.927], [2181428, 1690309420001.927], [2181428, 1690309421002.05], [2181428, 1690309421002.054], [2181428, 1690309421002.055], [2181428, 1690309421002.057], [2181428, 1690309422001.194], [2181428, 1690309422001.196], [2181428, 1690309422001.198], [2181428, 1690309422001.202], [2181428, 1690309423001.3499], [2181428, 1690309423001.353], [2181428, 1690309423001.355], [2181428, 1690309423001.3582], [2181424, 1690309424001.507], [2181424, 1690309424001.514], [2181424, 1690309424001.517], [2181424, 1690309424001.519], [2181428, 1690309425001.663], [2181428, 1690309425001.665], [2181428, 1690309425001.667], [2181428, 1690309425001.667], [2181428, 1690309426001.801], [2181428, 1690309426001.802], [2181428, 1690309426001.804], [2181428, 1690309426001.804], [2181428, 1690309427001.94], [2181428, 1690309427001.94], [2181428, 1690309427001.943], [2181428, 1690309427001.945], [2181428, 1690309428002.084], [2181428, 1690309428002.0852], [2181428, 1690309428002.088], [2181428, 1690309428002.0889], [2181428, 1690309429000.291], [2181428, 1690309429000.359], [2181428, 1690309429000.9841], [2181428, 1690309429001.137], [2181428, 1690309430001.2952], [2181428, 1690309430001.2969], [2181428, 1690309430001.434], [2181428, 1690309430002.1062], [2181428, 1690309431001.2559], [2181428, 1690309431001.262], [2181428, 1690309431001.4148], [2181428, 1690309431001.585], [2181428, 1690309432001.397], [2181428, 1690309432001.408], [2181428, 1690309432001.4102], [2181428, 1690309432001.7139], [2181428, 1690309433001.1782], [2181428, 1690309433001.188], [2181428, 1690309433001.51], [2181428, 1690309433001.521], [2181428, 1690309434001.311], [2181428, 1690309434001.322], [2181428, 1690309434001.323], [2181428, 1690309434001.639], [2181428, 1690309435001.466], [2181428, 1690309435001.468], [2181428, 1690309435001.482], [2181428, 1690309435001.493], [2181428, 1690309436000.627], [2181428, 1690309436001.579], [2181428, 1690309436001.6], [2181428, 1690309436001.687], [2181428, 1690309437001.7212], [2181428, 1690309437001.7332], [2181428, 1690309437001.7332], [2181428, 1690309437001.8171], [2181428, 1690309438001.8582], [2181428, 1690309438001.865], [2181428, 1690309438001.867], [2181428, 1690309438001.937], [2181428, 1690309439001.6262], [2181428, 1690309439001.731], [2181428, 1690309439001.972], [2181428, 1690309439001.987], [2181428, 1690309440001.783], [2181428, 1690309440001.7979], [2181428, 1690309440001.874], [2181428, 1690309440002.1418], [2181428, 1690309441001.2852], [2181428, 1690309441001.92], [2181428, 1690309441001.928], [2181428, 1690309441001.997], [2181428, 1690309442001.13], [2181428, 1690309442001.4248], [2181428, 1690309442002.0989], [2181428, 1690309442002.1099], [2181428, 1690309443001.2212], [2181428, 1690309443001.222], [2181428, 1690309443001.239], [2181428, 1690309443001.559], [2181428, 1690309444001.366], [2181428, 1690309444001.3682], [2181428, 1690309444001.375], [2181428, 1690309444001.7039], [2181428, 1690309445001.521], [2181428, 1690309445001.523], [2181428, 1690309445001.527], [2181428, 1690309445001.854], [2181428, 1690309446001.513], [2181428, 1690309446001.633], [2181428, 1690309446001.639], [2181428, 1690309446001.999], [2181428, 1690309447001.134], [2181428, 1690309447001.655], [2181428, 1690309447001.656], [2181428, 1690309447001.755], [2181428, 1690309448001.2668], [2181428, 1690309448001.7842], [2181428, 1690309448001.791], [2181428, 1690309448001.878], [2181428, 1690309449001.4148], [2181428, 1690309449001.933], [2181428, 1690309449001.9358], [2181428, 1690309449002.011], [2181428, 1690309450001.167], [2181428, 1690309450001.566], [2181428, 1690309450002.091], [2181428, 1690309450002.0962], [2181428, 1690309451001.218], [2181428, 1690309451001.222], [2181428, 1690309451001.2852], [2181428, 1690309451001.697], [2181428, 1690309452001.3608], [2181428, 1690309452001.3618], [2181428, 1690309452001.4111], [2181428, 1690309452001.8381], [2181428, 1690309453001.513], [2181428, 1690309453001.516], [2181428, 1690309453001.548], [2181428, 1690309453001.989], [2181428, 1690309454001.644], [2181428, 1690309454001.651], [2181428, 1690309454001.667], [2181428, 1690309454002.128], [2181428, 1690309455000.856], [2181428, 1690309455001.228], [2181428, 1690309455001.7239], [2181428, 1690309455001.7458], [2181428, 1690309456001.378], [2181428, 1690309456001.8801], [2181428, 1690309456001.8838], [2181428, 1690309456001.8838], [2181428, 1690309457001.512], [2181428, 1690309457002.012], [2181428, 1690309457002.012], [2181428, 1690309457002.014], [2181428, 1690309458001.156], [2181428, 1690309458001.163], [2181428, 1690309458001.163], [2181428, 1690309458001.657], [2181428, 1690309459001.3052], [2181428, 1690309459001.307], [2181428, 1690309459001.3079], [2181428, 1690309459001.802], [2181428, 1690309460001.4358], [2181428, 1690309460001.439], [2181428, 1690309460001.439], [2181428, 1690309460001.9321], [2181428, 1690309461001.588], [2181428, 1690309461001.59], [2181428, 1690309461001.593], [2181428, 1690309461002.077], [2181428, 1690309462001.226], [2181428, 1690309462001.7349], [2181428, 1690309462001.741], [2181428, 1690309462001.741], [2181428, 1690309463001.3792], [2181428, 1690309463001.886], [2181428, 1690309463001.891], [2181428, 1690309463001.892], [2181428, 1690309464001.311], [2181428, 1690309464001.513], [2181428, 1690309464001.8088], [2181428, 1690309464002.014], [2181428, 1690309465001.146], [2181428, 1690309465001.445], [2181428, 1690309465001.6472], [2181428, 1690309465001.945], [2181428, 1690309466001.296], [2181428, 1690309466001.595], [2181428, 1690309466001.8], [2181428, 1690309466002.102], [2181428, 1690309467001.236], [2181428, 1690309467001.4321], [2181428, 1690309467001.723], [2181428, 1690309467001.931], [2181428, 1690309468001.3801], [2181428, 1690309468001.579], [2181428, 1690309468001.867], [2181428, 1690309468002.072], [2181428, 1690309469001.217], [2181428, 1690309469001.532], [2181428, 1690309469001.7258], [2181428, 1690309469002.014], [2181428, 1690309470001.147], [2181428, 1690309470001.3472], [2181428, 1690309470001.664], [2181428, 1690309470001.8591], [2181428, 1690309471001.301], [2181428, 1690309471001.496], [2181428, 1690309471001.8188], [2181428, 1690309471002.016], [2181428, 1690309472001.155], [2181428, 1690309472001.447], [2181428, 1690309472001.6309], [2181428, 1690309472001.961], [2181428, 1690309473001.0618], [2181428, 1690309473001.272], [2181428, 1690309473001.5842], [2181428, 1690309473002.0999], [2181428, 1690309474001.207], [2181428, 1690309474001.208], [2181428, 1690309474001.2249], [2181428, 1690309474001.699], [2181428, 1690309475001.343], [2181428, 1690309475001.3582], [2181428, 1690309475001.3608], [2181428, 1690309475001.464], [2181428, 1690309476001.504], [2181428, 1690309476001.508], [2181428, 1690309476001.517], [2181428, 1690309476001.52], [2181428, 1690309477001.4521], [2181428, 1690309477001.622], [2181428, 1690309477001.628], [2181428, 1690309477001.633], [2181428, 1690309478001.597], [2181428, 1690309478001.607], [2181428, 1690309478001.7568], [2181428, 1690309478001.769], [2181428, 1690309479001.7239], [2181428, 1690309479001.7258], [2181428, 1690309479001.887], [2181428, 1690309479001.891], [2181428, 1690309480001.864], [2181428, 1690309480001.868], [2181428, 1690309480002.02], [2181428, 1690309480002.0212], [2181428, 1690309481001.177], [2181428, 1690309481001.18], [2181428, 1690309481002.019], [2181428, 1690309481002.019], [2181428, 1690309482001.1572], [2181428, 1690309482001.1619], [2181428, 1690309482001.3079], [2181428, 1690309482001.3079], [2181428, 1690309483001.303], [2181428, 1690309483001.312], [2181428, 1690309483001.443], [2181428, 1690309483001.445], [2181428, 1690309484001.4458], [2181428, 1690309484001.447], [2181428, 1690309484001.574], [2181428, 1690309484001.577], [2181428, 1690309485001.589], [2181428, 1690309485001.594], [2181428, 1690309485001.702], [2181428, 1690309485001.705], [2181428, 1690309486001.6938], [2181428, 1690309486001.72], [2181428, 1690309486001.803], [2181428, 1690309486001.8198], [2181428, 1690309487001.832], [2181428, 1690309487001.845], [2181428, 1690309487001.93], [2181428, 1690309487001.946], [2181428, 1690309488001.971], [2181428, 1690309488001.975], [2181428, 1690309488002.055], [2181428, 1690309488002.073], [2181428, 1690309489001.208], [2181428, 1690309489001.2102], [2181428, 1690309489002.125], [2181428, 1690309489002.1309], [2181428, 1690309490001.269], [2181428, 1690309490001.2742], [2181428, 1690309490001.334], [2181428, 1690309490001.344], [2181428, 1690309491001.4011], [2181428, 1690309491001.405], [2181428, 1690309491001.456], [2181428, 1690309491001.47], [2181428, 1690309492001.554], [2181428, 1690309492001.556], [2181428, 1690309492001.604], [2181428, 1690309492001.621]]}, {"target": "component_id", "datapoints": [[4, 1690309392002.022], [3, 1690309392002.029], [1, 1690309392002.034], [2, 1690309392002.036], [3, 1690309393000.749], [1, 1690309393001.149], [4, 1690309393001.1892], [2, 1690309393001.191], [3, 1690309394001.292], [1, 1690309394001.293], [2, 1690309394001.301], [4, 1690309394001.304], [2, 1690309395001.438], [4, 1690309395001.44], [3, 1690309395001.4421], [1, 1690309395001.444], [3, 1690309396001.586], [4, 1690309396001.59], [2, 1690309396001.593], [1, 1690309396001.594], [2, 1690309397001.723], [1, 1690309397001.7258], [4, 1690309397001.729], [3, 1690309397001.73], [4, 1690309398001.043], [2, 1690309398001.176], [3, 1690309398001.395], [1, 1690309398001.814], [2, 1690309399001.205], [4, 1690309399001.209], [3, 1690309399001.517], [1, 1690309399001.958], [4, 1690309400001.331], [2, 1690309400001.332], [3, 1690309400001.3381], [1, 1690309400002.081], [3, 1690309401001.219], [1, 1690309401001.2239], [2, 1690309401001.445], [4, 1690309401001.459], [1, 1690309402001.375], [2, 1690309402001.3801], [3, 1690309402001.381], [4, 1690309402001.581], [1, 1690309403001.529], [3, 1690309403001.5308], [2, 1690309403001.533], [4, 1690309403001.536], [4, 1690309404001.709], [1, 1690309404001.71], [3, 1690309404001.7122], [2, 1690309404001.7139], [4, 1690309405001.836], [1, 1690309405001.8408], [3, 1690309405001.8408], [2, 1690309405001.844], [4, 1690309406001.974], [3, 1690309406001.977], [1, 1690309406001.981], [2, 1690309406001.984], [3, 1690309407002.1162], [1, 1690309407002.1199], [4, 1690309407002.1199], [2, 1690309407002.1208], [1, 1690309408001.2422], [4, 1690309408001.2458], [2, 1690309408001.247], [3, 1690309408001.25], [1, 1690309409001.4011], [2, 1690309409001.408], [4, 1690309409001.408], [3, 1690309409001.409], [1, 1690309410001.54], [2, 1690309410001.549], [3, 1690309410001.55], [4, 1690309410001.551], [1, 1690309411001.664], [4, 1690309411001.667], [3, 1690309411001.6682], [2, 1690309411001.674], [3, 1690309412001.8171], [4, 1690309412001.8171], [1, 1690309412001.818], [2, 1690309412001.821], [1, 1690309413001.957], [4, 1690309413001.969], [2, 1690309413001.9788], [3, 1690309413001.9788], [1, 1690309414002.1172], [4, 1690309414002.1199], [2, 1690309414002.129], [3, 1690309414002.1309], [2, 1690309415001.255], [3, 1690309415001.257], [1, 1690309415001.259], [4, 1690309415001.26], [2, 1690309416001.388], [4, 1690309416001.391], [1, 1690309416001.3938], [3, 1690309416001.397], [1, 1690309417000.974], [2, 1690309417001.501], [3, 1690309417001.5051], [4, 1690309417001.509], [3, 1690309418001.633], [4, 1690309418001.636], [1, 1690309418001.64], [2, 1690309418001.6482], [1, 1690309419001.7878], [2, 1690309419001.794], [3, 1690309419001.7961], [4, 1690309419001.7961], [4, 1690309420001.925], [2, 1690309420001.9258], [1, 1690309420001.927], [3, 1690309420001.927], [2, 1690309421002.05], [1, 1690309421002.054], [3, 1690309421002.055], [4, 1690309421002.057], [1, 1690309422001.194], [2, 1690309422001.196], [3, 1690309422001.198], [4, 1690309422001.202], [1, 1690309423001.3499], [2, 1690309423001.353], [3, 1690309423001.355], [4, 1690309423001.3582], [2, 1690309424001.507], [4, 1690309424001.514], [1, 1690309424001.517], [3, 1690309424001.519], [2, 1690309425001.663], [4, 1690309425001.665], [1, 1690309425001.667], [3, 1690309425001.667], [1, 1690309426001.801], [2, 1690309426001.802], [3, 1690309426001.804], [4, 1690309426001.804], [3, 1690309427001.94], [4, 1690309427001.94], [1, 1690309427001.943], [2, 1690309427001.945], [4, 1690309428002.084], [2, 1690309428002.0852], [3, 1690309428002.088], [1, 1690309428002.0889], [4, 1690309429000.291], [1, 1690309429000.359], [3, 1690309429000.9841], [2, 1690309429001.137], [2, 1690309430001.2952], [1, 1690309430001.2969], [4, 1690309430001.434], [3, 1690309430002.1062], [1, 1690309431001.2559], [3, 1690309431001.262], [2, 1690309431001.4148], [4, 1690309431001.585], [3, 1690309432001.397], [2, 1690309432001.408], [1, 1690309432001.4102], [4, 1690309432001.7139], [2, 1690309433001.1782], [4, 1690309433001.188], [1, 1690309433001.51], [3, 1690309433001.521], [4, 1690309434001.311], [1, 1690309434001.322], [2, 1690309434001.323], [3, 1690309434001.639], [4, 1690309435001.466], [2, 1690309435001.468], [3, 1690309435001.482], [1, 1690309435001.493], [4, 1690309436000.627], [2, 1690309436001.579], [3, 1690309436001.6], [1, 1690309436001.687], [2, 1690309437001.7212], [3, 1690309437001.7332], [4, 1690309437001.7332], [1, 1690309437001.8171], [4, 1690309438001.8582], [2, 1690309438001.865], [3, 1690309438001.867], [1, 1690309438001.937], [1, 1690309439001.6262], [3, 1690309439001.731], [2, 1690309439001.972], [4, 1690309439001.987], [1, 1690309440001.783], [2, 1690309440001.7979], [3, 1690309440001.874], [4, 1690309440002.1418], [4, 1690309441001.2852], [1, 1690309441001.92], [2, 1690309441001.928], [3, 1690309441001.997], [3, 1690309442001.13], [4, 1690309442001.4248], [1, 1690309442002.0989], [2, 1690309442002.1099], [2, 1690309443001.2212], [1, 1690309443001.222], [3, 1690309443001.239], [4, 1690309443001.559], [2, 1690309444001.366], [3, 1690309444001.3682], [1, 1690309444001.375], [4, 1690309444001.7039], [2, 1690309445001.521], [3, 1690309445001.523], [1, 1690309445001.527], [4, 1690309445001.854], [2, 1690309446001.513], [1, 1690309446001.633], [3, 1690309446001.639], [4, 1690309446001.999], [4, 1690309447001.134], [1, 1690309447001.655], [2, 1690309447001.656], [3, 1690309447001.755], [4, 1690309448001.2668], [1, 1690309448001.7842], [2, 1690309448001.791], [3, 1690309448001.878], [4, 1690309449001.4148], [2, 1690309449001.933], [1, 1690309449001.9358], [3, 1690309449002.011], [3, 1690309450001.167], [4, 1690309450001.566], [1, 1690309450002.091], [2, 1690309450002.0962], [2, 1690309451001.218], [1, 1690309451001.222], [3, 1690309451001.2852], [4, 1690309451001.697], [1, 1690309452001.3608], [2, 1690309452001.3618], [3, 1690309452001.4111], [4, 1690309452001.8381], [1, 1690309453001.513], [2, 1690309453001.516], [3, 1690309453001.548], [4, 1690309453001.989], [2, 1690309454001.644], [1, 1690309454001.651], [3, 1690309454001.667], [4, 1690309454002.128], [3, 1690309455000.856], [4, 1690309455001.228], [1, 1690309455001.7239], [2, 1690309455001.7458], [4, 1690309456001.378], [3, 1690309456001.8801], [1, 1690309456001.8838], [2, 1690309456001.8838], [4, 1690309457001.512], [2, 1690309457002.012], [3, 1690309457002.012], [1, 1690309457002.014], [3, 1690309458001.156], [1, 1690309458001.163], [2, 1690309458001.163], [4, 1690309458001.657], [3, 1690309459001.3052], [1, 1690309459001.307], [2, 1690309459001.3079], [4, 1690309459001.802], [2, 1690309460001.4358], [1, 1690309460001.439], [3, 1690309460001.439], [4, 1690309460001.9321], [3, 1690309461001.588], [1, 1690309461001.59], [2, 1690309461001.593], [4, 1690309461002.077], [4, 1690309462001.226], [2, 1690309462001.7349], [1, 1690309462001.741], [3, 1690309462001.741], [4, 1690309463001.3792], [2, 1690309463001.886], [3, 1690309463001.891], [1, 1690309463001.892], [2, 1690309464001.311], [4, 1690309464001.513], [1, 1690309464001.8088], [3, 1690309464002.014], [3, 1690309465001.146], [2, 1690309465001.445], [4, 1690309465001.6472], [1, 1690309465001.945], [3, 1690309466001.296], [2, 1690309466001.595], [4, 1690309466001.8], [1, 1690309466002.102], [1, 1690309467001.236], [3, 1690309467001.4321], [2, 1690309467001.723], [4, 1690309467001.931], [1, 1690309468001.3801], [3, 1690309468001.579], [2, 1690309468001.867], [4, 1690309468002.072], [4, 1690309469001.217], [1, 1690309469001.532], [3, 1690309469001.7258], [2, 1690309469002.014], [2, 1690309470001.147], [4, 1690309470001.3472], [1, 1690309470001.664], [3, 1690309470001.8591], [2, 1690309471001.301], [4, 1690309471001.496], [1, 1690309471001.8188], [3, 1690309471002.016], [3, 1690309472001.155], [2, 1690309472001.447], [4, 1690309472001.6309], [1, 1690309472001.961], [4, 1690309473001.0618], [3, 1690309473001.272], [2, 1690309473001.5842], [1, 1690309473002.0999], [4, 1690309474001.207], [3, 1690309474001.208], [1, 1690309474001.2249], [2, 1690309474001.699], [4, 1690309475001.343], [3, 1690309475001.3582], [1, 1690309475001.3608], [2, 1690309475001.464], [1, 1690309476001.504], [3, 1690309476001.508], [2, 1690309476001.517], [4, 1690309476001.52], [4, 1690309477001.4521], [2, 1690309477001.622], [1, 1690309477001.628], [3, 1690309477001.633], [4, 1690309478001.597], [2, 1690309478001.607], [1, 1690309478001.7568], [3, 1690309478001.769], [2, 1690309479001.7239], [4, 1690309479001.7258], [1, 1690309479001.887], [3, 1690309479001.891], [2, 1690309480001.864], [4, 1690309480001.868], [3, 1690309480002.02], [1, 1690309480002.0212], [1, 1690309481001.177], [3, 1690309481001.18], [2, 1690309481002.019], [4, 1690309481002.019], [2, 1690309482001.1572], [4, 1690309482001.1619], [1, 1690309482001.3079], [3, 1690309482001.3079], [2, 1690309483001.303], [4, 1690309483001.312], [1, 1690309483001.443], [3, 1690309483001.445], [2, 1690309484001.4458], [4, 1690309484001.447], [1, 1690309484001.574], [3, 1690309484001.577], [2, 1690309485001.589], [4, 1690309485001.594], [1, 1690309485001.702], [3, 1690309485001.705], [4, 1690309486001.6938], [2, 1690309486001.72], [3, 1690309486001.803], [1, 1690309486001.8198], [4, 1690309487001.832], [2, 1690309487001.845], [3, 1690309487001.93], [1, 1690309487001.946], [4, 1690309488001.971], [2, 1690309488001.975], [3, 1690309488002.055], [1, 1690309488002.073], [1, 1690309489001.208], [3, 1690309489001.2102], [4, 1690309489002.125], [2, 1690309489002.1309], [4, 1690309490001.269], [2, 1690309490001.2742], [1, 1690309490001.334], [3, 1690309490001.344], [4, 1690309491001.4011], [2, 1690309491001.405], [1, 1690309491001.456], [3, 1690309491001.47], [4, 1690309492001.554], [2, 1690309492001.556], [1, 1690309492001.604], [3, 1690309492001.621]]}, {"target": "job_id", "datapoints": [[0, 1690309392002.022], [0, 1690309392002.029], [0, 1690309392002.034], [0, 1690309392002.036], [0, 1690309393000.749], [0, 1690309393001.149], [0, 1690309393001.1892], [0, 1690309393001.191], [0, 1690309394001.292], [0, 1690309394001.293], [0, 1690309394001.301], [0, 1690309394001.304], [0, 1690309395001.438], [0, 1690309395001.44], [0, 1690309395001.4421], [0, 1690309395001.444], [0, 1690309396001.586], [0, 1690309396001.59], [0, 1690309396001.593], [0, 1690309396001.594], [0, 1690309397001.723], [0, 1690309397001.7258], [0, 1690309397001.729], [0, 1690309397001.73], [0, 1690309398001.043], [0, 1690309398001.176], [0, 1690309398001.395], [0, 1690309398001.814], [0, 1690309399001.205], [0, 1690309399001.209], [0, 1690309399001.517], [0, 1690309399001.958], [0, 1690309400001.331], [0, 1690309400001.332], [0, 1690309400001.3381], [0, 1690309400002.081], [0, 1690309401001.219], [0, 1690309401001.2239], [0, 1690309401001.445], [0, 1690309401001.459], [0, 1690309402001.375], [0, 1690309402001.3801], [0, 1690309402001.381], [0, 1690309402001.581], [0, 1690309403001.529], [0, 1690309403001.5308], [0, 1690309403001.533], [0, 1690309403001.536], [0, 1690309404001.709], [0, 1690309404001.71], [0, 1690309404001.7122], [0, 1690309404001.7139], [0, 1690309405001.836], [0, 1690309405001.8408], [0, 1690309405001.8408], [0, 1690309405001.844], [0, 1690309406001.974], [0, 1690309406001.977], [0, 1690309406001.981], [0, 1690309406001.984], [0, 1690309407002.1162], [0, 1690309407002.1199], [0, 1690309407002.1199], [0, 1690309407002.1208], [0, 1690309408001.2422], [0, 1690309408001.2458], [0, 1690309408001.247], [0, 1690309408001.25], [0, 1690309409001.4011], [0, 1690309409001.408], [0, 1690309409001.408], [0, 1690309409001.409], [0, 1690309410001.54], [0, 1690309410001.549], [0, 1690309410001.55], [0, 1690309410001.551], [0, 1690309411001.664], [0, 1690309411001.667], [0, 1690309411001.6682], [0, 1690309411001.674], [0, 1690309412001.8171], [0, 1690309412001.8171], [0, 1690309412001.818], [0, 1690309412001.821], [0, 1690309413001.957], [0, 1690309413001.969], [0, 1690309413001.9788], [0, 1690309413001.9788], [0, 1690309414002.1172], [0, 1690309414002.1199], [0, 1690309414002.129], [0, 1690309414002.1309], [0, 1690309415001.255], [0, 1690309415001.257], [0, 1690309415001.259], [0, 1690309415001.26], [0, 1690309416001.388], [0, 1690309416001.391], [0, 1690309416001.3938], [0, 1690309416001.397], [0, 1690309417000.974], [0, 1690309417001.501], [0, 1690309417001.5051], [0, 1690309417001.509], [0, 1690309418001.633], [0, 1690309418001.636], [0, 1690309418001.64], [0, 1690309418001.6482], [0, 1690309419001.7878], [0, 1690309419001.794], [0, 1690309419001.7961], [0, 1690309419001.7961], [0, 1690309420001.925], [0, 1690309420001.9258], [0, 1690309420001.927], [0, 1690309420001.927], [0, 1690309421002.05], [0, 1690309421002.054], [0, 1690309421002.055], [0, 1690309421002.057], [0, 1690309422001.194], [0, 1690309422001.196], [0, 1690309422001.198], [0, 1690309422001.202], [0, 1690309423001.3499], [0, 1690309423001.353], [0, 1690309423001.355], [0, 1690309423001.3582], [0, 1690309424001.507], [0, 1690309424001.514], [0, 1690309424001.517], [0, 1690309424001.519], [0, 1690309425001.663], [0, 1690309425001.665], [0, 1690309425001.667], [0, 1690309425001.667], [0, 1690309426001.801], [0, 1690309426001.802], [0, 1690309426001.804], [0, 1690309426001.804], [0, 1690309427001.94], [0, 1690309427001.94], [0, 1690309427001.943], [0, 1690309427001.945], [0, 1690309428002.084], [0, 1690309428002.0852], [0, 1690309428002.088], [0, 1690309428002.0889], [0, 1690309429000.291], [0, 1690309429000.359], [0, 1690309429000.9841], [0, 1690309429001.137], [0, 1690309430001.2952], [0, 1690309430001.2969], [0, 1690309430001.434], [0, 1690309430002.1062], [0, 1690309431001.2559], [0, 1690309431001.262], [0, 1690309431001.4148], [0, 1690309431001.585], [0, 1690309432001.397], [0, 1690309432001.408], [0, 1690309432001.4102], [0, 1690309432001.7139], [0, 1690309433001.1782], [0, 1690309433001.188], [0, 1690309433001.51], [0, 1690309433001.521], [0, 1690309434001.311], [0, 1690309434001.322], [0, 1690309434001.323], [0, 1690309434001.639], [0, 1690309435001.466], [0, 1690309435001.468], [0, 1690309435001.482], [0, 1690309435001.493], [0, 1690309436000.627], [0, 1690309436001.579], [0, 1690309436001.6], [0, 1690309436001.687], [0, 1690309437001.7212], [0, 1690309437001.7332], [0, 1690309437001.7332], [0, 1690309437001.8171], [0, 1690309438001.8582], [0, 1690309438001.865], [0, 1690309438001.867], [0, 1690309438001.937], [0, 1690309439001.6262], [0, 1690309439001.731], [0, 1690309439001.972], [0, 1690309439001.987], [0, 1690309440001.783], [0, 1690309440001.7979], [0, 1690309440001.874], [0, 1690309440002.1418], [0, 1690309441001.2852], [0, 1690309441001.92], [0, 1690309441001.928], [0, 1690309441001.997], [0, 1690309442001.13], [0, 1690309442001.4248], [0, 1690309442002.0989], [0, 1690309442002.1099], [0, 1690309443001.2212], [0, 1690309443001.222], [0, 1690309443001.239], [0, 1690309443001.559], [0, 1690309444001.366], [0, 1690309444001.3682], [0, 1690309444001.375], [0, 1690309444001.7039], [0, 1690309445001.521], [0, 1690309445001.523], [0, 1690309445001.527], [0, 1690309445001.854], [0, 1690309446001.513], [0, 1690309446001.633], [0, 1690309446001.639], [0, 1690309446001.999], [0, 1690309447001.134], [0, 1690309447001.655], [0, 1690309447001.656], [0, 1690309447001.755], [0, 1690309448001.2668], [0, 1690309448001.7842], [0, 1690309448001.791], [0, 1690309448001.878], [0, 1690309449001.4148], [0, 1690309449001.933], [0, 1690309449001.9358], [0, 1690309449002.011], [0, 1690309450001.167], [0, 1690309450001.566], [0, 1690309450002.091], [0, 1690309450002.0962], [0, 1690309451001.218], [0, 1690309451001.222], [0, 1690309451001.2852], [0, 1690309451001.697], [0, 1690309452001.3608], [0, 1690309452001.3618], [0, 1690309452001.4111], [0, 1690309452001.8381], [0, 1690309453001.513], [0, 1690309453001.516], [0, 1690309453001.548], [0, 1690309453001.989], [0, 1690309454001.644], [0, 1690309454001.651], [0, 1690309454001.667], [0, 1690309454002.128], [0, 1690309455000.856], [0, 1690309455001.228], [0, 1690309455001.7239], [0, 1690309455001.7458], [0, 1690309456001.378], [0, 1690309456001.8801], [0, 1690309456001.8838], [0, 1690309456001.8838], [0, 1690309457001.512], [0, 1690309457002.012], [0, 1690309457002.012], [0, 1690309457002.014], [0, 1690309458001.156], [0, 1690309458001.163], [0, 1690309458001.163], [0, 1690309458001.657], [0, 1690309459001.3052], [0, 1690309459001.307], [0, 1690309459001.3079], [0, 1690309459001.802], [0, 1690309460001.4358], [0, 1690309460001.439], [0, 1690309460001.439], [0, 1690309460001.9321], [0, 1690309461001.588], [0, 1690309461001.59], [0, 1690309461001.593], [0, 1690309461002.077], [0, 1690309462001.226], [0, 1690309462001.7349], [0, 1690309462001.741], [0, 1690309462001.741], [0, 1690309463001.3792], [0, 1690309463001.886], [0, 1690309463001.891], [0, 1690309463001.892], [0, 1690309464001.311], [0, 1690309464001.513], [0, 1690309464001.8088], [0, 1690309464002.014], [0, 1690309465001.146], [0, 1690309465001.445], [0, 1690309465001.6472], [0, 1690309465001.945], [0, 1690309466001.296], [0, 1690309466001.595], [0, 1690309466001.8], [0, 1690309466002.102], [0, 1690309467001.236], [0, 1690309467001.4321], [0, 1690309467001.723], [0, 1690309467001.931], [0, 1690309468001.3801], [0, 1690309468001.579], [0, 1690309468001.867], [0, 1690309468002.072], [0, 1690309469001.217], [0, 1690309469001.532], [0, 1690309469001.7258], [0, 1690309469002.014], [0, 1690309470001.147], [0, 1690309470001.3472], [0, 1690309470001.664], [0, 1690309470001.8591], [0, 1690309471001.301], [0, 1690309471001.496], [0, 1690309471001.8188], [0, 1690309471002.016], [0, 1690309472001.155], [0, 1690309472001.447], [0, 1690309472001.6309], [0, 1690309472001.961], [0, 1690309473001.0618], [0, 1690309473001.272], [0, 1690309473001.5842], [0, 1690309473002.0999], [0, 1690309474001.207], [0, 1690309474001.208], [0, 1690309474001.2249], [0, 1690309474001.699], [0, 1690309475001.343], [0, 1690309475001.3582], [0, 1690309475001.3608], [0, 1690309475001.464], [0, 1690309476001.504], [0, 1690309476001.508], [0, 1690309476001.517], [0, 1690309476001.52], [0, 1690309477001.4521], [0, 1690309477001.622], [0, 1690309477001.628], [0, 1690309477001.633], [0, 1690309478001.597], [0, 1690309478001.607], [0, 1690309478001.7568], [0, 1690309478001.769], [0, 1690309479001.7239], [0, 1690309479001.7258], [0, 1690309479001.887], [0, 1690309479001.891], [0, 1690309480001.864], [0, 1690309480001.868], [0, 1690309480002.02], [0, 1690309480002.0212], [0, 1690309481001.177], [0, 1690309481001.18], [0, 1690309481002.019], [0, 1690309481002.019], [0, 1690309482001.1572], [0, 1690309482001.1619], [0, 1690309482001.3079], [0, 1690309482001.3079], [0, 1690309483001.303], [0, 1690309483001.312], [0, 1690309483001.443], [0, 1690309483001.445], [0, 1690309484001.4458], [0, 1690309484001.447], [0, 1690309484001.574], [0, 1690309484001.577], [0, 1690309485001.589], [0, 1690309485001.594], [0, 1690309485001.702], [0, 1690309485001.705], [0, 1690309486001.6938], [0, 1690309486001.72], [0, 1690309486001.803], [0, 1690309486001.8198], [0, 1690309487001.832], [0, 1690309487001.845], [0, 1690309487001.93], [0, 1690309487001.946], [0, 1690309488001.971], [0, 1690309488001.975], [0, 1690309488002.055], [0, 1690309488002.073], [0, 1690309489001.208], [0, 1690309489001.2102], [0, 1690309489002.125], [0, 1690309489002.1309], [0, 1690309490001.269], [0, 1690309490001.2742], [0, 1690309490001.334], [0, 1690309490001.344], [0, 1690309491001.4011], [0, 1690309491001.405], [0, 1690309491001.456], [0, 1690309491001.47], [0, 1690309492001.554], [0, 1690309492001.556], [0, 1690309492001.604], [0, 1690309492001.621]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-25T13:25:05-05:00 INFO: query check RC: 0
1d25a313381e4584a5096a5dcbe7362c91bdd30e6def4aa05a00210fe89dd609
2023-07-25T13:25:36-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2497    771 --:--:-- --:--:-- --:--:--  3280
{"datasource":{"id":1,"uid":"SNBzjv34z","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-25T13:25:38-05:00 INFO: Checking grafana data
2023-07-25T13:25:38-05:00 INFO: Grafana data check, rc: 0
2023-07-25T13:25:38-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-25T13:25:43-05:00 INFO: DONE
2023-07-25 13:25:53 INFO: ----------------------------------------------
2023-07-25 13:25:53 INFO: ======== test-maestro-hostmunge ========
2023-07-25 13:25:53 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-07-25T13:25:53-05:00 INFO: Checking munge on localhost
2023-07-25T13:25:53-05:00 INFO: munge encode/decode successfully
2023-07-25T13:25:53-05:00 INFO: starting mtest-maestro
797ed4abb79f4639cccf04ddb14cedff9e5dfb00cf8547da0c5049cf491f25b1
2023-07-25T13:25:55-05:00 INFO: starting mtest-samp-1
a2aaeb2800f31fbc3b4097f9698ae6189e8f6b0f87a6f2248b473925fc0ac7bd
2023-07-25T13:25:57-05:00 INFO: starting mtest-samp-2
23579b41fbec8e074dbac3b288968a1d000516952f7efbda7e7f49459c500e87
2023-07-25T13:25:59-05:00 INFO: starting mtest-samp-3
d336e8287d195b64fe99db1d2573a4b03ab705030a22a5f8bf4589299a790159
2023-07-25T13:26:01-05:00 INFO: starting mtest-samp-4
ae3f94869b91a39de48c6d42c350d94e1ae2f716844c915ce0caea031a667c5d
2023-07-25T13:26:02-05:00 INFO: mtest-samp-1 is running
2023-07-25T13:26:02-05:00 INFO: mtest-samp-2 is running
2023-07-25T13:26:02-05:00 INFO: mtest-samp-3 is running
2023-07-25T13:26:02-05:00 INFO: mtest-samp-4 is running
2023-07-25T13:26:02-05:00 INFO: starting mtest-agg-11
bb62072a781e4828c54f01b61f41e9ee1871ebea06e9ce7e8ce57f68cbad0d5e
2023-07-25T13:26:04-05:00 INFO: starting mtest-agg-12
05e19edb9f465cd5e47baa7033d43a1c1489aa790886a218baf6962e3375c00c
2023-07-25T13:26:05-05:00 INFO: mtest-agg-11 is running
2023-07-25T13:26:05-05:00 INFO: mtest-agg-12 is running
2023-07-25T13:26:05-05:00 INFO: starting mtest-agg-2
75822f79d24d529e3d7c40a4a025e1c6150742f803067d02dd4ccfb376cc8d5a
2023-07-25T13:26:06-05:00 INFO: mtest-agg-2 is running
2023-07-25T13:26:06-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-25T13:28:07-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-25T13:28:10-05:00 INFO: sos check rc: 0
2023-07-25T13:28:11-05:00 INFO: starting mtest-ui
3f249917768191cc89501b9a8716e8745e1fc416d0a905b18f9e19d119759247
2023-07-25T13:28:12-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2181804, 1690309587001.385], [2181804, 1690309587001.395], [2181804, 1690309587001.398], [2181804, 1690309587001.434], [2182104, 1690309588001.537], [2182104, 1690309588001.544], [2182104, 1690309588001.545], [2182104, 1690309588001.579], [2182176, 1690309589001.6892], [2182176, 1690309589001.692], [2182176, 1690309589001.6929], [2182176, 1690309589001.72], [2182176, 1690309590001.826], [2182176, 1690309590001.8281], [2182176, 1690309590001.8281], [2182176, 1690309590001.842], [2182176, 1690309591001.974], [2182176, 1690309591001.976], [2182176, 1690309591001.978], [2182176, 1690309591001.985], [2182176, 1690309592002.135], [2182176, 1690309592002.1372], [2182176, 1690309592002.139], [2182176, 1690309592002.14], [2182176, 1690309593001.276], [2182176, 1690309593001.279], [2182176, 1690309593001.28], [2182176, 1690309593001.281], [2182176, 1690309594001.417], [2182176, 1690309594001.417], [2182176, 1690309594001.426], [2182176, 1690309594001.426], [2182176, 1690309595001.571], [2182176, 1690309595001.571], [2182176, 1690309595001.574], [2182176, 1690309595001.575], [2182176, 1690309596001.703], [2182176, 1690309596001.713], [2182176, 1690309596001.713], [2182176, 1690309596001.7148], [2182176, 1690309597001.844], [2182176, 1690309597001.853], [2182176, 1690309597001.854], [2182176, 1690309597001.854], [2182176, 1690309598002.012], [2182176, 1690309598002.015], [2182176, 1690309598002.017], [2182176, 1690309598002.019], [2182176, 1690309599001.141], [2182176, 1690309599001.144], [2182176, 1690309599001.146], [2182176, 1690309599001.148], [2182176, 1690309600001.259], [2182176, 1690309600001.262], [2182176, 1690309600001.2659], [2182176, 1690309600001.2668], [2182176, 1690309601001.392], [2182176, 1690309601001.397], [2182176, 1690309601001.4028], [2182176, 1690309601001.4038], [2182176, 1690309602001.515], [2182176, 1690309602001.525], [2182176, 1690309602001.535], [2182176, 1690309602001.535], [2182176, 1690309603001.676], [2182176, 1690309603001.678], [2182176, 1690309603001.68], [2182176, 1690309603001.68], [2182176, 1690309604001.7878], [2182176, 1690309604001.7961], [2182176, 1690309604001.8062], [2182176, 1690309604001.8088], [2182176, 1690309605001.9148], [2182176, 1690309605001.918], [2182176, 1690309605001.923], [2182176, 1690309605001.927], [2182176, 1690309606002.048], [2182176, 1690309606002.051], [2182176, 1690309606002.051], [2182176, 1690309606002.055], [2182176, 1690309607001.1829], [2182176, 1690309607001.185], [2182176, 1690309607001.187], [2182176, 1690309607001.1892], [2182176, 1690309608001.3362], [2182176, 1690309608001.346], [2182176, 1690309608001.3481], [2182176, 1690309608001.3499], [2182176, 1690309609001.489], [2182176, 1690309609001.493], [2182176, 1690309609001.493], [2182176, 1690309609001.494], [2182176, 1690309610001.607], [2182176, 1690309610001.613], [2182176, 1690309610001.618], [2182176, 1690309610001.618], [2182176, 1690309611001.7332], [2182176, 1690309611001.739], [2182176, 1690309611001.74], [2182176, 1690309611001.74], [2182176, 1690309612001.876], [2182176, 1690309612001.876], [2182176, 1690309612001.877], [2182176, 1690309612001.877], [2182176, 1690309613002.033], [2182176, 1690309613002.035], [2182176, 1690309613002.037], [2182176, 1690309613002.039], [2182176, 1690309614001.153], [2182176, 1690309614001.154], [2182176, 1690309614001.158], [2182176, 1690309614001.159], [2182176, 1690309615001.2869], [2182176, 1690309615001.293], [2182176, 1690309615001.296], [2182176, 1690309615001.2969], [2182176, 1690309616001.3901], [2182176, 1690309616001.4312], [2182176, 1690309616001.44], [2182176, 1690309616001.44], [2182176, 1690309617001.538], [2182176, 1690309617001.574], [2182176, 1690309617001.578], [2182176, 1690309617001.579], [2182176, 1690309618001.6938], [2182176, 1690309618001.7222], [2182176, 1690309618001.7258], [2182176, 1690309618001.731], [2182176, 1690309619001.8271], [2182176, 1690309619001.842], [2182176, 1690309619001.8481], [2182176, 1690309619001.8508], [2182176, 1690309620001.962], [2182176, 1690309620001.97], [2182176, 1690309620001.974], [2182176, 1690309620001.974], [2182176, 1690309621002.0962], [2182176, 1690309621002.1062], [2182176, 1690309621002.1062], [2182176, 1690309621002.1062], [2182176, 1690309622001.2458], [2182176, 1690309622001.248], [2182176, 1690309622001.25], [2182176, 1690309622001.255], [2182176, 1690309623001.4038], [2182176, 1690309623001.406], [2182176, 1690309623001.412], [2182176, 1690309623001.416], [2182176, 1690309624001.555], [2182176, 1690309624001.556], [2182176, 1690309624001.558], [2182176, 1690309624001.56], [2182176, 1690309625001.696], [2182176, 1690309625001.698], [2182176, 1690309625001.699], [2182176, 1690309625001.702], [2182176, 1690309626001.824], [2182176, 1690309626001.826], [2182176, 1690309626001.8271], [2182176, 1690309626001.833], [2182176, 1690309627001.96], [2182176, 1690309627001.969], [2182176, 1690309627001.969], [2182176, 1690309627001.97], [2182176, 1690309628001.099], [2182176, 1690309628001.8052], [2182176, 1690309628001.813], [2182176, 1690309628001.814], [2182176, 1690309629001.232], [2182176, 1690309629001.2458], [2182176, 1690309629001.2532], [2182176, 1690309629001.906], [2182176, 1690309630001.367], [2182176, 1690309630001.37], [2182176, 1690309630001.374], [2182176, 1690309630001.483], [2182176, 1690309631001.483], [2182176, 1690309631001.494], [2182176, 1690309631001.506], [2182176, 1690309631001.507], [2182176, 1690309632001.618], [2182176, 1690309632001.619], [2182176, 1690309632001.624], [2182176, 1690309632001.629], [2182176, 1690309633001.7432], [2182176, 1690309633001.744], [2182176, 1690309633001.7458], [2182176, 1690309633001.75], [2182116, 1690309634001.902], [2182116, 1690309634001.908], [2182116, 1690309634001.909], [2182116, 1690309634001.912], [2182176, 1690309635002.03], [2182176, 1690309635002.0322], [2182176, 1690309635002.034], [2182176, 1690309635002.037], [2182176, 1690309636001.154], [2182176, 1690309636001.155], [2182176, 1690309636001.161], [2182176, 1690309636001.165], [2182176, 1690309637001.276], [2182176, 1690309637001.29], [2182176, 1690309637001.303], [2182176, 1690309637001.304], [2182176, 1690309638001.398], [2182176, 1690309638001.407], [2182176, 1690309638001.4248], [2182176, 1690309638001.426], [2182176, 1690309639000.5818], [2182176, 1690309639001.538], [2182176, 1690309639001.558], [2182176, 1690309639001.561], [2182176, 1690309640001.6729], [2182176, 1690309640001.674], [2182176, 1690309640001.674], [2182176, 1690309640001.674], [2182176, 1690309641001.7861], [2182176, 1690309641001.7861], [2182176, 1690309641001.79], [2182176, 1690309641001.793], [2182176, 1690309642001.944], [2182176, 1690309642001.9468], [2182176, 1690309642001.95], [2182176, 1690309642001.951], [2182176, 1690309643002.078], [2182176, 1690309643002.08], [2182176, 1690309643002.08], [2182176, 1690309643002.082], [2182176, 1690309644001.236], [2182176, 1690309644001.238], [2182176, 1690309644001.24], [2182176, 1690309644001.241], [2182176, 1690309645001.3572], [2182176, 1690309645001.3618], [2182176, 1690309645001.3618], [2182176, 1690309645001.363], [2182176, 1690309646001.494], [2182176, 1690309646001.497], [2182176, 1690309646001.4988], [2182176, 1690309646001.503], [2182176, 1690309647001.636], [2182176, 1690309647001.638], [2182176, 1690309647001.639], [2182176, 1690309647001.64], [2182176, 1690309648001.773], [2182176, 1690309648001.7742], [2182176, 1690309648001.782], [2182176, 1690309648001.783], [2182176, 1690309649000.9631], [2182176, 1690309649001.089], [2182176, 1690309649001.897], [2182176, 1690309649001.897], [2182176, 1690309650001.247], [2182176, 1690309650001.248], [2182176, 1690309650002.022], [2182176, 1690309650002.05], [2182176, 1690309651001.163], [2182176, 1690309651001.165], [2182176, 1690309651001.1719], [2182176, 1690309651001.3599], [2182176, 1690309652001.304], [2182176, 1690309652001.311], [2182176, 1690309652001.313], [2182176, 1690309652001.3188], [2182176, 1690309653001.434], [2182176, 1690309653001.438], [2182176, 1690309653001.441], [2182176, 1690309653001.445], [2182176, 1690309654001.59], [2182176, 1690309654001.596], [2182176, 1690309654001.598], [2182176, 1690309654001.6], [2182176, 1690309655001.731], [2182176, 1690309655001.734], [2182176, 1690309655001.737], [2182176, 1690309655001.74], [2182176, 1690309656001.847], [2182176, 1690309656001.8508], [2182176, 1690309656001.86], [2182176, 1690309656001.8618], [2182176, 1690309657001.98], [2182176, 1690309657001.985], [2182176, 1690309657001.985], [2182176, 1690309657001.988], [2182176, 1690309658002.1062], [2182176, 1690309658002.109], [2182176, 1690309658002.114], [2182176, 1690309658002.115], [2182180, 1690309659001.262], [2182180, 1690309659001.265], [2182180, 1690309659001.272], [2182180, 1690309659001.2732], [2182176, 1690309660001.4028], [2182176, 1690309660001.409], [2182176, 1690309660001.412], [2182176, 1690309660001.4138], [2182176, 1690309661001.2852], [2182176, 1690309661001.5308], [2182176, 1690309661001.556], [2182176, 1690309661001.557], [2182176, 1690309662001.4321], [2182176, 1690309662001.4348], [2182176, 1690309662001.6938], [2182176, 1690309662001.699], [2182176, 1690309663001.576], [2182176, 1690309663001.579], [2182176, 1690309663001.832], [2182176, 1690309663001.835], [2182176, 1690309664001.352], [2182176, 1690309664001.3708], [2182176, 1690309664001.684], [2182176, 1690309664001.842], [2182176, 1690309665001.517], [2182176, 1690309665001.521], [2182176, 1690309665001.522], [2182176, 1690309665001.524], [2182176, 1690309666001.645], [2182176, 1690309666001.65], [2182176, 1690309666001.656], [2182176, 1690309666001.6582], [2182176, 1690309667001.7769], [2182176, 1690309667001.7869], [2182176, 1690309667001.7878], [2182176, 1690309667001.7888], [2182176, 1690309668001.914], [2182176, 1690309668001.917], [2182176, 1690309668001.919], [2182176, 1690309668001.919], [2182176, 1690309669002.064], [2182176, 1690309669002.065], [2182176, 1690309669002.0679], [2182176, 1690309669002.07], [2182176, 1690309670001.228], [2182176, 1690309670001.228], [2182176, 1690309670001.229], [2182176, 1690309670001.233], [2182176, 1690309671001.3572], [2182176, 1690309671001.3682], [2182176, 1690309671001.3708], [2182176, 1690309671001.377], [2182176, 1690309672001.174], [2182176, 1690309672001.3062], [2182176, 1690309672001.482], [2182176, 1690309672001.49], [2182176, 1690309673001.331], [2182176, 1690309673001.433], [2182176, 1690309673001.4348], [2182176, 1690309673001.604], [2182176, 1690309674001.471], [2182176, 1690309674001.565], [2182176, 1690309674001.567], [2182176, 1690309674001.573], [2182176, 1690309675001.6309], [2182176, 1690309675001.7112], [2182176, 1690309675001.7122], [2182176, 1690309675001.717], [2182176, 1690309676001.78], [2182176, 1690309676001.849], [2182176, 1690309676001.85], [2182176, 1690309676001.857], [2182176, 1690309677001.918], [2182176, 1690309677001.977], [2182176, 1690309677001.977], [2182176, 1690309677001.983], [2182176, 1690309678002.055], [2182176, 1690309678002.0989], [2182176, 1690309678002.104], [2182176, 1690309678002.1099], [2182176, 1690309679001.24], [2182176, 1690309679001.241], [2182176, 1690309679001.248], [2182176, 1690309679001.251], [2182176, 1690309680001.4102], [2182176, 1690309680001.412], [2182176, 1690309680001.413], [2182176, 1690309680001.4148], [2182176, 1690309681001.553], [2182176, 1690309681001.553], [2182176, 1690309681001.556], [2182176, 1690309681001.556], [2182176, 1690309682001.71], [2182176, 1690309682001.713], [2182176, 1690309682001.7148], [2182176, 1690309682001.717], [2182176, 1690309683001.462], [2182176, 1690309683001.822], [2182176, 1690309683001.8381], [2182176, 1690309683001.844], [2182176, 1690309684001.612], [2182176, 1690309684001.617], [2182176, 1690309684001.961], [2182176, 1690309684001.985], [2182176, 1690309685001.772], [2182176, 1690309685001.782], [2182176, 1690309685001.7861], [2182176, 1690309685002.1172], [2182176, 1690309686001.257], [2182176, 1690309686001.2632], [2182176, 1690309686001.892], [2182176, 1690309686001.917]]}, {"target": "component_id", "datapoints": [[2, 1690309587001.385], [4, 1690309587001.395], [3, 1690309587001.398], [1, 1690309587001.434], [2, 1690309588001.537], [3, 1690309588001.544], [4, 1690309588001.545], [1, 1690309588001.579], [4, 1690309589001.6892], [2, 1690309589001.692], [3, 1690309589001.6929], [1, 1690309589001.72], [4, 1690309590001.826], [2, 1690309590001.8281], [3, 1690309590001.8281], [1, 1690309590001.842], [3, 1690309591001.974], [2, 1690309591001.976], [4, 1690309591001.978], [1, 1690309591001.985], [2, 1690309592002.135], [1, 1690309592002.1372], [3, 1690309592002.139], [4, 1690309592002.14], [4, 1690309593001.276], [1, 1690309593001.279], [3, 1690309593001.28], [2, 1690309593001.281], [3, 1690309594001.417], [4, 1690309594001.417], [1, 1690309594001.426], [2, 1690309594001.426], [1, 1690309595001.571], [3, 1690309595001.571], [4, 1690309595001.574], [2, 1690309595001.575], [4, 1690309596001.703], [1, 1690309596001.713], [3, 1690309596001.713], [2, 1690309596001.7148], [2, 1690309597001.844], [3, 1690309597001.853], [1, 1690309597001.854], [4, 1690309597001.854], [1, 1690309598002.012], [3, 1690309598002.015], [2, 1690309598002.017], [4, 1690309598002.019], [3, 1690309599001.141], [2, 1690309599001.144], [4, 1690309599001.146], [1, 1690309599001.148], [1, 1690309600001.259], [2, 1690309600001.262], [3, 1690309600001.2659], [4, 1690309600001.2668], [1, 1690309601001.392], [2, 1690309601001.397], [3, 1690309601001.4028], [4, 1690309601001.4038], [2, 1690309602001.515], [4, 1690309602001.525], [1, 1690309602001.535], [3, 1690309602001.535], [3, 1690309603001.676], [4, 1690309603001.678], [1, 1690309603001.68], [2, 1690309603001.68], [3, 1690309604001.7878], [1, 1690309604001.7961], [2, 1690309604001.8062], [4, 1690309604001.8088], [1, 1690309605001.9148], [3, 1690309605001.918], [4, 1690309605001.923], [2, 1690309605001.927], [1, 1690309606002.048], [2, 1690309606002.051], [4, 1690309606002.051], [3, 1690309606002.055], [4, 1690309607001.1829], [3, 1690309607001.185], [1, 1690309607001.187], [2, 1690309607001.1892], [4, 1690309608001.3362], [1, 1690309608001.346], [2, 1690309608001.3481], [3, 1690309608001.3499], [1, 1690309609001.489], [3, 1690309609001.493], [4, 1690309609001.493], [2, 1690309609001.494], [1, 1690309610001.607], [4, 1690309610001.613], [2, 1690309610001.618], [3, 1690309610001.618], [1, 1690309611001.7332], [4, 1690309611001.739], [2, 1690309611001.74], [3, 1690309611001.74], [1, 1690309612001.876], [3, 1690309612001.876], [2, 1690309612001.877], [4, 1690309612001.877], [2, 1690309613002.033], [3, 1690309613002.035], [4, 1690309613002.037], [1, 1690309613002.039], [2, 1690309614001.153], [1, 1690309614001.154], [3, 1690309614001.158], [4, 1690309614001.159], [4, 1690309615001.2869], [3, 1690309615001.293], [2, 1690309615001.296], [1, 1690309615001.2969], [3, 1690309616001.3901], [4, 1690309616001.4312], [1, 1690309616001.44], [2, 1690309616001.44], [3, 1690309617001.538], [4, 1690309617001.574], [1, 1690309617001.578], [2, 1690309617001.579], [3, 1690309618001.6938], [4, 1690309618001.7222], [1, 1690309618001.7258], [2, 1690309618001.731], [3, 1690309619001.8271], [4, 1690309619001.842], [1, 1690309619001.8481], [2, 1690309619001.8508], [3, 1690309620001.962], [1, 1690309620001.97], [2, 1690309620001.974], [4, 1690309620001.974], [1, 1690309621002.0962], [2, 1690309621002.1062], [3, 1690309621002.1062], [4, 1690309621002.1062], [1, 1690309622001.2458], [3, 1690309622001.248], [2, 1690309622001.25], [4, 1690309622001.255], [3, 1690309623001.4038], [4, 1690309623001.406], [2, 1690309623001.412], [1, 1690309623001.416], [2, 1690309624001.555], [4, 1690309624001.556], [1, 1690309624001.558], [3, 1690309624001.56], [4, 1690309625001.696], [1, 1690309625001.698], [2, 1690309625001.699], [3, 1690309625001.702], [4, 1690309626001.824], [3, 1690309626001.826], [1, 1690309626001.8271], [2, 1690309626001.833], [4, 1690309627001.96], [1, 1690309627001.969], [3, 1690309627001.969], [2, 1690309627001.97], [4, 1690309628001.099], [1, 1690309628001.8052], [3, 1690309628001.813], [2, 1690309628001.814], [3, 1690309629001.232], [1, 1690309629001.2458], [4, 1690309629001.2532], [2, 1690309629001.906], [1, 1690309630001.367], [4, 1690309630001.37], [3, 1690309630001.374], [2, 1690309630001.483], [3, 1690309631001.483], [1, 1690309631001.494], [4, 1690309631001.506], [2, 1690309631001.507], [3, 1690309632001.618], [1, 1690309632001.619], [4, 1690309632001.624], [2, 1690309632001.629], [1, 1690309633001.7432], [2, 1690309633001.744], [3, 1690309633001.7458], [4, 1690309633001.75], [1, 1690309634001.902], [2, 1690309634001.908], [4, 1690309634001.909], [3, 1690309634001.912], [1, 1690309635002.03], [3, 1690309635002.0322], [4, 1690309635002.034], [2, 1690309635002.037], [1, 1690309636001.154], [4, 1690309636001.155], [3, 1690309636001.161], [2, 1690309636001.165], [3, 1690309637001.276], [1, 1690309637001.29], [2, 1690309637001.303], [4, 1690309637001.304], [3, 1690309638001.398], [1, 1690309638001.407], [4, 1690309638001.4248], [2, 1690309638001.426], [3, 1690309639000.5818], [2, 1690309639001.538], [1, 1690309639001.558], [4, 1690309639001.561], [3, 1690309640001.6729], [1, 1690309640001.674], [2, 1690309640001.674], [4, 1690309640001.674], [3, 1690309641001.7861], [4, 1690309641001.7861], [1, 1690309641001.79], [2, 1690309641001.793], [3, 1690309642001.944], [1, 1690309642001.9468], [4, 1690309642001.95], [2, 1690309642001.951], [3, 1690309643002.078], [1, 1690309643002.08], [4, 1690309643002.08], [2, 1690309643002.082], [2, 1690309644001.236], [4, 1690309644001.238], [3, 1690309644001.24], [1, 1690309644001.241], [2, 1690309645001.3572], [1, 1690309645001.3618], [3, 1690309645001.3618], [4, 1690309645001.363], [4, 1690309646001.494], [3, 1690309646001.497], [1, 1690309646001.4988], [2, 1690309646001.503], [3, 1690309647001.636], [4, 1690309647001.638], [2, 1690309647001.639], [1, 1690309647001.64], [3, 1690309648001.773], [4, 1690309648001.7742], [2, 1690309648001.782], [1, 1690309648001.783], [4, 1690309649000.9631], [1, 1690309649001.089], [2, 1690309649001.897], [3, 1690309649001.897], [3, 1690309650001.247], [1, 1690309650001.248], [2, 1690309650002.022], [4, 1690309650002.05], [3, 1690309651001.163], [2, 1690309651001.165], [4, 1690309651001.1719], [1, 1690309651001.3599], [4, 1690309652001.304], [2, 1690309652001.311], [3, 1690309652001.313], [1, 1690309652001.3188], [3, 1690309653001.434], [4, 1690309653001.438], [2, 1690309653001.441], [1, 1690309653001.445], [4, 1690309654001.59], [3, 1690309654001.596], [1, 1690309654001.598], [2, 1690309654001.6], [1, 1690309655001.731], [4, 1690309655001.734], [2, 1690309655001.737], [3, 1690309655001.74], [1, 1690309656001.847], [4, 1690309656001.8508], [2, 1690309656001.86], [3, 1690309656001.8618], [4, 1690309657001.98], [1, 1690309657001.985], [3, 1690309657001.985], [2, 1690309657001.988], [1, 1690309658002.1062], [4, 1690309658002.109], [2, 1690309658002.114], [3, 1690309658002.115], [2, 1690309659001.262], [4, 1690309659001.265], [3, 1690309659001.272], [1, 1690309659001.2732], [4, 1690309660001.4028], [3, 1690309660001.409], [1, 1690309660001.412], [2, 1690309660001.4138], [4, 1690309661001.2852], [2, 1690309661001.5308], [1, 1690309661001.556], [3, 1690309661001.557], [2, 1690309662001.4321], [4, 1690309662001.4348], [1, 1690309662001.6938], [3, 1690309662001.699], [4, 1690309663001.576], [2, 1690309663001.579], [3, 1690309663001.832], [1, 1690309663001.835], [4, 1690309664001.352], [3, 1690309664001.3708], [2, 1690309664001.684], [1, 1690309664001.842], [1, 1690309665001.517], [4, 1690309665001.521], [2, 1690309665001.522], [3, 1690309665001.524], [4, 1690309666001.645], [1, 1690309666001.65], [3, 1690309666001.656], [2, 1690309666001.6582], [4, 1690309667001.7769], [1, 1690309667001.7869], [3, 1690309667001.7878], [2, 1690309667001.7888], [1, 1690309668001.914], [3, 1690309668001.917], [2, 1690309668001.919], [4, 1690309668001.919], [4, 1690309669002.064], [1, 1690309669002.065], [3, 1690309669002.0679], [2, 1690309669002.07], [3, 1690309670001.228], [4, 1690309670001.228], [1, 1690309670001.229], [2, 1690309670001.233], [4, 1690309671001.3572], [2, 1690309671001.3682], [1, 1690309671001.3708], [3, 1690309671001.377], [4, 1690309672001.174], [3, 1690309672001.3062], [2, 1690309672001.482], [1, 1690309672001.49], [4, 1690309673001.331], [3, 1690309673001.433], [2, 1690309673001.4348], [1, 1690309673001.604], [4, 1690309674001.471], [3, 1690309674001.565], [2, 1690309674001.567], [1, 1690309674001.573], [4, 1690309675001.6309], [3, 1690309675001.7112], [2, 1690309675001.7122], [1, 1690309675001.717], [4, 1690309676001.78], [2, 1690309676001.849], [3, 1690309676001.85], [1, 1690309676001.857], [4, 1690309677001.918], [2, 1690309677001.977], [3, 1690309677001.977], [1, 1690309677001.983], [4, 1690309678002.055], [2, 1690309678002.0989], [3, 1690309678002.104], [1, 1690309678002.1099], [4, 1690309679001.24], [2, 1690309679001.241], [3, 1690309679001.248], [1, 1690309679001.251], [4, 1690309680001.4102], [2, 1690309680001.412], [3, 1690309680001.413], [1, 1690309680001.4148], [1, 1690309681001.553], [2, 1690309681001.553], [3, 1690309681001.556], [4, 1690309681001.556], [2, 1690309682001.71], [4, 1690309682001.713], [1, 1690309682001.7148], [3, 1690309682001.717], [3, 1690309683001.462], [4, 1690309683001.822], [1, 1690309683001.8381], [2, 1690309683001.844], [3, 1690309684001.612], [4, 1690309684001.617], [1, 1690309684001.961], [2, 1690309684001.985], [3, 1690309685001.772], [1, 1690309685001.782], [4, 1690309685001.7861], [2, 1690309685002.1172], [2, 1690309686001.257], [1, 1690309686001.2632], [4, 1690309686001.892], [3, 1690309686001.917]]}, {"target": "job_id", "datapoints": [[0, 1690309587001.385], [0, 1690309587001.395], [0, 1690309587001.398], [0, 1690309587001.434], [0, 1690309588001.537], [0, 1690309588001.544], [0, 1690309588001.545], [0, 1690309588001.579], [0, 1690309589001.6892], [0, 1690309589001.692], [0, 1690309589001.6929], [0, 1690309589001.72], [0, 1690309590001.826], [0, 1690309590001.8281], [0, 1690309590001.8281], [0, 1690309590001.842], [0, 1690309591001.974], [0, 1690309591001.976], [0, 1690309591001.978], [0, 1690309591001.985], [0, 1690309592002.135], [0, 1690309592002.1372], [0, 1690309592002.139], [0, 1690309592002.14], [0, 1690309593001.276], [0, 1690309593001.279], [0, 1690309593001.28], [0, 1690309593001.281], [0, 1690309594001.417], [0, 1690309594001.417], [0, 1690309594001.426], [0, 1690309594001.426], [0, 1690309595001.571], [0, 1690309595001.571], [0, 1690309595001.574], [0, 1690309595001.575], [0, 1690309596001.703], [0, 1690309596001.713], [0, 1690309596001.713], [0, 1690309596001.7148], [0, 1690309597001.844], [0, 1690309597001.853], [0, 1690309597001.854], [0, 1690309597001.854], [0, 1690309598002.012], [0, 1690309598002.015], [0, 1690309598002.017], [0, 1690309598002.019], [0, 1690309599001.141], [0, 1690309599001.144], [0, 1690309599001.146], [0, 1690309599001.148], [0, 1690309600001.259], [0, 1690309600001.262], [0, 1690309600001.2659], [0, 1690309600001.2668], [0, 1690309601001.392], [0, 1690309601001.397], [0, 1690309601001.4028], [0, 1690309601001.4038], [0, 1690309602001.515], [0, 1690309602001.525], [0, 1690309602001.535], [0, 1690309602001.535], [0, 1690309603001.676], [0, 1690309603001.678], [0, 1690309603001.68], [0, 1690309603001.68], [0, 1690309604001.7878], [0, 1690309604001.7961], [0, 1690309604001.8062], [0, 1690309604001.8088], [0, 1690309605001.9148], [0, 1690309605001.918], [0, 1690309605001.923], [0, 1690309605001.927], [0, 1690309606002.048], [0, 1690309606002.051], [0, 1690309606002.051], [0, 1690309606002.055], [0, 1690309607001.1829], [0, 1690309607001.185], [0, 1690309607001.187], [0, 1690309607001.1892], [0, 1690309608001.3362], [0, 1690309608001.346], [0, 1690309608001.3481], [0, 1690309608001.3499], [0, 1690309609001.489], [0, 1690309609001.493], [0, 1690309609001.493], [0, 1690309609001.494], [0, 1690309610001.607], [0, 1690309610001.613], [0, 1690309610001.618], [0, 1690309610001.618], [0, 1690309611001.7332], [0, 1690309611001.739], [0, 1690309611001.74], [0, 1690309611001.74], [0, 1690309612001.876], [0, 1690309612001.876], [0, 1690309612001.877], [0, 1690309612001.877], [0, 1690309613002.033], [0, 1690309613002.035], [0, 1690309613002.037], [0, 1690309613002.039], [0, 1690309614001.153], [0, 1690309614001.154], [0, 1690309614001.158], [0, 1690309614001.159], [0, 1690309615001.2869], [0, 1690309615001.293], [0, 1690309615001.296], [0, 1690309615001.2969], [0, 1690309616001.3901], [0, 1690309616001.4312], [0, 1690309616001.44], [0, 1690309616001.44], [0, 1690309617001.538], [0, 1690309617001.574], [0, 1690309617001.578], [0, 1690309617001.579], [0, 1690309618001.6938], [0, 1690309618001.7222], [0, 1690309618001.7258], [0, 1690309618001.731], [0, 1690309619001.8271], [0, 1690309619001.842], [0, 1690309619001.8481], [0, 1690309619001.8508], [0, 1690309620001.962], [0, 1690309620001.97], [0, 1690309620001.974], [0, 1690309620001.974], [0, 1690309621002.0962], [0, 1690309621002.1062], [0, 1690309621002.1062], [0, 1690309621002.1062], [0, 1690309622001.2458], [0, 1690309622001.248], [0, 1690309622001.25], [0, 1690309622001.255], [0, 1690309623001.4038], [0, 1690309623001.406], [0, 1690309623001.412], [0, 1690309623001.416], [0, 1690309624001.555], [0, 1690309624001.556], [0, 1690309624001.558], [0, 1690309624001.56], [0, 1690309625001.696], [0, 1690309625001.698], [0, 1690309625001.699], [0, 1690309625001.702], [0, 1690309626001.824], [0, 1690309626001.826], [0, 1690309626001.8271], [0, 1690309626001.833], [0, 1690309627001.96], [0, 1690309627001.969], [0, 1690309627001.969], [0, 1690309627001.97], [0, 1690309628001.099], [0, 1690309628001.8052], [0, 1690309628001.813], [0, 1690309628001.814], [0, 1690309629001.232], [0, 1690309629001.2458], [0, 1690309629001.2532], [0, 1690309629001.906], [0, 1690309630001.367], [0, 1690309630001.37], [0, 1690309630001.374], [0, 1690309630001.483], [0, 1690309631001.483], [0, 1690309631001.494], [0, 1690309631001.506], [0, 1690309631001.507], [0, 1690309632001.618], [0, 1690309632001.619], [0, 1690309632001.624], [0, 1690309632001.629], [0, 1690309633001.7432], [0, 1690309633001.744], [0, 1690309633001.7458], [0, 1690309633001.75], [0, 1690309634001.902], [0, 1690309634001.908], [0, 1690309634001.909], [0, 1690309634001.912], [0, 1690309635002.03], [0, 1690309635002.0322], [0, 1690309635002.034], [0, 1690309635002.037], [0, 1690309636001.154], [0, 1690309636001.155], [0, 1690309636001.161], [0, 1690309636001.165], [0, 1690309637001.276], [0, 1690309637001.29], [0, 1690309637001.303], [0, 1690309637001.304], [0, 1690309638001.398], [0, 1690309638001.407], [0, 1690309638001.4248], [0, 1690309638001.426], [0, 1690309639000.5818], [0, 1690309639001.538], [0, 1690309639001.558], [0, 1690309639001.561], [0, 1690309640001.6729], [0, 1690309640001.674], [0, 1690309640001.674], [0, 1690309640001.674], [0, 1690309641001.7861], [0, 1690309641001.7861], [0, 1690309641001.79], [0, 1690309641001.793], [0, 1690309642001.944], [0, 1690309642001.9468], [0, 1690309642001.95], [0, 1690309642001.951], [0, 1690309643002.078], [0, 1690309643002.08], [0, 1690309643002.08], [0, 1690309643002.082], [0, 1690309644001.236], [0, 1690309644001.238], [0, 1690309644001.24], [0, 1690309644001.241], [0, 1690309645001.3572], [0, 1690309645001.3618], [0, 1690309645001.3618], [0, 1690309645001.363], [0, 1690309646001.494], [0, 1690309646001.497], [0, 1690309646001.4988], [0, 1690309646001.503], [0, 1690309647001.636], [0, 1690309647001.638], [0, 1690309647001.639], [0, 1690309647001.64], [0, 1690309648001.773], [0, 1690309648001.7742], [0, 1690309648001.782], [0, 1690309648001.783], [0, 1690309649000.9631], [0, 1690309649001.089], [0, 1690309649001.897], [0, 1690309649001.897], [0, 1690309650001.247], [0, 1690309650001.248], [0, 1690309650002.022], [0, 1690309650002.05], [0, 1690309651001.163], [0, 1690309651001.165], [0, 1690309651001.1719], [0, 1690309651001.3599], [0, 1690309652001.304], [0, 1690309652001.311], [0, 1690309652001.313], [0, 1690309652001.3188], [0, 1690309653001.434], [0, 1690309653001.438], [0, 1690309653001.441], [0, 1690309653001.445], [0, 1690309654001.59], [0, 1690309654001.596], [0, 1690309654001.598], [0, 1690309654001.6], [0, 1690309655001.731], [0, 1690309655001.734], [0, 1690309655001.737], [0, 1690309655001.74], [0, 1690309656001.847], [0, 1690309656001.8508], [0, 1690309656001.86], [0, 1690309656001.8618], [0, 1690309657001.98], [0, 1690309657001.985], [0, 1690309657001.985], [0, 1690309657001.988], [0, 1690309658002.1062], [0, 1690309658002.109], [0, 1690309658002.114], [0, 1690309658002.115], [0, 1690309659001.262], [0, 1690309659001.265], [0, 1690309659001.272], [0, 1690309659001.2732], [0, 1690309660001.4028], [0, 1690309660001.409], [0, 1690309660001.412], [0, 1690309660001.4138], [0, 1690309661001.2852], [0, 1690309661001.5308], [0, 1690309661001.556], [0, 1690309661001.557], [0, 1690309662001.4321], [0, 1690309662001.4348], [0, 1690309662001.6938], [0, 1690309662001.699], [0, 1690309663001.576], [0, 1690309663001.579], [0, 1690309663001.832], [0, 1690309663001.835], [0, 1690309664001.352], [0, 1690309664001.3708], [0, 1690309664001.684], [0, 1690309664001.842], [0, 1690309665001.517], [0, 1690309665001.521], [0, 1690309665001.522], [0, 1690309665001.524], [0, 1690309666001.645], [0, 1690309666001.65], [0, 1690309666001.656], [0, 1690309666001.6582], [0, 1690309667001.7769], [0, 1690309667001.7869], [0, 1690309667001.7878], [0, 1690309667001.7888], [0, 1690309668001.914], [0, 1690309668001.917], [0, 1690309668001.919], [0, 1690309668001.919], [0, 1690309669002.064], [0, 1690309669002.065], [0, 1690309669002.0679], [0, 1690309669002.07], [0, 1690309670001.228], [0, 1690309670001.228], [0, 1690309670001.229], [0, 1690309670001.233], [0, 1690309671001.3572], [0, 1690309671001.3682], [0, 1690309671001.3708], [0, 1690309671001.377], [0, 1690309672001.174], [0, 1690309672001.3062], [0, 1690309672001.482], [0, 1690309672001.49], [0, 1690309673001.331], [0, 1690309673001.433], [0, 1690309673001.4348], [0, 1690309673001.604], [0, 1690309674001.471], [0, 1690309674001.565], [0, 1690309674001.567], [0, 1690309674001.573], [0, 1690309675001.6309], [0, 1690309675001.7112], [0, 1690309675001.7122], [0, 1690309675001.717], [0, 1690309676001.78], [0, 1690309676001.849], [0, 1690309676001.85], [0, 1690309676001.857], [0, 1690309677001.918], [0, 1690309677001.977], [0, 1690309677001.977], [0, 1690309677001.983], [0, 1690309678002.055], [0, 1690309678002.0989], [0, 1690309678002.104], [0, 1690309678002.1099], [0, 1690309679001.24], [0, 1690309679001.241], [0, 1690309679001.248], [0, 1690309679001.251], [0, 1690309680001.4102], [0, 1690309680001.412], [0, 1690309680001.413], [0, 1690309680001.4148], [0, 1690309681001.553], [0, 1690309681001.553], [0, 1690309681001.556], [0, 1690309681001.556], [0, 1690309682001.71], [0, 1690309682001.713], [0, 1690309682001.7148], [0, 1690309682001.717], [0, 1690309683001.462], [0, 1690309683001.822], [0, 1690309683001.8381], [0, 1690309683001.844], [0, 1690309684001.612], [0, 1690309684001.617], [0, 1690309684001.961], [0, 1690309684001.985], [0, 1690309685001.772], [0, 1690309685001.782], [0, 1690309685001.7861], [0, 1690309685002.1172], [0, 1690309686001.257], [0, 1690309686001.2632], [0, 1690309686001.892], [0, 1690309686001.917]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-25T13:28:15-05:00 INFO: query check RC: 0
58d819f86c632bbda7c2376d8f71b1802e99a0092b6f7054ee8741e6cde58dfc
2023-07-25T13:28:46-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2692    831 --:--:-- --:--:-- --:--:--  3548
{"datasource":{"id":1,"uid":"IdtICDqVz","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-25T13:28:47-05:00 INFO: Checking grafana data
2023-07-25T13:28:47-05:00 INFO: Grafana data check, rc: 0
2023-07-25T13:28:47-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-25T13:28:52-05:00 INFO: DONE
2023-07-25 13:29:02 INFO: ----------------------------------------------
2023-07-25 13:29:02 INFO: ======== test-maestro-munge ========
2023-07-25 13:29:02 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000310929 s, 13.2 MB/s
2023-07-25T13:29:03-05:00 INFO: starting mtest-maestro
462c8bb3112511cfa7175fdc9d64acb834a758eb11e8d5445a7d208e0dcb733b
2023-07-25T13:29:05-05:00 INFO: starting mtest-samp-1
bb4c7bfd085807a63a4b36f59c69ce431913f9bbead6cbd498a4c8c4cd4dcd99
2023-07-25T13:29:07-05:00 INFO: starting mtest-samp-2
0c1103c2300babe80f13fef33fec7a8cd8f1447e0699638b91b9a47d0637c60e
2023-07-25T13:29:08-05:00 INFO: starting mtest-samp-3
cbec9b2f03fada5b36c471dce1252f7b9e89f792bd4bec3b6f576470480f2334
2023-07-25T13:29:10-05:00 INFO: starting mtest-samp-4
f90baccec00aa4278f7a4ec04b99783f68fc3f0c1e48e656c1f6d7b105b4b835
2023-07-25T13:29:12-05:00 INFO: mtest-samp-1 is running
2023-07-25T13:29:12-05:00 INFO: mtest-samp-2 is running
2023-07-25T13:29:12-05:00 INFO: mtest-samp-3 is running
2023-07-25T13:29:12-05:00 INFO: mtest-samp-4 is running
2023-07-25T13:29:12-05:00 INFO: starting mtest-agg-11
4d75107d36135be914b1696297e94d1a8322ab392fce4cb8beb2c6a3b8b7b363
2023-07-25T13:29:13-05:00 INFO: starting mtest-agg-12
7f128de95f6766147c696c36f6eea855ee4364160ea7ff4aa91f9b25f4122724
2023-07-25T13:29:15-05:00 INFO: mtest-agg-11 is running
2023-07-25T13:29:15-05:00 INFO: mtest-agg-12 is running
2023-07-25T13:29:15-05:00 INFO: starting mtest-agg-2
9d2f735d6c3d786229a25e5a7a6e115e12b35a48138c6480836050ff2e0fb977
2023-07-25T13:29:16-05:00 INFO: mtest-agg-2 is running
2023-07-25T13:29:16-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-25T13:31:17-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-25T13:31:19-05:00 INFO: sos check rc: 0
2023-07-25T13:31:20-05:00 INFO: starting mtest-ui
692488c89ea5101ee526066a1b5789fe324a31e9b0a28d075aebc6becb5792f4
2023-07-25T13:31:22-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2182728, 1690309777000.673], [2182728, 1690309777002.016], [2182728, 1690309777002.038], [2182728, 1690309777002.041], [2183100, 1690309778001.165], [2183100, 1690309778001.1829], [2183100, 1690309778001.187], [2183100, 1690309778001.822], [2183100, 1690309779001.314], [2183100, 1690309779001.3171], [2183100, 1690309779001.322], [2183100, 1690309779001.968], [2183100, 1690309780001.455], [2183100, 1690309780001.462], [2183100, 1690309780001.4631], [2183100, 1690309780002.113], [2183100, 1690309781001.251], [2183100, 1690309781001.588], [2183100, 1690309781001.59], [2183100, 1690309781001.591], [2183100, 1690309782001.42], [2183100, 1690309782001.7659], [2183100, 1690309782001.7659], [2183100, 1690309782001.769], [2183100, 1690309783001.573], [2183100, 1690309783001.923], [2183100, 1690309783001.924], [2183100, 1690309783001.927], [2183100, 1690309784001.719], [2183100, 1690309784002.071], [2183100, 1690309784002.072], [2183100, 1690309784002.072], [2183100, 1690309785001.226], [2183100, 1690309785001.228], [2183100, 1690309785001.2312], [2183100, 1690309785001.8628], [2183100, 1690309786001.356], [2183100, 1690309786001.3599], [2183100, 1690309786001.3618], [2183100, 1690309786002.003], [2183100, 1690309787001.1362], [2183100, 1690309787001.494], [2183100, 1690309787001.4988], [2183100, 1690309787001.501], [2183100, 1690309788001.292], [2183100, 1690309788001.65], [2183100, 1690309788001.6519], [2183100, 1690309788001.654], [2183100, 1690309789001.443], [2183100, 1690309789001.797], [2183100, 1690309789001.797], [2183100, 1690309789001.7979], [2183100, 1690309790001.587], [2183100, 1690309790001.928], [2183100, 1690309790001.931], [2183100, 1690309790001.935], [2183100, 1690309791001.7222], [2183100, 1690309791002.0532], [2183100, 1690309791002.056], [2183100, 1690309791002.061], [2183100, 1690309792000.883], [2183100, 1690309792001.169], [2183100, 1690309792001.1829], [2183100, 1690309792001.8372], [2183100, 1690309793001.322], [2183100, 1690309793001.3281], [2183100, 1690309793001.331], [2183100, 1690309793001.991], [2183100, 1690309794001.455], [2183100, 1690309794001.461], [2183100, 1690309794001.4631], [2183100, 1690309794002.122], [2183100, 1690309795001.251], [2183100, 1690309795001.583], [2183100, 1690309795001.583], [2183100, 1690309795001.5842], [2183100, 1690309796000.795], [2183100, 1690309796001.3818], [2183100, 1690309796001.698], [2183100, 1690309796001.717], [2183100, 1690309797001.525], [2183100, 1690309797001.839], [2183100, 1690309797001.8408], [2183100, 1690309797001.842], [2183100, 1690309798001.681], [2183100, 1690309798001.992], [2183100, 1690309798001.995], [2183100, 1690309798001.996], [2183100, 1690309799001.3582], [2183100, 1690309799001.811], [2183100, 1690309799002.103], [2183100, 1690309799002.118], [2183100, 1690309800001.237], [2183100, 1690309800001.239], [2183100, 1690309800001.241], [2183100, 1690309800001.946], [2183100, 1690309801001.3718], [2183100, 1690309801001.377], [2183100, 1690309801001.378], [2183100, 1690309801002.0852], [2183100, 1690309802001.232], [2183100, 1690309802001.5261], [2183100, 1690309802001.529], [2183100, 1690309802001.5308], [2183100, 1690309803000.9841], [2183100, 1690309803001.664], [2183100, 1690309803001.687], [2183100, 1690309803001.6892], [2183100, 1690309804001.812], [2183100, 1690309804001.812], [2183100, 1690309804001.813], [2183100, 1690309804001.815], [2183100, 1690309805001.9358], [2183100, 1690309805001.9358], [2183100, 1690309805001.939], [2183100, 1690309805001.939], [2183100, 1690309806002.0742], [2183100, 1690309806002.078], [2183100, 1690309806002.08], [2183100, 1690309806002.081], [2182844, 1690309807001.1829], [2182844, 1690309807001.219], [2182844, 1690309807001.2202], [2182844, 1690309807001.229], [2183100, 1690309808001.3408], [2183100, 1690309808001.363], [2183100, 1690309808001.365], [2183100, 1690309808001.376], [2183100, 1690309809001.003], [2183100, 1690309809001.489], [2183100, 1690309809001.492], [2183100, 1690309809001.502], [2183100, 1690309810000.649], [2183100, 1690309810001.114], [2183100, 1690309810001.13], [2183100, 1690309810001.614], [2183100, 1690309811001.25], [2183100, 1690309811001.2522], [2183100, 1690309811001.3591], [2183100, 1690309811001.749], [2183100, 1690309812000.425], [2183100, 1690309812001.399], [2183100, 1690309812001.4111], [2183100, 1690309812001.888], [2183100, 1690309813000.975], [2183100, 1690309813001.555], [2183100, 1690309813001.703], [2183100, 1690309813001.803], [2183100, 1690309814000.71], [2183100, 1690309814001.243], [2183100, 1690309814001.96], [2183100, 1690309814002.09], [2183100, 1690309815001.239], [2183100, 1690309815001.243], [2183100, 1690309815001.392], [2183100, 1690309815002.1099], [2183100, 1690309816000.407], [2183100, 1690309816000.4229], [2183100, 1690309816001.2349], [2183100, 1690309816001.516], [2183100, 1690309817001.373], [2183100, 1690309817001.377], [2183100, 1690309817001.54], [2183100, 1690309817001.629], [2183100, 1690309818001.2988], [2183100, 1690309818001.506], [2183100, 1690309818001.512], [2183100, 1690309818001.516], [2183100, 1690309819001.2942], [2183100, 1690309819001.439], [2183100, 1690309819001.504], [2183100, 1690309819001.613], [2183100, 1690309820001.438], [2183100, 1690309820001.554], [2183100, 1690309820001.568], [2183100, 1690309820001.623], [2183100, 1690309821001.59], [2183100, 1690309821001.601], [2183100, 1690309821001.682], [2183100, 1690309821001.7002], [2183100, 1690309822001.099], [2183100, 1690309822001.707], [2183100, 1690309822001.717], [2183100, 1690309822001.794], [2183100, 1690309823001.243], [2183100, 1690309823001.248], [2183100, 1690309823001.832], [2183100, 1690309823001.9148], [2183100, 1690309824001.396], [2183100, 1690309824001.4138], [2183100, 1690309824001.42], [2183100, 1690309824002.061], [2183100, 1690309825000.8162], [2183100, 1690309825001.131], [2183100, 1690309825001.187], [2183100, 1690309825001.54], [2183100, 1690309826001.268], [2183100, 1690309826001.269], [2183100, 1690309826001.3079], [2183100, 1690309826001.944], [2183100, 1690309827001.4028], [2183100, 1690309827001.408], [2183100, 1690309827001.434], [2183100, 1690309827002.082], [2183100, 1690309828001.212], [2183100, 1690309828001.536], [2183100, 1690309828001.539], [2183100, 1690309828001.559], [2183100, 1690309829001.364], [2183100, 1690309829001.6892], [2183100, 1690309829001.691], [2183100, 1690309829001.697], [2183100, 1690309830001.494], [2183100, 1690309830001.8171], [2183100, 1690309830001.8188], [2183100, 1690309830001.8198], [2183100, 1690309831001.639], [2183100, 1690309831001.933], [2183100, 1690309831001.94], [2183100, 1690309831001.944], [2183100, 1690309832001.7761], [2183100, 1690309832002.063], [2183100, 1690309832002.0679], [2183100, 1690309832002.077], [2183100, 1690309833000.9119], [2183100, 1690309833001.177], [2183100, 1690309833001.2002], [2183100, 1690309833001.9148], [2183100, 1690309834001.3381], [2183100, 1690309834001.344], [2183100, 1690309834001.3472], [2183100, 1690309834002.065], [2183100, 1690309835001.203], [2183100, 1690309835001.4731], [2183100, 1690309835001.475], [2183100, 1690309835001.477], [2183100, 1690309836001.3381], [2183100, 1690309836001.613], [2183100, 1690309836001.615], [2183100, 1690309836001.616], [2183100, 1690309837001.48], [2183100, 1690309837001.76], [2183100, 1690309837001.76], [2183100, 1690309837001.7642], [2183100, 1690309838001.623], [2183100, 1690309838001.903], [2183100, 1690309838001.904], [2183100, 1690309838001.9111], [2183100, 1690309839001.779], [2183100, 1690309839002.057], [2183100, 1690309839002.067], [2183100, 1690309839002.069], [2183100, 1690309840000.369], [2183100, 1690309840000.48], [2183100, 1690309840001.195], [2183100, 1690309840001.8938], [2183100, 1690309841001.342], [2183100, 1690309841001.344], [2183100, 1690309841001.502], [2183100, 1690309841001.522], [2183100, 1690309842001.48], [2183100, 1690309842001.486], [2183100, 1690309842001.4878], [2183100, 1690309842001.636], [2183100, 1690309843001.607], [2183100, 1690309843001.609], [2183100, 1690309843001.615], [2183100, 1690309843001.618], [2183100, 1690309844001.047], [2183100, 1690309844001.134], [2183100, 1690309844001.333], [2183100, 1690309844001.728], [2183100, 1690309845001.2139], [2183100, 1690309845001.219], [2183100, 1690309845001.276], [2183100, 1690309845001.874], [2183100, 1690309846001.3362], [2183100, 1690309846001.3398], [2183100, 1690309846001.381], [2183100, 1690309846001.615], [2183100, 1690309847001.4778], [2183100, 1690309847001.4841], [2183100, 1690309847001.506], [2183100, 1690309847001.525], [2183100, 1690309848001.088], [2183100, 1690309848001.623], [2183100, 1690309848001.623], [2183100, 1690309848001.6482], [2183100, 1690309849001.238], [2183100, 1690309849001.7769], [2183100, 1690309849001.78], [2183100, 1690309849001.7869], [2183100, 1690309850001.386], [2183100, 1690309850001.931], [2183100, 1690309850001.931], [2183100, 1690309850001.938], [2183100, 1690309851001.524], [2183100, 1690309851002.057], [2183100, 1690309851002.061], [2183100, 1690309851002.063], [2183100, 1690309852001.195], [2183100, 1690309852001.198], [2183100, 1690309852001.198], [2183100, 1690309852001.663], [2183100, 1690309853001.323], [2183100, 1690309853001.3271], [2183100, 1690309853001.3298], [2183100, 1690309853001.7878], [2183100, 1690309854001.454], [2183100, 1690309854001.459], [2183100, 1690309854001.4631], [2183100, 1690309854001.923], [2183100, 1690309855001.6052], [2183100, 1690309855001.608], [2183100, 1690309855001.614], [2183100, 1690309855002.076], [2183100, 1690309856000.9768], [2183100, 1690309856001.206], [2183100, 1690309856001.7432], [2183100, 1690309856001.7468], [2183100, 1690309857001.3508], [2183100, 1690309857001.3508], [2183100, 1690309857001.886], [2183100, 1690309857001.886], [2183100, 1690309858000.61], [2183100, 1690309858001.455], [2183100, 1690309858001.985], [2183100, 1690309858002.03], [2183100, 1690309859001.176], [2183100, 1690309859001.596], [2183100, 1690309859001.7249], [2183100, 1690309859001.749], [2183100, 1690309860001.3389], [2183100, 1690309860001.344], [2183100, 1690309860001.7458], [2183100, 1690309860001.8582], [2183100, 1690309861001.4668], [2183100, 1690309861001.482], [2183100, 1690309861001.483], [2183100, 1690309861001.885], [2183100, 1690309862001.591], [2183100, 1690309862001.604], [2183100, 1690309862001.606], [2183100, 1690309862002.0212], [2183100, 1690309863001.161], [2183100, 1690309863001.7239], [2183100, 1690309863001.728], [2183100, 1690309863001.73], [2183100, 1690309864001.289], [2183100, 1690309864001.85], [2183100, 1690309864001.855], [2183100, 1690309864001.857], [2183100, 1690309865001.438], [2183100, 1690309865002.01], [2183100, 1690309865002.011], [2183100, 1690309865002.013], [2183100, 1690309866001.153], [2183100, 1690309866001.161], [2183100, 1690309866001.1619], [2183100, 1690309866001.534], [2183100, 1690309867001.2852], [2183100, 1690309867001.293], [2183100, 1690309867001.2942], [2183100, 1690309867001.301], [2183100, 1690309868000.602], [2183100, 1690309868001.377], [2183100, 1690309868001.412], [2183100, 1690309868001.423], [2183100, 1690309869001.532], [2183100, 1690309869001.562], [2183100, 1690309869001.563], [2183100, 1690309869001.567], [2183100, 1690309870001.6829], [2183100, 1690309870001.697], [2183100, 1690309870001.716], [2183100, 1690309870001.7642], [2183100, 1690309871001.8281], [2183100, 1690309871001.8281], [2183100, 1690309871001.857], [2183100, 1690309871001.904], [2183100, 1690309872001.951], [2183100, 1690309872001.958], [2183100, 1690309872001.98], [2183100, 1690309872002.03], [2183100, 1690309873001.186], [2183100, 1690309873002.082], [2183100, 1690309873002.1592], [2183100, 1690309873002.166], [2183100, 1690309874001.2312], [2183100, 1690309874001.2952], [2183100, 1690309874001.307], [2183100, 1690309874001.321], [2183104, 1690309875001.3892], [2183104, 1690309875001.449], [2183104, 1690309875001.449], [2183104, 1690309875001.4568], [2183100, 1690309876001.525], [2183100, 1690309876001.567], [2183100, 1690309876001.575], [2183100, 1690309876001.579]]}, {"target": "component_id", "datapoints": [[3, 1690309777000.673], [1, 1690309777002.016], [4, 1690309777002.038], [2, 1690309777002.041], [1, 1690309778001.165], [2, 1690309778001.1829], [4, 1690309778001.187], [3, 1690309778001.822], [1, 1690309779001.314], [2, 1690309779001.3171], [4, 1690309779001.322], [3, 1690309779001.968], [2, 1690309780001.455], [4, 1690309780001.462], [1, 1690309780001.4631], [3, 1690309780002.113], [3, 1690309781001.251], [1, 1690309781001.588], [2, 1690309781001.59], [4, 1690309781001.591], [3, 1690309782001.42], [2, 1690309782001.7659], [4, 1690309782001.7659], [1, 1690309782001.769], [3, 1690309783001.573], [1, 1690309783001.923], [4, 1690309783001.924], [2, 1690309783001.927], [3, 1690309784001.719], [2, 1690309784002.071], [1, 1690309784002.072], [4, 1690309784002.072], [1, 1690309785001.226], [4, 1690309785001.228], [2, 1690309785001.2312], [3, 1690309785001.8628], [2, 1690309786001.356], [1, 1690309786001.3599], [4, 1690309786001.3618], [3, 1690309786002.003], [3, 1690309787001.1362], [1, 1690309787001.494], [4, 1690309787001.4988], [2, 1690309787001.501], [3, 1690309788001.292], [2, 1690309788001.65], [1, 1690309788001.6519], [4, 1690309788001.654], [3, 1690309789001.443], [1, 1690309789001.797], [2, 1690309789001.797], [4, 1690309789001.7979], [3, 1690309790001.587], [1, 1690309790001.928], [4, 1690309790001.931], [2, 1690309790001.935], [3, 1690309791001.7222], [2, 1690309791002.0532], [1, 1690309791002.056], [4, 1690309791002.061], [2, 1690309792000.883], [4, 1690309792001.169], [1, 1690309792001.1829], [3, 1690309792001.8372], [1, 1690309793001.322], [4, 1690309793001.3281], [2, 1690309793001.331], [3, 1690309793001.991], [4, 1690309794001.455], [2, 1690309794001.461], [1, 1690309794001.4631], [3, 1690309794002.122], [3, 1690309795001.251], [2, 1690309795001.583], [4, 1690309795001.583], [1, 1690309795001.5842], [2, 1690309796000.795], [3, 1690309796001.3818], [4, 1690309796001.698], [1, 1690309796001.717], [3, 1690309797001.525], [4, 1690309797001.839], [1, 1690309797001.8408], [2, 1690309797001.842], [3, 1690309798001.681], [2, 1690309798001.992], [1, 1690309798001.995], [4, 1690309798001.996], [2, 1690309799001.3582], [3, 1690309799001.811], [4, 1690309799002.103], [1, 1690309799002.118], [4, 1690309800001.237], [1, 1690309800001.239], [2, 1690309800001.241], [3, 1690309800001.946], [1, 1690309801001.3718], [2, 1690309801001.377], [4, 1690309801001.378], [3, 1690309801002.0852], [3, 1690309802001.232], [4, 1690309802001.5261], [2, 1690309802001.529], [1, 1690309802001.5308], [3, 1690309803000.9841], [1, 1690309803001.664], [4, 1690309803001.687], [2, 1690309803001.6892], [3, 1690309804001.812], [4, 1690309804001.812], [1, 1690309804001.813], [2, 1690309804001.815], [3, 1690309805001.9358], [4, 1690309805001.9358], [1, 1690309805001.939], [2, 1690309805001.939], [3, 1690309806002.0742], [4, 1690309806002.078], [1, 1690309806002.08], [2, 1690309806002.081], [2, 1690309807001.1829], [4, 1690309807001.219], [3, 1690309807001.2202], [1, 1690309807001.229], [2, 1690309808001.3408], [3, 1690309808001.363], [4, 1690309808001.365], [1, 1690309808001.376], [4, 1690309809001.003], [2, 1690309809001.489], [1, 1690309809001.492], [3, 1690309809001.502], [3, 1690309810000.649], [4, 1690309810001.114], [1, 1690309810001.13], [2, 1690309810001.614], [4, 1690309811001.25], [1, 1690309811001.2522], [3, 1690309811001.3591], [2, 1690309811001.749], [4, 1690309812000.425], [1, 1690309812001.399], [3, 1690309812001.4111], [2, 1690309812001.888], [3, 1690309813000.975], [1, 1690309813001.555], [4, 1690309813001.703], [2, 1690309813001.803], [1, 1690309814000.71], [4, 1690309814001.243], [2, 1690309814001.96], [3, 1690309814002.09], [1, 1690309815001.239], [3, 1690309815001.243], [4, 1690309815001.392], [2, 1690309815002.1099], [3, 1690309816000.407], [1, 1690309816000.4229], [2, 1690309816001.2349], [4, 1690309816001.516], [3, 1690309817001.373], [2, 1690309817001.377], [1, 1690309817001.54], [4, 1690309817001.629], [4, 1690309818001.2988], [2, 1690309818001.506], [1, 1690309818001.512], [3, 1690309818001.516], [1, 1690309819001.2942], [4, 1690309819001.439], [3, 1690309819001.504], [2, 1690309819001.613], [1, 1690309820001.438], [4, 1690309820001.554], [2, 1690309820001.568], [3, 1690309820001.623], [1, 1690309821001.59], [3, 1690309821001.601], [4, 1690309821001.682], [2, 1690309821001.7002], [2, 1690309822001.099], [3, 1690309822001.707], [1, 1690309822001.717], [4, 1690309822001.794], [3, 1690309823001.243], [2, 1690309823001.248], [1, 1690309823001.832], [4, 1690309823001.9148], [2, 1690309824001.396], [1, 1690309824001.4138], [3, 1690309824001.42], [4, 1690309824002.061], [2, 1690309825000.8162], [1, 1690309825001.131], [4, 1690309825001.187], [3, 1690309825001.54], [1, 1690309826001.268], [3, 1690309826001.269], [4, 1690309826001.3079], [2, 1690309826001.944], [3, 1690309827001.4028], [1, 1690309827001.408], [4, 1690309827001.434], [2, 1690309827002.082], [2, 1690309828001.212], [1, 1690309828001.536], [3, 1690309828001.539], [4, 1690309828001.559], [2, 1690309829001.364], [1, 1690309829001.6892], [3, 1690309829001.691], [4, 1690309829001.697], [2, 1690309830001.494], [1, 1690309830001.8171], [4, 1690309830001.8188], [3, 1690309830001.8198], [2, 1690309831001.639], [4, 1690309831001.933], [1, 1690309831001.94], [3, 1690309831001.944], [2, 1690309832001.7761], [1, 1690309832002.063], [4, 1690309832002.0679], [3, 1690309832002.077], [3, 1690309833000.9119], [1, 1690309833001.177], [4, 1690309833001.2002], [2, 1690309833001.9148], [4, 1690309834001.3381], [3, 1690309834001.344], [1, 1690309834001.3472], [2, 1690309834002.065], [2, 1690309835001.203], [1, 1690309835001.4731], [4, 1690309835001.475], [3, 1690309835001.477], [2, 1690309836001.3381], [3, 1690309836001.613], [1, 1690309836001.615], [4, 1690309836001.616], [2, 1690309837001.48], [1, 1690309837001.76], [4, 1690309837001.76], [3, 1690309837001.7642], [2, 1690309838001.623], [4, 1690309838001.903], [1, 1690309838001.904], [3, 1690309838001.9111], [2, 1690309839001.779], [4, 1690309839002.057], [1, 1690309839002.067], [3, 1690309839002.069], [4, 1690309840000.369], [1, 1690309840000.48], [3, 1690309840001.195], [2, 1690309840001.8938], [1, 1690309841001.342], [3, 1690309841001.344], [2, 1690309841001.502], [4, 1690309841001.522], [1, 1690309842001.48], [2, 1690309842001.486], [3, 1690309842001.4878], [4, 1690309842001.636], [3, 1690309843001.607], [2, 1690309843001.609], [1, 1690309843001.615], [4, 1690309843001.618], [3, 1690309844001.047], [1, 1690309844001.134], [4, 1690309844001.333], [2, 1690309844001.728], [3, 1690309845001.2139], [4, 1690309845001.219], [1, 1690309845001.276], [2, 1690309845001.874], [3, 1690309846001.3362], [4, 1690309846001.3398], [1, 1690309846001.381], [2, 1690309846001.615], [3, 1690309847001.4778], [4, 1690309847001.4841], [1, 1690309847001.506], [2, 1690309847001.525], [1, 1690309848001.088], [3, 1690309848001.623], [4, 1690309848001.623], [2, 1690309848001.6482], [1, 1690309849001.238], [4, 1690309849001.7769], [3, 1690309849001.78], [2, 1690309849001.7869], [1, 1690309850001.386], [2, 1690309850001.931], [4, 1690309850001.931], [3, 1690309850001.938], [1, 1690309851001.524], [4, 1690309851002.057], [3, 1690309851002.061], [2, 1690309851002.063], [4, 1690309852001.195], [2, 1690309852001.198], [3, 1690309852001.198], [1, 1690309852001.663], [4, 1690309853001.323], [3, 1690309853001.3271], [2, 1690309853001.3298], [1, 1690309853001.7878], [4, 1690309854001.454], [3, 1690309854001.459], [2, 1690309854001.4631], [1, 1690309854001.923], [2, 1690309855001.6052], [4, 1690309855001.608], [3, 1690309855001.614], [1, 1690309855002.076], [2, 1690309856000.9768], [1, 1690309856001.206], [4, 1690309856001.7432], [3, 1690309856001.7468], [1, 1690309857001.3508], [2, 1690309857001.3508], [3, 1690309857001.886], [4, 1690309857001.886], [1, 1690309858000.61], [2, 1690309858001.455], [4, 1690309858001.985], [3, 1690309858002.03], [3, 1690309859001.176], [2, 1690309859001.596], [4, 1690309859001.7249], [1, 1690309859001.749], [4, 1690309860001.3389], [3, 1690309860001.344], [2, 1690309860001.7458], [1, 1690309860001.8582], [3, 1690309861001.4668], [4, 1690309861001.482], [1, 1690309861001.483], [2, 1690309861001.885], [3, 1690309862001.591], [4, 1690309862001.604], [1, 1690309862001.606], [2, 1690309862002.0212], [2, 1690309863001.161], [4, 1690309863001.7239], [3, 1690309863001.728], [1, 1690309863001.73], [2, 1690309864001.289], [4, 1690309864001.85], [3, 1690309864001.855], [1, 1690309864001.857], [2, 1690309865001.438], [4, 1690309865002.01], [3, 1690309865002.011], [1, 1690309865002.013], [3, 1690309866001.153], [4, 1690309866001.161], [1, 1690309866001.1619], [2, 1690309866001.534], [3, 1690309867001.2852], [4, 1690309867001.293], [1, 1690309867001.2942], [2, 1690309867001.301], [2, 1690309868000.602], [1, 1690309868001.377], [4, 1690309868001.412], [3, 1690309868001.423], [1, 1690309869001.532], [2, 1690309869001.562], [4, 1690309869001.563], [3, 1690309869001.567], [1, 1690309870001.6829], [2, 1690309870001.697], [3, 1690309870001.716], [4, 1690309870001.7642], [1, 1690309871001.8281], [2, 1690309871001.8281], [3, 1690309871001.857], [4, 1690309871001.904], [1, 1690309872001.951], [2, 1690309872001.958], [3, 1690309872001.98], [4, 1690309872002.03], [4, 1690309873001.186], [3, 1690309873002.082], [1, 1690309873002.1592], [2, 1690309873002.166], [3, 1690309874001.2312], [1, 1690309874001.2952], [2, 1690309874001.307], [4, 1690309874001.321], [3, 1690309875001.3892], [1, 1690309875001.449], [4, 1690309875001.449], [2, 1690309875001.4568], [3, 1690309876001.525], [4, 1690309876001.567], [2, 1690309876001.575], [1, 1690309876001.579]]}, {"target": "job_id", "datapoints": [[0, 1690309777000.673], [0, 1690309777002.016], [0, 1690309777002.038], [0, 1690309777002.041], [0, 1690309778001.165], [0, 1690309778001.1829], [0, 1690309778001.187], [0, 1690309778001.822], [0, 1690309779001.314], [0, 1690309779001.3171], [0, 1690309779001.322], [0, 1690309779001.968], [0, 1690309780001.455], [0, 1690309780001.462], [0, 1690309780001.4631], [0, 1690309780002.113], [0, 1690309781001.251], [0, 1690309781001.588], [0, 1690309781001.59], [0, 1690309781001.591], [0, 1690309782001.42], [0, 1690309782001.7659], [0, 1690309782001.7659], [0, 1690309782001.769], [0, 1690309783001.573], [0, 1690309783001.923], [0, 1690309783001.924], [0, 1690309783001.927], [0, 1690309784001.719], [0, 1690309784002.071], [0, 1690309784002.072], [0, 1690309784002.072], [0, 1690309785001.226], [0, 1690309785001.228], [0, 1690309785001.2312], [0, 1690309785001.8628], [0, 1690309786001.356], [0, 1690309786001.3599], [0, 1690309786001.3618], [0, 1690309786002.003], [0, 1690309787001.1362], [0, 1690309787001.494], [0, 1690309787001.4988], [0, 1690309787001.501], [0, 1690309788001.292], [0, 1690309788001.65], [0, 1690309788001.6519], [0, 1690309788001.654], [0, 1690309789001.443], [0, 1690309789001.797], [0, 1690309789001.797], [0, 1690309789001.7979], [0, 1690309790001.587], [0, 1690309790001.928], [0, 1690309790001.931], [0, 1690309790001.935], [0, 1690309791001.7222], [0, 1690309791002.0532], [0, 1690309791002.056], [0, 1690309791002.061], [0, 1690309792000.883], [0, 1690309792001.169], [0, 1690309792001.1829], [0, 1690309792001.8372], [0, 1690309793001.322], [0, 1690309793001.3281], [0, 1690309793001.331], [0, 1690309793001.991], [0, 1690309794001.455], [0, 1690309794001.461], [0, 1690309794001.4631], [0, 1690309794002.122], [0, 1690309795001.251], [0, 1690309795001.583], [0, 1690309795001.583], [0, 1690309795001.5842], [0, 1690309796000.795], [0, 1690309796001.3818], [0, 1690309796001.698], [0, 1690309796001.717], [0, 1690309797001.525], [0, 1690309797001.839], [0, 1690309797001.8408], [0, 1690309797001.842], [0, 1690309798001.681], [0, 1690309798001.992], [0, 1690309798001.995], [0, 1690309798001.996], [0, 1690309799001.3582], [0, 1690309799001.811], [0, 1690309799002.103], [0, 1690309799002.118], [0, 1690309800001.237], [0, 1690309800001.239], [0, 1690309800001.241], [0, 1690309800001.946], [0, 1690309801001.3718], [0, 1690309801001.377], [0, 1690309801001.378], [0, 1690309801002.0852], [0, 1690309802001.232], [0, 1690309802001.5261], [0, 1690309802001.529], [0, 1690309802001.5308], [0, 1690309803000.9841], [0, 1690309803001.664], [0, 1690309803001.687], [0, 1690309803001.6892], [0, 1690309804001.812], [0, 1690309804001.812], [0, 1690309804001.813], [0, 1690309804001.815], [0, 1690309805001.9358], [0, 1690309805001.9358], [0, 1690309805001.939], [0, 1690309805001.939], [0, 1690309806002.0742], [0, 1690309806002.078], [0, 1690309806002.08], [0, 1690309806002.081], [0, 1690309807001.1829], [0, 1690309807001.219], [0, 1690309807001.2202], [0, 1690309807001.229], [0, 1690309808001.3408], [0, 1690309808001.363], [0, 1690309808001.365], [0, 1690309808001.376], [0, 1690309809001.003], [0, 1690309809001.489], [0, 1690309809001.492], [0, 1690309809001.502], [0, 1690309810000.649], [0, 1690309810001.114], [0, 1690309810001.13], [0, 1690309810001.614], [0, 1690309811001.25], [0, 1690309811001.2522], [0, 1690309811001.3591], [0, 1690309811001.749], [0, 1690309812000.425], [0, 1690309812001.399], [0, 1690309812001.4111], [0, 1690309812001.888], [0, 1690309813000.975], [0, 1690309813001.555], [0, 1690309813001.703], [0, 1690309813001.803], [0, 1690309814000.71], [0, 1690309814001.243], [0, 1690309814001.96], [0, 1690309814002.09], [0, 1690309815001.239], [0, 1690309815001.243], [0, 1690309815001.392], [0, 1690309815002.1099], [0, 1690309816000.407], [0, 1690309816000.4229], [0, 1690309816001.2349], [0, 1690309816001.516], [0, 1690309817001.373], [0, 1690309817001.377], [0, 1690309817001.54], [0, 1690309817001.629], [0, 1690309818001.2988], [0, 1690309818001.506], [0, 1690309818001.512], [0, 1690309818001.516], [0, 1690309819001.2942], [0, 1690309819001.439], [0, 1690309819001.504], [0, 1690309819001.613], [0, 1690309820001.438], [0, 1690309820001.554], [0, 1690309820001.568], [0, 1690309820001.623], [0, 1690309821001.59], [0, 1690309821001.601], [0, 1690309821001.682], [0, 1690309821001.7002], [0, 1690309822001.099], [0, 1690309822001.707], [0, 1690309822001.717], [0, 1690309822001.794], [0, 1690309823001.243], [0, 1690309823001.248], [0, 1690309823001.832], [0, 1690309823001.9148], [0, 1690309824001.396], [0, 1690309824001.4138], [0, 1690309824001.42], [0, 1690309824002.061], [0, 1690309825000.8162], [0, 1690309825001.131], [0, 1690309825001.187], [0, 1690309825001.54], [0, 1690309826001.268], [0, 1690309826001.269], [0, 1690309826001.3079], [0, 1690309826001.944], [0, 1690309827001.4028], [0, 1690309827001.408], [0, 1690309827001.434], [0, 1690309827002.082], [0, 1690309828001.212], [0, 1690309828001.536], [0, 1690309828001.539], [0, 1690309828001.559], [0, 1690309829001.364], [0, 1690309829001.6892], [0, 1690309829001.691], [0, 1690309829001.697], [0, 1690309830001.494], [0, 1690309830001.8171], [0, 1690309830001.8188], [0, 1690309830001.8198], [0, 1690309831001.639], [0, 1690309831001.933], [0, 1690309831001.94], [0, 1690309831001.944], [0, 1690309832001.7761], [0, 1690309832002.063], [0, 1690309832002.0679], [0, 1690309832002.077], [0, 1690309833000.9119], [0, 1690309833001.177], [0, 1690309833001.2002], [0, 1690309833001.9148], [0, 1690309834001.3381], [0, 1690309834001.344], [0, 1690309834001.3472], [0, 1690309834002.065], [0, 1690309835001.203], [0, 1690309835001.4731], [0, 1690309835001.475], [0, 1690309835001.477], [0, 1690309836001.3381], [0, 1690309836001.613], [0, 1690309836001.615], [0, 1690309836001.616], [0, 1690309837001.48], [0, 1690309837001.76], [0, 1690309837001.76], [0, 1690309837001.7642], [0, 1690309838001.623], [0, 1690309838001.903], [0, 1690309838001.904], [0, 1690309838001.9111], [0, 1690309839001.779], [0, 1690309839002.057], [0, 1690309839002.067], [0, 1690309839002.069], [0, 1690309840000.369], [0, 1690309840000.48], [0, 1690309840001.195], [0, 1690309840001.8938], [0, 1690309841001.342], [0, 1690309841001.344], [0, 1690309841001.502], [0, 1690309841001.522], [0, 1690309842001.48], [0, 1690309842001.486], [0, 1690309842001.4878], [0, 1690309842001.636], [0, 1690309843001.607], [0, 1690309843001.609], [0, 1690309843001.615], [0, 1690309843001.618], [0, 1690309844001.047], [0, 1690309844001.134], [0, 1690309844001.333], [0, 1690309844001.728], [0, 1690309845001.2139], [0, 1690309845001.219], [0, 1690309845001.276], [0, 1690309845001.874], [0, 1690309846001.3362], [0, 1690309846001.3398], [0, 1690309846001.381], [0, 1690309846001.615], [0, 1690309847001.4778], [0, 1690309847001.4841], [0, 1690309847001.506], [0, 1690309847001.525], [0, 1690309848001.088], [0, 1690309848001.623], [0, 1690309848001.623], [0, 1690309848001.6482], [0, 1690309849001.238], [0, 1690309849001.7769], [0, 1690309849001.78], [0, 1690309849001.7869], [0, 1690309850001.386], [0, 1690309850001.931], [0, 1690309850001.931], [0, 1690309850001.938], [0, 1690309851001.524], [0, 1690309851002.057], [0, 1690309851002.061], [0, 1690309851002.063], [0, 1690309852001.195], [0, 1690309852001.198], [0, 1690309852001.198], [0, 1690309852001.663], [0, 1690309853001.323], [0, 1690309853001.3271], [0, 1690309853001.3298], [0, 1690309853001.7878], [0, 1690309854001.454], [0, 1690309854001.459], [0, 1690309854001.4631], [0, 1690309854001.923], [0, 1690309855001.6052], [0, 1690309855001.608], [0, 1690309855001.614], [0, 1690309855002.076], [0, 1690309856000.9768], [0, 1690309856001.206], [0, 1690309856001.7432], [0, 1690309856001.7468], [0, 1690309857001.3508], [0, 1690309857001.3508], [0, 1690309857001.886], [0, 1690309857001.886], [0, 1690309858000.61], [0, 1690309858001.455], [0, 1690309858001.985], [0, 1690309858002.03], [0, 1690309859001.176], [0, 1690309859001.596], [0, 1690309859001.7249], [0, 1690309859001.749], [0, 1690309860001.3389], [0, 1690309860001.344], [0, 1690309860001.7458], [0, 1690309860001.8582], [0, 1690309861001.4668], [0, 1690309861001.482], [0, 1690309861001.483], [0, 1690309861001.885], [0, 1690309862001.591], [0, 1690309862001.604], [0, 1690309862001.606], [0, 1690309862002.0212], [0, 1690309863001.161], [0, 1690309863001.7239], [0, 1690309863001.728], [0, 1690309863001.73], [0, 1690309864001.289], [0, 1690309864001.85], [0, 1690309864001.855], [0, 1690309864001.857], [0, 1690309865001.438], [0, 1690309865002.01], [0, 1690309865002.011], [0, 1690309865002.013], [0, 1690309866001.153], [0, 1690309866001.161], [0, 1690309866001.1619], [0, 1690309866001.534], [0, 1690309867001.2852], [0, 1690309867001.293], [0, 1690309867001.2942], [0, 1690309867001.301], [0, 1690309868000.602], [0, 1690309868001.377], [0, 1690309868001.412], [0, 1690309868001.423], [0, 1690309869001.532], [0, 1690309869001.562], [0, 1690309869001.563], [0, 1690309869001.567], [0, 1690309870001.6829], [0, 1690309870001.697], [0, 1690309870001.716], [0, 1690309870001.7642], [0, 1690309871001.8281], [0, 1690309871001.8281], [0, 1690309871001.857], [0, 1690309871001.904], [0, 1690309872001.951], [0, 1690309872001.958], [0, 1690309872001.98], [0, 1690309872002.03], [0, 1690309873001.186], [0, 1690309873002.082], [0, 1690309873002.1592], [0, 1690309873002.166], [0, 1690309874001.2312], [0, 1690309874001.2952], [0, 1690309874001.307], [0, 1690309874001.321], [0, 1690309875001.3892], [0, 1690309875001.449], [0, 1690309875001.449], [0, 1690309875001.4568], [0, 1690309876001.525], [0, 1690309876001.567], [0, 1690309876001.575], [0, 1690309876001.579]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-25T13:31:24-05:00 INFO: query check RC: 0
abc838890b8cad0d5a6850aa52cf0068ad1f416b093f1a27d5c709941b16d955
2023-07-25T13:31:55-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2564    791 --:--:-- --:--:-- --:--:--  3373
{"datasource":{"id":1,"uid":"ccVtCDq4k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-25T13:31:57-05:00 INFO: Checking grafana data
2023-07-25T13:31:57-05:00 INFO: Grafana data check, rc: 0
2023-07-25T13:31:57-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-25T13:32:02-05:00 INFO: DONE
2023-07-25 13:32:12 INFO: ----------------------------------------------
2023-07-25 13:32:12 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;32mPASSED[0m
papi_store_test: [01;32mPASSED[0m
updtr_status_test: [01;32mPASSED[0m
ldms_stream_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;32mPASSED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;32mPASSED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;32mPASSED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;32mPASSED[0m
agg_slurm_test: [01;32mPASSED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;32mPASSED[0m
ldmsd_auth_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;32mPASSED[0m
cont-test-ldms: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;32mPASSED[0m
ldmsd_long_config_test: [01;32mPASSED[0m
ldmsd_stream_test2: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
libovis_log_test: [01;32mPASSED[0m
cont-test-maestro: [01;32mPASSED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
ldms_rail_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;32mPASSED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 49/49
------------------------------------------
