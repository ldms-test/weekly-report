2023-11-07 00:30:02 INFO: WORK_DIR: /mnt/300G/data/2023-11-07-003001
2023-11-07 00:30:02 INFO: LOG: /mnt/300G/data/2023-11-07-003001/cygnus-weekly.log
2023-11-07 00:30:02 INFO: OVIS_NEW_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:30:02 INFO: OVIS_OLD_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:30:02 INFO: CONT_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:30:02 INFO: -----------------------------------------------
2023-11-07 00:30:02 INFO: LDMS_TEST_REPO: https://github.com/ovis-hpc/ldms-test
2023-11-07 00:30:02 INFO: LDMS_TEST_BRANCH: master
2023-11-07 00:30:02 INFO: LDMS_TEST_NEW_GIT_SHA: 1cfe9e4c12e8e2fa43bc2c7c80f91f732da9428d
2023-11-07 00:30:02 INFO: LDMS_TEST_OLD_GIT_SHA: 
~/cron/ldms-test ~
/mnt/300G/data/2023-11-07-003001 ~/cron/ldms-test ~
2023-11-07 00:30:03 INFO: Skip building on host because GIT SHA has not changed: 
32d5252f2776353a2e7fc9be6a15930f6c1f575b
OVIS_LDMS_OVIS_GIT_LONG "32d5252f2776353a2e7fc9be6a15930f6c1f575b"
2023-11-07 00:30:03 INFO: Skip building containerized binary because GIT SHA has not changed: 
2023-11-07 00:30:03 INFO: -- Installation process succeeded --
2023-11-07 00:30:03 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-11-07-003001
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-11-07-003001
HEAD is now at 0d94fd0 2023-11-06-003002
[master ce2729d] 2023-11-07-003001
 2 files changed, 23 insertions(+), 2842 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   0d94fd0..ce2729d  master -> master
~/cron/ldms-test /mnt/300G/data/2023-11-07-003001
2023-11-07 00:30:05 INFO: ==== OVIS+SOS Installation Completed ====
2023-11-07 00:30:05 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-11-07-003001 ~/cron/ldms-test ~
2023-11-07 00:30:05 INFO: ======== direct_ldms_ls_conn_test ========
2023-11-07 00:30:05 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/direct_ldms_ls_conn_test
2023-11-07 00:30:06,147 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-11-07 00:30:06,147 TADA INFO   test-id: 0aa0597136334cc51a5cca5a71d9beb43c33f7b59280d3abb9a361a94ac81f83
2023-11-07 00:30:06,147 TADA INFO   test-suite: LDMSD
2023-11-07 00:30:06,147 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-11-07 00:30:06,147 TADA INFO   test-user: narate
2023-11-07 00:30:06,147 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:30:06,379 __main__ INFO starting munged on cygnus-01-iw
2023-11-07 00:30:06,855 __main__ INFO starting munged on localhost
2023-11-07 00:30:07,084 __main__ INFO starting ldmsd on cygnus-01-iw
2023-11-07 00:30:07,383 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-11-07 00:30:12,595 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-11-07 00:30:12,595 __main__ INFO Stopping sampler daemon ...
2023-11-07 00:30:18,008 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-11-07 00:30:18,050 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-11-07 00:30:18,092 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-11-07 00:30:18,093 TADA INFO test direct_ldms_ls_conn_test ended
2023-11-07 00:30:18,301 __main__ INFO stopping munged on cygnus-01-iw
2023-11-07 00:30:18,712 __main__ INFO stopping munged on localhost
2023-11-07 00:30:18 INFO: ----------------------------------------------
2023-11-07 00:30:18 INFO: ======== direct_prdcr_subscribe_test ========
2023-11-07 00:30:18 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/direct_prdcr_subscribe_test
2023-11-07 00:30:19,551 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-11-07 00:30:19,551 TADA INFO   test-id: d9fa20595afa93e937b0341a5477cc2874f668189d5c05028c8077a82218083c
2023-11-07 00:30:19,551 TADA INFO   test-suite: LDMSD
2023-11-07 00:30:19,551 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-11-07 00:30:19,551 TADA INFO   test-user: narate
2023-11-07 00:30:19,551 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:30:21,488 __main__ INFO starting munged on cygnus-01-iw
2023-11-07 00:30:22,056 __main__ INFO starting munged on cygnus-05-iw
2023-11-07 00:30:22,599 __main__ INFO starting munged on cygnus-03-iw
2023-11-07 00:30:23,134 __main__ INFO starting munged on cygnus-04-iw
2023-11-07 00:30:23,454 __main__ INFO starting munged on localhost
2023-11-07 00:30:23,678 __main__ INFO starting ldmsd on cygnus-01-iw
2023-11-07 00:30:24,226 __main__ INFO starting ldmsd on cygnus-05-iw
2023-11-07 00:30:24,759 __main__ INFO starting ldmsd on cygnus-03-iw
2023-11-07 00:30:25,292 __main__ INFO starting ldmsd on cygnus-04-iw
2023-11-07 00:30:32,301 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-11-07 00:30:32,301 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-11-07 00:30:32,302 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-11-07 00:30:32,302 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-11-07 00:30:32,303 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
2023-11-07 00:30:32,350 TADA INFO assertion 5, Stopping the producers succeeds: agg-1 producers stopped, passed
2023-11-07 00:30:33,352 TADA INFO assertion 6, Restarting the producers succeeds: agg-1 producers started, passed
2023-11-07 00:30:40,101 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-11-07 00:30:40,102 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-11-07 00:30:40,102 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-11-07 00:30:40,103 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-11-07 00:30:40,104 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verified, passed
2023-11-07 00:30:40,104 __main__ INFO stopping sampler-1
2023-11-07 00:30:41,524 TADA INFO assertion 12, stream-sampler-1 is not running: sampler-1 stopped, passed
2023-11-07 00:30:41,525 __main__ INFO starting sampler-1
2023-11-07 00:30:42,765 TADA INFO assertion 13, stream-sampler-1 has restarted: sampler-1 running, passed
2023-11-07 00:30:42,765 __main__ INFO allow some time for prdcr to reconnect ...
2023-11-07 00:30:48,682 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-11-07 00:30:48,683 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-11-07 00:30:48,684 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verified, passed
2023-11-07 00:30:48,685 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: unsubscribed, passed
2023-11-07 00:30:51,078 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-11-07 00:30:51,083 __main__ INFO stopping agg-1
2023-11-07 00:30:56,337 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-11-07 00:30:56,338 TADA INFO test direct_prdcr_subscribe_test ended
2023-11-07 00:30:56,549 __main__ INFO stopping munged on cygnus-01-iw
2023-11-07 00:30:56,961 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-11-07 00:30:57,423 __main__ INFO stopping munged on cygnus-05-iw
2023-11-07 00:30:57,902 __main__ INFO stopping ldmsd on cygnus-05-iw
2023-11-07 00:30:58,367 __main__ INFO stopping munged on cygnus-03-iw
2023-11-07 00:30:59,005 __main__ INFO stopping munged on cygnus-04-iw
2023-11-07 00:30:59,429 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-11-07 00:30:59,646 __main__ INFO stopping munged on localhost
2023-11-07 00:30:59 INFO: ----------------------------------------------
2023-11-07 00:30:59 INFO: ======== agg_slurm_test ========
2023-11-07 00:30:59 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/agg_slurm_test
2023-11-07 00:31:00,548 TADA INFO starting test `agg_slurm_test`
2023-11-07 00:31:00,549 TADA INFO   test-id: 8624329c1a6cbedc0953f65e0a78ebc8d6fef74981ffacc15b473567491f1ca6
2023-11-07 00:31:00,549 TADA INFO   test-suite: LDMSD
2023-11-07 00:31:00,549 TADA INFO   test-name: agg_slurm_test
2023-11-07 00:31:00,549 TADA INFO   test-user: narate
2023-11-07 00:31:00,549 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:31:00,550 __main__ INFO -- Get or create the cluster --
2023-11-07 00:31:26,367 __main__ INFO -- Preparing syspapi JSON file --
2023-11-07 00:31:26,469 __main__ INFO -- Preparing jobpapi JSON file --
2023-11-07 00:31:26,585 __main__ INFO -- Preparing job script & programs --
2023-11-07 00:31:28,116 __main__ INFO -- Start daemons --
2023-11-07 00:31:59,913 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:32:04,916 __main__ INFO -- ldms_ls to agg-2 --
2023-11-07 00:32:05,038 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-11-07 00:32:05,169 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-11-07 00:32:10,172 __main__ INFO -- Submitting jobs --
2023-11-07 00:32:10,288 __main__ INFO job_one: 1
2023-11-07 00:32:10,411 __main__ INFO job_two: 2
2023-11-07 00:32:20,422 __main__ INFO -- Cancelling jobs --
2023-11-07 00:32:20,422 __main__ INFO job_one: 1
2023-11-07 00:32:20,557 __main__ INFO job_two: 2
2023-11-07 00:33:32,475 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-11-07 00:33:32,476 TADA INFO assertion 3, meminfo data verification: No data missing, failed
Traceback (most recent call last):
  File "agg_slurm_test", line 592, in <module>
    test.assert_test(3, len(meminfo) > 5 and missing_counts == 0, "No data missing")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: LDMSD 2-level agg with slurm, No data missing: FAILED
2023-11-07 00:33:32,477 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: skipped
2023-11-07 00:33:32,477 TADA INFO test agg_slurm_test ended
2023-11-07 00:33:46 INFO: ----------------------------------------------
2023-11-07 00:33:47 INFO: ======== papi_sampler_test ========
2023-11-07 00:33:47 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/papi_sampler_test
2023-11-07 00:33:48,570 TADA INFO starting test `papi_sampler_test`
2023-11-07 00:33:48,570 TADA INFO   test-id: 370d282d2393b2500b2c296ff984343854b7f34441414a444528c20429c03106
2023-11-07 00:33:48,570 TADA INFO   test-suite: LDMSD
2023-11-07 00:33:48,570 TADA INFO   test-name: papi_sampler_test
2023-11-07 00:33:48,570 TADA INFO   test-user: narate
2023-11-07 00:33:48,570 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:33:48,571 __main__ INFO -- Get or create the cluster --
2023-11-07 00:34:01,875 __main__ INFO -- Start daemons --
2023-11-07 00:34:15,610 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-11-07 00:34:15,831 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-11-07 00:34:20,970 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-11-07 00:34:21,173 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-11-07 00:34:21,173 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-11-07 00:34:35,033 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-11-07 00:34:35,034 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-11-07 00:34:35,034 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-11-07 00:34:35,034 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-11-07 00:34:35,240 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-11-07 00:34:41,093 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-11-07 00:34:41,093 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-11-07 00:34:41,094 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-11-07 00:34:41,094 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-11-07 00:34:41,312 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-11-07 00:34:41,313 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/papi0/2.0', 'node-1/meminfo', 'node-1/papi1/3.0'}), passed
2023-11-07 00:34:52,005 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-11-07 00:35:32,361 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-11-07 00:35:34,717 TADA INFO assertion 8, Missing config file attribute is logged: : sampler.papi_sampler: papi_sampler[515]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-11-07 00:35:40,176 TADA INFO assertion 9, Bad config file is logged: : sampler.papi_sampler: configuration file syntax error., passed
2023-11-07 00:35:40,176 __main__ INFO -- Finishing Test --
2023-11-07 00:35:40,176 TADA INFO test papi_sampler_test ended
2023-11-07 00:35:40,177 __main__ INFO -- Cleaning up files --
2023-11-07 00:35:40,177 __main__ INFO -- Removing the virtual cluster --
2023-11-07 00:35:51 INFO: ----------------------------------------------
2023-11-07 00:35:52 INFO: ======== papi_store_test ========
2023-11-07 00:35:52 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/papi_store_test
2023-11-07 00:35:53,369 TADA INFO starting test `papi_store_test`
2023-11-07 00:35:53,370 TADA INFO   test-id: f2c6ea5ada1e280456f7566d192af12edcf0a33757cf6af97eacd14ad9be24c8
2023-11-07 00:35:53,370 TADA INFO   test-suite: LDMSD
2023-11-07 00:35:53,370 TADA INFO   test-name: papi_store_test
2023-11-07 00:35:53,370 TADA INFO   test-user: narate
2023-11-07 00:35:53,371 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:35:53,371 __main__ INFO -- Get or create the cluster --
2023-11-07 00:36:10,180 __main__ INFO -- Start daemons --
2023-11-07 00:36:52,839 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-11-07 00:36:52,839 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-11-07 00:36:52,840 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-11-07 00:36:52,840 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-11-07 00:36:52,840 TADA INFO test papi_store_test ended
2023-11-07 00:37:05 INFO: ----------------------------------------------
2023-11-07 00:37:06 INFO: ======== store_app_test ========
2023-11-07 00:37:06 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/store_app_test
2023-11-07 00:37:06,917 TADA INFO starting test `store_app_test`
2023-11-07 00:37:06,917 TADA INFO   test-id: fe8a42677cc162290bbe9e54bc838c223210401c7bcbdecd45891f7cdc631fb4
2023-11-07 00:37:06,917 TADA INFO   test-suite: LDMSD
2023-11-07 00:37:06,918 TADA INFO   test-name: store_app_test
2023-11-07 00:37:06,918 TADA INFO   test-user: narate
2023-11-07 00:37:06,918 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:37:06,919 __main__ INFO -- Get or create the cluster --
2023-11-07 00:37:33,067 __main__ INFO -- Preparing job script & programs --
2023-11-07 00:37:33,489 __main__ INFO -- Start daemons --
2023-11-07 00:38:05,384 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:38:10,391 __main__ INFO -- Submitting jobs --
2023-11-07 00:38:10,604 __main__ INFO job_one: 1
2023-11-07 00:38:15,844 __main__ INFO job_two: 2
2023-11-07 00:39:18,914 __main__ INFO Verifying data ...
2023-11-07 00:41:23,470 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-11-07 00:41:23,471 TADA INFO test store_app_test ended
2023-11-07 00:41:37 INFO: ----------------------------------------------
2023-11-07 00:41:38 INFO: ======== syspapi_test ========
2023-11-07 00:41:38 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/syspapi_test
2023-11-07 00:41:39,432 TADA INFO starting test `syspapi_test`
2023-11-07 00:41:39,432 TADA INFO   test-id: 9aca36d6088b5083c104dc4ba75fc15fd88c842dea31b6e048dcf63baf1c7bf0
2023-11-07 00:41:39,432 TADA INFO   test-suite: LDMSD
2023-11-07 00:41:39,432 TADA INFO   test-name: syspapi_test
2023-11-07 00:41:39,432 TADA INFO   test-user: narate
2023-11-07 00:41:39,432 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:41:39,433 __main__ INFO -- Get or create the cluster --
2023-11-07 00:42:01,699 __main__ INFO -- Write syspapi JSON config files --
2023-11-07 00:42:01,699 __main__ INFO    - db/syspapi-1.json
2023-11-07 00:42:01,700 __main__ INFO    - db/syspapi-bad.json
2023-11-07 00:42:01,700 __main__ INFO -- Start daemons --
2023-11-07 00:42:21,903 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:42:26,908 __main__ INFO -- Verifying --
2023-11-07 00:42:27,045 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-11-07 00:42:27,046 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-11-07 00:42:27,165 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-11-07 00:42:29,290 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-11-07 00:42:29,407 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-11-07 00:42:29,541 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-11-07 00:42:51,654 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-11-07 00:42:51,654 __main__ INFO  events succeeded: 77
2023-11-07 00:42:51,654 __main__ INFO  events failed: 114
2023-11-07 00:42:51,655 TADA INFO test syspapi_test ended
2023-11-07 00:43:05 INFO: ----------------------------------------------
2023-11-07 00:43:06 INFO: ======== agg_test ========
2023-11-07 00:43:06 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/agg_test
2023-11-07 00:43:07,054 TADA INFO starting test `agg_test`
2023-11-07 00:43:07,054 TADA INFO   test-id: fa5385e85d24fe65a82f7ca4e131c36f48a5e3c5a1ad9697ad1c23fac4cc9afc
2023-11-07 00:43:07,054 TADA INFO   test-suite: LDMSD
2023-11-07 00:43:07,054 TADA INFO   test-name: agg_test
2023-11-07 00:43:07,054 TADA INFO   test-user: narate
2023-11-07 00:43:07,054 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:43:07,055 __main__ INFO -- Get or create the cluster --
2023-11-07 00:43:38,734 __main__ INFO -- Start daemons --
2023-11-07 00:44:15,548 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:44:20,553 __main__ INFO -- ldms_ls to agg-2 --
2023-11-07 00:44:20,695 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-11-07 00:44:21,514 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-11-07 00:44:21,515 __main__ INFO -- Terminating ldmsd on node-1 --
2023-11-07 00:44:23,882 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-07 00:44:24,098 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-3/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-3/meminfo', 'node-4/meminfo'}), passed
2023-11-07 00:44:24,098 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-11-07 00:44:33,650 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-07 00:44:33,765 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-2/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-4/meminfo'}), passed
2023-11-07 00:44:33,765 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-11-07 00:44:36,134 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-11-07 00:44:36,264 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-11-07 00:44:36,379 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-07 00:44:36,379 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-11-07 00:44:45,945 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-2/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-4/meminfo'}), passed
2023-11-07 00:44:45,946 TADA INFO test agg_test ended
2023-11-07 00:45:01 INFO: ----------------------------------------------
2023-11-07 00:45:02 INFO: ======== failover_test ========
2023-11-07 00:45:02 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/failover_test
2023-11-07 00:45:03,542 TADA INFO starting test `failover_test`
2023-11-07 00:45:03,543 TADA INFO   test-id: 465319cf64afafe8a8ee39d13e23e0717c659156f9a732e60c63c3b4a3b6fee3
2023-11-07 00:45:03,543 TADA INFO   test-suite: LDMSD
2023-11-07 00:45:03,543 TADA INFO   test-name: failover_test
2023-11-07 00:45:03,543 TADA INFO   test-user: narate
2023-11-07 00:45:03,543 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:45:03,544 __main__ INFO -- Get or create the cluster --
2023-11-07 00:45:35,313 __main__ INFO -- Start daemons --
2023-11-07 00:46:12,164 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:46:27,179 __main__ INFO -- ldms_ls to agg-2 --
2023-11-07 00:46:27,326 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-11-07 00:46:28,146 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-11-07 00:46:28,146 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-11-07 00:46:33,504 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:46:33,640 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:46:33,754 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-11-07 00:46:33,883 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-07 00:46:33,883 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-11-07 00:46:58,493 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-4/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:46:58,617 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:46:58,617 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-11-07 00:47:03,998 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:47:04,124 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:47:04,230 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-11-07 00:47:04,341 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-11-07 00:47:04,341 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-11-07 00:47:28,896 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-07 00:47:29,020 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-11-07 00:47:29,021 TADA INFO test failover_test ended
2023-11-07 00:47:44 INFO: ----------------------------------------------
2023-11-07 00:47:45 INFO: ======== ldmsd_auth_ovis_test ========
2023-11-07 00:47:45 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_auth_ovis_test
2023-11-07 00:47:46,538 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-11-07 00:47:46,538 TADA INFO   test-id: 0201cc5c60ca2597247019a24dd106e4506bc112a0f0fc828a078c8d5d0e4872
2023-11-07 00:47:46,538 TADA INFO   test-suite: LDMSD
2023-11-07 00:47:46,538 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-11-07 00:47:46,538 TADA INFO   test-user: narate
2023-11-07 00:47:46,538 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:47:46,539 __main__ INFO -- Get or create the cluster --
2023-11-07 00:47:59,914 __main__ INFO -- Start daemons --
2023-11-07 00:48:05,828 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:48:10,963 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-11-07 00:48:11,096 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-11-07 00:48:11,219 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-11-07 00:48:11,516 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-11-07 00:48:11,516 TADA INFO test ldmsd_auth_ovis_test ended
2023-11-07 00:48:23 INFO: ----------------------------------------------
2023-11-07 00:48:24 INFO: ======== ldmsd_auth_test ========
2023-11-07 00:48:24 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_auth_test
2023-11-07 00:48:24,775 TADA INFO starting test `ldmsd_auth_test`
2023-11-07 00:48:24,775 TADA INFO   test-id: 3e57e99ffc7fd1e75afa72d6ef5e5265d4074b54f2e68a01bb9ba62c3c8f1cfa
2023-11-07 00:48:24,775 TADA INFO   test-suite: LDMSD
2023-11-07 00:48:24,775 TADA INFO   test-name: ldmsd_auth_test
2023-11-07 00:48:24,775 TADA INFO   test-user: narate
2023-11-07 00:48:24,775 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:48:24,776 __main__ INFO -- Get or create the cluster --
2023-11-07 00:48:56,686 __main__ INFO -- Start daemons --
2023-11-07 00:49:43,540 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:49:48,683 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-11-07 00:49:48,807 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-11-07 00:49:48,922 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-11-07 00:49:49,034 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-11-07 00:49:49,146 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-11-07 00:49:49,146 TADA INFO test ldmsd_auth_test ended
2023-11-07 00:50:05 INFO: ----------------------------------------------
2023-11-07 00:50:06 INFO: ======== ldmsd_ctrl_test ========
2023-11-07 00:50:06 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_ctrl_test
2023-11-07 00:50:07,118 TADA INFO starting test `ldmsd_ctrl_test`
2023-11-07 00:50:07,118 TADA INFO   test-id: 0dd145fb8ba81ce39f7cc1cebcdfa6f3792daf1ac00f7b52fe9ab6057bc31c29
2023-11-07 00:50:07,118 TADA INFO   test-suite: LDMSD
2023-11-07 00:50:07,118 TADA INFO   test-name: ldmsd_ctrl_test
2023-11-07 00:50:07,118 TADA INFO   test-user: narate
2023-11-07 00:50:07,118 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:50:07,119 __main__ INFO -- Get or create the cluster --
2023-11-07 00:50:26,261 __main__ INFO -- Start daemons --
2023-11-07 00:50:42,408 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:50:48,531 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-11-07 00:50:49,647 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-11-07 00:50:50,248 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-11-07 00:50:50,850 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-11-07 00:50:51,451 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-11-07 00:50:52,053 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-11-07 00:50:52,654 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-11-07 00:50:53,256 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-11-07 00:51:10,465 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-11-07 00:51:27,681 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-11-07 00:51:27,681 TADA INFO test ldmsd_ctrl_test ended
2023-11-07 00:51:40 INFO: ----------------------------------------------
2023-11-07 00:51:41 INFO: ======== ldmsd_stream_test2 ========
2023-11-07 00:51:41 INFO: CMD: python3 ldmsd_stream_test2 --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_stream_test2
2023-11-07 00:51:42,416 TADA INFO starting test `ldmsd_stream_test`
2023-11-07 00:51:42,416 TADA INFO   test-id: 79305239dad1f2d178f0860190338355d10295d67e1be0176628386244652428
2023-11-07 00:51:42,417 TADA INFO   test-suite: LDMSD
2023-11-07 00:51:42,417 TADA INFO   test-name: ldmsd_stream_test
2023-11-07 00:51:42,417 TADA INFO   test-user: narate
2023-11-07 00:51:42,417 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:51:42,418 __main__ INFO -- Get or create the cluster --
2023-11-07 00:52:01,565 __main__ INFO -- Start daemons --
2023-11-07 00:52:19,189 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 00:52:21,191 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-32d5252-new 
2023-11-07 00:52:24,209 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-32d5252-agg-2 
2023-11-07 00:52:34,665 TADA INFO assertion 1, Check data from old ldmsd_stream at agg-1: , passed
2023-11-07 00:52:34,665 TADA INFO assertion 2, Check data from old ldmsd_stream at agg-2: , passed
2023-11-07 00:52:34,665 TADA INFO assertion 3, Check data from old ldmsd_stream at the last subscriber: , passed
2023-11-07 00:52:34,666 TADA INFO assertion 4, Check data from the matching new ldms stream at agg-1: , passed
2023-11-07 00:52:34,666 TADA INFO assertion 5, Check data from the matching new ldms stream at agg-2: , passed
2023-11-07 00:52:34,666 TADA INFO assertion 6, Check data from the matching new ldms stream at the last subscriber: , passed
2023-11-07 00:52:34,667 TADA INFO assertion 7, Check data from the non-matching new ldms stream at agg-1: , passed
2023-11-07 00:52:34,667 TADA INFO assertion 8, Check data from the non-matching new ldms stream at agg-2: , passed
2023-11-07 00:52:34,667 TADA INFO assertion 9, Check data from the non-matching new ldms stream at last subscriber: , passed
2023-11-07 00:52:35,227 TADA INFO assertion 10, Check stream_stats before stream data transfer: , passed
2023-11-07 00:52:35,227 TADA INFO assertion 11, Check stream_client_stats before stream data transfer: , passed
2023-11-07 00:52:35,228 TADA INFO assertion 12, Check stream_stats after stream data transfer: , passed
2023-11-07 00:52:35,228 TADA INFO assertion 13, Check stream_client_stats after stream data transfer: , passed
2023-11-07 00:52:35,228 TADA INFO test ldmsd_stream_test ended
2023-11-07 00:52:48 INFO: ----------------------------------------------
2023-11-07 00:52:49 INFO: ======== maestro_cfg_test ========
2023-11-07 00:52:49 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/maestro_cfg_test
2023-11-07 00:52:49,841 TADA INFO starting test `maestro_cfg_test`
2023-11-07 00:52:49,841 TADA INFO   test-id: 97a4c4c9886586a7e4562b754ca30f566b2f98a06b875ac8c6dfcbdebae5b861
2023-11-07 00:52:49,841 TADA INFO   test-suite: LDMSD
2023-11-07 00:52:49,841 TADA INFO   test-name: maestro_cfg_test
2023-11-07 00:52:49,841 TADA INFO   test-user: narate
2023-11-07 00:52:49,841 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:52:59,853 __main__ INFO -- Get or create cluster --
2023-11-07 00:53:43,758 __main__ INFO -- Start daemons --
2023-11-07 00:54:45,870 __main__ INFO ... make sure ldmsd's are up
2023-11-07 00:54:51,359 TADA INFO assertion 1, load maestro etcd cluster: Unexpected output: , failed
---Wait for config to write to file---
Traceback (most recent call last):
  File "maestro_cfg_test", line 324, in <module>
    test.assert_test(1, False, "Unexpected output: {}".format(out))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test for ldmsd configuration with maestro_ctrl, Unexpected output: : FAILED
2023-11-07 00:54:51,360 TADA INFO assertion 2, config ldmsd cluster with maestro: skipped
2023-11-07 00:54:51,360 TADA INFO assertion 3, verify sampler daemons: skipped
2023-11-07 00:54:51,361 TADA INFO assertion 4, verify L1 aggregator daemons: skipped
2023-11-07 00:54:51,361 TADA INFO assertion 5, verify L2 aggregator daemon: skipped
2023-11-07 00:54:51,361 TADA INFO assertion 6, verify data storage: skipped
2023-11-07 00:54:51,361 TADA INFO test maestro_cfg_test ended
2023-11-07 00:55:10 INFO: ----------------------------------------------
2023-11-07 00:55:10 INFO: ======== mt-slurm-test ========
2023-11-07 00:55:10 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1699340180.958816', '1,1699340180.958816', '2,1699340180.958816', '3,1699340181.963825', '4,1699340181.963825', '5,1699340181.963825', '6,1699340181.963825', '7,1699340181.963825', '8,1699340181.963825', '9,1699340182.967661', '10,1699340183.998939', '11,1699340183.998939', '12,1699340183.998939', '13,1699340183.998939', '14,1699340183.998939', '15,1699340183.998939', '16,1699340183.998939', '17,1699340183.998939', '18,1699340185.950795', '19,1699340185.950795', '20,1699340185.950795', '21,1699340185.950795', '22,1699340185.950795', '23,1699340185.950795', '24,1699340185.950795', '25,1699340186.962406', '26,1699340186.962406', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-11-07 00:57:01 INFO: ----------------------------------------------
2023-11-07 00:57:02 INFO: ======== ovis_ev_test ========
2023-11-07 00:57:02 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ovis_ev_test
2023-11-07 00:57:02,975 __main__ INFO -- Create the cluster -- 
2023-11-07 00:57:19,381 TADA INFO starting test `ovis_ev_test`
2023-11-07 00:57:19,381 TADA INFO   test-id: 8d4bc0b5fb1703b8d9db3d6fc4a27c3a6c17a2b993ec96b9bf1ecf7b35e8bfa1
2023-11-07 00:57:19,382 TADA INFO   test-suite: test_ovis_ev
2023-11-07 00:57:19,382 TADA INFO   test-name: ovis_ev_test
2023-11-07 00:57:19,382 TADA INFO   test-user: narate
2023-11-07 00:57:19,382 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:57:19,382 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-11-07 00:57:19,383 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-11-07 00:57:19,383 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-11-07 00:57:19,383 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-11-07 00:57:19,383 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-11-07 00:57:19,383 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-11-07 00:57:19,383 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-11-07 00:57:19,384 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-11-07 00:57:19,384 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-11-07 00:57:19,384 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-11-07 00:57:19,384 TADA INFO test ovis_ev_test ended
2023-11-07 00:57:30 INFO: ----------------------------------------------
2023-11-07 00:57:31 INFO: ======== prdcr_subscribe_test ========
2023-11-07 00:57:31 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/prdcr_subscribe_test
2023-11-07 00:57:31,909 TADA INFO starting test `prdcr_subscribe_test`
2023-11-07 00:57:31,909 TADA INFO   test-id: 76f4d0da8670881495d5e4af95a617ed182ad69b774f889e0477ae5d4ba88ef4
2023-11-07 00:57:31,909 TADA INFO   test-suite: LDMSD
2023-11-07 00:57:31,909 TADA INFO   test-name: prdcr_subscribe_test
2023-11-07 00:57:31,909 TADA INFO   test-user: narate
2023-11-07 00:57:31,909 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:58:29,530 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-11-07 00:58:29,530 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-11-07 00:58:29,530 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-11-07 00:58:29,531 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-11-07 00:58:29,531 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-11-07 00:58:29,900 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-11-07 00:58:30,261 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-11-07 00:58:38,320 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-11-07 00:58:38,320 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-11-07 00:58:38,320 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-11-07 00:58:38,321 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-11-07 00:58:38,321 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-11-07 00:58:39,575 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-11-07 00:58:45,093 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-11-07 00:58:52,691 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-11-07 00:58:52,691 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-11-07 00:58:52,691 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-11-07 00:58:53,074 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-11-07 00:58:56,433 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-11-07 00:59:02,263 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-11-07 00:59:02,264 TADA INFO test prdcr_subscribe_test ended
2023-11-07 00:59:15 INFO: ----------------------------------------------
2023-11-07 00:59:16 INFO: ======== set_array_test ========
2023-11-07 00:59:16 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/set_array_test
2023-11-07 00:59:16,849 TADA INFO starting test `set_array_test`
2023-11-07 00:59:16,849 TADA INFO   test-id: 84d7c23e725893d0913a517441c1d1791c735c7c985eb50420e47b3438b39e60
2023-11-07 00:59:16,849 TADA INFO   test-suite: LDMSD
2023-11-07 00:59:16,849 TADA INFO   test-name: set_array_test
2023-11-07 00:59:16,849 TADA INFO   test-user: narate
2023-11-07 00:59:16,849 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 00:59:16,850 __main__ INFO -- Get or create the cluster --
2023-11-07 00:59:30,015 __main__ INFO -- Start daemons --
2023-11-07 00:59:35,909 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:00:05,540 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 5 snapshots, passed
2023-11-07 01:00:05,540 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-11-07 01:00:05,540 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-11-07 01:00:05,541 TADA INFO test set_array_test ended
2023-11-07 01:00:17 INFO: ----------------------------------------------
2023-11-07 01:00:18 INFO: ======== setgroup_test ========
2023-11-07 01:00:18 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/setgroup_test
2023-11-07 01:00:18,788 TADA INFO starting test `setgroup_test`
2023-11-07 01:00:18,789 TADA INFO   test-id: cd8238f4c57c2d20c7052df626e6bf312ab65ed90526e5610213b3e9633444a3
2023-11-07 01:00:18,789 TADA INFO   test-suite: LDMSD
2023-11-07 01:00:18,789 TADA INFO   test-name: setgroup_test
2023-11-07 01:00:18,789 TADA INFO   test-user: narate
2023-11-07 01:00:18,789 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:00:18,790 __main__ INFO -- Get or create the cluster --
2023-11-07 01:00:37,913 __main__ INFO -- Start daemons --
2023-11-07 01:00:54,143 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:00:59,148 __main__ INFO -- ldms_ls to agg-2 --
2023-11-07 01:00:59,269 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-11-07 01:01:01,514 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-11-07 01:01:01,514 __main__ INFO -- Removing test_2 from grp --
2023-11-07 01:01:02,023 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-11-07 01:01:06,148 TADA INFO assertion 4, test_2 is removed from grp on agg-1: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-11-07 01:01:10,267 TADA INFO assertion 5, test_2 is removed from grp on agg-2: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-11-07 01:01:14,271 __main__ INFO -- Adding test_2 back into grp --
2023-11-07 01:01:14,795 TADA INFO assertion 6, test_2 is added back to grp on sampler: expect {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, got {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, passed
2023-11-07 01:01:18,945 TADA INFO assertion 7, test_2 is added back to grp on agg-1: expect {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, got {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, passed
2023-11-07 01:01:21,067 TADA INFO assertion 8, test_2 is added back to grp on agg-2: expect {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, got {'node-1/grp', 'node-1/test_1', 'node-1/test_2'}, passed
2023-11-07 01:01:23,069 TADA INFO test setgroup_test ended
2023-11-07 01:01:35 INFO: ----------------------------------------------
2023-11-07 01:01:36 INFO: ======== slurm_stream_test ========
2023-11-07 01:01:36 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/slurm_stream_test
2023-11-07 01:01:37,562 TADA INFO starting test `slurm_stream_test`
2023-11-07 01:01:37,563 TADA INFO   test-id: 56d12cad98222840a5ffd71d4bc7521b8d2666afae2a7d7a0bb9930c77ee5d87
2023-11-07 01:01:37,563 TADA INFO   test-suite: LDMSD
2023-11-07 01:01:37,563 TADA INFO   test-name: slurm_stream_test
2023-11-07 01:01:37,563 TADA INFO   test-user: narate
2023-11-07 01:01:37,563 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:01:37,563 __main__ INFO -- Get or create the cluster --
2023-11-07 01:01:52,488 __main__ INFO -- Start daemons --
2023-11-07 01:02:03,087 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:02:33,509 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:33,509 __main__ INFO 12345
2023-11-07 01:02:33,510 __main__ INFO 12345
2023-11-07 01:02:33,510 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,510 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-11-07 01:02:33,510 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,511 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,511 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,511 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,643 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:33,643 __main__ INFO 12345
2023-11-07 01:02:33,643 __main__ INFO 12345
2023-11-07 01:02:33,643 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,643 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-11-07 01:02:33,644 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,644 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,644 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,644 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-07 01:02:33,755 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:33,755 __main__ INFO 12346
2023-11-07 01:02:33,755 __main__ INFO 12346
2023-11-07 01:02:33,755 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,755 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-11-07 01:02:33,756 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,756 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,756 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,756 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,874 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:33,874 __main__ INFO 12346
2023-11-07 01:02:33,874 __main__ INFO 12346
2023-11-07 01:02:33,874 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,874 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-11-07 01:02:33,875 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,875 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,875 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,875 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-07 01:02:33,983 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:33,983 __main__ INFO 12347
2023-11-07 01:02:33,983 __main__ INFO 12347
2023-11-07 01:02:33,983 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-11-07 01:02:33,983 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-11-07 01:02:33,983 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:33,983 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:33,984 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:33,984 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,093 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,094 __main__ INFO 12347
2023-11-07 01:02:34,094 __main__ INFO 12347
2023-11-07 01:02:34,094 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,094 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-11-07 01:02:34,094 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,094 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,094 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,095 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-07 01:02:34,215 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,216 __main__ INFO 12348
2023-11-07 01:02:34,216 __main__ INFO 12348
2023-11-07 01:02:34,216 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,216 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-11-07 01:02:34,216 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,216 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,217 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,217 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,323 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,323 __main__ INFO 12348
2023-11-07 01:02:34,323 __main__ INFO 12348
2023-11-07 01:02:34,323 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,323 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-11-07 01:02:34,324 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,324 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,324 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,324 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-07 01:02:34,456 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,456 __main__ INFO 12355
2023-11-07 01:02:34,456 __main__ INFO 12355
2023-11-07 01:02:34,457 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,457 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-11-07 01:02:34,457 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,457 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,457 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,457 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,458 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,458 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,458 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,458 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,577 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,577 __main__ INFO 12355
2023-11-07 01:02:34,577 __main__ INFO 12355
2023-11-07 01:02:34,577 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,577 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-11-07 01:02:34,577 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,578 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,579 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-07 01:02:34,699 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,699 __main__ INFO 12356
2023-11-07 01:02:34,699 __main__ INFO 12356
2023-11-07 01:02:34,699 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,699 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-11-07 01:02:34,699 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,700 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,701 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,810 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,810 __main__ INFO 12356
2023-11-07 01:02:34,810 __main__ INFO 12356
2023-11-07 01:02:34,811 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,811 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-11-07 01:02:34,811 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,811 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,811 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,811 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,812 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,812 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,812 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,812 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-07 01:02:34,922 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:34,922 __main__ INFO 12357
2023-11-07 01:02:34,922 __main__ INFO 12357
2023-11-07 01:02:34,922 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,923 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,924 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,924 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:34,924 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,026 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:35,026 __main__ INFO 12357
2023-11-07 01:02:35,026 __main__ INFO 12357
2023-11-07 01:02:35,027 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,027 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-11-07 01:02:35,027 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,027 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,027 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,027 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,028 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,028 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,028 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,028 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-07 01:02:35,137 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:35,137 __main__ INFO 12358
2023-11-07 01:02:35,138 __main__ INFO 12358
2023-11-07 01:02:35,138 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,138 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-11-07 01:02:35,138 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,138 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,138 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,138 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,139 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,139 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,139 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,139 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,264 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-07 01:02:35,265 __main__ INFO 12358
2023-11-07 01:02:35,265 __main__ INFO 12358
2023-11-07 01:02:35,265 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,265 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-11-07 01:02:35,265 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,265 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,265 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,265 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,266 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,266 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,266 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:35,266 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-07 01:02:37,435 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-11-07 01:02:37,435 __main__ INFO 12353
2023-11-07 01:02:37,436 __main__ INFO 12353
2023-11-07 01:02:37,436 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,436 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-11-07 01:02:37,436 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,436 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,436 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-07 01:02:37,437 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-11-07 01:02:37,438 TADA INFO test slurm_stream_test ended
2023-11-07 01:02:49 INFO: ----------------------------------------------
2023-11-07 01:02:49 INFO: ======== spank_notifier_test ========
2023-11-07 01:02:49 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/spank_notifier_test
2023-11-07 01:02:50,596 TADA INFO starting test `spank_notifier_test`
2023-11-07 01:02:50,596 TADA INFO   test-id: 6bde2921dbc0c35989bdc77c9c83e5669005b9709cd02c36bdc61341f8768620
2023-11-07 01:02:50,596 TADA INFO   test-suite: Slurm_Plugins
2023-11-07 01:02:50,596 TADA INFO   test-name: spank_notifier_test
2023-11-07 01:02:50,596 TADA INFO   test-user: narate
2023-11-07 01:02:50,596 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:02:50,597 __main__ INFO -- Create the cluster --
2023-11-07 01:03:31,385 __main__ INFO -- Cleanup output --
2023-11-07 01:03:31,723 __main__ INFO -- Test bad plugstack config --
2023-11-07 01:03:31,723 __main__ INFO Starting slurm ...
2023-11-07 01:03:45,839 __main__ INFO Starting slurm ... OK
2023-11-07 01:04:06,362 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-07 01:04:06,488 __main__ INFO   jobid = 1
2023-11-07 01:04:06,705 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-07 01:04:06,829 __main__ INFO   jobid = 2
2023-11-07 01:04:07,040 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-07 01:04:07,171 __main__ INFO   jobid = 3
2023-11-07 01:04:07,400 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-07 01:04:07,526 __main__ INFO   jobid = 4
2023-11-07 01:04:17,303 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-11-07 01:04:17,303 __main__ INFO Killin slurm ...
2023-11-07 01:04:20,260 __main__ INFO Killin slurm ... OK
2023-11-07 01:04:40,264 __main__ INFO -- Start daemons --
2023-11-07 01:04:50,889 __main__ INFO Starting slurm ... OK
2023-11-07 01:05:11,156 __main__ INFO -- Submitting job with no stream listener --
2023-11-07 01:05:11,392 __main__ INFO -- Submitting job with num_tasks 8 --
2023-11-07 01:05:11,534 __main__ INFO   jobid = 5
2023-11-07 01:05:27,570 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-11-07 01:05:27,570 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-11-07 01:05:33,484 __main__ INFO -- Submitting job with listener --
2023-11-07 01:05:33,703 __main__ INFO -- Submitting job with num_tasks 1 --
2023-11-07 01:05:33,820 __main__ INFO   jobid = 6
2023-11-07 01:05:34,032 __main__ INFO -- Submitting job with num_tasks 2 --
2023-11-07 01:05:34,134 __main__ INFO   jobid = 7
2023-11-07 01:05:34,343 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-07 01:05:34,472 __main__ INFO   jobid = 8
2023-11-07 01:05:34,686 __main__ INFO -- Submitting job with num_tasks 8 --
2023-11-07 01:05:34,828 __main__ INFO   jobid = 9
2023-11-07 01:05:35,048 __main__ INFO -- Submitting job with num_tasks 27 --
2023-11-07 01:05:35,152 __main__ INFO   jobid = 10
2023-11-07 01:05:56,935 __main__ INFO -- Verifying Events --
2023-11-07 01:05:56,935 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-11-07 01:05:56,935 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-07 01:05:56,936 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-07 01:05:56,937 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-07 01:05:56,937 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-11-07 01:05:56,937 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-11-07 01:05:56,937 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-07 01:05:56,937 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-07 01:05:56,938 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-07 01:05:56,938 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-11-07 01:05:56,938 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-11-07 01:05:56,938 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-07 01:05:56,938 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-07 01:05:56,939 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-07 01:05:56,939 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-11-07 01:05:56,939 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-11-07 01:05:56,939 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-07 01:05:56,940 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-07 01:05:56,940 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-07 01:05:56,940 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-11-07 01:05:56,940 __main__ INFO job 6 multi-tenant with dict_keys([7])
2023-11-07 01:05:56,940 __main__ INFO job 10 multi-tenant with dict_keys([7, 6])
2023-11-07 01:05:56,940 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-11-07 01:05:56,940 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-11-07 01:05:56,940 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-11-07 01:05:56,941 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-11-07 01:05:57,157 __main__ INFO -- Submitting job that crashes listener --
2023-11-07 01:05:57,286 __main__ INFO   jobid = 11
2023-11-07 01:06:07,542 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-11-07 01:06:07,649 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-11-07 01:06:07,649 TADA INFO test spank_notifier_test ended
2023-11-07 01:06:24 INFO: ----------------------------------------------
2023-11-07 01:06:25 INFO: ======== ldms_list_test ========
2023-11-07 01:06:25 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_list_test
2023-11-07 01:06:26,082 TADA INFO starting test `ldms_list_test`
2023-11-07 01:06:26,083 TADA INFO   test-id: 769d5d94580bfb0943e260caee964548eeb5f4e54b41a59717a7f729f5b2b67e
2023-11-07 01:06:26,083 TADA INFO   test-suite: LDMSD
2023-11-07 01:06:26,083 TADA INFO   test-name: ldms_list_test
2023-11-07 01:06:26,083 TADA INFO   test-user: narate
2023-11-07 01:06:26,083 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:06:26,083 __main__ INFO -- Get or create the cluster --
2023-11-07 01:06:36,195 __main__ INFO -- Start daemons --
2023-11-07 01:06:44,616 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:06:46,619 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-11-07 01:06:52,654 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-11-07 01:06:52,654 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-11-07 01:06:52,654 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-11-07 01:06:52,655 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-11-07 01:06:52,655 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-11-07 01:06:52,655 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-11-07 01:06:52,656 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-11-07 01:06:52,656 __main__ INFO 2nd sampling on the sampler...
2023-11-07 01:06:59,865 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-11-07 01:06:59,865 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-11-07 01:06:59,865 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-11-07 01:06:59,866 __main__ INFO 2nd update on the aggregator...
2023-11-07 01:07:07,075 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-11-07 01:07:07,075 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-11-07 01:07:07,076 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-11-07 01:07:07,076 __main__ INFO 3rd sampling on the sampler...
2023-11-07 01:07:14,285 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-11-07 01:07:14,285 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-11-07 01:07:14,285 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-11-07 01:07:14,286 __main__ INFO 3rd update on the aggregator...
2023-11-07 01:07:21,495 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-11-07 01:07:21,495 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-11-07 01:07:21,495 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-11-07 01:07:21,496 __main__ INFO 4th sampling on the sampler...
2023-11-07 01:07:28,705 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-11-07 01:07:28,705 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-11-07 01:07:28,705 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-11-07 01:07:28,705 __main__ INFO 4th update on the aggregator...
2023-11-07 01:07:35,914 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-11-07 01:07:35,915 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-11-07 01:07:35,915 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-11-07 01:07:35,915 __main__ INFO 5th sampling on the sampler...
2023-11-07 01:07:43,125 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-11-07 01:07:43,125 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-11-07 01:07:43,125 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-11-07 01:07:43,125 __main__ INFO 5th update on the aggregator...
2023-11-07 01:07:50,334 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-11-07 01:07:50,335 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-11-07 01:07:50,335 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-11-07 01:07:50,335 __main__ INFO 6th sampling on the sampler...
2023-11-07 01:07:57,544 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-11-07 01:07:57,545 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-11-07 01:07:57,545 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-11-07 01:07:57,545 __main__ INFO 6th update on the updator...
2023-11-07 01:08:04,754 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-11-07 01:08:04,754 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-11-07 01:08:04,755 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-11-07 01:08:04,755 TADA INFO test ldms_list_test ended
2023-11-07 01:08:15 INFO: ----------------------------------------------
2023-11-07 01:08:16 INFO: ======== quick_set_add_rm_test ========
2023-11-07 01:08:16 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/quick_set_add_rm_test
2023-11-07 01:08:17,259 TADA INFO starting test `quick_set_add_rm_test`
2023-11-07 01:08:17,259 TADA INFO   test-id: 4957a24f4b86cfd04c1aca0bf9e80aedf5f94aba7a0ea763821f2a26a4742d8b
2023-11-07 01:08:17,259 TADA INFO   test-suite: LDMSD
2023-11-07 01:08:17,259 TADA INFO   test-name: quick_set_add_rm_test
2023-11-07 01:08:17,260 TADA INFO   test-user: narate
2023-11-07 01:08:17,260 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:08:17,260 __main__ INFO -- Get or create the cluster --
2023-11-07 01:08:33,457 __main__ INFO -- Start samp.py --
2023-11-07 01:08:38,573 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-11-07 01:08:38,573 __main__ INFO -- Start daemons --
2023-11-07 01:08:48,297 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:08:53,951 TADA INFO assertion 2, verify data: verified, passed
2023-11-07 01:08:58,608 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-11-07 01:09:03,248 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-11-07 01:09:07,868 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-11-07 01:09:13,010 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-11-07 01:09:13,010 TADA INFO test quick_set_add_rm_test ended
2023-11-07 01:09:25 INFO: ----------------------------------------------
2023-11-07 01:09:26 INFO: ======== set_array_hang_test ========
2023-11-07 01:09:26 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/set_array_hang_test
2023-11-07 01:09:26,865 TADA INFO starting test `set_array_hang_test`
2023-11-07 01:09:26,865 TADA INFO   test-id: 09539b131b5bd0f1000efa53c45547166651f4226b61e0692e4f434118d6830c
2023-11-07 01:09:26,866 TADA INFO   test-suite: LDMSD
2023-11-07 01:09:26,866 TADA INFO   test-name: set_array_hang_test
2023-11-07 01:09:26,866 TADA INFO   test-user: narate
2023-11-07 01:09:26,866 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:09:26,866 __main__ INFO -- Get or create the cluster --
2023-11-07 01:09:37,039 __main__ INFO -- Start processes --
2023-11-07 01:09:37,039 __main__ INFO starting interactive set_array_samp.py
2023-11-07 01:09:40,053 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-11-07 01:09:43,071 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-11-07 01:09:50,280 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-11-07 01:09:57,488 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-11-07 01:10:01,093 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-11-07 01:10:08,303 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-11-07 01:10:08,303 TADA INFO test set_array_hang_test ended
2023-11-07 01:10:19 INFO: ----------------------------------------------
2023-11-07 01:10:20 INFO: ======== ldmsd_autointerval_test ========
2023-11-07 01:10:20 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_autointerval_test
2023-11-07 01:10:20,840 TADA INFO starting test `ldmsd_autointerval_test`
2023-11-07 01:10:20,841 TADA INFO   test-id: 081afbc69aa53172910dbe4d4c2ba29582f9280b17b2968eb34e7cdd00826ed6
2023-11-07 01:10:20,841 TADA INFO   test-suite: LDMSD
2023-11-07 01:10:20,841 TADA INFO   test-name: ldmsd_autointerval_test
2023-11-07 01:10:20,841 TADA INFO   test-user: narate
2023-11-07 01:10:20,841 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:10:20,842 __main__ INFO -- Get or create the cluster --
2023-11-07 01:10:37,399 __main__ INFO -- Start daemons --
2023-11-07 01:10:52,926 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:10:59,448 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-11-07 01:11:01,695 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-11-07 01:11:01,695 __main__ INFO Let them run for a while to collect data ...
2023-11-07 01:11:11,706 __main__ INFO Setting sample interval to 1000000 ...
2023-11-07 01:11:19,962 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-11-07 01:11:19,963 __main__ INFO Let them run for a while to collect data ...
2023-11-07 01:11:29,963 __main__ INFO Setting sample interval to 2000000 ...
2023-11-07 01:11:38,219 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-11-07 01:11:38,219 __main__ INFO Let them run for a while to collect data ...
2023-11-07 01:11:48,455 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-11-07 01:11:48,590 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-11-07 01:11:48,591 TADA INFO test ldmsd_autointerval_test ended
2023-11-07 01:12:01 INFO: ----------------------------------------------
2023-11-07 01:12:01 INFO: ======== ldms_record_test ========
2023-11-07 01:12:01 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_record_test
2023-11-07 01:12:02,674 TADA INFO starting test `ldms_record_test`
2023-11-07 01:12:02,674 TADA INFO   test-id: da6693925c860a58857e652e1dbc0bce50ed60a3ae734e9d342b869d392bc840
2023-11-07 01:12:02,674 TADA INFO   test-suite: LDMSD
2023-11-07 01:12:02,674 TADA INFO   test-name: ldms_record_test
2023-11-07 01:12:02,675 TADA INFO   test-user: narate
2023-11-07 01:12:02,675 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:12:02,675 __main__ INFO -- Get or create the cluster --
2023-11-07 01:12:12,725 __main__ INFO -- Start daemons --
2023-11-07 01:12:21,164 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:12:23,166 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-11-07 01:12:29,202 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-11-07 01:12:29,202 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-11-07 01:12:29,202 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-11-07 01:12:29,203 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-11-07 01:12:29,203 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-11-07 01:12:29,203 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-11-07 01:12:29,204 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-11-07 01:12:29,204 __main__ INFO 2nd sampling on the sampler...
2023-11-07 01:12:36,413 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-11-07 01:12:36,413 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-11-07 01:12:36,414 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-11-07 01:12:36,414 __main__ INFO 2nd update on the aggregator...
2023-11-07 01:12:43,623 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-11-07 01:12:43,623 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-11-07 01:12:43,624 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-11-07 01:12:43,624 __main__ INFO 3rd sampling on the sampler...
2023-11-07 01:12:50,833 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-11-07 01:12:50,833 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-11-07 01:12:50,834 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-11-07 01:12:50,834 __main__ INFO 3rd update on the aggregator...
2023-11-07 01:12:58,042 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-11-07 01:12:58,042 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-11-07 01:12:58,043 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-11-07 01:12:58,043 __main__ INFO 4th sampling on the sampler...
2023-11-07 01:13:05,251 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-11-07 01:13:05,252 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-11-07 01:13:05,252 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-11-07 01:13:05,252 __main__ INFO 4th update on the aggregator...
2023-11-07 01:13:12,461 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-11-07 01:13:12,462 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-11-07 01:13:12,462 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-11-07 01:13:12,462 __main__ INFO 5th sampling on the sampler...
2023-11-07 01:13:19,671 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-11-07 01:13:19,672 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-11-07 01:13:19,672 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-11-07 01:13:19,672 __main__ INFO 5th update on the aggregator...
2023-11-07 01:13:26,881 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-11-07 01:13:26,882 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-11-07 01:13:26,882 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-11-07 01:13:26,882 __main__ INFO 6th sampling on the sampler...
2023-11-07 01:13:34,090 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-11-07 01:13:34,091 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-11-07 01:13:34,091 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-11-07 01:13:34,091 __main__ INFO 6th update on the updator...
2023-11-07 01:13:41,300 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-11-07 01:13:41,301 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-11-07 01:13:41,301 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-11-07 01:13:41,302 TADA INFO test ldms_record_test ended
2023-11-07 01:13:52 INFO: ----------------------------------------------
2023-11-07 01:13:53 INFO: ======== ldms_schema_digest_test ========
2023-11-07 01:13:53 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_schema_digest_test
2023-11-07 01:13:53,763 TADA INFO starting test `ldms_schema_digest_test`
2023-11-07 01:13:53,763 TADA INFO   test-id: becfec715253343eaadb8bae9513ac877a914a41c2c2fe0eb0e4fd0176698f6b
2023-11-07 01:13:53,763 TADA INFO   test-suite: LDMSD
2023-11-07 01:13:53,763 TADA INFO   test-name: ldms_schema_digest_test
2023-11-07 01:13:53,763 TADA INFO   test-user: narate
2023-11-07 01:13:53,763 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:13:53,764 __main__ INFO -- Get or create the cluster --
2023-11-07 01:14:09,972 __main__ INFO -- Start daemons --
2023-11-07 01:14:21,048 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:14:26,176 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-11-07 01:14:26,298 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-11-07 01:14:26,426 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-11-07 01:14:26,615 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-11-07 01:14:26,615 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-11-07 01:14:26,615 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-11-07 01:14:29,066 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-11-07 01:14:29,066 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-11-07 01:14:29,066 TADA INFO test ldms_schema_digest_test ended
2023-11-07 01:14:41 INFO: ----------------------------------------------
2023-11-07 01:14:42 INFO: ======== ldmsd_decomp_test ========
2023-11-07 01:14:42 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_decomp_test
2023-11-07 01:14:43,162 TADA INFO starting test `ldmsd_decomp_test`
2023-11-07 01:14:43,162 TADA INFO   test-id: 9ef359d458316e1f36519736caa80fb943246edca48c67ad455beddf9e52844e
2023-11-07 01:14:43,162 TADA INFO   test-suite: LDMSD
2023-11-07 01:14:43,162 TADA INFO   test-name: ldmsd_decomp_test
2023-11-07 01:14:43,162 TADA INFO   test-user: narate
2023-11-07 01:14:43,162 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:14:43,163 __main__ INFO -- Get or create the cluster --
2023-11-07 01:15:11,898 __main__ INFO -- Start daemons --
2023-11-07 01:15:41,724 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:16:36,557 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-11-07 01:16:36,557 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-11-07 01:16:36,557 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-11-07 01:16:36,557 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-11-07 01:16:36,558 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-11-07 01:16:36,559 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-11-07 01:16:36,560 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-11-07 01:16:36,560 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-11-07 01:16:36,562 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-11-07 01:16:36,564 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-11-07 01:16:36,646 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-11-07 01:16:36,651 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-11-07 01:16:36,654 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-11-07 01:16:36,664 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-11-07 01:16:36,666 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-11-07 01:16:36,667 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-11-07 01:16:36,745 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-11-07 01:16:36,750 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-11-07 01:16:36,753 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-11-07 01:16:36,764 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-11-07 01:16:36,764 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-11-07 01:16:36,765 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-11-07 01:16:36,795 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-11-07 01:16:36,796 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-11-07 01:16:36,798 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-11-07 01:16:36,803 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-11-07 01:16:36,803 TADA INFO test ldmsd_decomp_test ended
2023-11-07 01:16:36,804 TADA INFO test ldmsd_decomp_test ended
2023-11-07 01:16:52 INFO: ----------------------------------------------
2023-11-07 01:16:53 INFO: ======== ldmsd_stream_status_test ========
2023-11-07 01:16:53 INFO: CMD: python3 ldmsd_stream_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_stream_status_test
2023-11-07 01:16:53,823 __main__ INFO -- Get or create the cluster --
2023-11-07 01:16:53,824 TADA INFO starting test `ldmsd_stream_status`
2023-11-07 01:16:53,824 TADA INFO   test-id: 9b8c340573ab39d03cdf1a0ed8d1afe44c082cbff2b80d7042c259d095eaa978
2023-11-07 01:16:53,824 TADA INFO   test-suite: LDMSD
2023-11-07 01:16:53,824 TADA INFO   test-name: ldmsd_stream_status
2023-11-07 01:16:53,824 TADA INFO   test-user: narate
2023-11-07 01:16:53,824 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:17:10,708 __main__ INFO -- Start daemons --
2023-11-07 01:17:26,260 __main__ INFO waiting ... for all LDMSDs to start
2023-11-07 01:17:26,602 __main__ INFO All LDMSDs are up.
2023-11-07 01:17:27,819 TADA INFO assertion 1, No Stream data: [] == [], passed
2023-11-07 01:17:29,158 TADA INFO assertion 2, stream_status -- one stream message: [{'name': 'foo', 'rx': {'bytes': 6, 'count': 1, 'first_ts': 1699341447.92978, 'last_ts': 1699341447.92978}, 'sources': {'0.0.0.0:0': {'bytes': 6, 'count': 1, 'first_ts': 1699341447.92978, 'last_ts': 1699341447.92978}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 1, 'bytes': 6, 'first_ts': 1699341447.92978, 'last_ts': 1699341447.92978}, 'sources': {'0.0.0.0:0': {'count': 1, 'bytes': 6, 'first_ts': 1699341447.92978, 'last_ts': 1699341447.92978}}, 'clients': []}], passed
2023-11-07 01:17:31,654 TADA INFO assertion 3, stream_status --  multiple stream messages: [{'name': 'foo', 'rx': {'bytes': 18, 'count': 3, 'first_ts': 1699341447.92978, 'last_ts': 1699341450.4099758}, 'sources': {'0.0.0.0:0': {'bytes': 18, 'count': 3, 'first_ts': 1699341447.92978, 'last_ts': 1699341450.4099758}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 3, 'bytes': 18, 'first_ts': 1699341447.92978, 'last_ts': 1699341450.4099758}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 18, 'first_ts': 1699341447.92978, 'last_ts': 1699341450.4099758}}, 'clients': []}], passed
2023-11-07 01:17:35,462 TADA INFO assertion 4, stream_status -- mulitple streams: [{'name': 'bar', 'rx': {'bytes': 48, 'count': 3, 'first_ts': 1699341452.9904604, 'last_ts': 1699341454.2226846}, 'sources': {'0.0.0.0:0': {'bytes': 48, 'count': 3, 'first_ts': 1699341452.9904604, 'last_ts': 1699341454.2226846}}, 'clients': []}, {'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1699341451.7623754, 'last_ts': 1699341452.8884094}, 'sources': {'0.0.0.0:0': {'bytes': 12, 'count': 2, 'first_ts': 1699341451.7623754, 'last_ts': 1699341452.8884094}}, 'clients': []}] == [{'name': 'bar', 'rx': {'count': 3, 'bytes': 48, 'first_ts': 1699341452.9904604, 'last_ts': 1699341454.2226846}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 48, 'first_ts': 1699341452.9904604, 'last_ts': 1699341454.2226846}}, 'clients': []}, {'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1699341451.7623754, 'last_ts': 1699341452.8884094}, 'sources': {'0.0.0.0:0': {'count': 2, 'bytes': 12, 'first_ts': 1699341451.7623754, 'last_ts': 1699341452.8884094}}, 'clients': []}], passed
2023-11-07 01:17:39,150 TADA INFO assertion 5, stream_status to agg after one producer republished stream: [{'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}, 'sources': {'10.2.193.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}, 'sources': {'10.2.193.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}}, 'clients': []}], passed
2023-11-07 01:17:40,737 TADA INFO assertion 6, stream_status to agg after two producers republished stream: [{'name': 'foo', 'rx': {'bytes': 30, 'count': 5, 'first_ts': 1699341456.8012707, 'last_ts': 1699341459.5063212}, 'sources': {'10.2.193.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}, '10.2.193.4:10001': {'bytes': 18, 'count': 3, 'first_ts': 1699341459.261705, 'last_ts': 1699341459.5063212}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 5, 'bytes': 30, 'first_ts': 1699341456.8012707, 'last_ts': 1699341459.5063212}, 'sources': {'10.2.193.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1699341456.8012707, 'last_ts': 1699341457.9044273}, '10.2.193.4:10001': {'count': 3, 'bytes': 18, 'first_ts': 1699341459.261705, 'last_ts': 1699341459.5063212}}, 'clients': []}], passed
2023-11-07 01:17:40,738 TADA INFO test ldmsd_stream_status ended
2023-11-07 01:17:53 INFO: ----------------------------------------------
2023-11-07 01:17:53 INFO: ======== store_list_record_test ========
2023-11-07 01:17:53 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/store_list_record_test
2023-11-07 01:17:54,673 __main__ INFO -- Get or create the cluster --
2023-11-07 01:17:54,674 TADA INFO starting test `store_sos_lists_test`
2023-11-07 01:17:54,674 TADA INFO   test-id: 33bd0be6fee6d89396248ce2e9a7d6e10831fd2ec04632df2e3501affdb3798e
2023-11-07 01:17:54,674 TADA INFO   test-suite: LDMSD
2023-11-07 01:17:54,674 TADA INFO   test-name: store_sos_lists_test
2023-11-07 01:17:54,674 TADA INFO   test-user: narate
2023-11-07 01:17:54,674 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:18:11,111 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:18:26,756 __main__ INFO All sampler daemons are up.
2023-11-07 01:18:26,868 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-11-07 01:18:26,973 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-11-07 01:18:28,351 TADA INFO assertion 3, store_sos is storing data.: file_exists(a) for a in supported_schema, passed
2023-11-07 01:18:29,072 TADA INFO assertion 4, store_sos stores data correctly.: verify_data(db) for db in all_db, passed
2023-11-07 01:18:38,175 TADA INFO assertion 5, store_sos stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-11-07 01:18:38,868 TADA INFO assertion 6, store_sos reports multiple list errror messages resulted by the config file.: store_sos reported the multiple list error messages., passed
2023-11-07 01:18:43,060 TADA INFO assertion 7, store_sos reports multiple list errror messages resulted by ldmsd_controller.: 'store_sos: 'sampler/record_u64_array_u64_array' contains multiple lists' in the ldmsd log, failed
Traceback (most recent call last):
  File "store_list_record_test", line 533, in <module>
    test.assert_test(SOS_MULTI_LISTS_ERROR_LDMSD_CONTROLLER, res, cond)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test store_sos storing lists, 'store_sos: 'sampler/record_u64_array_u64_array' contains multiple lists' in the ldmsd log: FAILED
2023-11-07 01:18:43,061 TADA INFO assertion 8, store_csv is storing data.: skipped
2023-11-07 01:18:43,061 TADA INFO assertion 9, store_csv stores data correctly.: skipped
2023-11-07 01:18:43,061 TADA INFO assertion 10, store_csv stores data after restarted correctly.: skipped
2023-11-07 01:18:43,061 TADA INFO assertion 11, store_csv reports multiple list errror messages resulted by the config file.: skipped
2023-11-07 01:18:43,061 TADA INFO assertion 12, store_csv reports multiple list errror messages resulted by ldmsd_controller.: skipped
2023-11-07 01:18:43,061 TADA INFO test store_sos_lists_test ended
2023-11-07 01:18:55 INFO: ----------------------------------------------
2023-11-07 01:18:56 INFO: ======== maestro_raft_test ========
2023-11-07 01:18:56 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/maestro_raft_test
2023-11-07 01:18:57,076 TADA INFO starting test `maestro_raft_test`
2023-11-07 01:18:57,076 TADA INFO   test-id: 5920c7218c5ceebe4da03cb4b746cb27846f1153de2e2bcbb70e73523bcf1e62
2023-11-07 01:18:57,076 TADA INFO   test-suite: LDMSD
2023-11-07 01:18:57,076 TADA INFO   test-name: maestro_raft_test
2023-11-07 01:18:57,076 TADA INFO   test-user: narate
2023-11-07 01:18:57,077 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:19:07,088 __main__ INFO -- Get or create cluster --
2023-11-07 01:20:03,236 __main__ INFO -- Start daemons --
2023-11-07 01:21:38,967 __main__ INFO -- making known hosts (ssh) --
2023-11-07 01:21:45,980 __main__ INFO ... make sure ldmsd's are up
---Wait for config to write to file---
Traceback (most recent call last):
  File "maestro_raft_test", line 394, in <module>
    raise RuntimeError(f"maestro_ctl error rc: {rc}, out: {out}")
RuntimeError: maestro_ctl error rc: 1, out: 
2023-11-07 01:21:51,468 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: skipped
2023-11-07 01:21:51,468 TADA INFO assertion 2, All ldmsds are up and configured: skipped
2023-11-07 01:21:51,468 TADA INFO assertion 3, Data are being stored: skipped
2023-11-07 01:21:51,469 TADA INFO assertion 4, New leader elected: skipped
2023-11-07 01:21:51,469 TADA INFO assertion 5, Restarted ldmsd is configured: skipped
2023-11-07 01:21:51,469 TADA INFO assertion 6, New data are presented in the store: skipped
2023-11-07 01:21:51,469 TADA INFO assertion 7, The restarted maestro becomes a follower: skipped
2023-11-07 01:21:51,469 TADA INFO test maestro_raft_test ended
2023-11-07 01:22:12 INFO: ----------------------------------------------
2023-11-07 01:22:13 INFO: ======== ovis_json_test ========
2023-11-07 01:22:13 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ovis_json_test
2023-11-07 01:22:14,488 __main__ INFO -- Create the cluster -- 
2023-11-07 01:22:26,727 TADA INFO starting test `ovis_json_test`
2023-11-07 01:22:26,727 TADA INFO   test-id: bf50e3ed6a177c2bb6a80b90b8b76772399194d029c093f5261e304d21c0f8a3
2023-11-07 01:22:26,727 TADA INFO   test-suite: OVIS-LIB
2023-11-07 01:22:26,728 TADA INFO   test-name: ovis_json_test
2023-11-07 01:22:26,728 TADA INFO   test-user: narate
2023-11-07 01:22:26,728 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:22:26,728 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-11-07 01:22:26,729 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-11-07 01:22:26,729 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-11-07 01:22:26,729 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-11-07 01:22:26,729 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-11-07 01:22:26,729 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-11-07 01:22:26,729 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-11-07 01:22:26,730 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-11-07 01:22:26,730 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,730 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,730 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,730 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,730 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,731 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,731 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,731 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-07 01:22:26,731 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-11-07 01:22:26,731 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-11-07 01:22:26,731 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-11-07 01:22:26,732 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-11-07 01:22:26,732 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-11-07 01:22:26,732 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-11-07 01:22:26,732 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-11-07 01:22:26,732 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-11-07 01:22:26,732 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-11-07 01:22:26,733 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-11-07 01:22:26,733 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-11-07 01:22:26,733 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,733 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,733 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,733 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-07 01:22:26,734 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-11-07 01:22:26,735 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-11-07 01:22:26,735 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-11-07 01:22:26,735 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-11-07 01:22:26,735 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-11-07 01:22:26,735 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-11-07 01:22:26,735 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-11-07 01:22:26,735 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-11-07 01:22:26,736 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-11-07 01:22:26,736 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-11-07 01:22:26,736 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-11-07 01:22:26,736 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-11-07 01:22:26,736 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-11-07 01:22:26,736 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-11-07 01:22:26,737 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-11-07 01:22:26,737 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-11-07 01:22:26,737 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-11-07 01:22:26,737 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-11-07 01:22:26,737 TADA INFO test ovis_json_test ended
2023-11-07 01:22:37 INFO: ----------------------------------------------
2023-11-07 01:22:38 INFO: ======== updtr_add_test ========
2023-11-07 01:22:38 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_add_test
2023-11-07 01:22:39,380 __main__ INFO -- Get or create the cluster --
2023-11-07 01:22:39,380 TADA INFO starting test `updtr_add test`
2023-11-07 01:22:39,381 TADA INFO   test-id: 904f779b4e1b8b245085583793d7353d0102b5b0732ba945025e5b4566feb9c5
2023-11-07 01:22:39,381 TADA INFO   test-suite: LDMSD
2023-11-07 01:22:39,381 TADA INFO   test-name: updtr_add test
2023-11-07 01:22:39,381 TADA INFO   test-user: narate
2023-11-07 01:22:39,381 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:22:55,493 __main__ INFO -- Start daemons --
2023-11-07 01:23:10,998 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:23:11,331 __main__ INFO All LDMSDs are up.
2023-11-07 01:23:12,548 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:23:13,764 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:23:15,002 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:23:16,221 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:23:17,422 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:23:19,878 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:23:22,334 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:23:23,545 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-11-07 01:23:23,545 __main__ INFO --- done ---
2023-11-07 01:23:23,546 TADA INFO test updtr_add test ended
2023-11-07 01:23:35 INFO: ----------------------------------------------
2023-11-07 01:23:36 INFO: ======== updtr_del_test ========
2023-11-07 01:23:36 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_del_test
2023-11-07 01:23:37,529 __main__ INFO -- Get or create the cluster --
2023-11-07 01:23:37,529 TADA INFO starting test `updtr_add test`
2023-11-07 01:23:37,529 TADA INFO   test-id: ddcd9098eaf768cebc9338fec2ea402a5c64fe7f3a3118e515a592e510712b60
2023-11-07 01:23:37,530 TADA INFO   test-suite: LDMSD
2023-11-07 01:23:37,530 TADA INFO   test-name: updtr_add test
2023-11-07 01:23:37,530 TADA INFO   test-user: narate
2023-11-07 01:23:37,530 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:23:53,662 __main__ INFO -- Start daemons --
2023-11-07 01:24:09,110 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:24:09,419 __main__ INFO All LDMSDs are up.
2023-11-07 01:24:10,630 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:24:11,860 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-07 01:24:13,077 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:24:14,295 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:24:14,296 __main__ INFO --- done ---
2023-11-07 01:24:14,296 TADA INFO test updtr_add test ended
2023-11-07 01:24:26 INFO: ----------------------------------------------
2023-11-07 01:24:27 INFO: ======== updtr_match_add_test ========
2023-11-07 01:24:27 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_match_add_test
2023-11-07 01:24:28,234 __main__ INFO -- Get or create the cluster --
2023-11-07 01:24:28,234 TADA INFO starting test `updtr_add test`
2023-11-07 01:24:28,234 TADA INFO   test-id: f2eb0b21c9a5a75bcffbf7402d65d2bc9bbb5de5975b9f5dfb1cfb6544886558
2023-11-07 01:24:28,234 TADA INFO   test-suite: LDMSD
2023-11-07 01:24:28,234 TADA INFO   test-name: updtr_add test
2023-11-07 01:24:28,235 TADA INFO   test-user: narate
2023-11-07 01:24:28,235 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:24:44,383 __main__ INFO -- Start daemons --
2023-11-07 01:24:59,805 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:25:00,108 __main__ INFO All LDMSDs are up.
2023-11-07 01:25:01,341 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:25:02,559 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:25:03,781 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:25:05,011 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:25:06,240 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-07 01:25:06,240 __main__ INFO --- done ---
2023-11-07 01:25:06,240 TADA INFO test updtr_add test ended
2023-11-07 01:25:18 INFO: ----------------------------------------------
2023-11-07 01:25:19 INFO: ======== updtr_match_del_test ========
2023-11-07 01:25:19 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_match_del_test
2023-11-07 01:25:20,048 __main__ INFO -- Get or create the cluster --
2023-11-07 01:25:20,048 TADA INFO starting test `updtr_add test`
2023-11-07 01:25:20,048 TADA INFO   test-id: d7b98058f81e4595aa7367404838a6250d38eb2cfce53a32da58b09b85870cb1
2023-11-07 01:25:20,048 TADA INFO   test-suite: LDMSD
2023-11-07 01:25:20,048 TADA INFO   test-name: updtr_add test
2023-11-07 01:25:20,048 TADA INFO   test-user: narate
2023-11-07 01:25:20,048 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:25:36,140 __main__ INFO -- Start daemons --
2023-11-07 01:25:51,710 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:25:52,055 __main__ INFO All LDMSDs are up.
2023-11-07 01:25:53,276 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-11-07 01:25:54,516 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:25:55,739 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:25:56,962 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:25:58,183 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:25:59,416 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:26:00,632 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:26:00,632 __main__ INFO --- done ---
2023-11-07 01:26:00,633 TADA INFO test updtr_add test ended
2023-11-07 01:26:12 INFO: ----------------------------------------------
2023-11-07 01:26:13 INFO: ======== updtr_prdcr_add_test ========
2023-11-07 01:26:13 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_prdcr_add_test
2023-11-07 01:26:14,431 __main__ INFO -- Get or create the cluster --
2023-11-07 01:26:14,432 TADA INFO starting test `updtr_add test`
2023-11-07 01:26:14,432 TADA INFO   test-id: 67e686ccea151eb10da466b18113b4220fb546b13a2671759e0c9604d12fe05f
2023-11-07 01:26:14,432 TADA INFO   test-suite: LDMSD
2023-11-07 01:26:14,432 TADA INFO   test-name: updtr_add test
2023-11-07 01:26:14,432 TADA INFO   test-user: narate
2023-11-07 01:26:14,432 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:26:30,601 __main__ INFO -- Start daemons --
2023-11-07 01:26:46,077 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:26:46,398 __main__ INFO All LDMSDs are up.
2023-11-07 01:26:47,617 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:26:50,066 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:26:52,513 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:26:53,733 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-11-07 01:26:54,957 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:26:54,958 __main__ INFO --- done ---
2023-11-07 01:26:54,958 TADA INFO test updtr_add test ended
2023-11-07 01:27:07 INFO: ----------------------------------------------
2023-11-07 01:27:08 INFO: ======== updtr_prdcr_del_test ========
2023-11-07 01:27:08 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_prdcr_del_test
2023-11-07 01:27:08,815 __main__ INFO -- Get or create the cluster --
2023-11-07 01:27:08,815 TADA INFO starting test `updtr_add test`
2023-11-07 01:27:08,815 TADA INFO   test-id: d900748b41b17568ab2d887a8446e30a9a3d14ae044b5ef6dc246c7e88d0f696
2023-11-07 01:27:08,815 TADA INFO   test-suite: LDMSD
2023-11-07 01:27:08,815 TADA INFO   test-name: updtr_add test
2023-11-07 01:27:08,815 TADA INFO   test-user: narate
2023-11-07 01:27:08,815 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:27:25,198 __main__ INFO -- Start daemons --
2023-11-07 01:27:40,694 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:27:41,011 __main__ INFO All LDMSDs are up.
2023-11-07 01:27:42,225 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:27:43,446 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-07 01:27:44,680 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:27:47,133 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:27:47,133 __main__ INFO --- done ---
2023-11-07 01:27:47,133 TADA INFO test updtr_add test ended
2023-11-07 01:27:59 INFO: ----------------------------------------------
2023-11-07 01:28:00 INFO: ======== updtr_start_test ========
2023-11-07 01:28:00 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_start_test
2023-11-07 01:28:01,266 __main__ INFO -- Get or create the cluster --
2023-11-07 01:28:01,266 TADA INFO starting test `updtr_add test`
2023-11-07 01:28:01,266 TADA INFO   test-id: 275245949603c89abc62d269ddafa5abec7f842ae7596c143a7831a142a19ef3
2023-11-07 01:28:01,266 TADA INFO   test-suite: LDMSD
2023-11-07 01:28:01,266 TADA INFO   test-name: updtr_add test
2023-11-07 01:28:01,266 TADA INFO   test-user: narate
2023-11-07 01:28:01,266 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:28:17,293 __main__ INFO -- Start daemons --
2023-11-07 01:28:32,754 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:28:33,064 __main__ INFO All LDMSDs are up.
2023-11-07 01:28:34,283 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:28:35,501 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:28:36,734 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:28:37,965 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:28:39,182 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-11-07 01:28:41,609 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:28:42,829 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:28:45,270 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:28:47,718 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:28:50,158 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:28:51,381 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-07 01:28:51,381 __main__ INFO --- done ---
2023-11-07 01:28:51,381 TADA INFO test updtr_add test ended
2023-11-07 01:29:03 INFO: ----------------------------------------------
2023-11-07 01:29:04 INFO: ======== updtr_status_test ========
2023-11-07 01:29:04 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_status_test
2023-11-07 01:29:05,271 __main__ INFO -- Get or create the cluster --
2023-11-07 01:29:05,271 TADA INFO starting test `updtr_status test`
2023-11-07 01:29:05,271 TADA INFO   test-id: 11157a7d3e6462dca1674b86213378e512c8626d3f1c565a800c8c84783dbefe
2023-11-07 01:29:05,271 TADA INFO   test-suite: LDMSD
2023-11-07 01:29:05,271 TADA INFO   test-name: updtr_status test
2023-11-07 01:29:05,271 TADA INFO   test-user: narate
2023-11-07 01:29:05,271 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:29:24,394 __main__ INFO -- Start daemons --
2023-11-07 01:29:45,118 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:29:45,579 __main__ INFO All LDMSDs are up.
2023-11-07 01:29:46,800 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-11-07 01:29:48,019 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-11-07 01:29:49,269 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:29:50,489 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:29:51,724 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-07 01:29:51,725 __main__ INFO --- done ---
2023-11-07 01:29:51,725 TADA INFO test updtr_status test ended
2023-11-07 01:30:04 INFO: ----------------------------------------------
2023-11-07 01:30:05 INFO: ======== updtr_stop_test ========
2023-11-07 01:30:05 INFO: CMD: python3 updtr_stop_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/updtr_stop_test
2023-11-07 01:30:06,145 __main__ INFO -- Get or create the cluster --
2023-11-07 01:30:06,145 TADA INFO starting test `updtr_add test`
2023-11-07 01:30:06,145 TADA INFO   test-id: 06d20273020a8b0019228b43e9130902912e176f1dcced8a4486815cb724bf85
2023-11-07 01:30:06,145 TADA INFO   test-suite: LDMSD
2023-11-07 01:30:06,145 TADA INFO   test-name: updtr_add test
2023-11-07 01:30:06,146 TADA INFO   test-user: narate
2023-11-07 01:30:06,146 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:30:22,401 __main__ INFO -- Start daemons --
2023-11-07 01:30:37,840 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-07 01:30:38,160 __main__ INFO All LDMSDs are up.
2023-11-07 01:30:39,393 TADA INFO assertion 1, Send updtr_stop for a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-07 01:30:41,820 TADA INFO assertion 2, Send updtr_stop to a running updater: report(rc = 0, status = [{'name': 'running', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'running', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-07 01:30:43,034 TADA INFO assertion 3, Send updtr_stop to a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-11-07 01:30:43,034 __main__ INFO --- done ---
2023-11-07 01:30:43,035 TADA INFO test updtr_add test ended
2023-11-07 01:30:55 INFO: ----------------------------------------------
2023-11-07 01:30:56 INFO: ======== ldmsd_flex_decomp_test ========
2023-11-07 01:30:56 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_flex_decomp_test
2023-11-07 01:30:57,006 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-11-07 01:30:57,006 TADA INFO   test-id: b5dc1c10d74f0dc931867c077a20ac384804cb41755a0e337faaa173209d8c78
2023-11-07 01:30:57,006 TADA INFO   test-suite: LDMSD
2023-11-07 01:30:57,006 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-11-07 01:30:57,006 TADA INFO   test-user: narate
2023-11-07 01:30:57,006 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:30:57,007 __main__ INFO -- Get or create the cluster --
2023-11-07 01:31:25,420 __main__ INFO -- Start daemons --
2023-11-07 01:31:55,084 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:32:44,294 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-11-07 01:32:44,294 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-11-07 01:32:44,294 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-11-07 01:32:44,294 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 5, record sos schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-11-07 01:32:44,295 TADA INFO assertion 10, record csv schema check: OK, passed
2023-11-07 01:32:44,296 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-11-07 01:32:44,296 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-11-07 01:32:44,296 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-11-07 01:32:44,296 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-11-07 01:32:44,296 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-11-07 01:32:44,298 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-11-07 01:32:44,373 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-11-07 01:32:44,376 TADA INFO assertion 18, fill sos data check: OK, passed
2023-11-07 01:32:44,378 TADA INFO assertion 19, filter sos data check: OK, passed
2023-11-07 01:32:44,387 TADA INFO assertion 20, record sos data check: OK, passed
2023-11-07 01:32:44,388 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-11-07 01:32:44,463 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-11-07 01:32:44,466 TADA INFO assertion 23, fill csv data check: OK, passed
2023-11-07 01:32:44,468 TADA INFO assertion 24, filter csv data check: OK, passed
2023-11-07 01:32:44,477 TADA INFO assertion 25, record csv data check: OK, passed
2023-11-07 01:32:44,478 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-11-07 01:32:44,502 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-11-07 01:32:44,503 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-11-07 01:32:44,504 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-11-07 01:32:44,508 TADA INFO assertion 30, record kafka data check: OK, passed
2023-11-07 01:32:44,508 TADA INFO test ldmsd_flex_decomp_test ended
2023-11-07 01:32:44,508 TADA INFO test ldmsd_flex_decomp_test ended
2023-11-07 01:32:59 INFO: ----------------------------------------------
2023-11-07 01:33:00 INFO: ======== ldms_set_info_test ========
2023-11-07 01:33:00 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_set_info_test
2023-11-07 01:33:18,356 TADA INFO starting test `ldms_set_info_test`
2023-11-07 01:33:18,356 TADA INFO   test-id: e5025cc5efaa49cd61b8c7ea248ff47bef01836c4b160f03239ea10f19670fe6
2023-11-07 01:33:18,356 TADA INFO   test-suite: LDMSD
2023-11-07 01:33:18,356 TADA INFO   test-name: ldms_set_info_test
2023-11-07 01:33:18,356 TADA INFO   test-user: narate
2023-11-07 01:33:18,356 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:33:18,357 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-11-07 01:33:18,357 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-11-07 01:33:18,357 TADA INFO assertion 3, Get a value : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 4, Unset a pair : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 7, Server resetting a key : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 8, Server unset a key : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 9, Server add a key : -, passed
2023-11-07 01:33:18,358 TADA INFO assertion 10, Adding a key : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-11-07 01:33:18,359 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-11-07 01:33:18,360 TADA INFO test ldms_set_info_test ended
2023-11-07 01:33:29 INFO: ----------------------------------------------
2023-11-07 01:33:30 INFO: ======== slurm_sampler2_test ========
2023-11-07 01:33:30 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/slurm_sampler2_test
2023-11-07 01:33:30,867 TADA INFO starting test `slurm_sampler2_test`
2023-11-07 01:33:30,867 TADA INFO   test-id: 8161db9d73f7748c76cb673a89d3428a181ccae5d8f00991458c8b617c1a805c
2023-11-07 01:33:30,867 TADA INFO   test-suite: LDMSD
2023-11-07 01:33:30,867 TADA INFO   test-name: slurm_sampler2_test
2023-11-07 01:33:30,867 TADA INFO   test-user: narate
2023-11-07 01:33:30,867 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:33:30,868 __main__ INFO -- Get or create the cluster --
2023-11-07 01:33:56,375 __main__ INFO -- Add users --
2023-11-07 01:34:01,653 __main__ INFO -- Preparing job script & programs --
2023-11-07 01:34:02,370 __main__ INFO -- Start daemons --
2023-11-07 01:34:43,952 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-11-07 01:34:48,702 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-11-07 01:34:51,463 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-11-07 01:34:54,191 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-11-07 01:34:56,961 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:34:59,702 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:35:04,499 TADA INFO assertion 3.1, Expanding the set heap -- job_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:07,264 TADA INFO assertion 3.2, Expanding the set heap -- step_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:11,285 TADA INFO assertion 3.3, Expanding the set heap -- task_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:15,454 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:35:18,170 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:35:24,512 TADA INFO assertion 4.1, Multi-tenant -- job_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:26,202 TADA INFO assertion 4.2, Multi-tenant -- step_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:28,905 TADA INFO assertion 4.3, Multi-tenant -- task_init: The metric values are as expected on all nodes., passed
2023-11-07 01:35:31,564 TADA INFO assertion 4.4, Multi-tenant -- task_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:35:33,297 TADA INFO assertion 4.5, Multi-tenant -- job_exit: The metric values are as expected on all nodes., passed
2023-11-07 01:35:33,297 TADA INFO test slurm_sampler2_test ended
2023-11-07 01:35:47 INFO: ----------------------------------------------
2023-11-07 01:35:48 INFO: ======== libovis_log_test ========
2023-11-07 01:35:48 INFO: CMD: python3 libovis_log_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/libovis_log_test
2023-11-07 01:35:49,371 TADA INFO starting test `libovis_log_test`
2023-11-07 01:35:49,372 TADA INFO   test-id: 3d6eae5e48ae916da6470753882c36c71ac0227bcce9765f02d61630a75bd639
2023-11-07 01:35:49,372 TADA INFO   test-suite: LDMSD
2023-11-07 01:35:49,372 TADA INFO   test-name: libovis_log_test
2023-11-07 01:35:49,372 TADA INFO   test-user: narate
2023-11-07 01:35:49,372 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:35:49,372 __main__ INFO -- Create the cluster -- 
2023-11-07 01:36:01,697 __main__ INFO -- Start daemons --
2023-11-07 01:36:03,898 TADA INFO assertion 1, Call ovis_log_init() with valid arguments: 'return_code=0' and 'liovis_log_test' in 'Tue Nov 07 01:36:02 2023:         : libovis_log_test: return_code=0
', passed
2023-11-07 01:36:05,026 TADA INFO assertion 2, Call ovis_log_init() with name = NULL: ('return_code=0' and ': :') in 'Tue Nov 07 01:36:04 2023:         : : return_code=0
', passed
2023-11-07 01:36:06,150 TADA INFO assertion 3, Call ovis_log_init() with an invalid level: 'return_code=22' in 'Tue Nov 07 01:36:05 2023:         : : return_code=22
', passed
2023-11-07 01:36:07,262 TADA INFO assertion 4, Call ovis_log_init() with an invalid mode: 'return_code=22' in 'Tue Nov 07 01:36:06 2023:         : : return_code=22
', passed
2023-11-07 01:36:07,937 TADA INFO assertion 6, Log messages to a file: 0 == ovis_log_open(/var/log/6.log) # (0), passed
2023-11-07 01:36:09,050 TADA INFO assertion 5, Log messages to stdout: 'return_code=0' in 'Tue Nov 07 01:36:08 2023:         : : return_code=0
', passed
2023-11-07 01:36:09,181 TADA INFO assertion 7, Open the log file at a non-existing path: 'Could not open the log file' in 'Tue Nov 07 01:36:09 2023:         : test: result=0
Tue Nov 07 01:36:09 2023:    ERROR: test: Could not open the log file named '/data/log/foo/7.log'
Tue Nov 07 01:36:09 2023:    ERROR: test: Failed to open the log file at /data/log/foo/7.log. Error 22
', passed
2023-11-07 01:36:09,768 TADA INFO assertion 8, Reopen the log file at another path: ovis_log_open() closes and opens the second path successfully, passed
2023-11-07 01:36:10,223 TADA INFO assertion 9, Convert 'DEBUG,INFO' integer to a string: DEBUG,INFO == DEBUG,INFO (expected), passed
2023-11-07 01:36:10,337 TADA INFO assertion 10, Convert 'DEBUG,WARNING' integer to a string: DEBUG,WARNING == DEBUG,WARNING (expected), passed
2023-11-07 01:36:10,449 TADA INFO assertion 11, Convert 'DEBUG,ERROR' integer to a string: DEBUG,ERROR == DEBUG,ERROR (expected), passed
2023-11-07 01:36:10,563 TADA INFO assertion 12, Convert 'DEBUG,CRITICAL' integer to a string: DEBUG,CRITICAL == DEBUG,CRITICAL (expected), passed
2023-11-07 01:36:10,658 TADA INFO assertion 13, Convert 'INFO,WARNING' integer to a string: INFO,WARNING == INFO,WARNING (expected), passed
2023-11-07 01:36:10,764 TADA INFO assertion 14, Convert 'INFO,ERROR' integer to a string: INFO,ERROR == INFO,ERROR (expected), passed
2023-11-07 01:36:10,869 TADA INFO assertion 15, Convert 'INFO,CRITICAL' integer to a string: INFO,CRITICAL == INFO,CRITICAL (expected), passed
2023-11-07 01:36:10,974 TADA INFO assertion 16, Convert 'WARNING,ERROR' integer to a string: WARNING,ERROR == WARNING,ERROR (expected), passed
2023-11-07 01:36:11,088 TADA INFO assertion 17, Convert 'WARNING,CRITICAL' integer to a string: WARNING,CRITICAL == WARNING,CRITICAL (expected), passed
2023-11-07 01:36:11,195 TADA INFO assertion 18, Convert 'ERROR,CRITICAL' integer to a string: ERROR,CRITICAL == ERROR,CRITICAL (expected), passed
2023-11-07 01:36:11,306 TADA INFO assertion 19, Convert 'DEBUG,INFO,WARNING' integer to a string: DEBUG,INFO,WARNING == DEBUG,INFO,WARNING (expected), passed
2023-11-07 01:36:11,414 TADA INFO assertion 20, Convert 'DEBUG,INFO,ERROR' integer to a string: DEBUG,INFO,ERROR == DEBUG,INFO,ERROR (expected), passed
2023-11-07 01:36:11,536 TADA INFO assertion 21, Convert 'DEBUG,INFO,CRITICAL' integer to a string: DEBUG,INFO,CRITICAL == DEBUG,INFO,CRITICAL (expected), passed
2023-11-07 01:36:11,666 TADA INFO assertion 22, Convert 'DEBUG,WARNING,ERROR' integer to a string: DEBUG,WARNING,ERROR == DEBUG,WARNING,ERROR (expected), passed
2023-11-07 01:36:11,776 TADA INFO assertion 23, Convert 'DEBUG,WARNING,CRITICAL' integer to a string: DEBUG,WARNING,CRITICAL == DEBUG,WARNING,CRITICAL (expected), passed
2023-11-07 01:36:11,885 TADA INFO assertion 24, Convert 'DEBUG,ERROR,CRITICAL' integer to a string: DEBUG,ERROR,CRITICAL == DEBUG,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:11,996 TADA INFO assertion 25, Convert 'INFO,WARNING,ERROR' integer to a string: INFO,WARNING,ERROR == INFO,WARNING,ERROR (expected), passed
2023-11-07 01:36:12,110 TADA INFO assertion 26, Convert 'INFO,WARNING,CRITICAL' integer to a string: INFO,WARNING,CRITICAL == INFO,WARNING,CRITICAL (expected), passed
2023-11-07 01:36:12,215 TADA INFO assertion 27, Convert 'INFO,ERROR,CRITICAL' integer to a string: INFO,ERROR,CRITICAL == INFO,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:12,315 TADA INFO assertion 28, Convert 'WARNING,ERROR,CRITICAL' integer to a string: WARNING,ERROR,CRITICAL == WARNING,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:12,435 TADA INFO assertion 29, Convert 'DEBUG,INFO,WARNING,ERROR' integer to a string: DEBUG,INFO,WARNING,ERROR == DEBUG,INFO,WARNING,ERROR (expected), passed
2023-11-07 01:36:12,554 TADA INFO assertion 30, Convert 'DEBUG,INFO,WARNING,CRITICAL' integer to a string: DEBUG,INFO,WARNING,CRITICAL == DEBUG,INFO,WARNING,CRITICAL (expected), passed
2023-11-07 01:36:12,685 TADA INFO assertion 31, Convert 'DEBUG,INFO,ERROR,CRITICAL' integer to a string: DEBUG,INFO,ERROR,CRITICAL == DEBUG,INFO,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:12,809 TADA INFO assertion 32, Convert 'DEBUG,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,WARNING,ERROR,CRITICAL == DEBUG,WARNING,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:12,922 TADA INFO assertion 33, Convert 'INFO,WARNING,ERROR,CRITICAL' integer to a string: INFO,WARNING,ERROR,CRITICAL == INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:13,034 TADA INFO assertion 34, Convert 'DEBUG,INFO,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,INFO,WARNING,ERROR,CRITICAL == DEBUG,INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-11-07 01:36:13,146 TADA INFO assertion 35, Convert 'DEBUG,' integer to a string: DEBUG, == DEBUG, (expected), passed
2023-11-07 01:36:13,253 TADA INFO assertion 36, Convert 'INFO,' integer to a string: INFO, == INFO, (expected), passed
2023-11-07 01:36:13,355 TADA INFO assertion 37, Convert 'WARNING,' integer to a string: WARNING, == WARNING, (expected), passed
2023-11-07 01:36:13,477 TADA INFO assertion 38, Convert 'ERROR,' integer to a string: ERROR, == ERROR, (expected), passed
2023-11-07 01:36:13,599 TADA INFO assertion 39, Convert 'CRITICAL,' integer to a string: CRITICAL, == CRITICAL, (expected), passed
2023-11-07 01:36:13,715 TADA INFO assertion 40, Convert an invalid integer to a level string: (null) == (null) (expected), passed
2023-11-07 01:36:13,830 TADA INFO assertion 41, Convert the 'DEBUG,INFO' to an integer: 3 == 3 (expected), passed
2023-11-07 01:36:13,930 TADA INFO assertion 42, Convert the 'DEBUG,WARNING' to an integer: 5 == 5 (expected), passed
2023-11-07 01:36:14,036 TADA INFO assertion 43, Convert the 'DEBUG,ERROR' to an integer: 9 == 9 (expected), passed
2023-11-07 01:36:14,139 TADA INFO assertion 44, Convert the 'DEBUG,CRITICAL' to an integer: 17 == 17 (expected), passed
2023-11-07 01:36:14,261 TADA INFO assertion 45, Convert the 'INFO,WARNING' to an integer: 6 == 6 (expected), passed
2023-11-07 01:36:14,369 TADA INFO assertion 46, Convert the 'INFO,ERROR' to an integer: 10 == 10 (expected), passed
2023-11-07 01:36:14,499 TADA INFO assertion 47, Convert the 'INFO,CRITICAL' to an integer: 18 == 18 (expected), passed
2023-11-07 01:36:14,613 TADA INFO assertion 48, Convert the 'WARNING,ERROR' to an integer: 12 == 12 (expected), passed
2023-11-07 01:36:14,730 TADA INFO assertion 49, Convert the 'WARNING,CRITICAL' to an integer: 20 == 20 (expected), passed
2023-11-07 01:36:14,846 TADA INFO assertion 50, Convert the 'ERROR,CRITICAL' to an integer: 24 == 24 (expected), passed
2023-11-07 01:36:14,947 TADA INFO assertion 51, Convert the 'DEBUG,INFO,WARNING' to an integer: 7 == 7 (expected), passed
2023-11-07 01:36:15,048 TADA INFO assertion 52, Convert the 'DEBUG,INFO,ERROR' to an integer: 11 == 11 (expected), passed
2023-11-07 01:36:15,173 TADA INFO assertion 53, Convert the 'DEBUG,INFO,CRITICAL' to an integer: 19 == 19 (expected), passed
2023-11-07 01:36:15,277 TADA INFO assertion 54, Convert the 'DEBUG,WARNING,ERROR' to an integer: 13 == 13 (expected), passed
2023-11-07 01:36:15,402 TADA INFO assertion 55, Convert the 'DEBUG,WARNING,CRITICAL' to an integer: 21 == 21 (expected), passed
2023-11-07 01:36:15,516 TADA INFO assertion 56, Convert the 'DEBUG,ERROR,CRITICAL' to an integer: 25 == 25 (expected), passed
2023-11-07 01:36:15,645 TADA INFO assertion 57, Convert the 'INFO,WARNING,ERROR' to an integer: 14 == 14 (expected), passed
2023-11-07 01:36:15,760 TADA INFO assertion 58, Convert the 'INFO,WARNING,CRITICAL' to an integer: 22 == 22 (expected), passed
2023-11-07 01:36:15,869 TADA INFO assertion 59, Convert the 'INFO,ERROR,CRITICAL' to an integer: 26 == 26 (expected), passed
2023-11-07 01:36:15,982 TADA INFO assertion 60, Convert the 'WARNING,ERROR,CRITICAL' to an integer: 28 == 28 (expected), passed
2023-11-07 01:36:16,092 TADA INFO assertion 61, Convert the 'DEBUG,INFO,WARNING,ERROR' to an integer: 15 == 15 (expected), passed
2023-11-07 01:36:16,199 TADA INFO assertion 62, Convert the 'DEBUG,INFO,WARNING,CRITICAL' to an integer: 23 == 23 (expected), passed
2023-11-07 01:36:16,302 TADA INFO assertion 63, Convert the 'DEBUG,INFO,ERROR,CRITICAL' to an integer: 27 == 27 (expected), passed
2023-11-07 01:36:16,405 TADA INFO assertion 64, Convert the 'DEBUG,WARNING,ERROR,CRITICAL' to an integer: 29 == 29 (expected), passed
2023-11-07 01:36:16,521 TADA INFO assertion 65, Convert the 'INFO,WARNING,ERROR,CRITICAL' to an integer: 30 == 30 (expected), passed
2023-11-07 01:36:16,630 TADA INFO assertion 66, Convert the 'DEBUG,INFO,WARNING,ERROR,CRITICAL' to an integer: 31 == 31 (expected), passed
2023-11-07 01:36:16,727 TADA INFO assertion 67, Convert the 'DEBUG,' to an integer: 1 == 1 (expected), passed
2023-11-07 01:36:16,832 TADA INFO assertion 68, Convert the 'INFO,' to an integer: 2 == 2 (expected), passed
2023-11-07 01:36:16,945 TADA INFO assertion 69, Convert the 'WARNING,' to an integer: 4 == 4 (expected), passed
2023-11-07 01:36:17,057 TADA INFO assertion 70, Convert the 'ERROR,' to an integer: 8 == 8 (expected), passed
2023-11-07 01:36:17,178 TADA INFO assertion 71, Convert the 'CRITICAL,' to an integer: 16 == 16 (expected), passed
2023-11-07 01:36:17,298 TADA INFO assertion 72, Convert the 'DEBUG' to an integer: 31 == 31 (expected), passed
2023-11-07 01:36:17,401 TADA INFO assertion 73, Convert the 'INFO' to an integer: 30 == 30 (expected), passed
2023-11-07 01:36:17,511 TADA INFO assertion 74, Convert the 'WARNING' to an integer: 28 == 28 (expected), passed
2023-11-07 01:36:17,641 TADA INFO assertion 75, Convert the 'ERROR' to an integer: 24 == 24 (expected), passed
2023-11-07 01:36:17,747 TADA INFO assertion 76, Convert the 'CRITICAL' to an integer: 16 == 16 (expected), passed
2023-11-07 01:36:17,845 TADA INFO assertion 77, Convert an invalid level string to an integer: -22 == -22 (expected), passed
2023-11-07 01:36:18,480 TADA INFO assertion 78, Verify that no messages were printed when the level is QUIET.: No messages were printed., passed
2023-11-07 01:36:18,815 TADA INFO assertion 79, Verify that messages of DEBUG,INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:19,136 TADA INFO assertion 80, Verify that messages of DEBUG,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:19,495 TADA INFO assertion 81, Verify that messages of DEBUG,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:19,817 TADA INFO assertion 82, Verify that messages of DEBUG,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:20,152 TADA INFO assertion 83, Verify that messages of INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:20,495 TADA INFO assertion 84, Verify that messages of INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:20,825 TADA INFO assertion 85, Verify that messages of INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:21,168 TADA INFO assertion 86, Verify that messages of WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:21,546 TADA INFO assertion 87, Verify that messages of WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:21,867 TADA INFO assertion 88, Verify that messages of ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:22,186 TADA INFO assertion 89, Verify that messages of DEBUG,INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:22,508 TADA INFO assertion 90, Verify that messages of DEBUG,INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:22,850 TADA INFO assertion 91, Verify that messages of DEBUG,INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:23,193 TADA INFO assertion 92, Verify that messages of DEBUG,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:23,522 TADA INFO assertion 93, Verify that messages of DEBUG,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:23,851 TADA INFO assertion 94, Verify that messages of DEBUG,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:24,157 TADA INFO assertion 95, Verify that messages of INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:24,496 TADA INFO assertion 96, Verify that messages of INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:24,837 TADA INFO assertion 97, Verify that messages of INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:25,166 TADA INFO assertion 98, Verify that messages of WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:25,509 TADA INFO assertion 99, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:25,854 TADA INFO assertion 100, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:26,161 TADA INFO assertion 101, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:26,502 TADA INFO assertion 102, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:26,815 TADA INFO assertion 103, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:27,143 TADA INFO assertion 104, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:27,491 TADA INFO assertion 105, Verify that messages of DEBUG, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:27,826 TADA INFO assertion 106, Verify that messages of INFO, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:28,134 TADA INFO assertion 107, Verify that messages of WARNING, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:28,447 TADA INFO assertion 108, Verify that messages of ERROR, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:28,787 TADA INFO assertion 109, Verify that messages of CRITICAL, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:29,097 TADA INFO assertion 110, Verify that messages of DEBUG were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:29,404 TADA INFO assertion 111, Verify that messages of INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:29,761 TADA INFO assertion 112, Verify that messages of WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:30,077 TADA INFO assertion 113, Verify that messages of ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:30,439 TADA INFO assertion 114, Verify that messages of CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:31,102 TADA INFO assertion 116, Verify that ovis_log_close() works properly: ovis_log_close() works properly., passed
2023-11-07 01:36:32,970 TADA INFO assertion 115, Verify that applications can open, rename, and reopen log files to perform log rotation.: ovis_log supports open, rename (external), and reopen., passed
2023-11-07 01:36:33,370 TADA INFO assertion 117, Test a ovis_log_register() call with valid arguments: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}], passed
2023-11-07 01:36:33,489 TADA INFO assertion 118, Test a ovis_log_register() call with NULL name: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-11-07 01:36:33,603 TADA INFO assertion 119, Test a ovis_log_register() call with NULL desc: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-11-07 01:36:33,712 TADA INFO assertion 120, Test a ovis_log_register() call with an existing subsystem: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}], passed
2023-11-07 01:36:34,360 TADA INFO assertion 122, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:34,683 TADA INFO assertion 123, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:35,017 TADA INFO assertion 124, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:35,328 TADA INFO assertion 125, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:35,684 TADA INFO assertion 126, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:36,009 TADA INFO assertion 127, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:36,329 TADA INFO assertion 128, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:36,616 TADA INFO assertion 129, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:36,956 TADA INFO assertion 130, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:37,268 TADA INFO assertion 131, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:37,619 TADA INFO assertion 132, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:37,957 TADA INFO assertion 133, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:38,265 TADA INFO assertion 134, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:38,600 TADA INFO assertion 135, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:38,954 TADA INFO assertion 136, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:39,286 TADA INFO assertion 137, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:39,605 TADA INFO assertion 138, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:39,917 TADA INFO assertion 139, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:40,240 TADA INFO assertion 140, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:40,575 TADA INFO assertion 141, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:40,930 TADA INFO assertion 142, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:41,260 TADA INFO assertion 143, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:41,589 TADA INFO assertion 144, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:41,940 TADA INFO assertion 145, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:42,254 TADA INFO assertion 146, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:42,586 TADA INFO assertion 147, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:42,951 TADA INFO assertion 148, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:43,282 TADA INFO assertion 149, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:43,633 TADA INFO assertion 150, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:43,967 TADA INFO assertion 151, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:44,273 TADA INFO assertion 152, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:44,594 TADA INFO assertion 153, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:44,942 TADA INFO assertion 154, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:45,285 TADA INFO assertion 155, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:45,608 TADA INFO assertion 156, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:45,955 TADA INFO assertion 157, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:46,271 TADA INFO assertion 158, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:46,591 TADA INFO assertion 159, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:46,918 TADA INFO assertion 160, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:47,241 TADA INFO assertion 161, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:47,593 TADA INFO assertion 162, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:47,951 TADA INFO assertion 163, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:48,283 TADA INFO assertion 164, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:48,619 TADA INFO assertion 165, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:48,971 TADA INFO assertion 166, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:49,284 TADA INFO assertion 167, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:49,599 TADA INFO assertion 168, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:49,943 TADA INFO assertion 169, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:50,275 TADA INFO assertion 170, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:50,603 TADA INFO assertion 171, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:50,952 TADA INFO assertion 172, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:51,286 TADA INFO assertion 173, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:51,608 TADA INFO assertion 174, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:51,957 TADA INFO assertion 175, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:52,282 TADA INFO assertion 176, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:52,613 TADA INFO assertion 177, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:52,937 TADA INFO assertion 178, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:53,247 TADA INFO assertion 179, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:53,564 TADA INFO assertion 180, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:53,924 TADA INFO assertion 181, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:54,237 TADA INFO assertion 182, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:54,552 TADA INFO assertion 183, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:54,900 TADA INFO assertion 184, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:55,219 TADA INFO assertion 185, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:55,554 TADA INFO assertion 186, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:55,862 TADA INFO assertion 187, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:56,221 TADA INFO assertion 188, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:56,534 TADA INFO assertion 189, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:56,889 TADA INFO assertion 190, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:57,229 TADA INFO assertion 191, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:57,540 TADA INFO assertion 192, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:57,893 TADA INFO assertion 193, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-11-07 01:36:58,541 TADA INFO assertion 195, Verify that ovis_log_set_level_by_regex() returns an error when the given regular expression string is invalid.: 'result=22' in 'Tue Nov 07 01:36:58 2023:         : test: result=22
', passed
2023-11-07 01:36:58,886 TADA INFO assertion 194, Verify that ovis_log_set_level_by_regex() returns ENOENT when the given regular expression string doesn't match any logs.: 'result=2' in 'Tue Nov 07 01:36:58 2023:         : test: result=2
', passed
2023-11-07 01:36:59,224 TADA INFO assertion 196, Verify that ovis_log_set_level_by_regex() sets the level of the matched log subsystems to the given value.: ('config:' in 'Tue Nov 07 01:36:58 2023:         : config: ALWAYS' and (('CRITICAL' in 'Tue Nov 07 01:36:58 2023:         : config: ALWAYS') or ('ALWAYS' in Tue Nov 07 01:36:58 2023:         : config: ALWAYS)), passed
2023-11-07 01:36:59,908 TADA INFO assertion 197, Verify that ovis_log_list() works correctly.: '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]' == '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]', passed
2023-11-07 01:36:59,908 TADA INFO test libovis_log_test ended
2023-11-07 01:37:11 INFO: ----------------------------------------------
2023-11-07 01:37:12 INFO: ======== ldmsd_long_config_test ========
2023-11-07 01:37:12 INFO: CMD: python3 ldmsd_long_config_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_long_config_test
2023-11-07 01:37:12,764 TADA INFO starting test `ldmsd_long_config_line`
2023-11-07 01:37:12,765 TADA INFO   test-id: 3e3d19f33a2e15878a75cd36d237f670774d1515037bc5ab3a75fe129607ec62
2023-11-07 01:37:12,765 TADA INFO   test-suite: LDMSD
2023-11-07 01:37:12,765 TADA INFO   test-name: ldmsd_long_config_line
2023-11-07 01:37:12,765 TADA INFO   test-user: narate
2023-11-07 01:37:12,765 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:37:12,766 __main__ INFO ---Get or create the cluster --
2023-11-07 01:37:29,233 __main__ INFO --- Start daemons ---
2023-11-07 01:37:45,864 TADA INFO assertion 1, LDMSD correctly processes a config line in a config file: LDMSD processed the long config line in the config file correctly., passed
2023-11-07 01:37:46,408 TADA INFO assertion 2, LDMSD correctly handle a config line from ldmsd_controller: LDMSD receives the correct message from ldmsd_controller., passed
2023-11-07 01:37:47,094 TADA INFO assertion 3, LDMSD correctly handle a config line from ldmsctl: LDMSD receives the correct message from ldmsctl., failed
Traceback (most recent call last):
  File "ldmsd_long_config_test", line 202, in <module>
    "LDMSD receives the correct message from ldmsctl.")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test the code path that handles long config lines that larger than the max of the transport message size, LDMSD receives the correct message from ldmsctl.: FAILED
2023-11-07 01:37:47,095 TADA INFO test ldmsd_long_config_line ended
2023-11-07 01:37:59 INFO: ----------------------------------------------
2023-11-07 01:38:00 INFO: ======== ldms_rail_test ========
2023-11-07 01:38:00 INFO: CMD: python3 ldms_rail_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_rail_test
2023-11-07 01:38:00,967 TADA INFO starting test `ldms_rail_test`
2023-11-07 01:38:00,967 TADA INFO   test-id: 8d92e4a413206a499b2bbf1f399bf42e66c7e141d32c908eca092d6692622407
2023-11-07 01:38:00,967 TADA INFO   test-suite: LDMSD
2023-11-07 01:38:00,967 TADA INFO   test-name: ldms_rail_test
2023-11-07 01:38:00,967 TADA INFO   test-user: narate
2023-11-07 01:38:00,967 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:38:00,968 __main__ INFO -- Get or create the cluster --
2023-11-07 01:38:17,043 __main__ INFO -- Start daemons --
2023-11-07 01:38:21,828 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:38:23,831 __main__ INFO start ldms_rail_server.py and ldms_rail_client.py interactive sessions
2023-11-07 01:38:26,849 TADA INFO assertion 1, Start interactive LDMS server: OK, passed
2023-11-07 01:38:29,868 TADA INFO assertion 2, Start interactive LDMS client: OK, passed
2023-11-07 01:38:33,472 TADA INFO assertion 3, Client rail has 8 endpoints on 8 thread pools: OK, passed
2023-11-07 01:38:37,077 TADA INFO assertion 4, Server rail has 8 endpoints on 8 thread pools: OK, passed
2023-11-07 01:38:40,682 TADA INFO assertion 5, Sets on client are processed by different threads: OK, passed
2023-11-07 01:38:44,286 TADA INFO assertion 6, Verify sets on the client: OK, passed
2023-11-07 01:38:47,305 TADA INFO assertion 7, Start interactive LDMS client2: OK, passed
2023-11-07 01:38:50,909 TADA INFO assertion 8, Client2 rail has 8 endpoints on 4 thread pools: OK, passed
2023-11-07 01:38:53,929 TADA INFO assertion 9, Client3 (wrong auth) cannot connect: OK, passed
2023-11-07 01:38:56,947 TADA INFO assertion 10, Start interactive client4 (for push mode): OK, passed
2023-11-07 01:38:56,947 __main__ INFO waiting push ...
2023-11-07 01:38:58,950 __main__ INFO server: sampling new data (2)
2023-11-07 01:39:03,555 __main__ INFO client4: set pushes received
2023-11-07 01:39:03,556 __main__ INFO client4: verifying data in sets
2023-11-07 01:39:07,160 __main__ INFO client4: verifying threads-sets-endpoints spread
2023-11-07 01:39:17,974 TADA INFO assertion 11, Client4 got push callback from the corresponding thread: OK, passed
2023-11-07 01:39:20,998 TADA INFO assertion 12, Client5 started (for clean-up path test): OK, passed
2023-11-07 01:39:20,998 __main__ INFO xprt close by client1
2023-11-07 01:39:33,812 TADA INFO assertion 13, Active-side close: client1 clean up: OK, passed
2023-11-07 01:39:37,417 TADA INFO assertion 14, Active-side close: server-side clean up: OK, passed
2023-11-07 01:39:53,836 TADA INFO assertion 15, Passive-side close: client2 clean up: OK, passed
2023-11-07 01:39:53,837 TADA INFO assertion 16, Passive-side close: server-side clean up: OK, passed
2023-11-07 01:39:59,444 TADA INFO assertion 17, Active-side term: server-side clean up: OK, passed
2023-11-07 01:40:06,653 TADA INFO assertion 18, Passive-side term: client5 clean up: OK, passed
2023-11-07 01:40:27,123 TADA INFO assertion 19, server -> client overspending send: error message verified, passed
2023-11-07 01:40:37,936 TADA INFO assertion 20, client -> server overspending send: error message verified, passed
2023-11-07 01:40:41,541 TADA INFO assertion 21, verify send credits on the server: OK, passed
2023-11-07 01:40:45,146 TADA INFO assertion 22, verify send credits on the client: OK, passed
2023-11-07 01:40:53,256 TADA INFO assertion 23, server unblock, verify recv data: recv data verified, passed
2023-11-07 01:41:01,366 TADA INFO assertion 24, client unblock, verify recv data: recv data verified, passed
2023-11-07 01:41:04,971 TADA INFO assertion 25, verify send credits on the server: OK, passed
2023-11-07 01:41:08,576 TADA INFO assertion 26, verify send credits on the client: OK, passed
2023-11-07 01:41:12,181 TADA INFO assertion 27, server -> client send after credited back: OK, passed
2023-11-07 01:41:15,786 TADA INFO assertion 28, client -> server send after credited back: OK, passed
2023-11-07 01:41:19,391 TADA INFO assertion 29, verify send credits on the server: OK, passed
2023-11-07 01:41:22,995 TADA INFO assertion 30, verify send credits on the client: OK, passed
2023-11-07 01:41:26,600 TADA INFO assertion 31, server unblock, verify recv data: OK, passed
2023-11-07 01:41:30,204 TADA INFO assertion 32, client unblock, verify recv data: OK, passed
2023-11-07 01:41:33,809 TADA INFO assertion 33, verify send credits on the server: OK, passed
2023-11-07 01:41:37,414 TADA INFO assertion 34, verify send credits on the client: OK, passed
2023-11-07 01:41:41,019 TADA INFO assertion 35, verify send-credit deposits on the server: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-11-07 01:41:44,624 TADA INFO assertion 36, verify send-credit deposits on the client: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-11-07 01:41:44,625 TADA INFO test ldms_rail_test ended
2023-11-07 01:41:56 INFO: ----------------------------------------------
2023-11-07 01:41:57 INFO: ======== ldms_stream_test ========
2023-11-07 01:41:57 INFO: CMD: python3 ldms_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_stream_test
2023-11-07 01:41:58,560 TADA INFO starting test `ldms_stream_test`
2023-11-07 01:41:58,560 TADA INFO   test-id: 72fe7d1a43729e74af8cc1594926306b8d7b632c2e2423ddac3c861b1fb3542d
2023-11-07 01:41:58,560 TADA INFO   test-suite: LDMSD
2023-11-07 01:41:58,560 TADA INFO   test-name: ldms_stream_test
2023-11-07 01:41:58,560 TADA INFO   test-user: narate
2023-11-07 01:41:58,561 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:41:58,561 __main__ INFO -- Get or create the cluster --
2023-11-07 01:42:30,109 __main__ INFO -- Adding 'foo' and 'bar' users --
2023-11-07 01:42:40,425 __main__ INFO -- Start daemons --
2023-11-07 01:42:53,125 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:42:55,127 __main__ INFO start interactive stream servers
2023-11-07 01:42:55,128 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-7 
2023-11-07 01:42:58,147 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-6 
2023-11-07 01:43:01,165 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-5 
2023-11-07 01:43:04,181 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-4 
2023-11-07 01:43:07,200 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-3 
2023-11-07 01:43:10,222 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-2 
2023-11-07 01:43:13,240 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-1 
2023-11-07 01:43:16,259 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 
2023-11-07 01:43:19,778 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-5 
2023-11-07 01:43:23,297 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-6 
2023-11-07 01:43:26,816 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-7 
2023-11-07 01:43:30,336 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 as foo
2023-11-07 01:43:33,857 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 as bar
2023-11-07 01:43:37,377 __main__ INFO starting /tada-src/python/ldms_stream_client.py on narate-ldms_stream_test-32d5252-node-8 as foo
2023-11-07 01:43:40,896 TADA INFO assertion 1, Publishing oversize data results in an error: checking..., passed
2023-11-07 01:43:41,408 __main__ INFO getting data from srv1
2023-11-07 01:43:43,914 __main__ INFO getting data from srv2
2023-11-07 01:43:46,421 __main__ INFO getting data from srv3
2023-11-07 01:43:48,926 __main__ INFO getting data from srv4
2023-11-07 01:43:51,433 __main__ INFO getting data from srv5
2023-11-07 01:43:53,938 __main__ INFO getting data from srv6
2023-11-07 01:43:56,444 __main__ INFO getting data from srv7
2023-11-07 01:43:58,949 __main__ INFO getting data from cli8foo
2023-11-07 01:44:01,456 TADA INFO assertion 2, JSON support (l3-stream): client data verified, passed
2023-11-07 01:44:01,457 __main__ INFO publishing 'four' on l3-stream by pub4
2023-11-07 01:44:01,959 __main__ INFO publishing 'five' on l3-stream by pub5
2023-11-07 01:44:02,462 __main__ INFO publishing 'six' on l3-stream by pub6
2023-11-07 01:44:02,964 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-11-07 01:44:03,467 TADA INFO assertion 301, send-credit taken: credits: [114, 128, 128, 128], passed
2023-11-07 01:44:03,467 __main__ INFO obtaining all client data (0)
2023-11-07 01:44:03,467 __main__ INFO getting data from srv1
2023-11-07 01:44:05,973 __main__ INFO getting data from srv2
2023-11-07 01:44:08,479 __main__ INFO getting data from srv3
2023-11-07 01:44:10,986 __main__ INFO getting data from srv4
2023-11-07 01:44:13,492 __main__ INFO getting data from srv5
2023-11-07 01:44:15,998 __main__ INFO getting data from srv6
2023-11-07 01:44:18,505 __main__ INFO getting data from srv7
2023-11-07 01:44:21,011 __main__ INFO getting data from cli8foo
2023-11-07 01:44:23,517 __main__ INFO obtaining all client data (1)
2023-11-07 01:44:23,518 __main__ INFO getting data from srv1
2023-11-07 01:44:26,024 __main__ INFO getting data from srv2
2023-11-07 01:44:28,530 __main__ INFO getting data from srv3
2023-11-07 01:44:31,036 __main__ INFO getting data from srv4
2023-11-07 01:44:33,542 __main__ INFO getting data from srv5
2023-11-07 01:44:36,047 __main__ INFO getting data from srv6
2023-11-07 01:44:38,553 __main__ INFO getting data from srv7
2023-11-07 01:44:41,058 __main__ INFO getting data from cli8foo
2023-11-07 01:44:43,565 __main__ INFO obtaining all client data (2)
2023-11-07 01:44:43,565 __main__ INFO getting data from srv1
2023-11-07 01:44:46,071 __main__ INFO getting data from srv2
2023-11-07 01:44:48,577 __main__ INFO getting data from srv3
2023-11-07 01:44:51,083 __main__ INFO getting data from srv4
2023-11-07 01:44:53,588 __main__ INFO getting data from srv5
2023-11-07 01:44:56,094 __main__ INFO getting data from srv6
2023-11-07 01:44:58,599 __main__ INFO getting data from srv7
2023-11-07 01:45:01,105 __main__ INFO getting data from cli8foo
2023-11-07 01:45:03,611 __main__ INFO obtaining all client data (3)
2023-11-07 01:45:03,611 __main__ INFO getting data from srv1
2023-11-07 01:45:06,118 __main__ INFO getting data from srv2
2023-11-07 01:45:08,623 __main__ INFO getting data from srv3
2023-11-07 01:45:11,129 __main__ INFO getting data from srv4
2023-11-07 01:45:13,634 __main__ INFO getting data from srv5
2023-11-07 01:45:16,140 __main__ INFO getting data from srv6
2023-11-07 01:45:18,645 __main__ INFO getting data from srv7
2023-11-07 01:45:21,151 __main__ INFO getting data from cli8foo
2023-11-07 01:45:24,159 TADA INFO assertion 302, send-credit returned: credits: [128, 128, 128, 128], passed
2023-11-07 01:45:24,159 TADA INFO assertion 303, stream delivery spread among rails: tids: {208, 209, 206, 207}, passed
2023-11-07 01:45:24,160 TADA INFO assertion 3, l3-stream delivery: client data verified, passed
2023-11-07 01:45:24,160 __main__ INFO publishing 'four' on l2-stream by pub4
2023-11-07 01:45:24,662 __main__ INFO publishing 'five' on l2-stream by pub5
2023-11-07 01:45:25,164 __main__ INFO publishing 'six' on l2-stream by pub6
2023-11-07 01:45:25,667 __main__ INFO publishing 'seven' on l2-stream by pub7
2023-11-07 01:45:26,169 __main__ INFO obtaining all client data (0)
2023-11-07 01:45:26,170 __main__ INFO getting data from srv1
2023-11-07 01:45:28,676 __main__ INFO getting data from srv2
2023-11-07 01:45:31,182 __main__ INFO getting data from srv3
2023-11-07 01:45:33,688 __main__ INFO getting data from srv4
2023-11-07 01:45:36,195 __main__ INFO getting data from srv5
2023-11-07 01:45:38,701 __main__ INFO getting data from srv6
2023-11-07 01:45:41,207 __main__ INFO getting data from srv7
2023-11-07 01:45:43,713 __main__ INFO getting data from cli8foo
2023-11-07 01:45:46,220 __main__ INFO obtaining all client data (1)
2023-11-07 01:45:46,220 __main__ INFO getting data from srv1
2023-11-07 01:45:48,726 __main__ INFO getting data from srv2
2023-11-07 01:45:51,232 __main__ INFO getting data from srv3
2023-11-07 01:45:53,739 __main__ INFO getting data from srv4
2023-11-07 01:45:56,245 __main__ INFO getting data from srv5
2023-11-07 01:45:58,750 __main__ INFO getting data from srv6
2023-11-07 01:46:01,256 __main__ INFO getting data from srv7
2023-11-07 01:46:03,761 __main__ INFO getting data from cli8foo
2023-11-07 01:46:06,268 TADA INFO assertion 4, l2-stream delivery: client data verified, passed
2023-11-07 01:46:06,268 __main__ INFO publishing 'four' on l1-stream by pub4
2023-11-07 01:46:06,770 __main__ INFO publishing 'five' on l1-stream by pub5
2023-11-07 01:46:07,272 __main__ INFO publishing 'six' on l1-stream by pub6
2023-11-07 01:46:07,774 __main__ INFO publishing 'seven' on l1-stream by pub7
2023-11-07 01:46:08,277 __main__ INFO obtaining all client data (0)
2023-11-07 01:46:08,277 __main__ INFO getting data from srv1
2023-11-07 01:46:10,783 __main__ INFO getting data from srv2
2023-11-07 01:46:13,288 __main__ INFO getting data from srv3
2023-11-07 01:46:15,795 __main__ INFO getting data from srv4
2023-11-07 01:46:18,301 __main__ INFO getting data from srv5
2023-11-07 01:46:20,808 __main__ INFO getting data from srv6
2023-11-07 01:46:23,314 __main__ INFO getting data from srv7
2023-11-07 01:46:25,820 __main__ INFO getting data from cli8foo
2023-11-07 01:46:28,326 __main__ INFO obtaining all client data (1)
2023-11-07 01:46:28,327 __main__ INFO getting data from srv1
2023-11-07 01:46:30,833 __main__ INFO getting data from srv2
2023-11-07 01:46:33,338 __main__ INFO getting data from srv3
2023-11-07 01:46:35,844 __main__ INFO getting data from srv4
2023-11-07 01:46:38,350 __main__ INFO getting data from srv5
2023-11-07 01:46:40,856 __main__ INFO getting data from srv6
2023-11-07 01:46:43,361 __main__ INFO getting data from srv7
2023-11-07 01:46:45,867 __main__ INFO getting data from cli8foo
2023-11-07 01:46:48,374 TADA INFO assertion 5, l1-stream delivery: client data verified, passed
2023-11-07 01:46:48,374 __main__ INFO publishing 'four' on x-stream by pub4
2023-11-07 01:46:48,876 __main__ INFO publishing 'five' on x-stream by pub5
2023-11-07 01:46:49,378 __main__ INFO publishing 'six' on x-stream by pub6
2023-11-07 01:46:49,880 __main__ INFO publishing 'seven' on x-stream by pub7
2023-11-07 01:46:50,382 __main__ INFO obtaining all client data (0)
2023-11-07 01:46:50,382 __main__ INFO getting data from srv1
2023-11-07 01:46:52,888 __main__ INFO getting data from srv2
2023-11-07 01:46:55,394 __main__ INFO getting data from srv3
2023-11-07 01:46:57,900 __main__ INFO getting data from srv4
2023-11-07 01:47:00,407 __main__ INFO getting data from srv5
2023-11-07 01:47:02,913 __main__ INFO getting data from srv6
2023-11-07 01:47:05,419 __main__ INFO getting data from srv7
2023-11-07 01:47:07,925 __main__ INFO getting data from cli8foo
2023-11-07 01:47:10,431 __main__ INFO obtaining all client data (1)
2023-11-07 01:47:10,431 __main__ INFO getting data from srv1
2023-11-07 01:47:12,937 __main__ INFO getting data from srv2
2023-11-07 01:47:15,443 __main__ INFO getting data from srv3
2023-11-07 01:47:17,949 __main__ INFO getting data from srv4
2023-11-07 01:47:20,455 __main__ INFO getting data from srv5
2023-11-07 01:47:22,960 __main__ INFO getting data from srv6
2023-11-07 01:47:25,466 __main__ INFO getting data from srv7
2023-11-07 01:47:27,971 __main__ INFO getting data from cli8foo
2023-11-07 01:47:30,478 TADA INFO assertion 6, x-stream delivery: client data verified, passed
2023-11-07 01:47:30,478 __main__ INFO publishing 'four' on nada by pub4
2023-11-07 01:47:30,980 __main__ INFO publishing 'five' on nada by pub5
2023-11-07 01:47:31,481 __main__ INFO publishing 'six' on nada by pub6
2023-11-07 01:47:31,983 __main__ INFO publishing 'seven' on nada by pub7
2023-11-07 01:47:32,484 __main__ INFO obtaining all client data (0)
2023-11-07 01:47:32,485 __main__ INFO getting data from srv1
2023-11-07 01:47:34,990 __main__ INFO getting data from srv2
2023-11-07 01:47:37,496 __main__ INFO getting data from srv3
2023-11-07 01:47:40,001 __main__ INFO getting data from srv4
2023-11-07 01:47:42,507 __main__ INFO getting data from srv5
2023-11-07 01:47:45,013 __main__ INFO getting data from srv6
2023-11-07 01:47:47,519 __main__ INFO getting data from srv7
2023-11-07 01:47:50,025 __main__ INFO getting data from cli8foo
2023-11-07 01:47:52,531 __main__ INFO obtaining all client data (1)
2023-11-07 01:47:52,531 __main__ INFO getting data from srv1
2023-11-07 01:47:55,036 __main__ INFO getting data from srv2
2023-11-07 01:47:57,542 __main__ INFO getting data from srv3
2023-11-07 01:48:00,047 __main__ INFO getting data from srv4
2023-11-07 01:48:02,553 __main__ INFO getting data from srv5
2023-11-07 01:48:05,058 __main__ INFO getting data from srv6
2023-11-07 01:48:07,564 __main__ INFO getting data from srv7
2023-11-07 01:48:10,069 __main__ INFO getting data from cli8foo
2023-11-07 01:48:12,575 TADA INFO assertion 7, nada delivery: client data verified, passed
2023-11-07 01:48:12,576 __main__ INFO publishing 'four' on l3-stream by pub4 (0400)
2023-11-07 01:48:13,078 __main__ INFO publishing 'five' on l3-stream by pub5 (0400)
2023-11-07 01:48:13,581 __main__ INFO publishing 'six' on l3-stream by pub6 (0400)
2023-11-07 01:48:14,083 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400)
2023-11-07 01:48:14,586 __main__ INFO obtaining all client data (0)
2023-11-07 01:48:14,586 __main__ INFO getting data from srv1
2023-11-07 01:48:17,092 __main__ INFO getting data from srv2
2023-11-07 01:48:19,598 __main__ INFO getting data from srv3
2023-11-07 01:48:22,104 __main__ INFO getting data from srv4
2023-11-07 01:48:24,611 __main__ INFO getting data from srv5
2023-11-07 01:48:27,117 __main__ INFO getting data from srv6
2023-11-07 01:48:29,623 __main__ INFO getting data from srv7
2023-11-07 01:48:32,129 __main__ INFO getting data from cli8foo
2023-11-07 01:48:34,635 __main__ INFO obtaining all client data (1)
2023-11-07 01:48:34,635 __main__ INFO getting data from srv1
2023-11-07 01:48:37,141 __main__ INFO getting data from srv2
2023-11-07 01:48:39,648 __main__ INFO getting data from srv3
2023-11-07 01:48:42,154 __main__ INFO getting data from srv4
2023-11-07 01:48:44,660 __main__ INFO getting data from srv5
2023-11-07 01:48:47,165 __main__ INFO getting data from srv6
2023-11-07 01:48:49,671 __main__ INFO getting data from srv7
2023-11-07 01:48:52,177 __main__ INFO getting data from cli8foo
2023-11-07 01:48:54,682 __main__ INFO obtaining all client data (2)
2023-11-07 01:48:54,682 __main__ INFO getting data from srv1
2023-11-07 01:48:57,189 __main__ INFO getting data from srv2
2023-11-07 01:48:59,694 __main__ INFO getting data from srv3
2023-11-07 01:49:02,200 __main__ INFO getting data from srv4
2023-11-07 01:49:04,705 __main__ INFO getting data from srv5
2023-11-07 01:49:07,211 __main__ INFO getting data from srv6
2023-11-07 01:49:09,716 __main__ INFO getting data from srv7
2023-11-07 01:49:12,222 __main__ INFO getting data from cli8foo
2023-11-07 01:49:14,727 __main__ INFO obtaining all client data (3)
2023-11-07 01:49:14,728 __main__ INFO getting data from srv1
2023-11-07 01:49:17,234 __main__ INFO getting data from srv2
2023-11-07 01:49:19,739 __main__ INFO getting data from srv3
2023-11-07 01:49:22,245 __main__ INFO getting data from srv4
2023-11-07 01:49:24,751 __main__ INFO getting data from srv5
2023-11-07 01:49:27,256 __main__ INFO getting data from srv6
2023-11-07 01:49:29,762 __main__ INFO getting data from srv7
2023-11-07 01:49:32,267 __main__ INFO getting data from cli8foo
2023-11-07 01:49:34,774 TADA INFO assertion 8, l3-stream by 'root' with 0400 permission: client data verified, passed
2023-11-07 01:49:34,774 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as foo
2023-11-07 01:49:35,277 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as foo
2023-11-07 01:49:35,779 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as foo
2023-11-07 01:49:36,282 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as foo
2023-11-07 01:49:36,784 __main__ INFO obtaining all client data (0)
2023-11-07 01:49:36,784 __main__ INFO getting data from srv1
2023-11-07 01:49:39,291 __main__ INFO getting data from srv2
2023-11-07 01:49:41,797 __main__ INFO getting data from srv3
2023-11-07 01:49:44,303 __main__ INFO getting data from srv4
2023-11-07 01:49:46,810 __main__ INFO getting data from srv5
2023-11-07 01:49:49,316 __main__ INFO getting data from srv6
2023-11-07 01:49:51,823 __main__ INFO getting data from srv7
2023-11-07 01:49:54,329 __main__ INFO getting data from cli8foo
2023-11-07 01:49:56,835 __main__ INFO obtaining all client data (1)
2023-11-07 01:49:56,836 __main__ INFO getting data from srv1
2023-11-07 01:49:59,342 __main__ INFO getting data from srv2
2023-11-07 01:50:01,848 __main__ INFO getting data from srv3
2023-11-07 01:50:04,354 __main__ INFO getting data from srv4
2023-11-07 01:50:06,860 __main__ INFO getting data from srv5
2023-11-07 01:50:09,365 __main__ INFO getting data from srv6
2023-11-07 01:50:11,871 __main__ INFO getting data from srv7
2023-11-07 01:50:14,376 __main__ INFO getting data from cli8foo
2023-11-07 01:50:16,883 __main__ INFO obtaining all client data (2)
2023-11-07 01:50:16,883 __main__ INFO getting data from srv1
2023-11-07 01:50:19,389 __main__ INFO getting data from srv2
2023-11-07 01:50:21,895 __main__ INFO getting data from srv3
2023-11-07 01:50:24,401 __main__ INFO getting data from srv4
2023-11-07 01:50:26,906 __main__ INFO getting data from srv5
2023-11-07 01:50:29,411 __main__ INFO getting data from srv6
2023-11-07 01:50:31,917 __main__ INFO getting data from srv7
2023-11-07 01:50:34,422 __main__ INFO getting data from cli8foo
2023-11-07 01:50:36,928 __main__ INFO obtaining all client data (3)
2023-11-07 01:50:36,929 __main__ INFO getting data from srv1
2023-11-07 01:50:39,435 __main__ INFO getting data from srv2
2023-11-07 01:50:41,941 __main__ INFO getting data from srv3
2023-11-07 01:50:44,446 __main__ INFO getting data from srv4
2023-11-07 01:50:46,952 __main__ INFO getting data from srv5
2023-11-07 01:50:49,457 __main__ INFO getting data from srv6
2023-11-07 01:50:51,963 __main__ INFO getting data from srv7
2023-11-07 01:50:54,469 __main__ INFO getting data from cli8foo
2023-11-07 01:50:56,976 TADA INFO assertion 9, l3-stream by 'root' as 'foo' with 0400 permission: client data verified, passed
2023-11-07 01:50:56,976 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as bar
2023-11-07 01:50:57,478 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as bar
2023-11-07 01:50:57,981 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as bar
2023-11-07 01:50:58,483 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as bar
2023-11-07 01:50:58,986 __main__ INFO obtaining all client data (0)
2023-11-07 01:50:58,986 __main__ INFO getting data from srv1
2023-11-07 01:51:01,492 __main__ INFO getting data from srv2
2023-11-07 01:51:03,999 __main__ INFO getting data from srv3
2023-11-07 01:51:06,505 __main__ INFO getting data from srv4
2023-11-07 01:51:09,011 __main__ INFO getting data from srv5
2023-11-07 01:51:11,517 __main__ INFO getting data from srv6
2023-11-07 01:51:14,023 __main__ INFO getting data from srv7
2023-11-07 01:51:16,530 __main__ INFO getting data from cli8foo
2023-11-07 01:51:19,035 __main__ INFO obtaining all client data (1)
2023-11-07 01:51:19,036 __main__ INFO getting data from srv1
2023-11-07 01:51:21,542 __main__ INFO getting data from srv2
2023-11-07 01:51:24,048 __main__ INFO getting data from srv3
2023-11-07 01:51:26,554 __main__ INFO getting data from srv4
2023-11-07 01:51:29,060 __main__ INFO getting data from srv5
2023-11-07 01:51:31,565 __main__ INFO getting data from srv6
2023-11-07 01:51:34,071 __main__ INFO getting data from srv7
2023-11-07 01:51:36,576 __main__ INFO getting data from cli8foo
2023-11-07 01:51:39,082 __main__ INFO obtaining all client data (2)
2023-11-07 01:51:39,082 __main__ INFO getting data from srv1
2023-11-07 01:51:41,588 __main__ INFO getting data from srv2
2023-11-07 01:51:44,094 __main__ INFO getting data from srv3
2023-11-07 01:51:46,600 __main__ INFO getting data from srv4
2023-11-07 01:51:49,105 __main__ INFO getting data from srv5
2023-11-07 01:51:51,611 __main__ INFO getting data from srv6
2023-11-07 01:51:54,116 __main__ INFO getting data from srv7
2023-11-07 01:51:56,622 __main__ INFO getting data from cli8foo
2023-11-07 01:51:59,127 __main__ INFO obtaining all client data (3)
2023-11-07 01:51:59,128 __main__ INFO getting data from srv1
2023-11-07 01:52:01,634 __main__ INFO getting data from srv2
2023-11-07 01:52:04,139 __main__ INFO getting data from srv3
2023-11-07 01:52:06,645 __main__ INFO getting data from srv4
2023-11-07 01:52:09,150 __main__ INFO getting data from srv5
2023-11-07 01:52:11,656 __main__ INFO getting data from srv6
2023-11-07 01:52:14,162 __main__ INFO getting data from srv7
2023-11-07 01:52:16,667 __main__ INFO getting data from cli8foo
2023-11-07 01:52:19,174 TADA INFO assertion 10, l3-stream by 'root' as 'bar' with 0400 permission: client data verified, passed
2023-11-07 01:52:19,174 __main__ INFO publishing 'four' on l3-stream by pub4 (0440) root as bar
2023-11-07 01:52:19,676 __main__ INFO publishing 'five' on l3-stream by pub5 (0440) root as bar
2023-11-07 01:52:20,179 __main__ INFO publishing 'six' on l3-stream by pub6 (0440) root as bar
2023-11-07 01:52:20,681 __main__ INFO publishing 'seven' on l3-stream by pub7 (0440) root as bar
2023-11-07 01:52:21,184 __main__ INFO obtaining all client data (0)
2023-11-07 01:52:21,184 __main__ INFO getting data from srv1
2023-11-07 01:52:23,690 __main__ INFO getting data from srv2
2023-11-07 01:52:26,197 __main__ INFO getting data from srv3
2023-11-07 01:52:28,703 __main__ INFO getting data from srv4
2023-11-07 01:52:31,209 __main__ INFO getting data from srv5
2023-11-07 01:52:33,715 __main__ INFO getting data from srv6
2023-11-07 01:52:36,221 __main__ INFO getting data from srv7
2023-11-07 01:52:38,728 __main__ INFO getting data from cli8foo
2023-11-07 01:52:41,233 __main__ INFO obtaining all client data (1)
2023-11-07 01:52:41,234 __main__ INFO getting data from srv1
2023-11-07 01:52:43,740 __main__ INFO getting data from srv2
2023-11-07 01:52:46,246 __main__ INFO getting data from srv3
2023-11-07 01:52:48,752 __main__ INFO getting data from srv4
2023-11-07 01:52:51,258 __main__ INFO getting data from srv5
2023-11-07 01:52:53,764 __main__ INFO getting data from srv6
2023-11-07 01:52:56,269 __main__ INFO getting data from srv7
2023-11-07 01:52:58,775 __main__ INFO getting data from cli8foo
2023-11-07 01:53:01,280 __main__ INFO obtaining all client data (2)
2023-11-07 01:53:01,281 __main__ INFO getting data from srv1
2023-11-07 01:53:03,787 __main__ INFO getting data from srv2
2023-11-07 01:53:06,292 __main__ INFO getting data from srv3
2023-11-07 01:53:08,798 __main__ INFO getting data from srv4
2023-11-07 01:53:11,304 __main__ INFO getting data from srv5
2023-11-07 01:53:13,809 __main__ INFO getting data from srv6
2023-11-07 01:53:16,315 __main__ INFO getting data from srv7
2023-11-07 01:53:18,820 __main__ INFO getting data from cli8foo
2023-11-07 01:53:21,326 __main__ INFO obtaining all client data (3)
2023-11-07 01:53:21,326 __main__ INFO getting data from srv1
2023-11-07 01:53:23,833 __main__ INFO getting data from srv2
2023-11-07 01:53:26,338 __main__ INFO getting data from srv3
2023-11-07 01:53:28,843 __main__ INFO getting data from srv4
2023-11-07 01:53:31,349 __main__ INFO getting data from srv5
2023-11-07 01:53:33,854 __main__ INFO getting data from srv6
2023-11-07 01:53:36,360 __main__ INFO getting data from srv7
2023-11-07 01:53:38,866 __main__ INFO getting data from cli8foo
2023-11-07 01:53:41,372 TADA INFO assertion 11, l3-stream by 'root' as 'bar' with 0440 permission: client data verified, passed
2023-11-07 01:53:41,874 TADA INFO assertion 12, l3-stream by 'foo' as 'bar' results in an error: checking..., passed
2023-11-07 01:53:41,874 __main__ INFO publishing 'four' on l3-stream by pub4foo (0440)
2023-11-07 01:53:42,377 __main__ INFO obtaining all client data (0)
2023-11-07 01:53:42,377 __main__ INFO getting data from srv1
2023-11-07 01:53:44,883 __main__ INFO getting data from srv2
2023-11-07 01:53:47,390 __main__ INFO getting data from srv3
2023-11-07 01:53:49,895 __main__ INFO getting data from srv4
2023-11-07 01:53:52,401 __main__ INFO getting data from srv5
2023-11-07 01:53:54,907 __main__ INFO getting data from srv6
2023-11-07 01:53:57,413 __main__ INFO getting data from srv7
2023-11-07 01:53:59,918 __main__ INFO getting data from cli8foo
2023-11-07 01:54:02,425 TADA INFO assertion 13, l3-stream by 'foo' with 0440 permission: client data verified, passed
2023-11-07 01:54:02,425 __main__ INFO publishing 'four' on l3-stream by pub4bar (0440)
2023-11-07 01:54:02,928 __main__ INFO obtaining all client data (0)
2023-11-07 01:54:02,928 __main__ INFO getting data from srv1
2023-11-07 01:54:05,434 __main__ INFO getting data from srv2
2023-11-07 01:54:07,940 __main__ INFO getting data from srv3
2023-11-07 01:54:10,446 __main__ INFO getting data from srv4
2023-11-07 01:54:12,952 __main__ INFO getting data from srv5
2023-11-07 01:54:15,458 __main__ INFO getting data from srv6
2023-11-07 01:54:17,963 __main__ INFO getting data from srv7
2023-11-07 01:54:20,469 __main__ INFO getting data from cli8foo
2023-11-07 01:54:22,975 TADA INFO assertion 14, l3-stream by 'bar' with 0440 permission: client data verified, passed
2023-11-07 01:54:26,483 TADA INFO assertion 15, Blocking client and asynchronous client have the same data: verified, passed
2023-11-07 01:54:27,986 __main__ INFO publishing 'four' on l3-stream by srv4
2023-11-07 01:54:28,488 __main__ INFO obtaining all client data (0)
2023-11-07 01:54:28,489 __main__ INFO getting data from srv1
2023-11-07 01:54:30,995 __main__ INFO getting data from srv2
2023-11-07 01:54:33,501 __main__ INFO getting data from srv3
2023-11-07 01:54:36,007 __main__ INFO getting data from srv4
2023-11-07 01:54:38,513 __main__ INFO getting data from srv5
2023-11-07 01:54:41,019 __main__ INFO getting data from srv6
2023-11-07 01:54:43,524 __main__ INFO getting data from srv7
2023-11-07 01:54:46,030 __main__ INFO getting data from cli8foo
2023-11-07 01:54:48,537 TADA INFO assertion 20, l3-stream publish from L1 (srv4): client data verified, passed
2023-11-07 01:54:48,537 __main__ INFO publishing 'four' on nada by srv4
2023-11-07 01:54:49,038 __main__ INFO obtaining all client data (0)
2023-11-07 01:54:49,039 __main__ INFO getting data from srv1
2023-11-07 01:54:51,544 __main__ INFO getting data from srv2
2023-11-07 01:54:54,050 __main__ INFO getting data from srv3
2023-11-07 01:54:56,555 __main__ INFO getting data from srv4
2023-11-07 01:54:59,061 __main__ INFO getting data from srv5
2023-11-07 01:55:01,567 __main__ INFO getting data from srv6
2023-11-07 01:55:04,072 __main__ INFO getting data from srv7
2023-11-07 01:55:06,578 __main__ INFO getting data from cli8foo
2023-11-07 01:55:09,084 TADA INFO assertion 21, nada publish from L1 (srv4): client data verified, passed
2023-11-07 01:55:12,678 TADA INFO assertion 22, Check stream stats in each process: verified, passed
2023-11-07 01:55:16,288 TADA INFO assertion 23, Check stream client stats in each process: verified, passed
2023-11-07 01:55:18,792 TADA INFO assertion 16, srv-6 clean up properly after srv-3 exited: checking..., passed
2023-11-07 01:55:18,793 TADA INFO assertion 17, srv-7 clean up properly after srv-3 exited: checking..., passed
2023-11-07 01:55:18,793 TADA INFO assertion 18, srv-1 clean up properly after srv-3 exited: checking..., passed
2023-11-07 01:55:18,793 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-3 
2023-11-07 01:55:22,312 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-11-07 01:55:22,815 __main__ INFO obtaining all client data (0)
2023-11-07 01:55:22,815 __main__ INFO getting data from srv1
2023-11-07 01:55:25,321 __main__ INFO getting data from srv2
2023-11-07 01:55:27,827 __main__ INFO getting data from srv3
2023-11-07 01:55:30,333 __main__ INFO getting data from srv4
2023-11-07 01:55:32,839 __main__ INFO getting data from srv5
2023-11-07 01:55:35,345 __main__ INFO getting data from srv6
2023-11-07 01:55:37,850 __main__ INFO getting data from srv7
2023-11-07 01:55:40,357 __main__ INFO getting data from cli8foo
2023-11-07 01:55:42,863 TADA INFO assertion 19, l3-stream successfully delivered after srv-3 restarted: client data verified, passed
2023-11-07 01:55:42,864 TADA INFO test ldms_stream_test ended
2023-11-07 01:55:58 INFO: ----------------------------------------------
2023-11-07 01:55:59 INFO: ======== set_sec_mod_test ========
2023-11-07 01:55:59 INFO: CMD: python3 set_sec_mod_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/set_sec_mod_test
2023-11-07 01:56:00,407 TADA INFO starting test `set_sec_test`
2023-11-07 01:56:00,407 TADA INFO   test-id: 7e2bd7221afb772f282fb9ab73c59e3f810a9567cae387cbf783b1249a206370
2023-11-07 01:56:00,407 TADA INFO   test-suite: LDMSD
2023-11-07 01:56:00,407 TADA INFO   test-name: set_sec_test
2023-11-07 01:56:00,407 TADA INFO   test-user: narate
2023-11-07 01:56:00,407 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:56:00,408 __main__ INFO ---Get or create the cluster ---
2023-11-07 01:56:13,536 __main__ INFO --- Start daemons ---
2023-11-07 01:56:27,198 TADA INFO assertion 1, Change UID to an existing username: {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-07 01:56:28,441 TADA INFO assertion 2, Change UID to a not-existing username: errcode (22) == expected (22), passed
2023-11-07 01:56:29,883 TADA INFO assertion 3, Change UID to a valid UID: {'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}} == {'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-07 01:56:31,112 TADA INFO assertion 4, Change UID to an invalid UID: errcode (22) == expected (22), passed
2023-11-07 01:56:32,549 TADA INFO assertion 5, Change GID to an existing groupname: {'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}} == {'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}}, passed
2023-11-07 01:56:33,765 TADA INFO assertion 6, Change GID to a not-existing groupname: errcode (22) == expected (22), passed
2023-11-07 01:56:35,212 TADA INFO assertion 7, Change GID to a valid GID: {'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}} == {'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}}, passed
2023-11-07 01:56:36,435 TADA INFO assertion 8, Change GID to an invalid GID: errcode (22) == expected (22), passed
2023-11-07 01:56:37,875 TADA INFO assertion 9, Change permission bits to a valid permission value: {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}} == {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}}, passed
2023-11-07 01:56:39,099 TADA INFO assertion 10, Change permission bits to an invalid permission value: errcode (22) == expected (22), passed
2023-11-07 01:56:39,319 TADA INFO assertion 11, Verify that the aggregator got sets' new security info: {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-07 01:56:39,538 TADA INFO assertion 12.1, Clients with different UID and the same GID cannot access 0400 sets.: {'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-07 01:56:39,746 TADA INFO assertion 12.2, Clients with different UID and GID cannot access 04## sets.: {} == {}, passed
2023-11-07 01:56:39,747 __main__ INFO --- done ---
2023-11-07 01:56:39,747 TADA INFO test set_sec_test ended
2023-11-07 01:56:51 INFO: ----------------------------------------------
2023-11-07 01:56:52 INFO: ======== dump_cfg_test ========
2023-11-07 01:56:52 INFO: CMD: python3 dump_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/dump_cfg_test
2023-11-07 01:56:52,992 TADA INFO starting test `dump_cfg_test`
2023-11-07 01:56:52,992 TADA INFO   test-id: 412eefce6b9ba34b7ff3d7ff810137abb91c9b8e9fec42528e09893970f5a00a
2023-11-07 01:56:52,992 TADA INFO   test-suite: LDMSD
2023-11-07 01:56:52,992 TADA INFO   test-name: dump_cfg_test
2023-11-07 01:56:52,992 TADA INFO   test-user: narate
2023-11-07 01:56:52,992 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:56:52,993 __main__ INFO -- Get or create the cluster --
2023-11-07 01:57:12,296 __main__ INFO -- Start daemons --
2023-11-07 01:57:35,457 __main__ INFO -- Begin the test --
2023-11-07 01:57:41,056 TADA INFO assertion 1.1, Specify the command-line options: The generated configuration is as expected., passed
2023-11-07 01:57:46,787 TADA INFO assertion 1.2, Specify host at the command-line: The generated configuration is as expected., passed
2023-11-07 01:57:52,494 TADA INFO assertion 1.3, Specify auth_opt at the command-line: The generated configuration is as expected., passed
2023-11-07 01:57:58,321 TADA INFO assertion 2.1, Specify the command-line options in a configuration file: The generated configuration is as expected., passed
2023-11-07 01:58:04,155 TADA INFO assertion 3.1, Sampler configuration commands: The generated configuration is as expected., passed
2023-11-07 01:58:09,963 TADA INFO assertion 3.2, Sampler configuration commands with plugin-specific attributes: The generated configuration is as expected., passed
2023-11-07 01:58:15,895 TADA INFO assertion 4.1, Simple aggregator configuration commands: The generated configuration is as expected., passed
2023-11-07 01:58:21,685 TADA INFO assertion 5.1, prdcr_subscribe configuration commands: The generated configuration is as expected., passed
2023-11-07 01:58:21,809 __main__ INFO --- done ---
2023-11-07 01:58:21,809 TADA INFO test dump_cfg_test ended
2023-11-07 01:58:34 INFO: ----------------------------------------------
2023-11-07 01:58:35 INFO: ======== ldmsd_stream_rate_test ========
2023-11-07 01:58:35 INFO: CMD: python3 ldmsd_stream_rate_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldmsd_stream_rate_test
2023-11-07 01:58:36,500 TADA INFO starting test `ldmsd_stream_rate_test`
2023-11-07 01:58:36,501 TADA INFO   test-id: a189ffe11416dd49b602dc3211968d8b31c807d820ad8cab0e1bdb217045e9f4
2023-11-07 01:58:36,501 TADA INFO   test-suite: LDMSD
2023-11-07 01:58:36,501 TADA INFO   test-name: ldmsd_stream_rate_test
2023-11-07 01:58:36,501 TADA INFO   test-user: narate
2023-11-07 01:58:36,501 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:58:36,501 __main__ INFO -- Get or create the cluster --
2023-11-07 01:58:49,643 __main__ INFO -- Start daemons --
2023-11-07 01:59:00,356 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:59:02,358 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_rate_test-32d5252-samp 
2023-11-07 01:59:05,376 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_rate_test-32d5252-agg-1 
2023-11-07 01:59:18,909 TADA INFO assertion 1, Sampler cannot publish all data (prdcr rate limit): received stream data 34 is limited by prdcr rx_rate, passed
2023-11-07 01:59:18,909 TADA INFO assertion 2, After the wait, the sampler can publish: stream data received, passed
2023-11-07 01:59:18,910 TADA INFO assertion 3, Sampler cannot publish all data (stream rate limit): received stream data 34 is limited by stream rx_rate, passed
2023-11-07 01:59:18,910 TADA INFO assertion 4, After the wait, the sampler can publish: stream data received, passed
2023-11-07 01:59:18,910 TADA INFO test ldmsd_stream_rate_test ended
2023-11-07 01:59:30 INFO: ----------------------------------------------
2023-11-07 01:59:31 INFO: ======== ldms_rate_test ========
2023-11-07 01:59:31 INFO: CMD: python3 ldms_rate_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_rate_test
2023-11-07 01:59:32,019 TADA INFO starting test `ldms_rate_test`
2023-11-07 01:59:32,019 TADA INFO   test-id: cb9e9f925f56290e0c2c49b612e55b6140cf3b37330ad8f361326afb1e0bf6ec
2023-11-07 01:59:32,019 TADA INFO   test-suite: LDMSD
2023-11-07 01:59:32,019 TADA INFO   test-name: ldms_rate_test
2023-11-07 01:59:32,019 TADA INFO   test-user: narate
2023-11-07 01:59:32,019 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 01:59:32,020 __main__ INFO -- Get or create the cluster --
2023-11-07 01:59:42,112 __main__ INFO -- Start daemons --
2023-11-07 01:59:42,984 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 01:59:44,986 root INFO starting /tada-src/python/ldms_rate.py on narate-ldms_rate_test-32d5252-node-1 
2023-11-07 01:59:48,004 root INFO starting /tada-src/python/ldms_rate.py on narate-ldms_rate_test-32d5252-node-1 
2023-11-07 02:00:22,983 TADA INFO assertion 1, Publisher cannot publish all data (rail rate limit): received stream data is limited, passed
2023-11-07 02:00:23,486 TADA INFO assertion 2, Publisher get a rate limit error: publisher got rate limit errors, passed
2023-11-07 02:00:23,486 TADA INFO assertion 3, After the wait, the publisher can publish: stream data received, passed
2023-11-07 02:00:23,486 TADA INFO assertion 4, Publisher cannot publish all data (stream rate limit): received stream data is limited, passed
2023-11-07 02:00:23,988 TADA INFO assertion 5, Publisher get a rate limit error (by stream): publisher got rate limit errors, passed
2023-11-07 02:00:23,989 TADA INFO assertion 6, After the wait, the publisher can publish (by stream): stream data received, passed
2023-11-07 02:00:23,989 TADA INFO test ldms_rate_test ended
2023-11-07 02:00:35 INFO: ----------------------------------------------
2023-11-07 02:00:35 INFO: ======== ldms_ipv6_test ========
2023-11-07 02:00:35 INFO: CMD: python3 ldms_ipv6_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-07-003001/data/ldms_ipv6_test
2023-11-07 02:00:36,609 TADA INFO starting test `ldms_ipv6_test`
2023-11-07 02:00:36,609 TADA INFO   test-id: 3c30a173b51b6ed0276504e0f4cd097d94483c7d2c39a67f3ad62b7e6f624908
2023-11-07 02:00:36,609 TADA INFO   test-suite: LDMSD
2023-11-07 02:00:36,609 TADA INFO   test-name: ldms_ipv6_test
2023-11-07 02:00:36,609 TADA INFO   test-user: narate
2023-11-07 02:00:36,609 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-07 02:00:36,610 __main__ INFO -- Get or create the cluster --
2023-11-07 02:00:53,781 __main__ INFO -- Start daemons --
2023-11-07 02:01:20,310 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-07 02:01:22,313 root INFO starting /tada-src/python/pypubsub.py on narate-ldms_ipv6_test-32d5252-samp 
2023-11-07 02:01:25,332 root INFO starting /tada-src/python/pypubsub.py on narate-ldms_ipv6_test-32d5252-agg-2 
2023-11-07 02:01:30,485 TADA INFO assertion 1, ldms_ls to samp using IPv6: expecting ['fd00:0:0:1::5'], got ['fd00:0:0:1::5'], passed
2023-11-07 02:01:30,601 TADA INFO assertion 2, ldms_ls to agg-2 using IPv6: expecting ['fd00:0:0:1::5'], got ['fd00:0:0:1::5'], passed
2023-11-07 02:01:30,602 TADA INFO assertion 3, ldms_ls to agg-2 contains 'samp/meminfo': samp/meminfo found, passed
2023-11-07 02:01:33,606 TADA INFO assertion 4, python stream publish using IPv6: verified, passed
2023-11-07 02:01:36,611 TADA INFO assertion 5, python stream subscribe using IPv6: verified, passed
2023-11-07 02:01:37,113 TADA INFO assertion 6, steam data contain IPv6 addressing: expecting fd00:0:0:1::5, got fd00:0:0:1::5, passed
2023-11-07 02:01:37,389 TADA INFO assertion 7, stream stats reported IPv6 addresses: Expecting fd00:0:0:1::3, got fd00:0:0:1::3, passed
2023-11-07 02:01:37,390 TADA INFO test ldms_ipv6_test ended
2023-11-07 02:01:56 INFO: ----------------------------------------------
2023-11-07 02:01:56 INFO: ======== test-ldms ========
2023-11-07 02:01:56 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-11-07T02:01:56-06:00 INFO: starting test-samp-1
351d2626a939c5e7d76236b13919b974e203da5a86883307ec3b549a774a398d
2023-11-07T02:01:59-06:00 INFO: starting test-samp-2
0fbdb2c0505ea0cfbe44d399a6549138264fa29f93bdb612be75d43f9c113d51
2023-11-07T02:02:01-06:00 INFO: starting test-samp-3
dd5132f8d3f858c319c211b0bc1431711333c886a39c3ed8b8e416970bc09402
2023-11-07T02:02:02-06:00 INFO: starting test-samp-4
a8357c11b520f96cb2fffe315856488eabad782535ee6fe1114381da1287e0ba
2023-11-07T02:02:04-06:00 INFO: test-samp-1 is running
2023-11-07T02:02:04-06:00 INFO: test-samp-2 is running
2023-11-07T02:02:04-06:00 INFO: test-samp-3 is running
2023-11-07T02:02:04-06:00 INFO: test-samp-4 is running
2023-11-07T02:02:04-06:00 INFO: starting test-agg-11
ea6cd33869c08c9e6bf975dc1fbcb82bd66525a092b4abb3965b51dad62d6ad9
2023-11-07T02:02:06-06:00 INFO: starting test-agg-12
323747ecacd738e7d15904d0051503085686106b5a82b695e3298af3fc785775
2023-11-07T02:02:08-06:00 INFO: test-agg-11 is running
2023-11-07T02:02:08-06:00 INFO: test-agg-12 is running
2023-11-07T02:02:08-06:00 INFO: starting test-agg-2
dbc1da7bc1f26f6337aa9e04084b9c07586f8b2d025dc3f0ddef4acd0699b979
2023-11-07T02:02:10-06:00 INFO: test-agg-2 is running
2023-11-07T02:02:10-06:00 INFO: Collecting data (into SOS)
2023-11-07T02:02:20-06:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-11-07T02:02:22-06:00 INFO: check rc: 0
2023-11-07T02:02:22-06:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-11-07T02:02:26-06:00 INFO: DONE
2023-11-07 02:02:36 INFO: ----------------------------------------------
2023-11-07 02:02:36 INFO: ======== test-maestro ========
2023-11-07 02:02:36 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-11-07T02:02:36-06:00 INFO: starting mtest-maestro
db66ce1b3b5a7f19677a069d9d59642e4ae7951b402080bd3fe17a01d6841e72
2023-11-07T02:02:39-06:00 INFO: starting mtest-samp-1
12c4b80b716262242fe9bd283537140a8fb7bcd003cc77e2b6b001efa8c93eca
2023-11-07T02:02:40-06:00 INFO: starting mtest-samp-2
07b3927471e5a1309ecb7c01e2fc82948d601b9003fd7b43315367d756ddc162
2023-11-07T02:02:41-06:00 INFO: starting mtest-samp-3
791a2d486e62ee41227ce8facf4c6c1058a20d2b43dab902226881b533cd998a
2023-11-07T02:02:43-06:00 INFO: starting mtest-samp-4
1a188ff40f178abe8dfd9209dabfcf6fdd60a56234c58a7e5ebdda49c69a1a1f
2023-11-07T02:02:44-06:00 INFO: mtest-samp-1 is running
2023-11-07T02:02:44-06:00 INFO: mtest-samp-2 is running
2023-11-07T02:02:44-06:00 INFO: mtest-samp-3 is running
2023-11-07T02:02:45-06:00 INFO: mtest-samp-4 is running
2023-11-07T02:02:45-06:00 INFO: starting mtest-agg-11
1094a7546c5de084480ac5330597537642e7a452a609514b5e38dad66b6d1776
2023-11-07T02:02:46-06:00 INFO: starting mtest-agg-12
bc26a1ca294ccc2996b2680b525afeb0bec8bdc71706f2832633ca7a271e38b7
2023-11-07T02:02:47-06:00 INFO: mtest-agg-11 is running
2023-11-07T02:02:47-06:00 INFO: mtest-agg-12 is running
2023-11-07T02:02:47-06:00 INFO: starting mtest-agg-2
4addd40e2f61a4744ebc4e5a772a7cd5e377a594672853ba9d68b44963e1eeb4
2023-11-07T02:02:49-06:00 INFO: mtest-agg-2 is running
2023-11-07T02:02:49-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-07T02:04:49-06:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-07T02:04:51-06:00 INFO: sos check rc: 1
2023-11-07T02:04:51-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-07T02:04:55-06:00 INFO: DONE
2023-11-07 02:05:05 INFO: ----------------------------------------------
2023-11-07 02:05:05 INFO: ======== test-maestro-hostmunge ========
2023-11-07 02:05:05 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-11-07T02:05:05-06:00 INFO: Checking munge on localhost
2023-11-07T02:05:05-06:00 INFO: munge encode/decode successfully
2023-11-07T02:05:05-06:00 INFO: starting mtest-maestro
ae65b3b260ac54a895dfd96c2a7137504c7620cd7c20d3000993ba6778f25afb
2023-11-07T02:05:07-06:00 INFO: starting mtest-samp-1
109c50f487aae6109ed572152abf317df04d87d2364272a3223648d9d4d991e9
2023-11-07T02:05:09-06:00 INFO: starting mtest-samp-2
62001bb1c0dcfe3dd4808a77729ca76d85facd7e733260e5dae657a13aae63c8
2023-11-07T02:05:10-06:00 INFO: starting mtest-samp-3
883ac7755000827e581f033801d6f05dbd21926fa9ef8a3f8d77ac69f5a36793
2023-11-07T02:05:12-06:00 INFO: starting mtest-samp-4
f0fa94c1c0aebc9456a6ccddfc85825da278f1e8a012551c83239dddc54d8d77
2023-11-07T02:05:13-06:00 INFO: mtest-samp-1 is running
2023-11-07T02:05:13-06:00 INFO: mtest-samp-2 is running
2023-11-07T02:05:13-06:00 INFO: mtest-samp-3 is running
2023-11-07T02:05:13-06:00 INFO: mtest-samp-4 is running
2023-11-07T02:05:13-06:00 INFO: starting mtest-agg-11
512f8f046961e950dea61face1e58844ec8023efbdfb12a10e9814cfdeea8329
2023-11-07T02:05:14-06:00 INFO: starting mtest-agg-12
ebd1afad84a849598d2544bacd5e2f9cdbeab9653b26583aea3941e0a3b4a2a9
2023-11-07T02:05:16-06:00 INFO: mtest-agg-11 is running
2023-11-07T02:05:16-06:00 INFO: mtest-agg-12 is running
2023-11-07T02:05:16-06:00 INFO: starting mtest-agg-2
877dbc48baf858515faae544c49a50777a8e24f763ef8a3b7e10044d1c308338
2023-11-07T02:05:17-06:00 INFO: mtest-agg-2 is running
2023-11-07T02:05:17-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-07T02:07:18-06:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-07T02:07:20-06:00 INFO: sos check rc: 1
2023-11-07T02:07:20-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-07T02:07:23-06:00 INFO: DONE
2023-11-07 02:07:33 INFO: ----------------------------------------------
2023-11-07 02:07:33 INFO: ======== test-maestro-munge ========
2023-11-07 02:07:33 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000234786 s, 17.4 MB/s
2023-11-07T02:07:35-06:00 INFO: starting mtest-maestro
d47caf3d4a16fbdbcdf3cfda8870889da8b5bdfa956d8dacee91288555170eec
2023-11-07T02:07:37-06:00 INFO: starting mtest-samp-1
c8c7c5ea01246fbbac595827bb3e9d09cf8786a5c7aff4a6f08ad39c113ec1a8
2023-11-07T02:07:38-06:00 INFO: starting mtest-samp-2
c22ccd02306944be68bcb29065c24c71ba01d8b92d9ae7af303bd83d356199ab
2023-11-07T02:07:40-06:00 INFO: starting mtest-samp-3
b9cc56ed597cfafec987997fbcd81d8d0ae8b424ed0ff3f44af7f7908d7a620c
2023-11-07T02:07:41-06:00 INFO: starting mtest-samp-4
9070b0614f56ed8c494e50fde2cbe19345e29b1b5e8ee8bde9c0a0facd0adb14
2023-11-07T02:07:42-06:00 INFO: mtest-samp-1 is running
2023-11-07T02:07:42-06:00 INFO: mtest-samp-2 is running
2023-11-07T02:07:43-06:00 INFO: mtest-samp-3 is running
2023-11-07T02:07:43-06:00 INFO: mtest-samp-4 is running
2023-11-07T02:07:43-06:00 INFO: starting mtest-agg-11
77e74fc740cc66a521f1a39fa2ccf51a6de5510a9c2bfdc908c3067d9e998b4c
2023-11-07T02:07:44-06:00 INFO: starting mtest-agg-12
1d56e91692a009991a0c94da49dc6b079b9629c46f7989e09b10f8dbc926634c
2023-11-07T02:07:45-06:00 INFO: mtest-agg-11 is running
2023-11-07T02:07:45-06:00 INFO: mtest-agg-12 is running
2023-11-07T02:07:45-06:00 INFO: starting mtest-agg-2
a6c627b6b48883f1b182aa9ba7987e5039befc528bbff8d0f3087b1df0ee2ec7
2023-11-07T02:07:47-06:00 INFO: mtest-agg-2 is running
2023-11-07T02:07:47-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-07T02:09:47-06:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-07T02:09:49-06:00 INFO: sos check rc: 1
2023-11-07T02:09:49-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-07T02:09:53-06:00 INFO: DONE
2023-11-07 02:10:03 INFO: ----------------------------------------------
2023-11-07 02:10:03 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;32mPASSED[0m
papi_store_test: [01;32mPASSED[0m
updtr_status_test: [01;32mPASSED[0m
ldms_stream_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;31mFAILED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;32mPASSED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;31mFAILED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;32mPASSED[0m
agg_slurm_test: [01;31mFAILED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;32mPASSED[0m
ldmsd_auth_test: [01;32mPASSED[0m
updtr_stop_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;31mFAILED[0m
cont-test-ldms: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;31mFAILED[0m
ldmsd_long_config_test: [01;31mFAILED[0m
maestro_raft_test: [01;31mFAILED[0m
dump_cfg_test: [01;32mPASSED[0m
ldmsd_stream_test2: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
libovis_log_test: [01;32mPASSED[0m
set_sec_mod_test: [01;32mPASSED[0m
cont-test-maestro: [01;31mFAILED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_stream_status_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
ldms_ipv6_test: [01;32mPASSED[0m
ldmsd_stream_rate_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
ldms_rail_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
ldms_rate_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;32mPASSED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 49/57
------------------------------------------
