2023-03-07 14:30:24 INFO: WORK_DIR: /mnt/300G/data/2023-03-07-143022
2023-03-07 14:30:24 INFO: LOG: /mnt/300G/data/2023-03-07-143022/cygnus-weekly.log
~/cron/ldms-test ~/cron/ldms-test
/mnt/300G/data/2023-03-07-143022 ~/cron/ldms-test ~/cron/ldms-test
2023-03-07 14:30:25 INFO: Skip building on host because GIT SHA has not changed: 661e35a010a7de2ebce0e7918406804bd1fbd726
661e35a010a7de2ebce0e7918406804bd1fbd726
OVIS_LDMS_OVIS_GIT_LONG "661e35a010a7de2ebce0e7918406804bd1fbd726"
2023-03-07 14:30:25 INFO: Skip building containerized binary because GIT SHA has not changed: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:30:25 INFO: -- Installation process succeeded --
2023-03-07 14:30:25 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-03-07-143022
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-03-07-143022
HEAD is now at 6e7a7c8 2023-03-07-141255
[master 4d7b9ea] 2023-03-07-143022
 1 file changed, 13 insertions(+), 14 deletions(-)
 rewrite test-all.log (61%)
To github.com:ldms-test/weekly-report
   6e7a7c8..4d7b9ea  master -> master
~/cron/ldms-test /mnt/300G/data/2023-03-07-143022
2023-03-07 14:30:27 INFO: ==== OVIS+SOS Installation Completed ====
2023-03-07 14:30:27 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-03-07-143022 ~/cron/ldms-test ~/cron/ldms-test
2023-03-07 14:30:27 INFO: ======== direct_ldms_ls_conn_test ========
2023-03-07 14:30:27 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/direct_ldms_ls_conn_test
2023-03-07 14:30:27,808 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-03-07 14:30:27,808 TADA INFO   test-id: 47b91f723268c493b48029c88acde005289ca6423f1c69934e615120068c5575
2023-03-07 14:30:27,808 TADA INFO   test-suite: LDMSD
2023-03-07 14:30:27,808 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-03-07 14:30:27,808 TADA INFO   test-user: narate
2023-03-07 14:30:27,808 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:30:28,030 __main__ INFO starting munged on cygnus-01-iw
2023-03-07 14:30:28,366 __main__ INFO starting munged on localhost
2023-03-07 14:30:28,600 __main__ INFO starting ldmsd on cygnus-01-iw
2023-03-07 14:30:28,913 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-03-07 14:30:34,106 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-03-07 14:30:34,106 __main__ INFO Stopping sampler daemon ...
2023-03-07 14:30:39,562 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-03-07 14:30:39,598 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-03-07 14:30:39,637 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-03-07 14:30:39,638 TADA INFO test direct_ldms_ls_conn_test ended
2023-03-07 14:30:39,842 __main__ INFO stopping munged on cygnus-01-iw
2023-03-07 14:30:40,246 __main__ INFO stopping munged on localhost
2023-03-07 14:30:40 INFO: ----------------------------------------------
2023-03-07 14:30:40 INFO: ======== direct_prdcr_subscribe_test ========
2023-03-07 14:30:40 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/direct_prdcr_subscribe_test
2023-03-07 14:30:41,076 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-03-07 14:30:41,076 TADA INFO   test-id: ea5031e6430c2b9929df5f64d41ad7a4b77ee8100716176526107024204eb991
2023-03-07 14:30:41,076 TADA INFO   test-suite: LDMSD
2023-03-07 14:30:41,076 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-03-07 14:30:41,076 TADA INFO   test-user: narate
2023-03-07 14:30:41,076 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:30:43,001 __main__ INFO starting munged on cygnus-01-iw
2023-03-07 14:30:43,766 __main__ INFO starting munged on cygnus-02-iw
2023-03-07 14:30:44,537 __main__ INFO starting munged on cygnus-03-iw
2023-03-07 14:30:45,302 __main__ INFO starting munged on cygnus-04-iw
2023-03-07 14:30:45,625 __main__ INFO starting munged on localhost
2023-03-07 14:30:45,865 __main__ INFO starting ldmsd on cygnus-01-iw
2023-03-07 14:30:46,370 __main__ INFO starting ldmsd on cygnus-02-iw
2023-03-07 14:30:46,848 __main__ INFO starting ldmsd on cygnus-03-iw
2023-03-07 14:30:47,358 __main__ INFO starting ldmsd on cygnus-04-iw
2023-03-07 14:30:54,203 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-03-07 14:30:54,204 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-03-07 14:30:54,204 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-03-07 14:30:54,205 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-03-07 14:30:54,205 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
Traceback (most recent call last):
  File "direct_prdcr_subscribe_test", line 524, in <module>
    ret = agg1.req("prdcr_stop_regex regex=.*")
  File "/home/narate/cron/ldms-test/LDMS_Test.py", line 2457, in req
    from ldmsd.ldmsd_request import LDMSD_Request, LDMSD_Req_Attr
ModuleNotFoundError: No module named 'ldmsd.ldmsd_request'
2023-03-07 14:30:54,246 TADA INFO assertion 5, Stopping the producers succeeds: skipped
2023-03-07 14:30:54,246 TADA INFO assertion 6, Restarting the producers succeeds: skipped
2023-03-07 14:30:54,247 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: skipped
2023-03-07 14:30:54,247 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: skipped
2023-03-07 14:30:54,247 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: skipped
2023-03-07 14:30:54,247 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: skipped
2023-03-07 14:30:54,248 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: skipped
2023-03-07 14:30:54,248 TADA INFO assertion 12, stream-sampler-1 is not running: skipped
2023-03-07 14:30:54,248 TADA INFO assertion 13, stream-sampler-1 has restarted: skipped
2023-03-07 14:30:54,248 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 14:30:54,248 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 14:30:54,249 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: skipped
2023-03-07 14:30:54,249 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: skipped
2023-03-07 14:30:54,249 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: skipped
2023-03-07 14:30:54,249 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: skipped
2023-03-07 14:30:54,249 TADA INFO test direct_prdcr_subscribe_test ended
2023-03-07 14:30:54,471 __main__ INFO stopping munged on cygnus-01-iw
2023-03-07 14:30:54,903 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-03-07 14:30:55,319 __main__ INFO stopping munged on cygnus-02-iw
2023-03-07 14:30:55,734 __main__ INFO stopping ldmsd on cygnus-02-iw
2023-03-07 14:30:56,151 __main__ INFO stopping munged on cygnus-03-iw
2023-03-07 14:30:56,582 __main__ INFO stopping ldmsd on cygnus-03-iw
2023-03-07 14:30:57,007 __main__ INFO stopping munged on cygnus-04-iw
2023-03-07 14:30:57,430 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-03-07 14:30:57,644 __main__ INFO stopping munged on localhost
2023-03-07 14:30:57 INFO: ----------------------------------------------
2023-03-07 14:30:57 INFO: ======== agg_slurm_test ========
2023-03-07 14:30:57 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/agg_slurm_test
2023-03-07 14:30:58,537 TADA INFO starting test `agg_slurm_test`
2023-03-07 14:30:58,537 TADA INFO   test-id: bd64ba63be9438f4cca31829b612f12dba80f0919ec826b3fdde29326642eeca
2023-03-07 14:30:58,537 TADA INFO   test-suite: LDMSD
2023-03-07 14:30:58,538 TADA INFO   test-name: agg_slurm_test
2023-03-07 14:30:58,538 TADA INFO   test-user: narate
2023-03-07 14:30:58,538 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:30:58,539 __main__ INFO -- Get or create the cluster --
2023-03-07 14:31:12,623 __main__ INFO -- Preparing syspapi JSON file --
2023-03-07 14:31:12,717 __main__ INFO -- Preparing jobpapi JSON file --
2023-03-07 14:31:12,828 __main__ INFO -- Preparing job script & programs --
2023-03-07 14:31:14,128 __main__ INFO -- Start daemons --
2023-03-07 14:31:26,613 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:31:31,614 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 14:31:31,740 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-03-07 14:31:31,848 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-03-07 14:31:36,850 __main__ INFO -- Submitting jobs --
2023-03-07 14:31:36,981 __main__ INFO job_one: 1
2023-03-07 14:31:37,100 __main__ INFO job_two: 2
2023-03-07 14:31:47,110 __main__ INFO -- Cancelling jobs --
2023-03-07 14:31:47,111 __main__ INFO job_one: 1
2023-03-07 14:31:47,235 __main__ INFO job_two: 2
2023-03-07 14:32:59,223 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-03-07 14:32:59,224 TADA INFO assertion 3, meminfo data verification: No data missing, passed
2023-03-07 14:32:59,225 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: No data missing, passed
2023-03-07 14:32:59,226 TADA INFO test agg_slurm_test ended
2023-03-07 14:33:13 INFO: ----------------------------------------------
2023-03-07 14:33:14 INFO: ======== papi_sampler_test ========
2023-03-07 14:33:14 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/papi_sampler_test
2023-03-07 14:33:15,042 TADA INFO starting test `papi_sampler_test`
2023-03-07 14:33:15,043 TADA INFO   test-id: 75ea0a3ea029bd760555e4351bab4f2a53d7c9090660d066d93efe2c75713570
2023-03-07 14:33:15,043 TADA INFO   test-suite: LDMSD
2023-03-07 14:33:15,043 TADA INFO   test-name: papi_sampler_test
2023-03-07 14:33:15,043 TADA INFO   test-user: narate
2023-03-07 14:33:15,043 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:33:15,044 __main__ INFO -- Get or create the cluster --
2023-03-07 14:33:20,424 __main__ INFO -- Start daemons --
2023-03-07 14:33:30,494 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-03-07 14:33:30,721 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-03-07 14:33:35,846 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-03-07 14:33:36,019 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-03-07 14:33:36,019 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-03-07 14:33:49,892 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-03-07 14:33:49,892 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-03-07 14:33:49,892 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-03-07 14:33:49,892 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-03-07 14:33:50,100 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-03-07 14:33:55,926 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-03-07 14:33:55,927 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-03-07 14:33:55,927 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-03-07 14:33:55,927 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-03-07 14:33:56,145 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-03-07 14:33:56,146 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/papi0/2.0', 'node-1/meminfo', 'node-1/papi1/3.0'}), passed
2023-03-07 14:34:06,679 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-03-07 14:34:47,020 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-03-07 14:34:49,379 TADA INFO assertion 8, Missing config file attribute is logged: : papi_sampler[519]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-03-07 14:34:54,851 TADA INFO assertion 9, Bad config file is logged: : papi_sampler: configuration file syntax error., passed
2023-03-07 14:34:54,851 __main__ INFO -- Finishing Test --
2023-03-07 14:34:54,851 TADA INFO test papi_sampler_test ended
2023-03-07 14:34:54,851 __main__ INFO -- Cleaning up files --
2023-03-07 14:34:54,852 __main__ INFO -- Removing the virtual cluster --
2023-03-07 14:35:06 INFO: ----------------------------------------------
2023-03-07 14:35:07 INFO: ======== papi_store_test ========
2023-03-07 14:35:07 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/papi_store_test
2023-03-07 14:35:07,905 TADA INFO starting test `papi_store_test`
2023-03-07 14:35:07,905 TADA INFO   test-id: b83b3c0c31e3c0165c5c996464b0bdefd83e4f60e2157e024292db910d7feda8
2023-03-07 14:35:07,905 TADA INFO   test-suite: LDMSD
2023-03-07 14:35:07,906 TADA INFO   test-name: papi_store_test
2023-03-07 14:35:07,906 TADA INFO   test-user: narate
2023-03-07 14:35:07,906 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:35:07,906 __main__ INFO -- Get or create the cluster --
2023-03-07 14:35:15,561 __main__ INFO -- Start daemons --
2023-03-07 14:35:48,295 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-03-07 14:35:48,296 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-03-07 14:35:48,296 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-03-07 14:35:48,296 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: missing tasks {(4, 5), (4, 7), (4, 4), (4, 6)}, excess tasks set(), failed
Traceback (most recent call last):
  File "papi_store_test", line 354, in <module>
    .format(missing, excess))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Multi-tenant PAPI store test., missing tasks {(4, 5), (4, 7), (4, 4), (4, 6)}, excess tasks set(): FAILED
2023-03-07 14:35:48,297 TADA INFO test papi_store_test ended
2023-03-07 14:36:00 INFO: ----------------------------------------------
2023-03-07 14:36:01 INFO: ======== store_app_test ========
2023-03-07 14:36:01 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/store_app_test
2023-03-07 14:36:02,283 TADA INFO starting test `store_app_test`
2023-03-07 14:36:02,283 TADA INFO   test-id: b483443ca391550f9eac11528b8c94471d70e690ec098b37316ebdfc087bd29c
2023-03-07 14:36:02,283 TADA INFO   test-suite: LDMSD
2023-03-07 14:36:02,284 TADA INFO   test-name: store_app_test
2023-03-07 14:36:02,284 TADA INFO   test-user: narate
2023-03-07 14:36:02,284 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:36:02,285 __main__ INFO -- Get or create the cluster --
2023-03-07 14:36:16,761 __main__ INFO -- Preparing job script & programs --
2023-03-07 14:36:17,156 __main__ INFO -- Start daemons --
2023-03-07 14:36:29,503 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:36:34,508 __main__ INFO -- Submitting jobs --
2023-03-07 14:36:34,726 __main__ INFO job_one: 1
2023-03-07 14:36:39,947 __main__ INFO job_two: 2
2023-03-07 14:36:49,184 __main__ INFO Verifying data ...
2023-03-07 14:38:54,963 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-03-07 14:38:54,964 TADA INFO test store_app_test ended
2023-03-07 14:39:09 INFO: ----------------------------------------------
2023-03-07 14:39:09 INFO: ======== syspapi_test ========
2023-03-07 14:39:09 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/syspapi_test
2023-03-07 14:39:10,605 TADA INFO starting test `syspapi_test`
2023-03-07 14:39:10,605 TADA INFO   test-id: 3691f50bbfba89def2647ac0902315caf509e3140482e3c6a829eda642766a84
2023-03-07 14:39:10,605 TADA INFO   test-suite: LDMSD
2023-03-07 14:39:10,605 TADA INFO   test-name: syspapi_test
2023-03-07 14:39:10,605 TADA INFO   test-user: narate
2023-03-07 14:39:10,605 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:39:10,606 __main__ INFO -- Get or create the cluster --
2023-03-07 14:39:21,909 __main__ INFO -- Write syspapi JSON config files --
2023-03-07 14:39:21,909 __main__ INFO    - db/syspapi-1.json
2023-03-07 14:39:21,910 __main__ INFO    - db/syspapi-bad.json
2023-03-07 14:39:21,911 __main__ INFO -- Start daemons --
2023-03-07 14:39:30,295 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:39:35,298 __main__ INFO -- Verifying --
2023-03-07 14:39:35,430 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-03-07 14:39:35,430 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-03-07 14:39:35,543 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-03-07 14:39:37,676 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-03-07 14:39:37,797 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-03-07 14:39:37,912 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-03-07 14:39:59,808 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-03-07 14:39:59,808 __main__ INFO  events succeeded: 77
2023-03-07 14:39:59,808 __main__ INFO  events failed: 114
2023-03-07 14:39:59,809 TADA INFO test syspapi_test ended
2023-03-07 14:40:13 INFO: ----------------------------------------------
2023-03-07 14:40:14 INFO: ======== agg_test ========
2023-03-07 14:40:14 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/agg_test
2023-03-07 14:40:14,811 TADA INFO starting test `agg_test`
2023-03-07 14:40:14,812 TADA INFO   test-id: 115af10f7aaffce4d3430d58a57cfdfae90b912ba74c5d908557445c71b8a5c5
2023-03-07 14:40:14,812 TADA INFO   test-suite: LDMSD
2023-03-07 14:40:14,812 TADA INFO   test-name: agg_test
2023-03-07 14:40:14,812 TADA INFO   test-user: narate
2023-03-07 14:40:14,812 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:40:14,813 __main__ INFO -- Get or create the cluster --
2023-03-07 14:40:32,465 __main__ INFO -- Start daemons --
2023-03-07 14:40:41,749 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:40:46,754 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 14:40:46,867 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-03-07 14:40:47,656 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-03-07 14:40:47,656 __main__ INFO -- Terminating ldmsd on node-1 --
2023-03-07 14:40:50,012 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 14:40:50,247 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-3/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-2/meminfo', 'node-4/meminfo'}), passed
2023-03-07 14:40:50,247 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-03-07 14:40:55,906 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-3/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-03-07 14:40:56,025 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}), passed
2023-03-07 14:40:56,025 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-03-07 14:40:58,349 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-03-07 14:40:58,458 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-03-07 14:40:58,571 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 14:40:58,571 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-03-07 14:41:04,250 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-1/meminfo', 'node-3/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}), passed
2023-03-07 14:41:04,250 TADA INFO test agg_test ended
2023-03-07 14:41:20 INFO: ----------------------------------------------
2023-03-07 14:41:20 INFO: ======== failover_test ========
2023-03-07 14:41:20 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/failover_test
2023-03-07 14:41:21,643 TADA INFO starting test `failover_test`
2023-03-07 14:41:21,644 TADA INFO   test-id: 67200d4569bd8d8f437b2a6baeaefb08292baa220808e322dc8f602994efbb68
2023-03-07 14:41:21,644 TADA INFO   test-suite: LDMSD
2023-03-07 14:41:21,644 TADA INFO   test-name: failover_test
2023-03-07 14:41:21,644 TADA INFO   test-user: narate
2023-03-07 14:41:21,644 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:41:21,645 __main__ INFO -- Get or create the cluster --
2023-03-07 14:41:39,362 __main__ INFO -- Start daemons --
2023-03-07 14:41:48,759 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:42:03,769 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 14:42:03,897 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-03-07 14:42:04,695 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-03-07 14:42:04,695 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-03-07 14:42:10,046 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:10,159 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:10,271 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-03-07 14:42:10,387 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 14:42:10,387 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-03-07 14:42:31,102 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-03-07 14:42:31,226 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:31,226 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-03-07 14:42:36,562 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:36,667 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:36,778 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-03-07 14:42:36,894 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-03-07 14:42:36,895 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-03-07 14:42:57,581 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:57,693 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-03-07 14:42:57,694 TADA INFO test failover_test ended
2023-03-07 14:43:13 INFO: ----------------------------------------------
2023-03-07 14:43:14 INFO: ======== ldmsd_auth_ovis_test ========
2023-03-07 14:43:14 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_auth_ovis_test
2023-03-07 14:43:14,726 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-03-07 14:43:14,726 TADA INFO   test-id: f858ee6cf7f348f79a1c4b0730c591aa1bce98270bbe47ae744b8a690e82f6ee
2023-03-07 14:43:14,726 TADA INFO   test-suite: LDMSD
2023-03-07 14:43:14,726 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-03-07 14:43:14,726 TADA INFO   test-user: narate
2023-03-07 14:43:14,726 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:43:14,727 __main__ INFO -- Get or create the cluster --
2023-03-07 14:43:19,758 __main__ INFO -- Start daemons --
2023-03-07 14:43:21,717 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:43:26,832 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-03-07 14:43:26,957 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-03-07 14:43:27,073 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-03-07 14:43:27,362 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-03-07 14:43:27,362 TADA INFO test ldmsd_auth_ovis_test ended
2023-03-07 14:43:38 INFO: ----------------------------------------------
2023-03-07 14:43:39 INFO: ======== ldmsd_auth_test ========
2023-03-07 14:43:39 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_auth_test
2023-03-07 14:43:40,304 TADA INFO starting test `ldmsd_auth_test`
2023-03-07 14:43:40,305 TADA INFO   test-id: bd3e36154c2b5f508798a2c5bd3c4d72b8c10ff4aa6dc17d132cccf0075a7da9
2023-03-07 14:43:40,305 TADA INFO   test-suite: LDMSD
2023-03-07 14:43:40,305 TADA INFO   test-name: ldmsd_auth_test
2023-03-07 14:43:40,305 TADA INFO   test-user: narate
2023-03-07 14:43:40,305 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:43:40,305 __main__ INFO -- Get or create the cluster --
2023-03-07 14:43:58,305 __main__ INFO -- Start daemons --
2023-03-07 14:44:17,393 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:44:22,509 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-03-07 14:44:22,636 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-03-07 14:44:22,745 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-03-07 14:44:22,855 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-03-07 14:44:22,972 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-03-07 14:44:22,972 TADA INFO test ldmsd_auth_test ended
2023-03-07 14:44:38 INFO: ----------------------------------------------
2023-03-07 14:44:39 INFO: ======== ldmsd_ctrl_test ========
2023-03-07 14:44:39 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_ctrl_test
2023-03-07 14:44:40,132 TADA INFO starting test `ldmsd_ctrl_test`
2023-03-07 14:44:40,132 TADA INFO   test-id: d4a7fb288ff417b7a6473a148fe2bffab359fd439f467819d2dd5ce50406b79b
2023-03-07 14:44:40,132 TADA INFO   test-suite: LDMSD
2023-03-07 14:44:40,132 TADA INFO   test-name: ldmsd_ctrl_test
2023-03-07 14:44:40,132 TADA INFO   test-user: narate
2023-03-07 14:44:40,132 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:44:40,133 __main__ INFO -- Get or create the cluster --
2023-03-07 14:44:49,909 __main__ INFO -- Start daemons --
2023-03-07 14:44:54,320 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 14:45:00,440 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-03-07 14:45:01,556 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-03-07 14:45:02,157 TADA INFO assertion 3, ldmsd_controller start bogus producer: Unexpected output: prdcr_start name=bogus
Error starting prdcr bogus: The producer specified does not exist.
sock:agg-11:10000> , failed
Traceback (most recent call last):
  File "ldmsd_ctrl_test", line 260, in <module>
    test.assert_test(3, False, "Unexpected output: {}".format(out))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test ldmsd_controller and ldmsctl, Unexpected output: prdcr_start name=bogus
Error starting prdcr bogus: The producer specified does not exist.
sock:agg-11:10000> : FAILED
2023-03-07 14:45:02,159 TADA INFO assertion 4, ldmsctl start bogus producer: skipped
2023-03-07 14:45:02,159 TADA INFO assertion 5, ldmsd_controller bogus command: skipped
2023-03-07 14:45:02,159 TADA INFO assertion 6, ldmsctl bogus command: skipped
2023-03-07 14:45:02,160 TADA INFO assertion 7, ldmsd_controller load bogus plugin: skipped
2023-03-07 14:45:02,160 TADA INFO assertion 8, ldmsctl load bogus plugin: skipped
2023-03-07 14:45:02,160 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: skipped
2023-03-07 14:45:02,161 TADA INFO assertion 10, ldmsctl prdcr/updtr: skipped
2023-03-07 14:45:02,161 TADA INFO test ldmsd_ctrl_test ended
2023-03-07 14:45:15 INFO: ----------------------------------------------
2023-03-07 14:45:15 INFO: ======== ldmsd_stream_test ========
2023-03-07 14:45:15 INFO: CMD: python3 ldmsd_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_stream_test
2023-03-07 14:45:16,582 TADA INFO starting test `ldmsd_stream_test`
2023-03-07 14:45:16,583 TADA INFO   test-id: 53a48ecc25c422f9cdeba964e08482bd07988bce625a668fd9173ad76fbf7d7a
2023-03-07 14:45:16,583 TADA INFO   test-suite: LDMSD
2023-03-07 14:45:16,583 TADA INFO   test-name: ldmsd_stream_test
2023-03-07 14:45:16,583 TADA INFO   test-user: narate
2023-03-07 14:45:16,583 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:45:27,792 __main__ INFO waiting for libraries to be available across all containers...
2023-03-07 14:45:28,660 __main__ INFO _lib_avail: True
2023-03-07 14:46:35,956 __main__ INFO test ldmsd_stream_subscribe with large json streams
2023-03-07 14:46:42,081 __main__ INFO --- Sending stream to ldmsd_stream_subscriber
2023-03-07 14:46:54,988 __main__ INFO --- Verifying the received streams
2023-03-07 14:46:56,575 TADA INFO assertion 1, ldmsd_stream_subscribe receives large json streams: Verify all streams were received correctly, passed
2023-03-07 14:46:56,822 __main__ INFO test LDMSD with large json streams
2023-03-07 14:47:02,870 __main__ INFO --- Sending stream to samplerd
2023-03-07 14:47:24,689 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:47:27,103 TADA INFO assertion 2, samplerd receives large json streams: Verify all streams were received correctly, passed
2023-03-07 14:47:27,103 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:47:29,474 TADA INFO assertion 3, agg receives large json streams: Verify all streams were received correctly, passed
2023-03-07 14:47:29,475 __main__ INFO test ldmsd_stream_subscribe with small json streams
2023-03-07 14:47:35,585 __main__ INFO --- Sending stream to ldmsd_stream_subscriber
2023-03-07 14:49:31,979 __main__ INFO --- Verifying the received streams
2023-03-07 14:49:33,789 TADA INFO assertion 4, ldmsd_stream_subscribe receives small json streams: Verify all streams were received correctly, passed
2023-03-07 14:49:34,029 __main__ INFO test LDMSD with small json streams
2023-03-07 14:49:40,115 __main__ INFO --- Sending stream to samplerd
2023-03-07 14:51:41,742 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:51:44,574 TADA INFO assertion 5, samplerd receives small json streams: Verify all streams were received correctly, passed
2023-03-07 14:51:44,575 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:51:47,792 TADA INFO assertion 6, agg receives small json streams: Verify all streams were received correctly, passed
2023-03-07 14:51:47,792 __main__ INFO test ldmsd_stream_subscribe with large string streams
2023-03-07 14:51:53,913 __main__ INFO --- Sending stream to ldmsd_stream_subscriber
2023-03-07 14:52:07,270 __main__ INFO --- Verifying the received streams
2023-03-07 14:52:08,700 TADA INFO assertion 7, ldmsd_stream_subscribe receives large string streams: Verify all streams were received correctly, passed
2023-03-07 14:52:08,926 __main__ INFO test LDMSD with large string streams
2023-03-07 14:52:14,973 __main__ INFO --- Sending stream to samplerd
2023-03-07 14:52:33,678 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:52:34,883 TADA INFO assertion 8, samplerd receives large string streams: Verify all streams were received correctly, passed
2023-03-07 14:52:34,884 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:52:36,055 TADA INFO assertion 9, agg receives large string streams: Verify all streams were received correctly, passed
2023-03-07 14:52:36,055 __main__ INFO test ldmsd_stream_subscribe with small string streams
2023-03-07 14:52:42,171 __main__ INFO --- Sending stream to ldmsd_stream_subscriber
2023-03-07 14:54:38,953 __main__ INFO --- Verifying the received streams
2023-03-07 14:54:40,879 TADA INFO assertion 10, ldmsd_stream_subscribe receives small string streams: Verify all streams were received correctly, passed
2023-03-07 14:54:41,140 __main__ INFO test LDMSD with small string streams
2023-03-07 14:54:47,214 __main__ INFO --- Sending stream to samplerd
2023-03-07 14:56:49,501 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:56:50,713 TADA INFO assertion 11, samplerd receives small string streams: Verify all streams were received correctly, passed
2023-03-07 14:56:50,713 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 14:56:51,900 TADA INFO assertion 12, agg receives small string streams: Verify all streams were received correctly, passed
2023-03-07 14:56:51,901 TADA INFO test ldmsd_stream_test ended
2023-03-07 14:57:06 INFO: ----------------------------------------------
2023-03-07 14:57:07 INFO: ======== maestro_cfg_test ========
2023-03-07 14:57:07 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/maestro_cfg_test
2023-03-07 14:57:08,169 TADA INFO starting test `maestro_cfg_test`
2023-03-07 14:57:08,170 TADA INFO   test-id: ec4f5a8ffe39d895d3b316838d1241251419d79afe6861c6c74a13b3c3cce06c
2023-03-07 14:57:08,170 TADA INFO   test-suite: LDMSD
2023-03-07 14:57:08,170 TADA INFO   test-name: maestro_cfg_test
2023-03-07 14:57:08,170 TADA INFO   test-user: narate
2023-03-07 14:57:08,170 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 14:57:18,181 __main__ INFO -- Get or create cluster --
2023-03-07 14:57:44,926 __main__ INFO -- Start daemons --
2023-03-07 14:58:00,033 __main__ INFO ... make sure ldmsd's are up
2023-03-07 14:58:07,889 TADA INFO assertion 1, load maestro etcd cluster: etcd cluster loaded successfully, passed
2023-03-07 14:58:47,919 TADA INFO assertion 2, config ldmsd cluster with maestro: Maestro ldmsd configuration successful, passed
2023-03-07 14:58:49,492 TADA INFO assertion 3, verify sampler daemons: OK, passed
2023-03-07 14:58:50,078 TADA INFO assertion 4, verify L1 aggregator daemons: OK, passed
2023-03-07 14:58:50,335 TADA INFO assertion 5, verify L2 aggregator daemon: OK, passed
2023-03-07 14:58:50,645 TADA INFO assertion 6, verify data storage: OK, passed
---Wait for config to write to file---
2023-03-07 14:58:50,646 TADA INFO test maestro_cfg_test ended
2023-03-07 14:59:08 INFO: ----------------------------------------------
2023-03-07 14:59:09 INFO: ======== mt-slurm-test ========
2023-03-07 14:59:09 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1678222789.941010', '1,1678222789.941010', '2,1678222789.941010', '3,1678222790.927065', '4,1678222790.927065', '5,1678222790.927065', '6,1678222790.927065', '7,1678222790.927065', '8,1678222790.927065', '9,1678222791.999485', '10,1678222791.999485', '11,1678222792.896789', '12,1678222792.896789', '13,1678222792.896789', '14,1678222792.896789', '15,1678222792.896789', '16,1678222792.896789', '17,1678222792.896789', '18,1678222794.961540', '19,1678222794.961540', '20,1678222794.961540', '21,1678222794.961540', '22,1678222794.961540', '23,1678222794.961540', '24,1678222794.961540', '25,1678222794.961540', '26,1678222794.961540', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-03-07 15:00:29 INFO: ----------------------------------------------
2023-03-07 15:00:30 INFO: ======== ovis_ev_test ========
2023-03-07 15:00:30 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ovis_ev_test
2023-03-07 15:00:30,776 __main__ INFO -- Create the cluster -- 
2023-03-07 15:00:40,495 TADA INFO starting test `ovis_ev_test`
2023-03-07 15:00:40,495 TADA INFO   test-id: d70b3c30395e9da126352d6f58082e7d141f60a2a2e34ae7bef9915196a5a237
2023-03-07 15:00:40,495 TADA INFO   test-suite: test_ovis_ev
2023-03-07 15:00:40,495 TADA INFO   test-name: ovis_ev_test
2023-03-07 15:00:40,495 TADA INFO   test-user: narate
2023-03-07 15:00:40,496 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:00:40,496 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-03-07 15:00:40,496 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-03-07 15:00:40,497 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-03-07 15:00:40,497 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-03-07 15:00:40,497 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-03-07 15:00:40,497 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-03-07 15:00:40,497 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-03-07 15:00:40,497 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-03-07 15:00:40,498 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-03-07 15:00:40,498 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-03-07 15:00:40,498 TADA INFO test ovis_ev_test ended
2023-03-07 15:00:51 INFO: ----------------------------------------------
2023-03-07 15:00:52 INFO: ======== prdcr_subscribe_test ========
2023-03-07 15:00:52 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/prdcr_subscribe_test
2023-03-07 15:00:52,846 TADA INFO starting test `prdcr_subscribe_test`
2023-03-07 15:00:52,846 TADA INFO   test-id: fa219bd97abb9191b3efe9d7515ce5a6743093352d62cf9fec92a4781794fb88
2023-03-07 15:00:52,846 TADA INFO   test-suite: LDMSD
2023-03-07 15:00:52,846 TADA INFO   test-name: prdcr_subscribe_test
2023-03-07 15:00:52,846 TADA INFO   test-user: narate
2023-03-07 15:00:52,846 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:01:28,367 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-03-07 15:01:28,368 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-03-07 15:01:28,368 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-03-07 15:01:28,368 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-03-07 15:01:28,369 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-03-07 15:01:28,732 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-03-07 15:01:29,083 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-03-07 15:01:37,076 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-03-07 15:01:37,076 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-03-07 15:01:37,077 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-03-07 15:01:37,077 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-03-07 15:01:37,077 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-03-07 15:01:38,280 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-03-07 15:01:39,849 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-03-07 15:01:47,452 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-03-07 15:01:47,453 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-03-07 15:01:47,453 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-03-07 15:01:47,815 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-03-07 15:01:51,133 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-03-07 15:01:56,934 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-03-07 15:01:56,934 TADA INFO test prdcr_subscribe_test ended
2023-03-07 15:02:09 INFO: ----------------------------------------------
2023-03-07 15:02:10 INFO: ======== set_array_test ========
2023-03-07 15:02:10 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/set_array_test
2023-03-07 15:02:11,244 TADA INFO starting test `set_array_test`
2023-03-07 15:02:11,244 TADA INFO   test-id: f9db4ed64033a4e904877d8d9f7872aa99406fdfa60db69942cfeea85874755b
2023-03-07 15:02:11,244 TADA INFO   test-suite: LDMSD
2023-03-07 15:02:11,244 TADA INFO   test-name: set_array_test
2023-03-07 15:02:11,244 TADA INFO   test-user: narate
2023-03-07 15:02:11,244 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:02:11,245 __main__ INFO -- Get or create the cluster --
2023-03-07 15:02:16,517 __main__ INFO -- Start daemons --
2023-03-07 15:02:18,507 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:02:45,534 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 8 snapshots, passed
2023-03-07 15:02:45,534 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-03-07 15:02:45,534 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-03-07 15:02:45,534 TADA INFO test set_array_test ended
2023-03-07 15:02:56 INFO: ----------------------------------------------
2023-03-07 15:02:57 INFO: ======== setgroup_test ========
2023-03-07 15:02:57 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/setgroup_test
2023-03-07 15:02:58,509 TADA INFO starting test `setgroup_test`
2023-03-07 15:02:58,509 TADA INFO   test-id: bb600e2786a7d1c930695d730b504656d6d97b839c5a2e2929ba855bfda3a238
2023-03-07 15:02:58,509 TADA INFO   test-suite: LDMSD
2023-03-07 15:02:58,510 TADA INFO   test-name: setgroup_test
2023-03-07 15:02:58,510 TADA INFO   test-user: narate
2023-03-07 15:02:58,510 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:02:58,511 __main__ INFO -- Get or create the cluster --
2023-03-07 15:03:07,939 __main__ INFO -- Start daemons --
2023-03-07 15:03:12,383 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:03:17,389 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 15:03:17,507 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-03-07 15:03:19,785 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-03-07 15:03:19,785 __main__ INFO -- Removing test_2 from grp --
2023-03-07 15:03:20,242 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, failed
Traceback (most recent call last):
  File "setgroup_test", line 357, in <module>
    test.assert_test(3, False, "expect {}, got {}".format(expect, sets))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: LDMSD setgroup 2-level aggregation test, expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}: FAILED
2023-03-07 15:03:20,243 TADA INFO assertion 4, test_2 is removed from grp on agg-1: skipped
2023-03-07 15:03:20,243 TADA INFO assertion 5, test_2 is removed from grp on agg-2: skipped
2023-03-07 15:03:20,244 TADA INFO assertion 6, test_2 is added back to grp on sampler: skipped
2023-03-07 15:03:20,244 TADA INFO assertion 7, test_2 is added back to grp on agg-1: skipped
2023-03-07 15:03:20,244 TADA INFO assertion 8, test_2 is added back to grp on agg-2: skipped
2023-03-07 15:03:20,244 TADA INFO test setgroup_test ended
2023-03-07 15:03:32 INFO: ----------------------------------------------
2023-03-07 15:03:33 INFO: ======== slurm_stream_test ========
2023-03-07 15:03:33 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/slurm_stream_test
2023-03-07 15:03:34,470 TADA INFO starting test `slurm_stream_test`
2023-03-07 15:03:34,471 TADA INFO   test-id: 38622fae7010f193fe0f9fbaeb34e6f459504451b9ac7757aed102b5b31e1d54
2023-03-07 15:03:34,471 TADA INFO   test-suite: LDMSD
2023-03-07 15:03:34,471 TADA INFO   test-name: slurm_stream_test
2023-03-07 15:03:34,471 TADA INFO   test-user: narate
2023-03-07 15:03:34,471 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:03:34,472 __main__ INFO -- Get or create the cluster --
2023-03-07 15:03:41,757 __main__ INFO -- Start daemons --
2023-03-07 15:03:44,431 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:04:14,424 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,424 __main__ INFO 12345
2023-03-07 15:04:14,424 __main__ INFO 12345
2023-03-07 15:04:14,425 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,425 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-03-07 15:04:14,425 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,425 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,425 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,426 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,547 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,547 __main__ INFO 12345
2023-03-07 15:04:14,547 __main__ INFO 12345
2023-03-07 15:04:14,547 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,547 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-03-07 15:04:14,548 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,548 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,548 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,548 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 15:04:14,658 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,658 __main__ INFO 12346
2023-03-07 15:04:14,658 __main__ INFO 12346
2023-03-07 15:04:14,658 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,658 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-03-07 15:04:14,658 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,659 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,659 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,659 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,769 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,770 __main__ INFO 12346
2023-03-07 15:04:14,770 __main__ INFO 12346
2023-03-07 15:04:14,770 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,770 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-03-07 15:04:14,770 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,770 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,770 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,771 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 15:04:14,883 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,883 __main__ INFO 12347
2023-03-07 15:04:14,883 __main__ INFO 12347
2023-03-07 15:04:14,883 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,883 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-03-07 15:04:14,884 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,884 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,884 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,884 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,991 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:14,991 __main__ INFO 12347
2023-03-07 15:04:14,991 __main__ INFO 12347
2023-03-07 15:04:14,991 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,992 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-03-07 15:04:14,992 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,992 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,992 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:14,992 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 15:04:15,087 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,087 __main__ INFO 12348
2023-03-07 15:04:15,088 __main__ INFO 12348
2023-03-07 15:04:15,088 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,088 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-03-07 15:04:15,088 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,088 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,088 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,089 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,198 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,198 __main__ INFO 12348
2023-03-07 15:04:15,198 __main__ INFO 12348
2023-03-07 15:04:15,198 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,198 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-03-07 15:04:15,198 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,199 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,199 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,199 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 15:04:15,316 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,316 __main__ INFO 12355
2023-03-07 15:04:15,316 __main__ INFO 12355
2023-03-07 15:04:15,316 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,316 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-03-07 15:04:15,316 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,317 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,318 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,424 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,424 __main__ INFO 12355
2023-03-07 15:04:15,424 __main__ INFO 12355
2023-03-07 15:04:15,424 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,424 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,425 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,426 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,426 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 15:04:15,530 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,530 __main__ INFO 12356
2023-03-07 15:04:15,530 __main__ INFO 12356
2023-03-07 15:04:15,531 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,531 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,532 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,532 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,532 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,646 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,646 __main__ INFO 12356
2023-03-07 15:04:15,646 __main__ INFO 12356
2023-03-07 15:04:15,646 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,646 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-03-07 15:04:15,646 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,646 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,647 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 15:04:15,760 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,760 __main__ INFO 12357
2023-03-07 15:04:15,760 __main__ INFO 12357
2023-03-07 15:04:15,761 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,761 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-03-07 15:04:15,761 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,761 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,761 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,762 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,762 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,762 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,762 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,762 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,896 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:15,896 __main__ INFO 12357
2023-03-07 15:04:15,896 __main__ INFO 12357
2023-03-07 15:04:15,896 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,896 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-03-07 15:04:15,896 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,896 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:15,897 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 15:04:16,008 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:16,008 __main__ INFO 12358
2023-03-07 15:04:16,008 __main__ INFO 12358
2023-03-07 15:04:16,008 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,008 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-03-07 15:04:16,008 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,008 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,009 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,009 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,009 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,009 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,009 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,010 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,113 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 15:04:16,113 __main__ INFO 12358
2023-03-07 15:04:16,113 __main__ INFO 12358
2023-03-07 15:04:16,113 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,114 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,115 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,115 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:16,115 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 15:04:18,216 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-03-07 15:04:18,216 __main__ INFO 12353
2023-03-07 15:04:18,216 __main__ INFO 12353
2023-03-07 15:04:18,216 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,216 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-03-07 15:04:18,217 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,217 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,217 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,217 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,217 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,218 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,218 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,218 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 15:04:18,218 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-03-07 15:04:18,218 TADA INFO test slurm_stream_test ended
2023-03-07 15:04:29 INFO: ----------------------------------------------
2023-03-07 15:04:30 INFO: ======== spank_notifier_test ========
2023-03-07 15:04:30 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/spank_notifier_test
2023-03-07 15:04:31,113 TADA INFO starting test `spank_notifier_test`
2023-03-07 15:04:31,113 TADA INFO   test-id: 12da46af5ed0061d1ee5dc9d5acabc232026994beda2e7b1ad2227143727f6ab
2023-03-07 15:04:31,113 TADA INFO   test-suite: Slurm_Plugins
2023-03-07 15:04:31,113 TADA INFO   test-name: spank_notifier_test
2023-03-07 15:04:31,114 TADA INFO   test-user: narate
2023-03-07 15:04:31,114 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:04:31,114 __main__ INFO -- Create the cluster --
2023-03-07 15:04:57,146 __main__ INFO -- Cleanup output --
2023-03-07 15:04:57,447 __main__ INFO -- Test bad plugstack config --
2023-03-07 15:04:57,447 __main__ INFO Starting slurm ...
2023-03-07 15:05:12,002 __main__ INFO Starting slurm ... OK
2023-03-07 15:05:32,571 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 15:05:32,697 __main__ INFO   jobid = 1
2023-03-07 15:05:32,901 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 15:05:33,016 __main__ INFO   jobid = 2
2023-03-07 15:05:33,237 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 15:05:33,348 __main__ INFO   jobid = 3
2023-03-07 15:05:33,563 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 15:05:33,678 __main__ INFO   jobid = 4
2023-03-07 15:05:43,429 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-03-07 15:05:43,429 __main__ INFO Killin slurm ...
2023-03-07 15:05:46,456 __main__ INFO Killin slurm ... OK
2023-03-07 15:06:06,473 __main__ INFO -- Start daemons --
2023-03-07 15:06:17,392 __main__ INFO Starting slurm ... OK
2023-03-07 15:06:37,645 __main__ INFO -- Submitting job with no stream listener --
2023-03-07 15:06:37,854 __main__ INFO -- Submitting job with num_tasks 8 --
2023-03-07 15:06:37,960 __main__ INFO   jobid = 5
2023-03-07 15:06:53,984 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-03-07 15:06:53,985 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-03-07 15:06:59,826 __main__ INFO -- Submitting job with listener --
2023-03-07 15:07:00,033 __main__ INFO -- Submitting job with num_tasks 1 --
2023-03-07 15:07:00,142 __main__ INFO   jobid = 6
2023-03-07 15:07:00,358 __main__ INFO -- Submitting job with num_tasks 2 --
2023-03-07 15:07:00,467 __main__ INFO   jobid = 7
2023-03-07 15:07:00,677 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 15:07:00,796 __main__ INFO   jobid = 8
2023-03-07 15:07:01,007 __main__ INFO -- Submitting job with num_tasks 8 --
2023-03-07 15:07:01,134 __main__ INFO   jobid = 9
2023-03-07 15:07:01,346 __main__ INFO -- Submitting job with num_tasks 27 --
2023-03-07 15:07:01,455 __main__ INFO   jobid = 10
2023-03-07 15:07:23,257 __main__ INFO -- Verifying Events --
2023-03-07 15:07:23,258 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-03-07 15:07:23,258 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 15:07:23,258 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 15:07:23,258 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 15:07:23,259 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 15:07:23,260 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-03-07 15:07:23,260 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 15:07:23,260 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 15:07:23,260 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 15:07:23,260 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 15:07:23,261 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-03-07 15:07:23,261 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 15:07:23,261 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 15:07:23,261 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 15:07:23,261 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 15:07:23,262 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-03-07 15:07:23,262 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 15:07:23,262 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 15:07:23,262 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 15:07:23,263 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 15:07:23,263 __main__ INFO job 6 multi-tenant with dict_keys([7])
2023-03-07 15:07:23,263 __main__ INFO job 10 multi-tenant with dict_keys([7, 6])
2023-03-07 15:07:23,263 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-03-07 15:07:23,263 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-03-07 15:07:23,263 __main__ INFO job 9 multi-tenant with dict_keys([10])
2023-03-07 15:07:23,263 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-03-07 15:07:23,503 __main__ INFO -- Submitting job that crashes listener --
2023-03-07 15:07:23,632 __main__ INFO   jobid = 11
2023-03-07 15:07:33,881 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-03-07 15:07:33,998 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-03-07 15:07:33,998 TADA INFO test spank_notifier_test ended
2023-03-07 15:07:50 INFO: ----------------------------------------------
2023-03-07 15:07:51 INFO: ======== ldms_list_test ========
2023-03-07 15:07:51 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldms_list_test
2023-03-07 15:07:52,068 TADA INFO starting test `ldms_list_test`
2023-03-07 15:07:52,069 TADA INFO   test-id: 117092f875d68deba7d7b15321f74b84d367d17d9fdc2b5cd907f2385fb13e26
2023-03-07 15:07:52,069 TADA INFO   test-suite: LDMSD
2023-03-07 15:07:52,069 TADA INFO   test-name: ldms_list_test
2023-03-07 15:07:52,069 TADA INFO   test-user: narate
2023-03-07 15:07:52,069 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:07:52,070 __main__ INFO -- Get or create the cluster --
2023-03-07 15:07:55,262 __main__ INFO -- Start daemons --
2023-03-07 15:08:01,601 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:08:03,602 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-03-07 15:08:09,635 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-03-07 15:08:09,636 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-03-07 15:08:09,636 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-03-07 15:08:09,636 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-03-07 15:08:09,636 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-03-07 15:08:09,637 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-03-07 15:08:09,637 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-03-07 15:08:09,637 __main__ INFO 2nd sampling on the sampler...
2023-03-07 15:08:16,847 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-03-07 15:08:16,847 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-03-07 15:08:16,847 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-03-07 15:08:16,848 __main__ INFO 2nd update on the aggregator...
2023-03-07 15:08:24,057 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-03-07 15:08:24,057 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-03-07 15:08:24,058 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-03-07 15:08:24,058 __main__ INFO 3rd sampling on the sampler...
2023-03-07 15:08:31,267 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-03-07 15:08:31,268 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-03-07 15:08:31,268 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-03-07 15:08:31,268 __main__ INFO 3rd update on the aggregator...
2023-03-07 15:08:38,477 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-03-07 15:08:38,478 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-03-07 15:08:38,478 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-03-07 15:08:38,478 __main__ INFO 4th sampling on the sampler...
2023-03-07 15:08:45,688 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-03-07 15:08:45,688 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-03-07 15:08:45,688 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-03-07 15:08:45,689 __main__ INFO 4th update on the aggregator...
2023-03-07 15:08:52,898 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-03-07 15:08:52,898 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-03-07 15:08:52,899 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-03-07 15:08:52,899 __main__ INFO 5th sampling on the sampler...
2023-03-07 15:09:00,107 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-03-07 15:09:00,108 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-03-07 15:09:00,108 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-03-07 15:09:00,108 __main__ INFO 5th update on the aggregator...
2023-03-07 15:09:07,316 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-03-07 15:09:07,317 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-03-07 15:09:07,317 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-03-07 15:09:07,317 __main__ INFO 6th sampling on the sampler...
2023-03-07 15:09:14,527 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-03-07 15:09:14,527 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-03-07 15:09:14,527 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-03-07 15:09:14,528 __main__ INFO 6th update on the updator...
2023-03-07 15:09:21,737 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-03-07 15:09:21,737 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-03-07 15:09:21,738 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-03-07 15:09:21,738 TADA INFO test ldms_list_test ended
2023-03-07 15:09:32 INFO: ----------------------------------------------
2023-03-07 15:09:33 INFO: ======== quick_set_add_rm_test ========
2023-03-07 15:09:33 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/quick_set_add_rm_test
2023-03-07 15:09:34,058 TADA INFO starting test `quick_set_add_rm_test`
2023-03-07 15:09:34,058 TADA INFO   test-id: 25e1394c66d8f6714f3530fba846d46304ddd48350ecc4ecd8e873465b526ba4
2023-03-07 15:09:34,058 TADA INFO   test-suite: LDMSD
2023-03-07 15:09:34,058 TADA INFO   test-name: quick_set_add_rm_test
2023-03-07 15:09:34,058 TADA INFO   test-user: narate
2023-03-07 15:09:34,058 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:09:34,059 __main__ INFO -- Get or create the cluster --
2023-03-07 15:09:41,349 __main__ INFO -- Start samp.py --
2023-03-07 15:09:46,466 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-03-07 15:09:46,466 __main__ INFO -- Start daemons --
2023-03-07 15:09:54,045 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:09:59,638 TADA INFO assertion 2, verify data: verified, passed
2023-03-07 15:10:04,264 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-03-07 15:10:08,869 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-03-07 15:10:13,451 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-03-07 15:10:18,564 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-03-07 15:10:18,565 TADA INFO test quick_set_add_rm_test ended
2023-03-07 15:10:30 INFO: ----------------------------------------------
2023-03-07 15:10:31 INFO: ======== set_array_hang_test ========
2023-03-07 15:10:31 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/set_array_hang_test
2023-03-07 15:10:32,166 TADA INFO starting test `set_array_hang_test`
2023-03-07 15:10:32,167 TADA INFO   test-id: ae43e2e523d542980e56dcecbd325b07f0311d43f8f86ee43fcd07fd43e800e5
2023-03-07 15:10:32,167 TADA INFO   test-suite: LDMSD
2023-03-07 15:10:32,167 TADA INFO   test-name: set_array_hang_test
2023-03-07 15:10:32,167 TADA INFO   test-user: narate
2023-03-07 15:10:32,167 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:10:32,168 __main__ INFO -- Get or create the cluster --
2023-03-07 15:10:35,268 __main__ INFO -- Start processes --
2023-03-07 15:10:35,268 __main__ INFO starting interactive set_array_samp.py
2023-03-07 15:10:38,283 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-03-07 15:10:41,300 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-03-07 15:10:48,510 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-03-07 15:10:55,719 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-03-07 15:10:59,324 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-03-07 15:11:06,533 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-03-07 15:11:06,534 TADA INFO test set_array_hang_test ended
2023-03-07 15:11:17 INFO: ----------------------------------------------
2023-03-07 15:11:18 INFO: ======== ldmsd_autointerval_test ========
2023-03-07 15:11:18 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_autointerval_test
2023-03-07 15:11:18,823 TADA INFO starting test `ldmsd_autointerval_test`
2023-03-07 15:11:18,823 TADA INFO   test-id: 59d4f71f43a7338c17cfdf8ca5e3381732223d75b21b05128e8fb27723cf11fc
2023-03-07 15:11:18,823 TADA INFO   test-suite: LDMSD
2023-03-07 15:11:18,823 TADA INFO   test-name: ldmsd_autointerval_test
2023-03-07 15:11:18,823 TADA INFO   test-user: narate
2023-03-07 15:11:18,823 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:11:18,824 __main__ INFO -- Get or create the cluster --
2023-03-07 15:11:26,271 __main__ INFO -- Start daemons --
2023-03-07 15:11:30,049 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:11:36,572 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-03-07 15:11:38,797 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-03-07 15:11:38,797 __main__ INFO Let them run for a while to collect data ...
2023-03-07 15:11:48,807 __main__ INFO Setting sample interval to 1000000 ...
2023-03-07 15:11:57,040 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-03-07 15:11:57,040 __main__ INFO Let them run for a while to collect data ...
2023-03-07 15:12:07,051 __main__ INFO Setting sample interval to 2000000 ...
2023-03-07 15:12:15,300 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-03-07 15:12:15,300 __main__ INFO Let them run for a while to collect data ...
2023-03-07 15:12:25,560 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-03-07 15:12:25,668 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-03-07 15:12:25,668 TADA INFO test ldmsd_autointerval_test ended
2023-03-07 15:12:37 INFO: ----------------------------------------------
2023-03-07 15:12:38 INFO: ======== ldms_record_test ========
2023-03-07 15:12:38 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldms_record_test
2023-03-07 15:12:39,426 TADA INFO starting test `ldms_record_test`
2023-03-07 15:12:39,426 TADA INFO   test-id: feabe69c52a38492fb57ac64ad539827c044106963c8fc6a70a6138f0ba5bdee
2023-03-07 15:12:39,426 TADA INFO   test-suite: LDMSD
2023-03-07 15:12:39,426 TADA INFO   test-name: ldms_record_test
2023-03-07 15:12:39,426 TADA INFO   test-user: narate
2023-03-07 15:12:39,426 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:12:39,427 __main__ INFO -- Get or create the cluster --
2023-03-07 15:12:42,486 __main__ INFO -- Start daemons --
2023-03-07 15:12:48,862 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:12:50,864 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-03-07 15:12:56,902 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-03-07 15:12:56,902 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-03-07 15:12:56,902 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-03-07 15:12:56,903 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-03-07 15:12:56,903 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-03-07 15:12:56,903 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-03-07 15:12:56,903 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-03-07 15:12:56,903 __main__ INFO 2nd sampling on the sampler...
2023-03-07 15:13:04,113 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-03-07 15:13:04,114 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-03-07 15:13:04,114 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-03-07 15:13:04,114 __main__ INFO 2nd update on the aggregator...
2023-03-07 15:13:11,322 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-03-07 15:13:11,323 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-03-07 15:13:11,323 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-03-07 15:13:11,323 __main__ INFO 3rd sampling on the sampler...
2023-03-07 15:13:18,533 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-03-07 15:13:18,533 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-03-07 15:13:18,534 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-03-07 15:13:18,534 __main__ INFO 3rd update on the aggregator...
2023-03-07 15:13:25,743 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-03-07 15:13:25,744 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-03-07 15:13:25,744 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-03-07 15:13:25,744 __main__ INFO 4th sampling on the sampler...
2023-03-07 15:13:32,953 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-03-07 15:13:32,954 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-03-07 15:13:32,954 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-03-07 15:13:32,954 __main__ INFO 4th update on the aggregator...
2023-03-07 15:13:40,164 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-03-07 15:13:40,164 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-03-07 15:13:40,165 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-03-07 15:13:40,165 __main__ INFO 5th sampling on the sampler...
2023-03-07 15:13:47,374 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-03-07 15:13:47,375 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-03-07 15:13:47,375 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-03-07 15:13:47,375 __main__ INFO 5th update on the aggregator...
2023-03-07 15:13:54,584 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-03-07 15:13:54,585 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-03-07 15:13:54,585 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-03-07 15:13:54,585 __main__ INFO 6th sampling on the sampler...
2023-03-07 15:14:01,795 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-03-07 15:14:01,795 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-03-07 15:14:01,795 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-03-07 15:14:01,796 __main__ INFO 6th update on the updator...
2023-03-07 15:14:09,005 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-03-07 15:14:09,005 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-03-07 15:14:09,005 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-03-07 15:14:09,006 TADA INFO test ldms_record_test ended
2023-03-07 15:14:19 INFO: ----------------------------------------------
2023-03-07 15:14:20 INFO: ======== ldms_schema_digest_test ========
2023-03-07 15:14:20 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldms_schema_digest_test
2023-03-07 15:14:21,266 TADA INFO starting test `ldms_schema_digest_test`
2023-03-07 15:14:21,267 TADA INFO   test-id: c87c861b969e94307aceff3016d080efc28f23d893d34136a2ff621ca820c994
2023-03-07 15:14:21,267 TADA INFO   test-suite: LDMSD
2023-03-07 15:14:21,267 TADA INFO   test-name: ldms_schema_digest_test
2023-03-07 15:14:21,267 TADA INFO   test-user: narate
2023-03-07 15:14:21,267 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:14:21,268 __main__ INFO -- Get or create the cluster --
2023-03-07 15:14:28,452 __main__ INFO -- Start daemons --
2023-03-07 15:14:31,697 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:14:36,834 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-03-07 15:14:36,937 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-03-07 15:14:37,068 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-03-07 15:14:37,271 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-03-07 15:14:37,272 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-03-07 15:14:37,272 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-03-07 15:14:39,728 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-03-07 15:14:39,728 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-03-07 15:14:39,728 TADA INFO test ldms_schema_digest_test ended
2023-03-07 15:14:51 INFO: ----------------------------------------------
2023-03-07 15:14:52 INFO: ======== ldmsd_decomp_test ========
2023-03-07 15:14:52 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_decomp_test
2023-03-07 15:14:53,412 TADA INFO starting test `ldmsd_decomp_test`
2023-03-07 15:14:53,412 TADA INFO   test-id: 808c311ca5df356d1b5baa3f083b3df06a769b871bf942e9eb5ec4acd6c8b3e0
2023-03-07 15:14:53,412 TADA INFO   test-suite: LDMSD
2023-03-07 15:14:53,412 TADA INFO   test-name: ldmsd_decomp_test
2023-03-07 15:14:53,412 TADA INFO   test-user: narate
2023-03-07 15:14:53,412 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:14:53,413 __main__ INFO -- Get or create the cluster --
2023-03-07 15:15:09,327 __main__ INFO -- Start daemons --
2023-03-07 15:15:19,727 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:16:14,765 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-03-07 15:16:14,765 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-03-07 15:16:14,766 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-03-07 15:16:14,767 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-03-07 15:16:14,768 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-03-07 15:16:14,768 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-03-07 15:16:14,768 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-03-07 15:16:14,768 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-03-07 15:16:14,770 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-03-07 15:16:14,771 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-03-07 15:16:14,846 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-03-07 15:16:14,850 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-03-07 15:16:14,853 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-03-07 15:16:14,862 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-03-07 15:16:14,863 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-03-07 15:16:14,865 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-03-07 15:16:14,938 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-03-07 15:16:14,942 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-03-07 15:16:14,945 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-03-07 15:16:14,953 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-03-07 15:16:14,954 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-03-07 15:16:14,955 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-03-07 15:16:14,984 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-03-07 15:16:14,986 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-03-07 15:16:14,988 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-03-07 15:16:14,992 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-03-07 15:16:14,992 TADA INFO test ldmsd_decomp_test ended
2023-03-07 15:16:14,993 TADA INFO test ldmsd_decomp_test ended
2023-03-07 15:16:30 INFO: ----------------------------------------------
2023-03-07 15:16:31 INFO: ======== ldmsd_stream_status_test ========
2023-03-07 15:16:31 INFO: CMD: python3 ldmsd_stream_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_stream_status_test
2023-03-07 15:16:31,836 __main__ INFO -- Get or create the cluster --
2023-03-07 15:16:31,836 TADA INFO starting test `ldmsd_stream_status`
2023-03-07 15:16:31,836 TADA INFO   test-id: 7294b363b86321eee032d28b21d5efc84ee3a5db41edceb1f1d905fe9ed607f6
2023-03-07 15:16:31,836 TADA INFO   test-suite: LDMSD
2023-03-07 15:16:31,836 TADA INFO   test-name: ldmsd_stream_status
2023-03-07 15:16:31,836 TADA INFO   test-user: narate
2023-03-07 15:16:31,837 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:16:40,363 __main__ INFO -- Start daemons --
2023-03-07 15:16:44,230 __main__ INFO waiting ... for all LDMSDs to start
2023-03-07 15:16:44,551 __main__ INFO All LDMSDs are up.
2023-03-07 15:16:45,778 TADA INFO assertion 1, No Stream data: {} == {}, passed
2023-03-07 15:16:47,126 TADA INFO assertion 2, stream_status -- one stream message: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223805, 'last_ts': 1678223805, 'count': 1, 'total_bytes': 6}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678223805, 'last_ts': 1678223805, 'count': 1, 'total_bytes': 6}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 1, 'total_bytes': 6, 'first_ts': 1678223805, 'last_ts': 1678223805}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 1, 'total_bytes': 6, 'first_ts': 1678223805, 'last_ts': 1678223805}, 'pub': {}}}, passed
2023-03-07 15:16:49,598 TADA INFO assertion 3, stream_status --  multiple stream messages: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223805, 'last_ts': 1678223808, 'count': 3, 'total_bytes': 18, 'msg/sec': 1.0, 'bytes/sec': 6.0}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678223805, 'last_ts': 1678223808, 'count': 3, 'total_bytes': 18, 'msg/sec': 1.0, 'bytes/sec': 6.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, passed
2023-03-07 15:16:50,825 TADA INFO assertion 4, prdcr_stream_status to agg -- one producer: {'_OVERALL_': {'samplerd-1': {'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678223808, 'first_ts': 1678223805, 'bytes/sec': 6.0, 'count': 3}}}, 'foo': {'samplerd-1': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678223808, 'first_ts': 1678223805, 'bytes/sec': 6.0, 'count': 3}}}} == {'foo': {'samplerd-1': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, '_OVERALL_': {'samplerd-1': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}}, passed
2023-03-07 15:16:54,654 TADA INFO assertion 5, stream_status -- mulitple streams: {'bar': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223812, 'last_ts': 1678223813, 'count': 3, 'total_bytes': 48, 'msg/sec': 3.0, 'bytes/sec': 48.0}, 'publishers': {}}, 'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223810, 'last_ts': 1678223812, 'count': 2, 'total_bytes': 12, 'msg/sec': 1.0, 'bytes/sec': 6.0}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678223810, 'last_ts': 1678223813, 'count': 5, 'total_bytes': 60, 'msg/sec': 1.666667, 'bytes/sec': 20.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223810, 'last_ts': 1678223812, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}, 'publishers': {}}, 'bar': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 48, 'first_ts': 1678223812, 'last_ts': 1678223813, 'bytes/sec': 48.0, 'msg/sec': 3.0}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 5, 'total_bytes': 60, 'first_ts': 1678223810, 'last_ts': 1678223813, 'bytes/sec': 20.0, 'msg/sec': 1.666667}, 'pub': {}}}, passed
2023-03-07 15:16:55,877 TADA INFO assertion 6, prdcr_stream_status to agg -- two producers: {'_OVERALL_': {'samplerd-2': {'pub': {}, 'recv': {'msg/sec': 1.666667, 'total_bytes': 60, 'last_ts': 1678223813, 'first_ts': 1678223810, 'bytes/sec': 20.0, 'count': 5}}, 'samplerd-1': {'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678223808, 'first_ts': 1678223805, 'bytes/sec': 6.0, 'count': 3}}}, 'foo': {'samplerd-2': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 12, 'last_ts': 1678223812, 'first_ts': 1678223810, 'bytes/sec': 6.0, 'count': 2}}, 'samplerd-1': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678223808, 'first_ts': 1678223805, 'bytes/sec': 6.0, 'count': 3}}}, 'bar': {'samplerd-2': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 3.0, 'total_bytes': 48, 'last_ts': 1678223813, 'first_ts': 1678223812, 'bytes/sec': 48.0, 'count': 3}}}} == {'foo': {'samplerd-1': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}, 'samplerd-2': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223810, 'last_ts': 1678223812, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, 'bar': {'samplerd-2': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 48, 'first_ts': 1678223812, 'last_ts': 1678223813, 'bytes/sec': 48.0, 'msg/sec': 3.0}, 'pub': {}}}, '_OVERALL_': {'samplerd-1': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223805, 'last_ts': 1678223808, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}, 'samplerd-2': {'recv': {'count': 5, 'total_bytes': 60, 'first_ts': 1678223810, 'last_ts': 1678223813, 'bytes/sec': 20.0, 'msg/sec': 1.666667}, 'pub': {}}}}, passed
2023-03-07 15:16:59,547 TADA INFO assertion 7, stream_status to agg after one producer republished stream: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223817, 'last_ts': 1678223818, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}, 'publishers': {'samplerd-1': {'recv': {'first_ts': 1678223817, 'last_ts': 1678223818, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678223817, 'last_ts': 1678223818, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223817, 'last_ts': 1678223818, 'bytes/sec': 12.0, 'msg/sec': 2.0}, 'pub': {}, 'publishers': {'samplerd-1': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223817, 'last_ts': 1678223818, 'bytes/sec': 12.0, 'msg/sec': 2.0}}}}, '_OVERALL_': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223817, 'last_ts': 1678223818, 'bytes/sec': 12.0, 'msg/sec': 2.0}, 'pub': {}}}, passed
2023-03-07 15:17:01,106 TADA INFO assertion 8, stream_status to agg after two producers republished stream: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678223817, 'last_ts': 1678223819, 'count': 5, 'total_bytes': 30, 'msg/sec': 2.5, 'bytes/sec': 15.0}, 'publishers': {'samplerd-1': {'recv': {'first_ts': 1678223817, 'last_ts': 1678223818, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}, 'samplerd-2': {'recv': {'first_ts': 1678223819, 'last_ts': 1678223819, 'count': 3, 'total_bytes': 18}}}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678223817, 'last_ts': 1678223819, 'count': 5, 'total_bytes': 30, 'msg/sec': 2.5, 'bytes/sec': 15.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 5, 'total_bytes': 30, 'first_ts': 1678223817, 'last_ts': 1678223819, 'bytes/sec': 15.0, 'msg/sec': 2.5}, 'pub': {}, 'publishers': {'samplerd-1': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678223817, 'last_ts': 1678223818, 'bytes/sec': 12.0, 'msg/sec': 2.0}}, 'samplerd-2': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678223819, 'last_ts': 1678223819}}}}, '_OVERALL_': {'recv': {'count': 5, 'total_bytes': 30, 'first_ts': 1678223817, 'last_ts': 1678223819, 'bytes/sec': 15.0, 'msg/sec': 2.5}, 'pub': {}}}, passed
2023-03-07 15:17:01,106 TADA INFO test ldmsd_stream_status ended
2023-03-07 15:17:13 INFO: ----------------------------------------------
2023-03-07 15:17:13 INFO: ======== store_list_record_test ========
2023-03-07 15:17:13 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/store_list_record_test
2023-03-07 15:17:14,636 __main__ INFO -- Get or create the cluster --
2023-03-07 15:17:14,637 TADA INFO starting test `store_sos_lists_test`
2023-03-07 15:17:14,637 TADA INFO   test-id: 0313b247c65956ff77c08f161edc84ba3471cacb59e89e11b1bfd59a08f2a99f
2023-03-07 15:17:14,637 TADA INFO   test-suite: LDMSD
2023-03-07 15:17:14,637 TADA INFO   test-name: store_sos_lists_test
2023-03-07 15:17:14,637 TADA INFO   test-user: narate
2023-03-07 15:17:14,637 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:17:22,090 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:17:25,894 __main__ INFO All sampler daemons are up.
2023-03-07 15:17:25,997 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-03-07 15:17:26,108 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-03-07 15:17:37,310 TADA INFO assertion 3, store_sos is storing data.: agg.file_exists/store/sos/record_record_record, failed
Traceback (most recent call last):
  File "store_list_record_test", line 427, in <module>
    test.assert_test(SOS_DATABASE_CREATED, result, reason)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test store_sos storing lists, agg.file_exists/store/sos/record_record_record: FAILED
2023-03-07 15:17:37,312 TADA INFO assertion 4, store_sos stores data correctly.: skipped
2023-03-07 15:17:37,313 TADA INFO assertion 5, store_sos stores data after restarted correctly.: skipped
2023-03-07 15:17:37,313 TADA INFO assertion 6, store_csv is storing data.: skipped
2023-03-07 15:17:37,313 TADA INFO assertion 7, store_csv stores data correctly.: skipped
2023-03-07 15:17:37,314 TADA INFO assertion 8, store_csv stores data after restarted correctly.: skipped
2023-03-07 15:17:37,314 TADA INFO test store_sos_lists_test ended
2023-03-07 15:17:49 INFO: ----------------------------------------------
2023-03-07 15:17:50 INFO: ======== maestro_raft_test ========
2023-03-07 15:17:50 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/maestro_raft_test
2023-03-07 15:17:51,482 TADA INFO starting test `maestro_raft_test`
2023-03-07 15:17:51,483 TADA INFO   test-id: 6a4252414827472f137703fbd6edf1444bdb0cb42a33302ff00fbdec2af2cab8
2023-03-07 15:17:51,483 TADA INFO   test-suite: LDMSD
2023-03-07 15:17:51,483 TADA INFO   test-name: maestro_raft_test
2023-03-07 15:17:51,483 TADA INFO   test-user: narate
2023-03-07 15:17:51,483 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:18:01,494 __main__ INFO -- Get or create cluster --
2023-03-07 15:18:36,013 __main__ INFO -- Start daemons --
2023-03-07 15:19:47,613 __main__ INFO -- making known hosts (ssh) --
2023-03-07 15:19:54,584 __main__ INFO ... make sure ldmsd's are up
2023-03-07 15:20:10,512 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: [('FOLLOWER', 2), ('LEADER', 1)], passed
2023-03-07 15:20:22,949 TADA INFO assertion 2, All ldmsds are up and configured: sets verified, passed
2023-03-07 15:20:23,219 TADA INFO assertion 3, Data are being stored: data check, passed
2023-03-07 15:20:28,143 TADA INFO assertion 4, New leader elected: checked, passed
2023-03-07 15:20:39,960 TADA INFO assertion 5, Restarted ldmsd is configured: sets verified, passed
2023-03-07 15:20:40,252 TADA INFO assertion 6, New data are presented in the store: data check, passed
2023-03-07 15:20:51,270 TADA INFO assertion 7, The restarted maestro becomes a follower: checked, passed
---Wait for config to write to file---
2023-03-07 15:20:51,271 TADA INFO test maestro_raft_test ended
2023-03-07 15:21:12 INFO: ----------------------------------------------
2023-03-07 15:21:13 INFO: ======== ovis_json_test ========
2023-03-07 15:21:13 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ovis_json_test
2023-03-07 15:21:14,233 __main__ INFO -- Create the cluster -- 
2023-03-07 15:21:19,636 TADA INFO starting test `ovis_json_test`
2023-03-07 15:21:19,637 TADA INFO   test-id: 9d2a33d7dbaddad178b0f6d5eb7e847bfdf172f5fca5c6e55b09ab7637dce60d
2023-03-07 15:21:19,637 TADA INFO   test-suite: OVIS-LIB
2023-03-07 15:21:19,637 TADA INFO   test-name: ovis_json_test
2023-03-07 15:21:19,637 TADA INFO   test-user: narate
2023-03-07 15:21:19,637 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:21:19,638 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-03-07 15:21:19,638 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-03-07 15:21:19,638 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-03-07 15:21:19,638 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-03-07 15:21:19,638 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-03-07 15:21:19,638 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-03-07 15:21:19,638 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-03-07 15:21:19,639 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-03-07 15:21:19,639 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,639 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,639 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,639 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,639 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,639 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,640 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,640 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 15:21:19,640 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-03-07 15:21:19,640 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-03-07 15:21:19,640 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-03-07 15:21:19,640 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-03-07 15:21:19,640 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-03-07 15:21:19,641 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-03-07 15:21:19,641 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-03-07 15:21:19,641 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-03-07 15:21:19,641 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-03-07 15:21:19,641 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-03-07 15:21:19,641 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-03-07 15:21:19,641 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,642 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,643 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 15:21:19,643 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-03-07 15:21:19,643 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-03-07 15:21:19,643 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-03-07 15:21:19,643 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-03-07 15:21:19,643 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-03-07 15:21:19,643 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-03-07 15:21:19,644 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-03-07 15:21:19,644 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-03-07 15:21:19,644 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-03-07 15:21:19,644 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-03-07 15:21:19,644 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-03-07 15:21:19,644 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-03-07 15:21:19,644 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-03-07 15:21:19,645 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-03-07 15:21:19,645 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-03-07 15:21:19,645 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-03-07 15:21:19,645 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-03-07 15:21:19,645 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-03-07 15:21:19,645 TADA INFO test ovis_json_test ended
2023-03-07 15:21:30 INFO: ----------------------------------------------
2023-03-07 15:21:31 INFO: ======== updtr_add_test ========
2023-03-07 15:21:31 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_add_test
2023-03-07 15:21:31,973 __main__ INFO -- Get or create the cluster --
2023-03-07 15:21:31,974 TADA INFO starting test `updtr_add test`
2023-03-07 15:21:31,974 TADA INFO   test-id: 88781ac5ab3b2417615449c968674f0ac07c8268bb80949c02621508ac37f989
2023-03-07 15:21:31,974 TADA INFO   test-suite: LDMSD
2023-03-07 15:21:31,974 TADA INFO   test-name: updtr_add test
2023-03-07 15:21:31,974 TADA INFO   test-user: narate
2023-03-07 15:21:31,974 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:21:40,187 __main__ INFO -- Start daemons --
2023-03-07 15:21:44,042 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:21:44,361 __main__ INFO All LDMSDs are up.
2023-03-07 15:21:45,583 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:21:46,804 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:21:48,015 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:21:49,223 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:21:50,448 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:21:52,915 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:21:55,358 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:21:56,578 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-03-07 15:21:56,578 __main__ INFO --- done ---
2023-03-07 15:21:56,578 TADA INFO test updtr_add test ended
2023-03-07 15:22:08 INFO: ----------------------------------------------
2023-03-07 15:22:09 INFO: ======== updtr_del_test ========
2023-03-07 15:22:09 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_del_test
2023-03-07 15:22:10,226 __main__ INFO -- Get or create the cluster --
2023-03-07 15:22:10,226 TADA INFO starting test `updtr_add test`
2023-03-07 15:22:10,226 TADA INFO   test-id: 9b699bc5fa317fa9439f3c22bc29c05fb3b52b6ff8a7b1aab8267dc68fa717dc
2023-03-07 15:22:10,226 TADA INFO   test-suite: LDMSD
2023-03-07 15:22:10,226 TADA INFO   test-name: updtr_add test
2023-03-07 15:22:10,226 TADA INFO   test-user: narate
2023-03-07 15:22:10,227 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:22:17,984 __main__ INFO -- Start daemons --
2023-03-07 15:22:21,689 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:22:21,997 __main__ INFO All LDMSDs are up.
2023-03-07 15:22:23,213 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:22:24,448 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 15:22:25,665 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:22:26,891 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:22:26,892 __main__ INFO --- done ---
2023-03-07 15:22:26,892 TADA INFO test updtr_add test ended
2023-03-07 15:22:39 INFO: ----------------------------------------------
2023-03-07 15:22:40 INFO: ======== updtr_match_add_test ========
2023-03-07 15:22:40 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_match_add_test
2023-03-07 15:22:40,772 __main__ INFO -- Get or create the cluster --
2023-03-07 15:22:40,772 TADA INFO starting test `updtr_add test`
2023-03-07 15:22:40,773 TADA INFO   test-id: 9f5f7db8ebbccd87fd50647150991afd3a3d847e51030c1000fbc2cf0f856371
2023-03-07 15:22:40,773 TADA INFO   test-suite: LDMSD
2023-03-07 15:22:40,773 TADA INFO   test-name: updtr_add test
2023-03-07 15:22:40,773 TADA INFO   test-user: narate
2023-03-07 15:22:40,773 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:22:48,655 __main__ INFO -- Start daemons --
2023-03-07 15:22:52,385 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:22:52,707 __main__ INFO All LDMSDs are up.
2023-03-07 15:22:53,919 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:22:55,134 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:22:56,350 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:22:57,569 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:22:58,805 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 15:22:58,805 __main__ INFO --- done ---
2023-03-07 15:22:58,805 TADA INFO test updtr_add test ended
2023-03-07 15:23:10 INFO: ----------------------------------------------
2023-03-07 15:23:11 INFO: ======== updtr_match_del_test ========
2023-03-07 15:23:11 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_match_del_test
2023-03-07 15:23:12,571 __main__ INFO -- Get or create the cluster --
2023-03-07 15:23:12,571 TADA INFO starting test `updtr_add test`
2023-03-07 15:23:12,571 TADA INFO   test-id: 2e6bfb325242f4d040c4b6f573fcea79fd66f21d3854994aa7262f209b95ab85
2023-03-07 15:23:12,571 TADA INFO   test-suite: LDMSD
2023-03-07 15:23:12,571 TADA INFO   test-name: updtr_add test
2023-03-07 15:23:12,571 TADA INFO   test-user: narate
2023-03-07 15:23:12,572 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:23:20,339 __main__ INFO -- Start daemons --
2023-03-07 15:23:24,061 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:23:24,378 __main__ INFO All LDMSDs are up.
2023-03-07 15:23:25,593 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-03-07 15:23:26,820 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:23:28,039 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:23:29,251 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:23:30,485 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:23:31,691 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:23:32,911 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:23:32,911 __main__ INFO --- done ---
2023-03-07 15:23:32,911 TADA INFO test updtr_add test ended
2023-03-07 15:23:45 INFO: ----------------------------------------------
2023-03-07 15:23:45 INFO: ======== updtr_prdcr_add_test ========
2023-03-07 15:23:45 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_prdcr_add_test
2023-03-07 15:23:46,574 __main__ INFO -- Get or create the cluster --
2023-03-07 15:23:46,574 TADA INFO starting test `updtr_add test`
2023-03-07 15:23:46,574 TADA INFO   test-id: 9e581046664d510c0f005dbd354b6a39dfde7e1ee948f6a3a012d160ad187a25
2023-03-07 15:23:46,574 TADA INFO   test-suite: LDMSD
2023-03-07 15:23:46,574 TADA INFO   test-name: updtr_add test
2023-03-07 15:23:46,575 TADA INFO   test-user: narate
2023-03-07 15:23:46,575 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:23:54,375 __main__ INFO -- Start daemons --
2023-03-07 15:23:58,069 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:23:58,414 __main__ INFO All LDMSDs are up.
2023-03-07 15:23:59,620 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:24:02,081 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:24:04,510 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:24:05,717 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-03-07 15:24:06,926 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:24:06,927 __main__ INFO --- done ---
2023-03-07 15:24:06,927 TADA INFO test updtr_add test ended
2023-03-07 15:24:19 INFO: ----------------------------------------------
2023-03-07 15:24:19 INFO: ======== updtr_prdcr_del_test ========
2023-03-07 15:24:19 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_prdcr_del_test
2023-03-07 15:24:20,655 __main__ INFO -- Get or create the cluster --
2023-03-07 15:24:20,655 TADA INFO starting test `updtr_add test`
2023-03-07 15:24:20,656 TADA INFO   test-id: d721cc8db796468c83cfb2bfd58d9f054147cfc6e7e5cc1494e8da1a630774e3
2023-03-07 15:24:20,656 TADA INFO   test-suite: LDMSD
2023-03-07 15:24:20,656 TADA INFO   test-name: updtr_add test
2023-03-07 15:24:20,656 TADA INFO   test-user: narate
2023-03-07 15:24:20,656 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:24:28,500 __main__ INFO -- Start daemons --
2023-03-07 15:24:32,255 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:24:32,578 __main__ INFO All LDMSDs are up.
2023-03-07 15:24:33,787 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:24:35,008 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 15:24:36,223 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:24:38,671 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 15:24:38,671 __main__ INFO --- done ---
2023-03-07 15:24:38,671 TADA INFO test updtr_add test ended
2023-03-07 15:24:50 INFO: ----------------------------------------------
2023-03-07 15:24:51 INFO: ======== updtr_start_test ========
2023-03-07 15:24:51 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_start_test
2023-03-07 15:24:52,390 __main__ INFO -- Get or create the cluster --
2023-03-07 15:24:52,391 TADA INFO starting test `updtr_add test`
2023-03-07 15:24:52,391 TADA INFO   test-id: 297e8956a9a27e4301bbf42313043f85a40bbaf8bce6cae15813a97c2e782403
2023-03-07 15:24:52,391 TADA INFO   test-suite: LDMSD
2023-03-07 15:24:52,391 TADA INFO   test-name: updtr_add test
2023-03-07 15:24:52,391 TADA INFO   test-user: narate
2023-03-07 15:24:52,391 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:25:00,332 __main__ INFO -- Start daemons --
2023-03-07 15:25:03,995 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:25:04,325 __main__ INFO All LDMSDs are up.
2023-03-07 15:25:05,552 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:25:06,780 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:25:08,007 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-03-07 15:25:09,225 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:25:10,449 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 15:25:12,903 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 15:25:14,125 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 15:25:16,595 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 15:25:19,019 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 15:25:21,461 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 15:25:22,690 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 15:25:22,690 __main__ INFO --- done ---
2023-03-07 15:25:22,691 TADA INFO test updtr_add test ended
2023-03-07 15:25:34 INFO: ----------------------------------------------
2023-03-07 15:25:35 INFO: ======== updtr_status_test ========
2023-03-07 15:25:35 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/updtr_status_test
2023-03-07 15:25:36,282 __main__ INFO -- Get or create the cluster --
2023-03-07 15:25:36,282 TADA INFO starting test `updtr_status test`
2023-03-07 15:25:36,282 TADA INFO   test-id: cc6df42887e76252a62f8bda5ca4280340d5d690dac1e45e3b60aafe4967dc9f
2023-03-07 15:25:36,282 TADA INFO   test-suite: LDMSD
2023-03-07 15:25:36,282 TADA INFO   test-name: updtr_status test
2023-03-07 15:25:36,282 TADA INFO   test-user: narate
2023-03-07 15:25:36,282 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:25:46,677 __main__ INFO -- Start daemons --
2023-03-07 15:25:51,635 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 15:25:52,063 __main__ INFO All LDMSDs are up.
2023-03-07 15:25:53,286 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-03-07 15:25:54,500 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-03-07 15:25:55,770 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:25:56,994 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:25:58,213 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 15:25:58,213 __main__ INFO --- done ---
2023-03-07 15:25:58,213 TADA INFO test updtr_status test ended
2023-03-07 15:26:10 INFO: ----------------------------------------------
2023-03-07 15:26:11 INFO: ======== ldmsd_flex_decomp_test ========
2023-03-07 15:26:11 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldmsd_flex_decomp_test
2023-03-07 15:26:12,574 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-03-07 15:26:12,574 TADA INFO   test-id: 346424713cbf18b247c19bf0297f60f47990746a1986c42e485f5a8421b065a9
2023-03-07 15:26:12,574 TADA INFO   test-suite: LDMSD
2023-03-07 15:26:12,575 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-03-07 15:26:12,575 TADA INFO   test-user: narate
2023-03-07 15:26:12,575 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:26:12,576 __main__ INFO -- Get or create the cluster --
2023-03-07 15:26:28,272 __main__ INFO -- Start daemons --
2023-03-07 15:26:38,552 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 15:27:27,713 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-03-07 15:27:27,713 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-03-07 15:27:27,713 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-03-07 15:27:27,713 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 5, record sos schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-03-07 15:27:27,714 TADA INFO assertion 10, record csv schema check: OK, passed
2023-03-07 15:27:27,715 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-03-07 15:27:27,715 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-03-07 15:27:27,715 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-03-07 15:27:27,715 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-03-07 15:27:27,715 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-03-07 15:27:27,717 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-03-07 15:27:27,786 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-03-07 15:27:27,789 TADA INFO assertion 18, fill sos data check: OK, passed
2023-03-07 15:27:27,790 TADA INFO assertion 19, filter sos data check: OK, passed
2023-03-07 15:27:27,798 TADA INFO assertion 20, record sos data check: OK, passed
2023-03-07 15:27:27,800 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-03-07 15:27:27,863 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-03-07 15:27:27,865 TADA INFO assertion 23, fill csv data check: OK, passed
2023-03-07 15:27:27,867 TADA INFO assertion 24, filter csv data check: OK, passed
2023-03-07 15:27:27,875 TADA INFO assertion 25, record csv data check: OK, passed
2023-03-07 15:27:27,876 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-03-07 15:27:27,899 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-03-07 15:27:27,900 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-03-07 15:27:27,901 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-03-07 15:27:27,905 TADA INFO assertion 30, record kafka data check: OK, passed
2023-03-07 15:27:27,905 TADA INFO test ldmsd_flex_decomp_test ended
2023-03-07 15:27:27,905 TADA INFO test ldmsd_flex_decomp_test ended
2023-03-07 15:27:43 INFO: ----------------------------------------------
2023-03-07 15:27:43 INFO: ======== ldms_set_info_test ========
2023-03-07 15:27:43 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/ldms_set_info_test
2023-03-07 15:27:54,390 TADA INFO starting test `ldms_set_info_test`
2023-03-07 15:27:54,390 TADA INFO   test-id: 3a2e028d980aaf16a5d4b65d77cae51b50254bc8f241df6fde7c2a9684048918
2023-03-07 15:27:54,390 TADA INFO   test-suite: LDMSD
2023-03-07 15:27:54,390 TADA INFO   test-name: ldms_set_info_test
2023-03-07 15:27:54,390 TADA INFO   test-user: narate
2023-03-07 15:27:54,390 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:27:54,391 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-03-07 15:27:54,391 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-03-07 15:27:54,391 TADA INFO assertion 3, Get a value : -, passed
2023-03-07 15:27:54,391 TADA INFO assertion 4, Unset a pair : -, passed
2023-03-07 15:27:54,391 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-03-07 15:27:54,391 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 7, Server resetting a key : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 8, Server unset a key : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 9, Server add a key : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 10, Adding a key : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-03-07 15:27:54,392 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-03-07 15:27:54,393 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-03-07 15:27:54,393 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-03-07 15:27:54,393 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-03-07 15:27:54,393 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-03-07 15:27:54,393 TADA INFO test ldms_set_info_test ended
2023-03-07 15:28:05 INFO: ----------------------------------------------
2023-03-07 15:28:05 INFO: ======== slurm_sampler2_test ========
2023-03-07 15:28:05 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-143022/data/slurm_sampler2_test
2023-03-07 15:28:06,600 TADA INFO starting test `slurm_sampler2_test`
2023-03-07 15:28:06,600 TADA INFO   test-id: 6242171b651310f691a7cf2230b241d992c6f177efaa9758e675002e7a113150
2023-03-07 15:28:06,600 TADA INFO   test-suite: LDMSD
2023-03-07 15:28:06,600 TADA INFO   test-name: slurm_sampler2_test
2023-03-07 15:28:06,600 TADA INFO   test-user: narate
2023-03-07 15:28:06,600 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 15:28:06,601 __main__ INFO -- Get or create the cluster --
2023-03-07 15:28:20,280 __main__ INFO -- Add users --
2023-03-07 15:28:25,683 __main__ INFO -- Preparing job script & programs --
2023-03-07 15:28:26,377 __main__ INFO -- Start daemons --
2023-03-07 15:28:48,480 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-03-07 15:28:52,115 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-03-07 15:28:53,757 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-03-07 15:28:55,405 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-03-07 15:28:57,021 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: [node-1]: The job_list is not as expected. {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1} != {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 3, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1}, failed
Traceback (most recent call last):
  File "slurm_sampler2_test", line 880, in <module>
    test_all_step(job_1, node_jobs, DELETE_COMPLETE_JOBS)
  File "slurm_sampler2_test", line 691, in test_all_step
    "The metric values are as expected on all nodes." if passed else reason)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test the slurm_sampler2 plugin, [node-1]: The job_list is not as expected. {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1} != {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 3, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1}: FAILED
2023-03-07 15:28:57,022 TADA INFO assertion 3.1, Expanding the set heap -- job_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 4.1, Multi-tenant -- job_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 3.2, Expanding the set heap -- step_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 4.2, Multi-tenant -- step_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 3.3, Expanding the set heap -- task_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 4.3, Multi-tenant -- task_init: skipped
2023-03-07 15:28:57,023 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: skipped
2023-03-07 15:28:57,024 TADA INFO assertion 4.4, Multi-tenant -- task_exit: skipped
2023-03-07 15:28:57,024 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: skipped
2023-03-07 15:28:57,024 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: skipped
2023-03-07 15:28:57,024 TADA INFO assertion 4.5, Multi-tenant -- job_exit: skipped
2023-03-07 15:28:57,024 TADA INFO test slurm_sampler2_test ended
2023-03-07 15:29:11 INFO: ----------------------------------------------
2023-03-07 15:29:12 INFO: ======== test-ldms ========
2023-03-07 15:29:12 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-03-07T15:29:12-06:00 INFO: starting test-samp-1
f1d40f4ab41744019856cf57e6aa55a726b6840e3009a3c5595d20daa75350ad
2023-03-07T15:29:15-06:00 INFO: starting test-samp-2
318baa2c2b61b62f8bbc80bc80700624daab8e9bbae6a96d0cecbf8cb0d1bdf3
2023-03-07T15:29:16-06:00 INFO: starting test-samp-3
54991aa2b597f9fb667a5e37e7cdf3c577e91f4a5dc658d849c5169e900ae712
2023-03-07T15:29:18-06:00 INFO: starting test-samp-4
2bbe1ba2793c277065998484e0a4154d84c403db28d0a11e903f8c883f1e2415
2023-03-07T15:29:20-06:00 INFO: test-samp-1 is running
2023-03-07T15:29:20-06:00 INFO: test-samp-2 is running
2023-03-07T15:29:20-06:00 INFO: test-samp-3 is running
2023-03-07T15:29:20-06:00 INFO: test-samp-4 is running
2023-03-07T15:29:20-06:00 INFO: starting test-agg-11
3524c48fbcf8d853ef39a186ebb0dd8502e5036662322fdd35e9dced2cd8c9e3
2023-03-07T15:29:22-06:00 INFO: starting test-agg-12
dc9b3021095c18ff57c01135df0f1d6c82703a823616e72f97dea6b13e495dd7
2023-03-07T15:29:24-06:00 INFO: test-agg-11 is running
2023-03-07T15:29:24-06:00 INFO: test-agg-12 is running
2023-03-07T15:29:24-06:00 INFO: starting test-agg-2
f1d096833e04b9ddf4d4310e25e3a3c79088f28f817d6c4f9e8659e3f6a7fb26
2023-03-07T15:29:26-06:00 INFO: test-agg-2 is running
2023-03-07T15:29:26-06:00 INFO: Collecting data (into SOS)
2023-03-07T15:29:36-06:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-03-07T15:29:38-06:00 INFO: check rc: 0
2023-03-07T15:29:38-06:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-03-07T15:29:43-06:00 INFO: DONE
2023-03-07 15:29:53 INFO: ----------------------------------------------
2023-03-07 15:29:53 INFO: ======== test-maestro ========
2023-03-07 15:29:53 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-03-07T15:29:53-06:00 INFO: starting mtest-maestro
f0417eb36fac3c967ff4b19853e6fcef35f575f7b206e43de149db321906f37d
2023-03-07T15:29:55-06:00 INFO: starting mtest-samp-1
311ec2b0fe49fd7ad113c4c2b3efe41960785b96ddc2b278533568db72503f97
2023-03-07T15:29:57-06:00 INFO: starting mtest-samp-2
e9f4aa504c48a7306b43e5f858e6fc28122cd7dce28d81897c66afe6f0af0d40
2023-03-07T15:29:59-06:00 INFO: starting mtest-samp-3
52a84ec810855a897697c627f1036f0a134869a7e61eb1781b5a390a9cd09c93
2023-03-07T15:30:00-06:00 INFO: starting mtest-samp-4
887de7d1c5095ba87e2b30c05bd5a494dff4f6f38b4ca831be9f9624813b18be
2023-03-07T15:30:02-06:00 INFO: mtest-samp-1 is running
2023-03-07T15:30:02-06:00 INFO: mtest-samp-2 is running
2023-03-07T15:30:02-06:00 INFO: mtest-samp-3 is running
2023-03-07T15:30:02-06:00 INFO: mtest-samp-4 is running
2023-03-07T15:30:02-06:00 INFO: starting mtest-agg-11
b7487a6230faf6ee3f60eae8b1780071de5026c46c07ecb20844bcf26f4f65dc
2023-03-07T15:30:04-06:00 INFO: starting mtest-agg-12
fe742b9b2cecacf7cd0b30d71f882abe7e0e6ed97e336865ca3a131c6716edfc
2023-03-07T15:30:05-06:00 INFO: mtest-agg-11 is running
2023-03-07T15:30:05-06:00 INFO: mtest-agg-12 is running
2023-03-07T15:30:05-06:00 INFO: starting mtest-agg-2
6fcfba36bd2d3e63ea4c62d379447537b0aad21a62983a07ad2660af9c619b94
2023-03-07T15:30:07-06:00 INFO: mtest-agg-2 is running
2023-03-07T15:30:07-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-03-07T15:30:18-06:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-03-07T15:30:20-06:00 INFO: sos check rc: 0
2023-03-07T15:30:21-06:00 INFO: starting mtest-ui
1b47483e1f7a32a91f5730c982a2269eeb3623d62de261ada580a0b7136c92d2
2023-03-07T15:30:27-06:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[7872612, 1678224609000.925], [7872928, 1678224610001.755], [7872928, 1678224610001.7678], [7872984, 1678224611001.2969], [7872984, 1678224611001.334], [7872984, 1678224611001.3472], [7872984, 1678224611001.601], [7872984, 1678224612001.456], [7872984, 1678224612001.493], [7872984, 1678224612001.498], [7872984, 1678224612001.502], [7872984, 1678224613001.322], [7872984, 1678224613001.5989], [7872984, 1678224613001.5989], [7872984, 1678224613001.616], [7872984, 1678224614001.476], [7872984, 1678224614001.477], [7872984, 1678224614001.719], [7872984, 1678224614001.7458], [7872984, 1678224615001.629], [7872984, 1678224615001.638], [7872984, 1678224615001.643], [7872984, 1678224615001.903], [7872984, 1678224616001.635], [7872984, 1678224616001.77], [7872984, 1678224616001.779], [7872984, 1678224616002.028]]}, {"target": "component_id", "datapoints": [[3, 1678224609000.925], [1, 1678224610001.755], [3, 1678224610001.7678], [2, 1678224611001.2969], [1, 1678224611001.334], [3, 1678224611001.3472], [4, 1678224611001.601], [2, 1678224612001.456], [1, 1678224612001.493], [4, 1678224612001.498], [3, 1678224612001.502], [1, 1678224613001.322], [2, 1678224613001.5989], [3, 1678224613001.5989], [4, 1678224613001.616], [4, 1678224614001.476], [1, 1678224614001.477], [3, 1678224614001.719], [2, 1678224614001.7458], [1, 1678224615001.629], [4, 1678224615001.638], [3, 1678224615001.643], [2, 1678224615001.903], [1, 1678224616001.635], [4, 1678224616001.77], [3, 1678224616001.779], [2, 1678224616002.028]]}, {"target": "job_id", "datapoints": [[0, 1678224609000.925], [0, 1678224610001.755], [0, 1678224610001.7678], [0, 1678224611001.2969], [0, 1678224611001.334], [0, 1678224611001.3472], [0, 1678224611001.601], [0, 1678224612001.456], [0, 1678224612001.493], [0, 1678224612001.498], [0, 1678224612001.502], [0, 1678224613001.322], [0, 1678224613001.5989], [0, 1678224613001.5989], [0, 1678224613001.616], [0, 1678224614001.476], [0, 1678224614001.477], [0, 1678224614001.719], [0, 1678224614001.7458], [0, 1678224615001.629], [0, 1678224615001.638], [0, 1678224615001.643], [0, 1678224615001.903], [0, 1678224616001.635], [0, 1678224616001.77], [0, 1678224616001.779], [0, 1678224616002.028]]}]'
comp_ids:{1, 2, 3, 4}
2023-03-07T15:30:30-06:00 INFO: query check RC: 0
5c72e4e1f90ff7d6279442952d3ab2341ea469025ae9994613b77034f56413f5
2023-03-07T15:31:01-06:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2212    683 --:--:-- --:--:-- --:--:--  2903
{"datasource":{"id":1,"uid":"RtAgUka4z","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-03-07T15:31:03-06:00 INFO: Checking grafana data
2023-03-07T15:31:03-06:00 INFO: Grafana data check, rc: 0
2023-03-07T15:31:03-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-03-07T15:31:08-06:00 INFO: DONE
2023-03-07 15:31:18 INFO: ----------------------------------------------
2023-03-07 15:31:18 INFO: ======== test-maestro-hostmunge ========
2023-03-07 15:31:18 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-03-07T15:31:18-06:00 INFO: Checking munge on localhost
2023-03-07T15:31:18-06:00 INFO: munge encode/decode successfully
2023-03-07T15:31:18-06:00 INFO: starting mtest-maestro
aac207e2c892e4713e35ea7b685aef4bac7160f01e107af2cee7f8f33969de63
2023-03-07T15:31:20-06:00 INFO: starting mtest-samp-1
5f9b0e2abfb80ae35b5252194aad9685083d6867d03200801012d6a2c52256a7
2023-03-07T15:31:22-06:00 INFO: starting mtest-samp-2
1c6f70ac3f4492774086fc9113315aff3dd49d9209cdb9e16d2a2aedf5fa34b0
2023-03-07T15:31:24-06:00 INFO: starting mtest-samp-3
79f4043b8ddacd13cdd31cb3cee77f67f031992e198c33a1ca40bd5c8c801812
2023-03-07T15:31:25-06:00 INFO: starting mtest-samp-4
8339fc22de215cecef61b64d369d7c6aa0a4db639a84caf242d8601424f7c2f9
2023-03-07T15:31:27-06:00 INFO: mtest-samp-1 is running
2023-03-07T15:31:27-06:00 INFO: mtest-samp-2 is running
2023-03-07T15:31:27-06:00 INFO: mtest-samp-3 is running
2023-03-07T15:31:27-06:00 INFO: mtest-samp-4 is running
2023-03-07T15:31:27-06:00 INFO: starting mtest-agg-11
604529f932d1aec62873ce565d89f07116c1d81341e6343c11bd2a565d542994
2023-03-07T15:31:28-06:00 INFO: starting mtest-agg-12
623a192e27a0705f26576d2064b8ac5d842bcb7f2286fe1e590f7a6fbf4d7511
2023-03-07T15:31:30-06:00 INFO: mtest-agg-11 is running
2023-03-07T15:31:30-06:00 INFO: mtest-agg-12 is running
2023-03-07T15:31:30-06:00 INFO: starting mtest-agg-2
fc468c0e98c7daeff47a491d2ac7494fef81e5d57e81e1a88832f0d55ed6b88e
2023-03-07T15:31:31-06:00 INFO: mtest-agg-2 is running
2023-03-07T15:31:31-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-03-07T15:31:42-06:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-03-07T15:31:44-06:00 INFO: sos check rc: 0
2023-03-07T15:31:46-06:00 INFO: starting mtest-ui
0534008e032504021890b9df6594488861b01aabdde625297f394b26234f8939
2023-03-07T15:31:48-06:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[7879556, 1678224695001.455], [7879556, 1678224695001.9001], [7879800, 1678224696001.332], [7879800, 1678224696001.3682], [7879800, 1678224696001.749], [7879800, 1678224696002.024], [7879944, 1678224697001.182], [7879944, 1678224697001.482], [7879944, 1678224697001.495], [7879944, 1678224697001.527], [7879944, 1678224698001.3408], [7879944, 1678224698001.6409], [7879944, 1678224698001.643], [7879944, 1678224698001.6682], [7879944, 1678224699001.498], [7879944, 1678224699001.7979], [7879944, 1678224699001.8], [7879944, 1678224699001.8088], [7879944, 1678224700001.65], [7879944, 1678224700001.9468], [7879944, 1678224700001.951], [7879944, 1678224700001.952], [7879948, 1678224701001.8079], [7879948, 1678224701002.098], [7879948, 1678224701002.0989], [7879948, 1678224701002.103]]}, {"target": "component_id", "datapoints": [[1, 1678224695001.455], [3, 1678224695001.9001], [4, 1678224696001.332], [2, 1678224696001.3682], [1, 1678224696001.749], [3, 1678224696002.024], [3, 1678224697001.182], [4, 1678224697001.482], [1, 1678224697001.495], [2, 1678224697001.527], [3, 1678224698001.3408], [1, 1678224698001.6409], [4, 1678224698001.643], [2, 1678224698001.6682], [3, 1678224699001.498], [4, 1678224699001.7979], [1, 1678224699001.8], [2, 1678224699001.8088], [3, 1678224700001.65], [1, 1678224700001.9468], [2, 1678224700001.951], [4, 1678224700001.952], [3, 1678224701001.8079], [1, 1678224701002.098], [4, 1678224701002.0989], [2, 1678224701002.103]]}, {"target": "job_id", "datapoints": [[0, 1678224695001.455], [0, 1678224695001.9001], [0, 1678224696001.332], [0, 1678224696001.3682], [0, 1678224696001.749], [0, 1678224696002.024], [0, 1678224697001.182], [0, 1678224697001.482], [0, 1678224697001.495], [0, 1678224697001.527], [0, 1678224698001.3408], [0, 1678224698001.6409], [0, 1678224698001.643], [0, 1678224698001.6682], [0, 1678224699001.498], [0, 1678224699001.7979], [0, 1678224699001.8], [0, 1678224699001.8088], [0, 1678224700001.65], [0, 1678224700001.9468], [0, 1678224700001.951], [0, 1678224700001.952], [0, 1678224701001.8079], [0, 1678224701002.098], [0, 1678224701002.0989], [0, 1678224701002.103]]}]'
comp_ids:{1, 2, 3, 4}
2023-03-07T15:31:50-06:00 INFO: query check RC: 0
a2164b53365e11a61b8abf278987da4b646dad5201d3c1c4ba8f10e524c44b83
2023-03-07T15:32:22-06:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2260    697 --:--:-- --:--:-- --:--:--  2975
{"datasource":{"id":1,"uid":"rw3iUza4k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-03-07T15:32:23-06:00 INFO: Checking grafana data
2023-03-07T15:32:23-06:00 INFO: Grafana data check, rc: 0
2023-03-07T15:32:23-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-03-07T15:32:28-06:00 INFO: DONE
2023-03-07 15:32:38 INFO: ----------------------------------------------
2023-03-07 15:32:38 INFO: ======== test-maestro-munge ========
2023-03-07 15:32:38 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000184419 s, 22.2 MB/s
2023-03-07T15:32:39-06:00 INFO: starting mtest-maestro
eb20fe9d93c64bcff1478e3a535297720317f537453b358c3efd3bfe7003b24e
2023-03-07T15:32:42-06:00 INFO: starting mtest-samp-1
52e22443b1ac456bbfdf3b873ff48337070786118857ce64c84f6b98f4d36814
2023-03-07T15:32:43-06:00 INFO: starting mtest-samp-2
98db62b4cbdbd20d59dcd98c2a29889f713d05ae5ef23e96cc0ec08a5199d62c
2023-03-07T15:32:45-06:00 INFO: starting mtest-samp-3
465e9db35dace14f55723a18cc0fef7937087f0691caad9efd6d733e4c257859
2023-03-07T15:32:47-06:00 INFO: starting mtest-samp-4
60581010a5c8d880b836173c19e111dcb5d6eb8956946b278bcfd0d6ec07c032
2023-03-07T15:32:48-06:00 INFO: mtest-samp-1 is running
2023-03-07T15:32:49-06:00 INFO: mtest-samp-2 is running
2023-03-07T15:32:49-06:00 INFO: mtest-samp-3 is running
2023-03-07T15:32:49-06:00 INFO: mtest-samp-4 is running
2023-03-07T15:32:49-06:00 INFO: starting mtest-agg-11
af02f55773597565dbae04bc1d9d05c3bb09451a4575803f42f00713358759a2
2023-03-07T15:32:50-06:00 INFO: starting mtest-agg-12
6c777bb2e2f146995952d3256b5a5c5326583c167cdbc072a2c080c6482664a3
2023-03-07T15:32:52-06:00 INFO: mtest-agg-11 is running
2023-03-07T15:32:52-06:00 INFO: mtest-agg-12 is running
2023-03-07T15:32:52-06:00 INFO: starting mtest-agg-2
90e72dc668e835ff919341730e61c891d65a2b51f4e589511416320958a5f939
2023-03-07T15:32:53-06:00 INFO: mtest-agg-2 is running
2023-03-07T15:32:53-06:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-03-07T15:33:04-06:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-03-07T15:33:06-06:00 INFO: sos check rc: 0
2023-03-07T15:33:08-06:00 INFO: starting mtest-ui
7aaf794185832a01328ba8fecc89920e37146c682fda0d293eec4a96a4561107
2023-03-07T15:33:09-06:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[7880140, 1678224777001.84], [7880140, 1678224777001.846], [7880512, 1678224778001.487], [7880512, 1678224778001.491], [7880512, 1678224778001.97], [7880512, 1678224778001.992], [7880516, 1678224779001.3198], [7880516, 1678224779001.332], [7880516, 1678224779001.3362], [7880516, 1678224779002.111], [7880516, 1678224780001.283], [7880516, 1678224780001.2852], [7880516, 1678224780001.3079], [7880516, 1678224780001.456], [7880516, 1678224781001.4458], [7880516, 1678224781001.449], [7880516, 1678224781001.4568], [7880516, 1678224781001.464], [7880516, 1678224782001.601], [7880516, 1678224782001.603], [7880516, 1678224782001.607], [7880516, 1678224782001.609], [7880516, 1678224783001.762], [7880516, 1678224783001.769], [7880516, 1678224783001.771], [7880516, 1678224783001.773]]}, {"target": "component_id", "datapoints": [[1, 1678224777001.84], [3, 1678224777001.846], [2, 1678224778001.487], [4, 1678224778001.491], [3, 1678224778001.97], [1, 1678224778001.992], [3, 1678224779001.3198], [4, 1678224779001.332], [2, 1678224779001.3362], [1, 1678224779002.111], [3, 1678224780001.283], [1, 1678224780001.2852], [4, 1678224780001.3079], [2, 1678224780001.456], [3, 1678224781001.4458], [1, 1678224781001.449], [4, 1678224781001.4568], [2, 1678224781001.464], [4, 1678224782001.601], [3, 1678224782001.603], [1, 1678224782001.607], [2, 1678224782001.609], [4, 1678224783001.762], [2, 1678224783001.769], [3, 1678224783001.771], [1, 1678224783001.773]]}, {"target": "job_id", "datapoints": [[0, 1678224777001.84], [0, 1678224777001.846], [0, 1678224778001.487], [0, 1678224778001.491], [0, 1678224778001.97], [0, 1678224778001.992], [0, 1678224779001.3198], [0, 1678224779001.332], [0, 1678224779001.3362], [0, 1678224779002.111], [0, 1678224780001.283], [0, 1678224780001.2852], [0, 1678224780001.3079], [0, 1678224780001.456], [0, 1678224781001.4458], [0, 1678224781001.449], [0, 1678224781001.4568], [0, 1678224781001.464], [0, 1678224782001.601], [0, 1678224782001.603], [0, 1678224782001.607], [0, 1678224782001.609], [0, 1678224783001.762], [0, 1678224783001.769], [0, 1678224783001.771], [0, 1678224783001.773]]}]'
comp_ids:{1, 2, 3, 4}
2023-03-07T15:33:11-06:00 INFO: query check RC: 0
5d00dba3db55e5be3cd16e43a355d88de590a8bf0383f21382d218c4050c2998
2023-03-07T15:33:43-06:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2154    665 --:--:-- --:--:-- --:--:--  2834
{"datasource":{"id":1,"uid":"vnh78zaVz","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-03-07T15:33:44-06:00 INFO: Checking grafana data
2023-03-07T15:33:44-06:00 INFO: Grafana data check, rc: 0
2023-03-07T15:33:44-06:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-03-07T15:33:50-06:00 INFO: DONE
2023-03-07 15:34:00 INFO: ----------------------------------------------
2023-03-07 15:34:00 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;31mFAILED[0m
papi_store_test: [01;31mFAILED[0m
updtr_status_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;31mFAILED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;31mFAILED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;32mPASSED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;32mPASSED[0m
agg_slurm_test: [01;32mPASSED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;31mFAILED[0m
ldmsd_auth_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;32mPASSED[0m
cont-test-ldms: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;32mPASSED[0m
maestro_raft_test: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
cont-test-maestro: [01;32mPASSED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_stream_status_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
ldmsd_stream_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;31mFAILED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 41/47
------------------------------------------
