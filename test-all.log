2023-11-03 00:30:02 INFO: WORK_DIR: /mnt/300G/data/2023-11-03-003001
2023-11-03 00:30:02 INFO: LOG: /mnt/300G/data/2023-11-03-003001/cygnus-weekly.log
2023-11-03 00:30:02 INFO: OVIS_NEW_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:30:02 INFO: OVIS_OLD_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:30:02 INFO: CONT_GIT_SHA: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:30:02 INFO: -----------------------------------------------
2023-11-03 00:30:02 INFO: LDMS_TEST_REPO: https://github.com/ovis-hpc/ldms-test
2023-11-03 00:30:02 INFO: LDMS_TEST_BRANCH: master
2023-11-03 00:30:02 INFO: LDMS_TEST_NEW_GIT_SHA: 1cfe9e4c12e8e2fa43bc2c7c80f91f732da9428d
2023-11-03 00:30:02 INFO: LDMS_TEST_OLD_GIT_SHA: 
~/cron/ldms-test ~
/mnt/300G/data/2023-11-03-003001 ~/cron/ldms-test ~
2023-11-03 00:30:03 INFO: Skip building on host because GIT SHA has not changed: 
32d5252f2776353a2e7fc9be6a15930f6c1f575b
OVIS_LDMS_OVIS_GIT_LONG "32d5252f2776353a2e7fc9be6a15930f6c1f575b"
2023-11-03 00:30:03 INFO: Skip building containerized binary because GIT SHA has not changed: 
2023-11-03 00:30:03 INFO: -- Installation process succeeded --
2023-11-03 00:30:03 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-11-03-003001
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-11-03-003001
HEAD is now at 1e2adcb 2023-11-02-003001
[master 35ed6c0] 2023-11-03-003001
 2 files changed, 23 insertions(+), 2840 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   1e2adcb..35ed6c0  master -> master
~/cron/ldms-test /mnt/300G/data/2023-11-03-003001
2023-11-03 00:30:05 INFO: ==== OVIS+SOS Installation Completed ====
2023-11-03 00:30:05 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-11-03-003001 ~/cron/ldms-test ~
2023-11-03 00:30:05 INFO: ======== direct_ldms_ls_conn_test ========
2023-11-03 00:30:05 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/direct_ldms_ls_conn_test
2023-11-03 00:30:05,990 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-11-03 00:30:05,990 TADA INFO   test-id: 3622f2a568bafd571890633393491bfd0044b6ca0cb73ae564be47993c30d069
2023-11-03 00:30:05,990 TADA INFO   test-suite: LDMSD
2023-11-03 00:30:05,990 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-11-03 00:30:05,990 TADA INFO   test-user: narate
2023-11-03 00:30:05,990 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:30:06,216 __main__ INFO starting munged on cygnus-01-iw
2023-11-03 00:30:06,556 __main__ INFO starting munged on localhost
2023-11-03 00:30:06,795 __main__ INFO starting ldmsd on cygnus-01-iw
2023-11-03 00:30:07,103 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-11-03 00:30:12,304 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-11-03 00:30:12,304 __main__ INFO Stopping sampler daemon ...
2023-11-03 00:30:17,719 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-11-03 00:30:17,758 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-11-03 00:30:17,792 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-11-03 00:30:17,793 TADA INFO test direct_ldms_ls_conn_test ended
2023-11-03 00:30:17,997 __main__ INFO stopping munged on cygnus-01-iw
2023-11-03 00:30:18,412 __main__ INFO stopping munged on localhost
2023-11-03 00:30:18 INFO: ----------------------------------------------
2023-11-03 00:30:18 INFO: ======== direct_prdcr_subscribe_test ========
2023-11-03 00:30:18 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/direct_prdcr_subscribe_test
2023-11-03 00:30:19,251 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-11-03 00:30:19,252 TADA INFO   test-id: d58fde92e9ac1582f0b810e38df76b911c2487ec11ce9b4e73342f2706982fd5
2023-11-03 00:30:19,252 TADA INFO   test-suite: LDMSD
2023-11-03 00:30:19,252 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-11-03 00:30:19,252 TADA INFO   test-user: narate
2023-11-03 00:30:19,252 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:30:21,179 __main__ INFO starting munged on cygnus-01-iw
2023-11-03 00:30:21,725 __main__ INFO starting munged on cygnus-05-iw
2023-11-03 00:30:22,291 __main__ INFO starting munged on cygnus-03-iw
2023-11-03 00:30:22,807 __main__ INFO starting munged on cygnus-04-iw
2023-11-03 00:30:23,143 __main__ INFO starting munged on localhost
2023-11-03 00:30:23,383 __main__ INFO starting ldmsd on cygnus-01-iw
2023-11-03 00:30:23,924 __main__ INFO starting ldmsd on cygnus-05-iw
2023-11-03 00:30:24,460 __main__ INFO starting ldmsd on cygnus-03-iw
2023-11-03 00:30:24,955 __main__ INFO starting ldmsd on cygnus-04-iw
2023-11-03 00:30:31,996 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-11-03 00:30:31,997 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-11-03 00:30:31,997 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-11-03 00:30:31,998 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-11-03 00:30:31,999 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
2023-11-03 00:30:32,042 TADA INFO assertion 5, Stopping the producers succeeds: agg-1 producers stopped, passed
2023-11-03 00:30:33,044 TADA INFO assertion 6, Restarting the producers succeeds: agg-1 producers started, passed
2023-11-03 00:30:39,760 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-11-03 00:30:39,761 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-11-03 00:30:39,761 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-11-03 00:30:39,762 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-11-03 00:30:39,763 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verified, passed
2023-11-03 00:30:39,763 __main__ INFO stopping sampler-1
2023-11-03 00:30:41,172 TADA INFO assertion 12, stream-sampler-1 is not running: sampler-1 stopped, passed
2023-11-03 00:30:41,173 __main__ INFO starting sampler-1
2023-11-03 00:30:42,430 TADA INFO assertion 13, stream-sampler-1 has restarted: sampler-1 running, passed
2023-11-03 00:30:42,430 __main__ INFO allow some time for prdcr to reconnect ...
2023-11-03 00:30:48,347 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-11-03 00:30:48,348 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-11-03 00:30:48,348 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verified, passed
2023-11-03 00:30:48,349 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: unsubscribed, passed
2023-11-03 00:30:50,676 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-11-03 00:30:50,681 __main__ INFO stopping agg-1
2023-11-03 00:30:55,901 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-11-03 00:30:55,902 TADA INFO test direct_prdcr_subscribe_test ended
2023-11-03 00:30:56,109 __main__ INFO stopping munged on cygnus-01-iw
2023-11-03 00:30:56,532 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-11-03 00:30:56,977 __main__ INFO stopping munged on cygnus-05-iw
2023-11-03 00:30:57,453 __main__ INFO stopping ldmsd on cygnus-05-iw
2023-11-03 00:30:57,903 __main__ INFO stopping munged on cygnus-03-iw
2023-11-03 00:30:58,544 __main__ INFO stopping munged on cygnus-04-iw
2023-11-03 00:30:58,961 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-11-03 00:30:59,166 __main__ INFO stopping munged on localhost
2023-11-03 00:30:59 INFO: ----------------------------------------------
2023-11-03 00:30:59 INFO: ======== agg_slurm_test ========
2023-11-03 00:30:59 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/agg_slurm_test
2023-11-03 00:31:00,065 TADA INFO starting test `agg_slurm_test`
2023-11-03 00:31:00,066 TADA INFO   test-id: 562f7b043cef1ff7724678a853b2f6c583b1e00d5faf48b8de98be0b642c4900
2023-11-03 00:31:00,066 TADA INFO   test-suite: LDMSD
2023-11-03 00:31:00,066 TADA INFO   test-name: agg_slurm_test
2023-11-03 00:31:00,066 TADA INFO   test-user: narate
2023-11-03 00:31:00,066 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:31:00,067 __main__ INFO -- Get or create the cluster --
2023-11-03 00:31:25,815 __main__ INFO -- Preparing syspapi JSON file --
2023-11-03 00:31:25,928 __main__ INFO -- Preparing jobpapi JSON file --
2023-11-03 00:31:26,028 __main__ INFO -- Preparing job script & programs --
2023-11-03 00:31:27,459 __main__ INFO -- Start daemons --
2023-11-03 00:31:59,369 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:32:04,372 __main__ INFO -- ldms_ls to agg-2 --
2023-11-03 00:32:04,512 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-11-03 00:32:04,633 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-11-03 00:32:09,636 __main__ INFO -- Submitting jobs --
2023-11-03 00:32:09,768 __main__ INFO job_one: 1
2023-11-03 00:32:09,904 __main__ INFO job_two: 2
2023-11-03 00:32:19,915 __main__ INFO -- Cancelling jobs --
2023-11-03 00:32:19,915 __main__ INFO job_one: 1
2023-11-03 00:32:20,054 __main__ INFO job_two: 2
2023-11-03 00:33:32,184 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-11-03 00:33:32,185 TADA INFO assertion 3, meminfo data verification: No data missing, failed
Traceback (most recent call last):
  File "agg_slurm_test", line 592, in <module>
    test.assert_test(3, len(meminfo) > 5 and missing_counts == 0, "No data missing")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: LDMSD 2-level agg with slurm, No data missing: FAILED
2023-11-03 00:33:32,186 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: skipped
2023-11-03 00:33:32,186 TADA INFO test agg_slurm_test ended
2023-11-03 00:33:47 INFO: ----------------------------------------------
2023-11-03 00:33:47 INFO: ======== papi_sampler_test ========
2023-11-03 00:33:47 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/papi_sampler_test
2023-11-03 00:33:48,703 TADA INFO starting test `papi_sampler_test`
2023-11-03 00:33:48,703 TADA INFO   test-id: ceacb2d3bf0c3b833725536d346194e56ee13630b55b009c0f374b8ed69a4b48
2023-11-03 00:33:48,703 TADA INFO   test-suite: LDMSD
2023-11-03 00:33:48,703 TADA INFO   test-name: papi_sampler_test
2023-11-03 00:33:48,703 TADA INFO   test-user: narate
2023-11-03 00:33:48,703 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:33:48,704 __main__ INFO -- Get or create the cluster --
2023-11-03 00:34:01,993 __main__ INFO -- Start daemons --
2023-11-03 00:34:15,817 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-11-03 00:34:16,079 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-11-03 00:34:21,209 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-11-03 00:34:21,399 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-11-03 00:34:21,400 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-11-03 00:34:35,265 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-11-03 00:34:35,266 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-11-03 00:34:35,266 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-11-03 00:34:35,266 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-11-03 00:34:35,483 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-11-03 00:34:41,349 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-11-03 00:34:41,350 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-11-03 00:34:41,350 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-11-03 00:34:41,350 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-11-03 00:34:41,558 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-11-03 00:34:41,559 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/papi1/3.0', 'node-1/papi0/2.0', 'node-1/meminfo'}), passed
2023-11-03 00:34:52,195 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-11-03 00:35:32,548 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-11-03 00:35:34,928 TADA INFO assertion 8, Missing config file attribute is logged: : sampler.papi_sampler: papi_sampler[515]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-11-03 00:35:40,388 TADA INFO assertion 9, Bad config file is logged: : sampler.papi_sampler: configuration file syntax error., passed
2023-11-03 00:35:40,388 __main__ INFO -- Finishing Test --
2023-11-03 00:35:40,389 TADA INFO test papi_sampler_test ended
2023-11-03 00:35:40,389 __main__ INFO -- Cleaning up files --
2023-11-03 00:35:40,389 __main__ INFO -- Removing the virtual cluster --
2023-11-03 00:35:52 INFO: ----------------------------------------------
2023-11-03 00:35:52 INFO: ======== papi_store_test ========
2023-11-03 00:35:52 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/papi_store_test
2023-11-03 00:35:53,735 TADA INFO starting test `papi_store_test`
2023-11-03 00:35:53,735 TADA INFO   test-id: 33dbb8e8f640ad7a67dbf20f18d2087a0d9faa4f42364267e2a217b28a14bbef
2023-11-03 00:35:53,735 TADA INFO   test-suite: LDMSD
2023-11-03 00:35:53,735 TADA INFO   test-name: papi_store_test
2023-11-03 00:35:53,736 TADA INFO   test-user: narate
2023-11-03 00:35:53,736 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:35:53,737 __main__ INFO -- Get or create the cluster --
2023-11-03 00:36:10,226 __main__ INFO -- Start daemons --
2023-11-03 00:36:51,550 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-11-03 00:36:51,551 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-11-03 00:36:51,551 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-11-03 00:36:51,551 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-11-03 00:36:51,551 TADA INFO test papi_store_test ended
2023-11-03 00:37:04 INFO: ----------------------------------------------
2023-11-03 00:37:04 INFO: ======== store_app_test ========
2023-11-03 00:37:04 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/store_app_test
2023-11-03 00:37:05,701 TADA INFO starting test `store_app_test`
2023-11-03 00:37:05,701 TADA INFO   test-id: 4fd9e62fed3e5f96cb366cab7015e7aeed3a23d579b64bc69c40f93d67407870
2023-11-03 00:37:05,701 TADA INFO   test-suite: LDMSD
2023-11-03 00:37:05,702 TADA INFO   test-name: store_app_test
2023-11-03 00:37:05,702 TADA INFO   test-user: narate
2023-11-03 00:37:05,702 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:37:05,703 __main__ INFO -- Get or create the cluster --
2023-11-03 00:37:32,196 __main__ INFO -- Preparing job script & programs --
2023-11-03 00:37:32,632 __main__ INFO -- Start daemons --
2023-11-03 00:38:04,459 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:38:09,465 __main__ INFO -- Submitting jobs --
2023-11-03 00:38:09,707 __main__ INFO job_one: 1
2023-11-03 00:38:14,936 __main__ INFO job_two: 2
2023-11-03 00:38:24,370 __main__ INFO Verifying data ...
2023-11-03 00:40:28,435 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-11-03 00:40:28,435 TADA INFO test store_app_test ended
2023-11-03 00:40:42 INFO: ----------------------------------------------
2023-11-03 00:40:43 INFO: ======== syspapi_test ========
2023-11-03 00:40:43 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/syspapi_test
2023-11-03 00:40:44,484 TADA INFO starting test `syspapi_test`
2023-11-03 00:40:44,484 TADA INFO   test-id: 41afa7aec46f70bbb2b10524f8ac60bf18e380d507c51eec38dedc0fe06e677e
2023-11-03 00:40:44,484 TADA INFO   test-suite: LDMSD
2023-11-03 00:40:44,484 TADA INFO   test-name: syspapi_test
2023-11-03 00:40:44,484 TADA INFO   test-user: narate
2023-11-03 00:40:44,485 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:40:44,485 __main__ INFO -- Get or create the cluster --
2023-11-03 00:41:06,828 __main__ INFO -- Write syspapi JSON config files --
2023-11-03 00:41:06,828 __main__ INFO    - db/syspapi-1.json
2023-11-03 00:41:06,829 __main__ INFO    - db/syspapi-bad.json
2023-11-03 00:41:06,829 __main__ INFO -- Start daemons --
2023-11-03 00:41:26,948 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:41:31,952 __main__ INFO -- Verifying --
2023-11-03 00:41:32,091 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-11-03 00:41:32,091 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-11-03 00:41:32,223 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-11-03 00:41:34,345 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-11-03 00:41:34,465 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-11-03 00:41:34,575 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-11-03 00:41:56,577 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-11-03 00:41:56,577 __main__ INFO  events succeeded: 77
2023-11-03 00:41:56,577 __main__ INFO  events failed: 114
2023-11-03 00:41:56,577 TADA INFO test syspapi_test ended
2023-11-03 00:42:10 INFO: ----------------------------------------------
2023-11-03 00:42:11 INFO: ======== agg_test ========
2023-11-03 00:42:11 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/agg_test
2023-11-03 00:42:11,961 TADA INFO starting test `agg_test`
2023-11-03 00:42:11,962 TADA INFO   test-id: 2727caf4ce940d39efb613743362ade5e37c89453974a1d79b8757035ad6790a
2023-11-03 00:42:11,962 TADA INFO   test-suite: LDMSD
2023-11-03 00:42:11,962 TADA INFO   test-name: agg_test
2023-11-03 00:42:11,962 TADA INFO   test-user: narate
2023-11-03 00:42:11,962 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:42:11,962 __main__ INFO -- Get or create the cluster --
2023-11-03 00:42:43,318 __main__ INFO -- Start daemons --
2023-11-03 00:43:20,076 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:43:25,080 __main__ INFO -- ldms_ls to agg-2 --
2023-11-03 00:43:25,202 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-11-03 00:43:26,007 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-11-03 00:43:26,007 __main__ INFO -- Terminating ldmsd on node-1 --
2023-11-03 00:43:28,386 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-03 00:43:28,616 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-3/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:43:28,616 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-11-03 00:43:38,225 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:43:38,345 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:43:38,345 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-11-03 00:43:40,709 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-11-03 00:43:40,838 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-11-03 00:43:40,955 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-03 00:43:40,955 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-11-03 00:43:50,542 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:43:50,542 TADA INFO test agg_test ended
2023-11-03 00:44:06 INFO: ----------------------------------------------
2023-11-03 00:44:07 INFO: ======== failover_test ========
2023-11-03 00:44:07 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/failover_test
2023-11-03 00:44:07,936 TADA INFO starting test `failover_test`
2023-11-03 00:44:07,937 TADA INFO   test-id: fa1d4a6c1d4e8c3ce4d95aff4f425071f362546d5718318e07cdee30fe067e18
2023-11-03 00:44:07,937 TADA INFO   test-suite: LDMSD
2023-11-03 00:44:07,937 TADA INFO   test-name: failover_test
2023-11-03 00:44:07,937 TADA INFO   test-user: narate
2023-11-03 00:44:07,937 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:44:07,938 __main__ INFO -- Get or create the cluster --
2023-11-03 00:44:39,124 __main__ INFO -- Start daemons --
2023-11-03 00:45:15,922 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:45:30,937 __main__ INFO -- ldms_ls to agg-2 --
2023-11-03 00:45:31,061 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-11-03 00:45:31,890 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-11-03 00:45:31,890 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-11-03 00:45:37,278 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:45:37,405 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:45:37,512 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-11-03 00:45:37,641 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-11-03 00:45:37,641 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-11-03 00:46:02,235 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-11-03 00:46:02,353 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:46:02,353 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-11-03 00:46:07,710 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:46:07,841 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:46:07,955 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-11-03 00:46:08,080 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-11-03 00:46:08,080 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-11-03 00:46:32,653 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:46:32,776 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}), passed
2023-11-03 00:46:32,776 TADA INFO test failover_test ended
2023-11-03 00:46:48 INFO: ----------------------------------------------
2023-11-03 00:46:49 INFO: ======== ldmsd_auth_ovis_test ========
2023-11-03 00:46:49 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_auth_ovis_test
2023-11-03 00:46:49,983 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-11-03 00:46:49,983 TADA INFO   test-id: 5b47396c377baa49629312556f57d2fd38b6dfda97579a4739d39ce9f439fd7d
2023-11-03 00:46:49,983 TADA INFO   test-suite: LDMSD
2023-11-03 00:46:49,983 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-11-03 00:46:49,983 TADA INFO   test-user: narate
2023-11-03 00:46:49,983 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:46:49,984 __main__ INFO -- Get or create the cluster --
2023-11-03 00:47:03,000 __main__ INFO -- Start daemons --
2023-11-03 00:47:08,893 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:47:14,015 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-11-03 00:47:14,157 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-11-03 00:47:14,278 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-11-03 00:47:14,565 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-11-03 00:47:14,565 TADA INFO test ldmsd_auth_ovis_test ended
2023-11-03 00:47:26 INFO: ----------------------------------------------
2023-11-03 00:47:26 INFO: ======== ldmsd_auth_test ========
2023-11-03 00:47:26 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_auth_test
2023-11-03 00:47:27,684 TADA INFO starting test `ldmsd_auth_test`
2023-11-03 00:47:27,684 TADA INFO   test-id: bc613df0316e5158fcc8d238b2bdd9e8fe74ae5533c01051fad97182c9f7017b
2023-11-03 00:47:27,684 TADA INFO   test-suite: LDMSD
2023-11-03 00:47:27,684 TADA INFO   test-name: ldmsd_auth_test
2023-11-03 00:47:27,684 TADA INFO   test-user: narate
2023-11-03 00:47:27,684 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:47:27,685 __main__ INFO -- Get or create the cluster --
2023-11-03 00:47:59,504 __main__ INFO -- Start daemons --
2023-11-03 00:48:46,279 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:48:51,425 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-11-03 00:48:51,541 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-11-03 00:48:51,672 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-11-03 00:48:51,799 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-11-03 00:48:51,906 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-11-03 00:48:51,906 TADA INFO test ldmsd_auth_test ended
2023-11-03 00:49:08 INFO: ----------------------------------------------
2023-11-03 00:49:08 INFO: ======== ldmsd_ctrl_test ========
2023-11-03 00:49:08 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_ctrl_test
2023-11-03 00:49:09,674 TADA INFO starting test `ldmsd_ctrl_test`
2023-11-03 00:49:09,674 TADA INFO   test-id: cd6b80fdff9191e1bb6a05e0ef2288832c42cd9e2a9d4d094832a042f7c2bfee
2023-11-03 00:49:09,674 TADA INFO   test-suite: LDMSD
2023-11-03 00:49:09,674 TADA INFO   test-name: ldmsd_ctrl_test
2023-11-03 00:49:09,674 TADA INFO   test-user: narate
2023-11-03 00:49:09,675 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:49:09,675 __main__ INFO -- Get or create the cluster --
2023-11-03 00:49:29,030 __main__ INFO -- Start daemons --
2023-11-03 00:49:45,258 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:49:51,381 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-11-03 00:49:52,497 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-11-03 00:49:53,099 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-11-03 00:49:53,700 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-11-03 00:49:54,302 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-11-03 00:49:54,903 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-11-03 00:49:55,504 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-11-03 00:49:56,105 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-11-03 00:50:13,307 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-11-03 00:50:30,535 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-11-03 00:50:30,536 TADA INFO test ldmsd_ctrl_test ended
2023-11-03 00:50:43 INFO: ----------------------------------------------
2023-11-03 00:50:44 INFO: ======== ldmsd_stream_test2 ========
2023-11-03 00:50:44 INFO: CMD: python3 ldmsd_stream_test2 --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_stream_test2
2023-11-03 00:50:45,208 TADA INFO starting test `ldmsd_stream_test`
2023-11-03 00:50:45,208 TADA INFO   test-id: c9a4b66d6613fe1aceb09af5bcae13acca39befd9ca4b2b2cfbd7e20e4155875
2023-11-03 00:50:45,208 TADA INFO   test-suite: LDMSD
2023-11-03 00:50:45,208 TADA INFO   test-name: ldmsd_stream_test
2023-11-03 00:50:45,208 TADA INFO   test-user: narate
2023-11-03 00:50:45,208 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:50:45,209 __main__ INFO -- Get or create the cluster --
2023-11-03 00:51:04,645 __main__ INFO -- Start daemons --
2023-11-03 00:51:22,177 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:51:24,179 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-32d5252-new 
2023-11-03 00:51:27,196 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-32d5252-agg-2 
2023-11-03 00:51:37,639 TADA INFO assertion 1, Check data from old ldmsd_stream at agg-1: , passed
2023-11-03 00:51:37,639 TADA INFO assertion 2, Check data from old ldmsd_stream at agg-2: , passed
2023-11-03 00:51:37,640 TADA INFO assertion 3, Check data from old ldmsd_stream at the last subscriber: , passed
2023-11-03 00:51:37,640 TADA INFO assertion 4, Check data from the matching new ldms stream at agg-1: , passed
2023-11-03 00:51:37,640 TADA INFO assertion 5, Check data from the matching new ldms stream at agg-2: , passed
2023-11-03 00:51:37,641 TADA INFO assertion 6, Check data from the matching new ldms stream at the last subscriber: , passed
2023-11-03 00:51:37,641 TADA INFO assertion 7, Check data from the non-matching new ldms stream at agg-1: , passed
2023-11-03 00:51:37,641 TADA INFO assertion 8, Check data from the non-matching new ldms stream at agg-2: , passed
2023-11-03 00:51:37,642 TADA INFO assertion 9, Check data from the non-matching new ldms stream at last subscriber: , passed
2023-11-03 00:51:38,175 TADA INFO assertion 10, Check stream_stats before stream data transfer: , passed
2023-11-03 00:51:38,175 TADA INFO assertion 11, Check stream_client_stats before stream data transfer: , passed
2023-11-03 00:51:38,175 TADA INFO assertion 12, Check stream_stats after stream data transfer: , passed
2023-11-03 00:51:38,175 TADA INFO assertion 13, Check stream_client_stats after stream data transfer: , passed
2023-11-03 00:51:38,176 TADA INFO test ldmsd_stream_test ended
2023-11-03 00:51:51 INFO: ----------------------------------------------
2023-11-03 00:51:52 INFO: ======== maestro_cfg_test ========
2023-11-03 00:51:52 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/maestro_cfg_test
2023-11-03 00:51:52,869 TADA INFO starting test `maestro_cfg_test`
2023-11-03 00:51:52,870 TADA INFO   test-id: 8b34324d37644540fc64184397fa074dbff3baae6a1467f37c1794fff705dfc7
2023-11-03 00:51:52,870 TADA INFO   test-suite: LDMSD
2023-11-03 00:51:52,870 TADA INFO   test-name: maestro_cfg_test
2023-11-03 00:51:52,870 TADA INFO   test-user: narate
2023-11-03 00:51:52,870 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:52:02,881 __main__ INFO -- Get or create cluster --
2023-11-03 00:52:46,946 __main__ INFO -- Start daemons --
2023-11-03 00:53:49,036 __main__ INFO ... make sure ldmsd's are up
2023-11-03 00:53:54,534 TADA INFO assertion 1, load maestro etcd cluster: Unexpected output: , failed
---Wait for config to write to file---
Traceback (most recent call last):
  File "maestro_cfg_test", line 324, in <module>
    test.assert_test(1, False, "Unexpected output: {}".format(out))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test for ldmsd configuration with maestro_ctrl, Unexpected output: : FAILED
2023-11-03 00:53:54,535 TADA INFO assertion 2, config ldmsd cluster with maestro: skipped
2023-11-03 00:53:54,535 TADA INFO assertion 3, verify sampler daemons: skipped
2023-11-03 00:53:54,535 TADA INFO assertion 4, verify L1 aggregator daemons: skipped
2023-11-03 00:53:54,535 TADA INFO assertion 5, verify L2 aggregator daemon: skipped
2023-11-03 00:53:54,535 TADA INFO assertion 6, verify data storage: skipped
2023-11-03 00:53:54,535 TADA INFO test maestro_cfg_test ended
2023-11-03 00:54:13 INFO: ----------------------------------------------
2023-11-03 00:54:13 INFO: ======== mt-slurm-test ========
2023-11-03 00:54:13 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1698990923.922078', '1,1698990923.922078', '2,1698990923.922078', '3,1698990923.922078', '4,1698990923.922078', '5,1698990923.922078', '6,1698990924.937850', '7,1698990924.937850', '8,1698990924.937850', '9,1698990925.945070', '10,1698990925.945070', '11,1698990925.945070', '12,1698990925.945070', '13,1698990926.973782', '14,1698990926.973782', '15,1698990926.973782', '16,1698990926.973782', '17,1698990926.973782', '18,1698990927.978358', '19,1698990927.978358', '20,1698990928.991700', '21,1698990928.991700', '22,1698990928.991700', '23,1698990928.991700', '24,1698990928.991700', '25,1698990928.991700', '26,1698990928.991700', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-11-03 00:56:03 INFO: ----------------------------------------------
2023-11-03 00:56:04 INFO: ======== ovis_ev_test ========
2023-11-03 00:56:04 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ovis_ev_test
2023-11-03 00:56:05,274 __main__ INFO -- Create the cluster -- 
2023-11-03 00:56:21,655 TADA INFO starting test `ovis_ev_test`
2023-11-03 00:56:21,655 TADA INFO   test-id: ef43570bcf785bb768bcfe7eea899f1e4d7093355252f4a01e3d9c0342ede1f0
2023-11-03 00:56:21,655 TADA INFO   test-suite: test_ovis_ev
2023-11-03 00:56:21,655 TADA INFO   test-name: ovis_ev_test
2023-11-03 00:56:21,655 TADA INFO   test-user: narate
2023-11-03 00:56:21,655 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:56:21,656 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-11-03 00:56:21,656 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-11-03 00:56:21,656 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-11-03 00:56:21,656 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-11-03 00:56:21,656 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-11-03 00:56:21,656 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-11-03 00:56:21,657 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-11-03 00:56:21,657 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-11-03 00:56:21,657 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-11-03 00:56:21,657 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-11-03 00:56:21,657 TADA INFO test ovis_ev_test ended
2023-11-03 00:56:32 INFO: ----------------------------------------------
2023-11-03 00:56:33 INFO: ======== prdcr_subscribe_test ========
2023-11-03 00:56:33 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/prdcr_subscribe_test
2023-11-03 00:56:34,184 TADA INFO starting test `prdcr_subscribe_test`
2023-11-03 00:56:34,185 TADA INFO   test-id: 0b49845dea35cc84c59f521294e33e2c0454abdadc618cb7adbeefb318b26191
2023-11-03 00:56:34,185 TADA INFO   test-suite: LDMSD
2023-11-03 00:56:34,185 TADA INFO   test-name: prdcr_subscribe_test
2023-11-03 00:56:34,185 TADA INFO   test-user: narate
2023-11-03 00:56:34,185 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:57:31,586 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-11-03 00:57:31,587 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-11-03 00:57:31,587 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-11-03 00:57:31,587 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-11-03 00:57:31,588 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-11-03 00:57:31,976 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-11-03 00:57:32,356 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-11-03 00:57:40,417 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-11-03 00:57:40,417 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-11-03 00:57:40,417 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-11-03 00:57:40,418 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-11-03 00:57:40,418 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-11-03 00:57:41,635 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-11-03 00:57:47,130 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-11-03 00:57:54,731 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-11-03 00:57:54,731 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-11-03 00:57:54,731 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-11-03 00:57:55,110 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-11-03 00:57:58,482 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-11-03 00:58:04,316 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-11-03 00:58:04,316 TADA INFO test prdcr_subscribe_test ended
2023-11-03 00:58:17 INFO: ----------------------------------------------
2023-11-03 00:58:18 INFO: ======== set_array_test ========
2023-11-03 00:58:18 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/set_array_test
2023-11-03 00:58:18,957 TADA INFO starting test `set_array_test`
2023-11-03 00:58:18,957 TADA INFO   test-id: 854fa201a1805005a86c4c86854bf915389fbc589771ebbb1b67c8edbb48be99
2023-11-03 00:58:18,957 TADA INFO   test-suite: LDMSD
2023-11-03 00:58:18,957 TADA INFO   test-name: set_array_test
2023-11-03 00:58:18,957 TADA INFO   test-user: narate
2023-11-03 00:58:18,957 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:58:18,958 __main__ INFO -- Get or create the cluster --
2023-11-03 00:58:31,939 __main__ INFO -- Start daemons --
2023-11-03 00:58:37,843 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:59:05,540 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 3 snapshots, passed
2023-11-03 00:59:05,540 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-11-03 00:59:05,540 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-11-03 00:59:05,540 TADA INFO test set_array_test ended
2023-11-03 00:59:17 INFO: ----------------------------------------------
2023-11-03 00:59:17 INFO: ======== setgroup_test ========
2023-11-03 00:59:17 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/setgroup_test
2023-11-03 00:59:18,708 TADA INFO starting test `setgroup_test`
2023-11-03 00:59:18,709 TADA INFO   test-id: 3a7c6510576f4d10bbd9984c302d7137c3e3b3106c57149a2acdd0a3492a2fe5
2023-11-03 00:59:18,709 TADA INFO   test-suite: LDMSD
2023-11-03 00:59:18,709 TADA INFO   test-name: setgroup_test
2023-11-03 00:59:18,709 TADA INFO   test-user: narate
2023-11-03 00:59:18,710 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 00:59:18,710 __main__ INFO -- Get or create the cluster --
2023-11-03 00:59:37,801 __main__ INFO -- Start daemons --
2023-11-03 00:59:54,008 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 00:59:59,014 __main__ INFO -- ldms_ls to agg-2 --
2023-11-03 00:59:59,134 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-11-03 01:00:01,389 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-11-03 01:00:01,390 __main__ INFO -- Removing test_2 from grp --
2023-11-03 01:00:01,886 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-11-03 01:00:06,037 TADA INFO assertion 4, test_2 is removed from grp on agg-1: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-11-03 01:00:10,188 TADA INFO assertion 5, test_2 is removed from grp on agg-2: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-11-03 01:00:14,192 __main__ INFO -- Adding test_2 back into grp --
2023-11-03 01:00:14,710 TADA INFO assertion 6, test_2 is added back to grp on sampler: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-11-03 01:00:18,842 TADA INFO assertion 7, test_2 is added back to grp on agg-1: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-11-03 01:00:20,986 TADA INFO assertion 8, test_2 is added back to grp on agg-2: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, passed
2023-11-03 01:00:22,989 TADA INFO test setgroup_test ended
2023-11-03 01:00:36 INFO: ----------------------------------------------
2023-11-03 01:00:37 INFO: ======== slurm_stream_test ========
2023-11-03 01:00:37 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/slurm_stream_test
2023-11-03 01:00:37,744 TADA INFO starting test `slurm_stream_test`
2023-11-03 01:00:37,744 TADA INFO   test-id: f3d1ad34651115160d32d656bf4ae821bf11578e1e9e2d0118809f8ad1aeed2a
2023-11-03 01:00:37,744 TADA INFO   test-suite: LDMSD
2023-11-03 01:00:37,744 TADA INFO   test-name: slurm_stream_test
2023-11-03 01:00:37,745 TADA INFO   test-user: narate
2023-11-03 01:00:37,745 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:00:37,745 __main__ INFO -- Get or create the cluster --
2023-11-03 01:00:52,561 __main__ INFO -- Start daemons --
2023-11-03 01:01:03,135 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:01:33,793 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:33,794 __main__ INFO 12345
2023-11-03 01:01:33,794 __main__ INFO 12345
2023-11-03 01:01:33,794 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,794 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-11-03 01:01:33,794 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,794 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,794 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,795 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,918 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:33,918 __main__ INFO 12345
2023-11-03 01:01:33,918 __main__ INFO 12345
2023-11-03 01:01:33,918 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,918 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-11-03 01:01:33,918 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,919 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,919 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:33,919 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-11-03 01:01:34,023 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,023 __main__ INFO 12346
2023-11-03 01:01:34,023 __main__ INFO 12346
2023-11-03 01:01:34,023 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,023 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-11-03 01:01:34,023 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,024 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,024 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,024 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,132 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,132 __main__ INFO 12346
2023-11-03 01:01:34,132 __main__ INFO 12346
2023-11-03 01:01:34,132 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,132 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-11-03 01:01:34,132 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,133 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,133 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,133 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-11-03 01:01:34,234 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,235 __main__ INFO 12347
2023-11-03 01:01:34,235 __main__ INFO 12347
2023-11-03 01:01:34,235 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,235 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-11-03 01:01:34,235 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,235 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,235 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,236 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,357 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,357 __main__ INFO 12347
2023-11-03 01:01:34,357 __main__ INFO 12347
2023-11-03 01:01:34,357 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,358 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-11-03 01:01:34,358 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,358 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,358 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,358 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-11-03 01:01:34,482 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,482 __main__ INFO 12348
2023-11-03 01:01:34,483 __main__ INFO 12348
2023-11-03 01:01:34,483 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,483 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-11-03 01:01:34,483 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,483 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,483 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,484 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,598 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,598 __main__ INFO 12348
2023-11-03 01:01:34,598 __main__ INFO 12348
2023-11-03 01:01:34,598 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,598 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-11-03 01:01:34,598 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,599 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,599 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,599 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-11-03 01:01:34,708 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,708 __main__ INFO 12355
2023-11-03 01:01:34,708 __main__ INFO 12355
2023-11-03 01:01:34,709 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,709 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-11-03 01:01:34,709 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,709 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,709 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,709 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,710 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,710 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,710 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,710 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,817 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,817 __main__ INFO 12355
2023-11-03 01:01:34,817 __main__ INFO 12355
2023-11-03 01:01:34,817 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,817 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-11-03 01:01:34,817 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,817 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,818 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-11-03 01:01:34,924 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:34,924 __main__ INFO 12356
2023-11-03 01:01:34,924 __main__ INFO 12356
2023-11-03 01:01:34,924 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,924 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,925 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,926 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:34,926 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,042 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:35,042 __main__ INFO 12356
2023-11-03 01:01:35,042 __main__ INFO 12356
2023-11-03 01:01:35,042 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,042 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,043 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,044 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-11-03 01:01:35,150 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:35,150 __main__ INFO 12357
2023-11-03 01:01:35,150 __main__ INFO 12357
2023-11-03 01:01:35,150 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,150 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-11-03 01:01:35,151 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,151 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,151 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,151 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,151 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,152 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,152 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,152 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,267 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:35,267 __main__ INFO 12357
2023-11-03 01:01:35,268 __main__ INFO 12357
2023-11-03 01:01:35,268 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,268 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-11-03 01:01:35,268 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,268 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,268 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,269 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,269 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,269 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,269 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,269 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-11-03 01:01:35,388 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:35,388 __main__ INFO 12358
2023-11-03 01:01:35,388 __main__ INFO 12358
2023-11-03 01:01:35,388 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,388 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,389 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,390 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,496 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-11-03 01:01:35,496 __main__ INFO 12358
2023-11-03 01:01:35,496 __main__ INFO 12358
2023-11-03 01:01:35,496 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,496 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,497 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,498 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:35,498 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-11-03 01:01:37,646 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-11-03 01:01:37,647 __main__ INFO 12353
2023-11-03 01:01:37,647 __main__ INFO 12353
2023-11-03 01:01:37,647 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,647 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-11-03 01:01:37,647 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,647 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,647 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-11-03 01:01:37,648 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-11-03 01:01:37,649 TADA INFO test slurm_stream_test ended
2023-11-03 01:01:49 INFO: ----------------------------------------------
2023-11-03 01:01:50 INFO: ======== spank_notifier_test ========
2023-11-03 01:01:50 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/spank_notifier_test
2023-11-03 01:01:50,785 TADA INFO starting test `spank_notifier_test`
2023-11-03 01:01:50,785 TADA INFO   test-id: 382df273949e1dcc39dfd589d8dceb58947513a6a472a7d66df94240964d753b
2023-11-03 01:01:50,785 TADA INFO   test-suite: Slurm_Plugins
2023-11-03 01:01:50,785 TADA INFO   test-name: spank_notifier_test
2023-11-03 01:01:50,785 TADA INFO   test-user: narate
2023-11-03 01:01:50,785 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:01:50,786 __main__ INFO -- Create the cluster --
2023-11-03 01:02:31,249 __main__ INFO -- Cleanup output --
2023-11-03 01:02:31,606 __main__ INFO -- Test bad plugstack config --
2023-11-03 01:02:31,606 __main__ INFO Starting slurm ...
2023-11-03 01:02:45,474 __main__ INFO Starting slurm ... OK
2023-11-03 01:03:05,995 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-03 01:03:06,133 __main__ INFO   jobid = 1
2023-11-03 01:03:06,327 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-03 01:03:06,444 __main__ INFO   jobid = 2
2023-11-03 01:03:06,663 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-03 01:03:06,787 __main__ INFO   jobid = 3
2023-11-03 01:03:07,032 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-03 01:03:07,152 __main__ INFO   jobid = 4
2023-11-03 01:03:16,903 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-11-03 01:03:16,903 __main__ INFO Killin slurm ...
2023-11-03 01:03:19,909 __main__ INFO Killin slurm ... OK
2023-11-03 01:03:39,918 __main__ INFO -- Start daemons --
2023-11-03 01:03:50,555 __main__ INFO Starting slurm ... OK
2023-11-03 01:04:10,821 __main__ INFO -- Submitting job with no stream listener --
2023-11-03 01:04:11,053 __main__ INFO -- Submitting job with num_tasks 8 --
2023-11-03 01:04:11,172 __main__ INFO   jobid = 5
2023-11-03 01:04:27,162 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-11-03 01:04:27,162 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-11-03 01:04:33,077 __main__ INFO -- Submitting job with listener --
2023-11-03 01:04:33,314 __main__ INFO -- Submitting job with num_tasks 1 --
2023-11-03 01:04:33,434 __main__ INFO   jobid = 6
2023-11-03 01:04:33,655 __main__ INFO -- Submitting job with num_tasks 2 --
2023-11-03 01:04:33,771 __main__ INFO   jobid = 7
2023-11-03 01:04:33,977 __main__ INFO -- Submitting job with num_tasks 4 --
2023-11-03 01:04:34,101 __main__ INFO   jobid = 8
2023-11-03 01:04:34,345 __main__ INFO -- Submitting job with num_tasks 8 --
2023-11-03 01:04:34,446 __main__ INFO   jobid = 9
2023-11-03 01:04:34,645 __main__ INFO -- Submitting job with num_tasks 27 --
2023-11-03 01:04:34,766 __main__ INFO   jobid = 10
2023-11-03 01:04:56,598 __main__ INFO -- Verifying Events --
2023-11-03 01:04:56,598 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-11-03 01:04:56,599 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-03 01:04:56,599 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-03 01:04:56,599 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-03 01:04:56,599 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-11-03 01:04:56,599 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-11-03 01:04:56,600 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-03 01:04:56,601 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-03 01:04:56,601 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-03 01:04:56,601 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-11-03 01:04:56,601 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-11-03 01:04:56,601 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-03 01:04:56,602 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-03 01:04:56,602 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-03 01:04:56,602 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-11-03 01:04:56,602 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-11-03 01:04:56,603 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-11-03 01:04:56,603 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-11-03 01:04:56,603 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-11-03 01:04:56,603 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-11-03 01:04:56,603 __main__ INFO job 7 multi-tenant with dict_keys([6])
2023-11-03 01:04:56,603 __main__ INFO job 10 multi-tenant with dict_keys([6, 7])
2023-11-03 01:04:56,603 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-11-03 01:04:56,603 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-11-03 01:04:56,604 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-11-03 01:04:56,817 __main__ INFO -- Submitting job that crashes listener --
2023-11-03 01:04:56,942 __main__ INFO   jobid = 11
2023-11-03 01:05:07,183 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-11-03 01:05:07,301 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-11-03 01:05:07,302 TADA INFO test spank_notifier_test ended
2023-11-03 01:05:24 INFO: ----------------------------------------------
2023-11-03 01:05:25 INFO: ======== ldms_list_test ========
2023-11-03 01:05:25 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_list_test
2023-11-03 01:05:26,160 TADA INFO starting test `ldms_list_test`
2023-11-03 01:05:26,160 TADA INFO   test-id: de47ffacda5fedd2c6b4f631d0dd8d2a0d13b716b4530036afdd6c3cfacf0d0f
2023-11-03 01:05:26,160 TADA INFO   test-suite: LDMSD
2023-11-03 01:05:26,160 TADA INFO   test-name: ldms_list_test
2023-11-03 01:05:26,161 TADA INFO   test-user: narate
2023-11-03 01:05:26,161 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:05:26,161 __main__ INFO -- Get or create the cluster --
2023-11-03 01:05:36,216 __main__ INFO -- Start daemons --
2023-11-03 01:05:44,646 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:05:46,648 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-11-03 01:05:52,684 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-11-03 01:05:52,684 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-11-03 01:05:52,685 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-11-03 01:05:52,685 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-11-03 01:05:52,685 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-11-03 01:05:52,685 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-11-03 01:05:52,686 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-11-03 01:05:52,686 __main__ INFO 2nd sampling on the sampler...
2023-11-03 01:05:59,895 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-11-03 01:05:59,895 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-11-03 01:05:59,896 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-11-03 01:05:59,896 __main__ INFO 2nd update on the aggregator...
2023-11-03 01:06:07,105 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-11-03 01:06:07,106 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-11-03 01:06:07,106 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-11-03 01:06:07,106 __main__ INFO 3rd sampling on the sampler...
2023-11-03 01:06:14,315 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-11-03 01:06:14,315 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-11-03 01:06:14,316 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-11-03 01:06:14,316 __main__ INFO 3rd update on the aggregator...
2023-11-03 01:06:21,525 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-11-03 01:06:21,526 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-11-03 01:06:21,526 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-11-03 01:06:21,526 __main__ INFO 4th sampling on the sampler...
2023-11-03 01:06:28,735 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-11-03 01:06:28,736 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-11-03 01:06:28,736 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-11-03 01:06:28,736 __main__ INFO 4th update on the aggregator...
2023-11-03 01:06:35,945 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-11-03 01:06:35,946 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-11-03 01:06:35,946 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-11-03 01:06:35,946 __main__ INFO 5th sampling on the sampler...
2023-11-03 01:06:43,155 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-11-03 01:06:43,156 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-11-03 01:06:43,156 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-11-03 01:06:43,156 __main__ INFO 5th update on the aggregator...
2023-11-03 01:06:50,365 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-11-03 01:06:50,366 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-11-03 01:06:50,366 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-11-03 01:06:50,366 __main__ INFO 6th sampling on the sampler...
2023-11-03 01:06:57,575 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-11-03 01:06:57,575 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-11-03 01:06:57,576 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-11-03 01:06:57,576 __main__ INFO 6th update on the updator...
2023-11-03 01:07:04,785 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-11-03 01:07:04,786 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-11-03 01:07:04,786 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-11-03 01:07:04,786 TADA INFO test ldms_list_test ended
2023-11-03 01:07:15 INFO: ----------------------------------------------
2023-11-03 01:07:16 INFO: ======== quick_set_add_rm_test ========
2023-11-03 01:07:16 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/quick_set_add_rm_test
2023-11-03 01:07:17,220 TADA INFO starting test `quick_set_add_rm_test`
2023-11-03 01:07:17,220 TADA INFO   test-id: 4d89e2c15cc2220a1ed5b353c25990c0782254bad2825645f8e0c7dd5ae37a9a
2023-11-03 01:07:17,220 TADA INFO   test-suite: LDMSD
2023-11-03 01:07:17,220 TADA INFO   test-name: quick_set_add_rm_test
2023-11-03 01:07:17,220 TADA INFO   test-user: narate
2023-11-03 01:07:17,220 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:07:17,221 __main__ INFO -- Get or create the cluster --
2023-11-03 01:07:33,298 __main__ INFO -- Start samp.py --
2023-11-03 01:07:38,414 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-11-03 01:07:38,414 __main__ INFO -- Start daemons --
2023-11-03 01:07:48,155 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:07:53,773 TADA INFO assertion 2, verify data: verified, passed
2023-11-03 01:07:58,425 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-11-03 01:08:03,034 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-11-03 01:08:07,646 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-11-03 01:08:12,776 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-11-03 01:08:12,777 TADA INFO test quick_set_add_rm_test ended
2023-11-03 01:08:25 INFO: ----------------------------------------------
2023-11-03 01:08:26 INFO: ======== set_array_hang_test ========
2023-11-03 01:08:26 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/set_array_hang_test
2023-11-03 01:08:26,759 TADA INFO starting test `set_array_hang_test`
2023-11-03 01:08:26,759 TADA INFO   test-id: 167c61acb68f785949ffe8b03a273a80fe6faa5fe8bccf731f112bf98ae6d768
2023-11-03 01:08:26,759 TADA INFO   test-suite: LDMSD
2023-11-03 01:08:26,760 TADA INFO   test-name: set_array_hang_test
2023-11-03 01:08:26,760 TADA INFO   test-user: narate
2023-11-03 01:08:26,760 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:08:26,760 __main__ INFO -- Get or create the cluster --
2023-11-03 01:08:36,747 __main__ INFO -- Start processes --
2023-11-03 01:08:36,747 __main__ INFO starting interactive set_array_samp.py
2023-11-03 01:08:39,763 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-11-03 01:08:42,781 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-11-03 01:08:49,990 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-11-03 01:08:57,198 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-11-03 01:09:00,803 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-11-03 01:09:08,013 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-11-03 01:09:08,014 TADA INFO test set_array_hang_test ended
2023-11-03 01:09:18 INFO: ----------------------------------------------
2023-11-03 01:09:19 INFO: ======== ldmsd_autointerval_test ========
2023-11-03 01:09:19 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_autointerval_test
2023-11-03 01:09:20,471 TADA INFO starting test `ldmsd_autointerval_test`
2023-11-03 01:09:20,472 TADA INFO   test-id: 378264644dc6228c935a400cd71a8c318a35e1b26b94287aa3671589a3533f63
2023-11-03 01:09:20,472 TADA INFO   test-suite: LDMSD
2023-11-03 01:09:20,472 TADA INFO   test-name: ldmsd_autointerval_test
2023-11-03 01:09:20,472 TADA INFO   test-user: narate
2023-11-03 01:09:20,472 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:09:20,472 __main__ INFO -- Get or create the cluster --
2023-11-03 01:09:36,706 __main__ INFO -- Start daemons --
2023-11-03 01:09:52,228 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:09:58,751 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-11-03 01:10:00,986 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-11-03 01:10:00,986 __main__ INFO Let them run for a while to collect data ...
2023-11-03 01:10:10,997 __main__ INFO Setting sample interval to 1000000 ...
2023-11-03 01:10:19,230 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-11-03 01:10:19,230 __main__ INFO Let them run for a while to collect data ...
2023-11-03 01:10:29,240 __main__ INFO Setting sample interval to 2000000 ...
2023-11-03 01:10:37,497 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-11-03 01:10:37,497 __main__ INFO Let them run for a while to collect data ...
2023-11-03 01:10:47,724 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-11-03 01:10:47,864 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-11-03 01:10:47,864 TADA INFO test ldmsd_autointerval_test ended
2023-11-03 01:11:00 INFO: ----------------------------------------------
2023-11-03 01:11:01 INFO: ======== ldms_record_test ========
2023-11-03 01:11:01 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_record_test
2023-11-03 01:11:01,777 TADA INFO starting test `ldms_record_test`
2023-11-03 01:11:01,778 TADA INFO   test-id: 63d240c6b24f8be1429615c849fe23b7bda0261afc3c345cd00cde81e66c923d
2023-11-03 01:11:01,778 TADA INFO   test-suite: LDMSD
2023-11-03 01:11:01,778 TADA INFO   test-name: ldms_record_test
2023-11-03 01:11:01,778 TADA INFO   test-user: narate
2023-11-03 01:11:01,778 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:11:01,778 __main__ INFO -- Get or create the cluster --
2023-11-03 01:11:11,777 __main__ INFO -- Start daemons --
2023-11-03 01:11:20,187 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:11:22,190 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-11-03 01:11:28,225 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-11-03 01:11:28,226 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-11-03 01:11:28,226 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-11-03 01:11:28,226 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-11-03 01:11:28,227 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-11-03 01:11:28,227 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-11-03 01:11:28,227 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-11-03 01:11:28,227 __main__ INFO 2nd sampling on the sampler...
2023-11-03 01:11:35,437 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-11-03 01:11:35,437 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-11-03 01:11:35,437 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-11-03 01:11:35,437 __main__ INFO 2nd update on the aggregator...
2023-11-03 01:11:42,647 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-11-03 01:11:42,647 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-11-03 01:11:42,647 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-11-03 01:11:42,648 __main__ INFO 3rd sampling on the sampler...
2023-11-03 01:11:49,857 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-11-03 01:11:49,857 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-11-03 01:11:49,857 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-11-03 01:11:49,857 __main__ INFO 3rd update on the aggregator...
2023-11-03 01:11:57,066 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-11-03 01:11:57,067 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-11-03 01:11:57,067 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-11-03 01:11:57,067 __main__ INFO 4th sampling on the sampler...
2023-11-03 01:12:04,276 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-11-03 01:12:04,277 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-11-03 01:12:04,277 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-11-03 01:12:04,277 __main__ INFO 4th update on the aggregator...
2023-11-03 01:12:11,486 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-11-03 01:12:11,486 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-11-03 01:12:11,487 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-11-03 01:12:11,487 __main__ INFO 5th sampling on the sampler...
2023-11-03 01:12:18,696 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-11-03 01:12:18,696 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-11-03 01:12:18,697 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-11-03 01:12:18,697 __main__ INFO 5th update on the aggregator...
2023-11-03 01:12:25,906 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-11-03 01:12:25,906 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-11-03 01:12:25,907 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-11-03 01:12:25,907 __main__ INFO 6th sampling on the sampler...
2023-11-03 01:12:33,116 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-11-03 01:12:33,116 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-11-03 01:12:33,117 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-11-03 01:12:33,117 __main__ INFO 6th update on the updator...
2023-11-03 01:12:40,326 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-11-03 01:12:40,326 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-11-03 01:12:40,327 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-11-03 01:12:40,327 TADA INFO test ldms_record_test ended
2023-11-03 01:12:51 INFO: ----------------------------------------------
2023-11-03 01:12:52 INFO: ======== ldms_schema_digest_test ========
2023-11-03 01:12:52 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_schema_digest_test
2023-11-03 01:12:52,793 TADA INFO starting test `ldms_schema_digest_test`
2023-11-03 01:12:52,794 TADA INFO   test-id: b762f186a374aba4671c60a1532503defd3d845eeeaae46823b6813c520b6a11
2023-11-03 01:12:52,794 TADA INFO   test-suite: LDMSD
2023-11-03 01:12:52,794 TADA INFO   test-name: ldms_schema_digest_test
2023-11-03 01:12:52,794 TADA INFO   test-user: narate
2023-11-03 01:12:52,794 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:12:52,795 __main__ INFO -- Get or create the cluster --
2023-11-03 01:13:09,229 __main__ INFO -- Start daemons --
2023-11-03 01:13:20,250 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:13:25,387 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-11-03 01:13:25,523 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-11-03 01:13:25,644 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-11-03 01:13:25,840 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-11-03 01:13:25,840 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-11-03 01:13:25,840 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-11-03 01:13:28,266 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-11-03 01:13:28,266 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-11-03 01:13:28,267 TADA INFO test ldms_schema_digest_test ended
2023-11-03 01:13:40 INFO: ----------------------------------------------
2023-11-03 01:13:41 INFO: ======== ldmsd_decomp_test ========
2023-11-03 01:13:41 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_decomp_test
2023-11-03 01:13:42,170 TADA INFO starting test `ldmsd_decomp_test`
2023-11-03 01:13:42,171 TADA INFO   test-id: 6b1ae095ed5efa50cae4a41ff0efd4de810a9ec796e0d6b888371338e451f1af
2023-11-03 01:13:42,171 TADA INFO   test-suite: LDMSD
2023-11-03 01:13:42,171 TADA INFO   test-name: ldmsd_decomp_test
2023-11-03 01:13:42,171 TADA INFO   test-user: narate
2023-11-03 01:13:42,171 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:13:42,172 __main__ INFO -- Get or create the cluster --
2023-11-03 01:14:10,643 __main__ INFO -- Start daemons --
2023-11-03 01:14:40,389 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:15:35,446 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-11-03 01:15:35,446 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-11-03 01:15:35,446 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-11-03 01:15:35,447 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-11-03 01:15:35,447 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-11-03 01:15:35,447 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-11-03 01:15:35,447 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-11-03 01:15:35,447 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-11-03 01:15:35,448 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-11-03 01:15:35,449 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-11-03 01:15:35,449 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-11-03 01:15:35,449 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-11-03 01:15:35,449 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-11-03 01:15:35,451 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-11-03 01:15:35,453 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-11-03 01:15:35,535 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-11-03 01:15:35,540 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-11-03 01:15:35,543 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-11-03 01:15:35,553 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-11-03 01:15:35,555 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-11-03 01:15:35,557 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-11-03 01:15:35,635 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-11-03 01:15:35,640 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-11-03 01:15:35,643 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-11-03 01:15:35,653 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-11-03 01:15:35,654 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-11-03 01:15:35,655 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-11-03 01:15:35,684 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-11-03 01:15:35,686 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-11-03 01:15:35,688 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-11-03 01:15:35,692 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-11-03 01:15:35,692 TADA INFO test ldmsd_decomp_test ended
2023-11-03 01:15:35,693 TADA INFO test ldmsd_decomp_test ended
2023-11-03 01:15:51 INFO: ----------------------------------------------
2023-11-03 01:15:52 INFO: ======== ldmsd_stream_status_test ========
2023-11-03 01:15:52 INFO: CMD: python3 ldmsd_stream_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_stream_status_test
2023-11-03 01:15:52,762 __main__ INFO -- Get or create the cluster --
2023-11-03 01:15:52,762 TADA INFO starting test `ldmsd_stream_status`
2023-11-03 01:15:52,762 TADA INFO   test-id: 4018fcc21c0eae380c3ee7313518873bddd443ebf28db3eddf87c51115775bd2
2023-11-03 01:15:52,762 TADA INFO   test-suite: LDMSD
2023-11-03 01:15:52,762 TADA INFO   test-name: ldmsd_stream_status
2023-11-03 01:15:52,762 TADA INFO   test-user: narate
2023-11-03 01:15:52,762 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:16:10,124 __main__ INFO -- Start daemons --
2023-11-03 01:16:25,681 __main__ INFO waiting ... for all LDMSDs to start
2023-11-03 01:16:26,033 __main__ INFO All LDMSDs are up.
2023-11-03 01:16:27,256 TADA INFO assertion 1, No Stream data: [] == [], passed
2023-11-03 01:16:28,574 TADA INFO assertion 2, stream_status -- one stream message: [{'name': 'foo', 'rx': {'bytes': 6, 'count': 1, 'first_ts': 1698992187.3519394, 'last_ts': 1698992187.3519394}, 'sources': {'0.0.0.0:0': {'bytes': 6, 'count': 1, 'first_ts': 1698992187.3519394, 'last_ts': 1698992187.3519394}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 1, 'bytes': 6, 'first_ts': 1698992187.3519394, 'last_ts': 1698992187.3519394}, 'sources': {'0.0.0.0:0': {'count': 1, 'bytes': 6, 'first_ts': 1698992187.3519394, 'last_ts': 1698992187.3519394}}, 'clients': []}], passed
2023-11-03 01:16:31,045 TADA INFO assertion 3, stream_status --  multiple stream messages: [{'name': 'foo', 'rx': {'bytes': 18, 'count': 3, 'first_ts': 1698992187.3519394, 'last_ts': 1698992189.8032873}, 'sources': {'0.0.0.0:0': {'bytes': 18, 'count': 3, 'first_ts': 1698992187.3519394, 'last_ts': 1698992189.8032873}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 3, 'bytes': 18, 'first_ts': 1698992187.3519394, 'last_ts': 1698992189.8032873}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 18, 'first_ts': 1698992187.3519394, 'last_ts': 1698992189.8032873}}, 'clients': []}], passed
2023-11-03 01:16:34,893 TADA INFO assertion 4, stream_status -- mulitple streams: [{'name': 'bar', 'rx': {'bytes': 48, 'count': 3, 'first_ts': 1698992192.428124, 'last_ts': 1698992193.6688144}, 'sources': {'0.0.0.0:0': {'bytes': 48, 'count': 3, 'first_ts': 1698992192.428124, 'last_ts': 1698992193.6688144}}, 'clients': []}, {'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1698992191.1885672, 'last_ts': 1698992192.3151298}, 'sources': {'0.0.0.0:0': {'bytes': 12, 'count': 2, 'first_ts': 1698992191.1885672, 'last_ts': 1698992192.3151298}}, 'clients': []}] == [{'name': 'bar', 'rx': {'count': 3, 'bytes': 48, 'first_ts': 1698992192.428124, 'last_ts': 1698992193.6688144}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 48, 'first_ts': 1698992192.428124, 'last_ts': 1698992193.6688144}}, 'clients': []}, {'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1698992191.1885672, 'last_ts': 1698992192.3151298}, 'sources': {'0.0.0.0:0': {'count': 2, 'bytes': 12, 'first_ts': 1698992191.1885672, 'last_ts': 1698992192.3151298}}, 'clients': []}], passed
2023-11-03 01:16:38,596 TADA INFO assertion 5, stream_status to agg after one producer republished stream: [{'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}, 'sources': {'10.1.249.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}, 'sources': {'10.1.249.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}}, 'clients': []}], passed
2023-11-03 01:16:40,169 TADA INFO assertion 6, stream_status to agg after two producers republished stream: [{'name': 'foo', 'rx': {'bytes': 30, 'count': 5, 'first_ts': 1698992196.204465, 'last_ts': 1698992198.948679}, 'sources': {'10.1.249.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}, '10.1.249.4:10001': {'bytes': 18, 'count': 3, 'first_ts': 1698992198.7248046, 'last_ts': 1698992198.948679}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 5, 'bytes': 30, 'first_ts': 1698992196.204465, 'last_ts': 1698992198.948679}, 'sources': {'10.1.249.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1698992196.204465, 'last_ts': 1698992197.3559427}, '10.1.249.4:10001': {'count': 3, 'bytes': 18, 'first_ts': 1698992198.7248046, 'last_ts': 1698992198.948679}}, 'clients': []}], passed
2023-11-03 01:16:40,170 TADA INFO test ldmsd_stream_status ended
2023-11-03 01:16:52 INFO: ----------------------------------------------
2023-11-03 01:16:53 INFO: ======== store_list_record_test ========
2023-11-03 01:16:53 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/store_list_record_test
2023-11-03 01:16:54,134 __main__ INFO -- Get or create the cluster --
2023-11-03 01:16:54,134 TADA INFO starting test `store_sos_lists_test`
2023-11-03 01:16:54,134 TADA INFO   test-id: 53806b567eacc3a225a06b51f991f3ee32ea67bf28e2a4cb82162ae38d42dd01
2023-11-03 01:16:54,134 TADA INFO   test-suite: LDMSD
2023-11-03 01:16:54,134 TADA INFO   test-name: store_sos_lists_test
2023-11-03 01:16:54,134 TADA INFO   test-user: narate
2023-11-03 01:16:54,134 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:17:10,426 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:17:26,037 __main__ INFO All sampler daemons are up.
2023-11-03 01:17:26,148 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-11-03 01:17:26,258 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-11-03 01:17:27,626 TADA INFO assertion 3, store_sos is storing data.: file_exists(a) for a in supported_schema, passed
2023-11-03 01:17:28,408 TADA INFO assertion 4, store_sos stores data correctly.: verify_data(db) for db in all_db, passed
2023-11-03 01:17:37,446 TADA INFO assertion 5, store_sos stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-11-03 01:17:38,115 TADA INFO assertion 6, store_sos reports multiple list errror messages resulted by the config file.: store_sos reported the multiple list error messages., passed
2023-11-03 01:17:42,238 TADA INFO assertion 7, store_sos reports multiple list errror messages resulted by ldmsd_controller.: 'store_sos: 'sampler/u64_array_u64_array_record' contains multiple lists' in the ldmsd log, failed
Traceback (most recent call last):
  File "store_list_record_test", line 533, in <module>
    test.assert_test(SOS_MULTI_LISTS_ERROR_LDMSD_CONTROLLER, res, cond)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test store_sos storing lists, 'store_sos: 'sampler/u64_array_u64_array_record' contains multiple lists' in the ldmsd log: FAILED
2023-11-03 01:17:42,239 TADA INFO assertion 8, store_csv is storing data.: skipped
2023-11-03 01:17:42,239 TADA INFO assertion 9, store_csv stores data correctly.: skipped
2023-11-03 01:17:42,240 TADA INFO assertion 10, store_csv stores data after restarted correctly.: skipped
2023-11-03 01:17:42,240 TADA INFO assertion 11, store_csv reports multiple list errror messages resulted by the config file.: skipped
2023-11-03 01:17:42,240 TADA INFO assertion 12, store_csv reports multiple list errror messages resulted by ldmsd_controller.: skipped
2023-11-03 01:17:42,240 TADA INFO test store_sos_lists_test ended
2023-11-03 01:17:54 INFO: ----------------------------------------------
2023-11-03 01:17:55 INFO: ======== maestro_raft_test ========
2023-11-03 01:17:55 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/maestro_raft_test
2023-11-03 01:17:56,365 TADA INFO starting test `maestro_raft_test`
2023-11-03 01:17:56,365 TADA INFO   test-id: 092121cc103fd39de42ea4339b73b10b7e30f02950241573731b5b5604e36020
2023-11-03 01:17:56,365 TADA INFO   test-suite: LDMSD
2023-11-03 01:17:56,365 TADA INFO   test-name: maestro_raft_test
2023-11-03 01:17:56,365 TADA INFO   test-user: narate
2023-11-03 01:17:56,365 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:18:06,377 __main__ INFO -- Get or create cluster --
2023-11-03 01:19:02,377 __main__ INFO -- Start daemons --
2023-11-03 01:20:38,237 __main__ INFO -- making known hosts (ssh) --
2023-11-03 01:20:45,343 __main__ INFO ... make sure ldmsd's are up
---Wait for config to write to file---
Traceback (most recent call last):
  File "maestro_raft_test", line 394, in <module>
    raise RuntimeError(f"maestro_ctl error rc: {rc}, out: {out}")
RuntimeError: maestro_ctl error rc: 1, out: 
2023-11-03 01:20:50,807 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: skipped
2023-11-03 01:20:50,807 TADA INFO assertion 2, All ldmsds are up and configured: skipped
2023-11-03 01:20:50,807 TADA INFO assertion 3, Data are being stored: skipped
2023-11-03 01:20:50,807 TADA INFO assertion 4, New leader elected: skipped
2023-11-03 01:20:50,808 TADA INFO assertion 5, Restarted ldmsd is configured: skipped
2023-11-03 01:20:50,808 TADA INFO assertion 6, New data are presented in the store: skipped
2023-11-03 01:20:50,808 TADA INFO assertion 7, The restarted maestro becomes a follower: skipped
2023-11-03 01:20:50,808 TADA INFO test maestro_raft_test ended
2023-11-03 01:21:12 INFO: ----------------------------------------------
2023-11-03 01:21:13 INFO: ======== ovis_json_test ========
2023-11-03 01:21:13 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ovis_json_test
2023-11-03 01:21:13,882 __main__ INFO -- Create the cluster -- 
2023-11-03 01:21:26,171 TADA INFO starting test `ovis_json_test`
2023-11-03 01:21:26,171 TADA INFO   test-id: e1d47ba3c6cada82cc978812a6f2bff5193bc7a7d88047a0b8f796b41dad4fb9
2023-11-03 01:21:26,171 TADA INFO   test-suite: OVIS-LIB
2023-11-03 01:21:26,171 TADA INFO   test-name: ovis_json_test
2023-11-03 01:21:26,171 TADA INFO   test-user: narate
2023-11-03 01:21:26,171 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:21:26,172 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-11-03 01:21:26,172 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-11-03 01:21:26,172 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-11-03 01:21:26,173 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-11-03 01:21:26,173 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-11-03 01:21:26,173 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-11-03 01:21:26,173 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-11-03 01:21:26,173 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-11-03 01:21:26,173 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,174 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-11-03 01:21:26,175 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-11-03 01:21:26,175 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-11-03 01:21:26,175 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-11-03 01:21:26,175 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-11-03 01:21:26,175 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-11-03 01:21:26,175 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-11-03 01:21:26,176 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-11-03 01:21:26,176 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-11-03 01:21:26,176 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-11-03 01:21:26,176 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-11-03 01:21:26,176 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-11-03 01:21:26,177 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,177 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,178 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,178 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-11-03 01:21:26,178 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-11-03 01:21:26,178 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-11-03 01:21:26,178 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-11-03 01:21:26,178 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-11-03 01:21:26,179 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-11-03 01:21:26,179 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-11-03 01:21:26,179 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-11-03 01:21:26,179 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-11-03 01:21:26,179 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-11-03 01:21:26,179 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-11-03 01:21:26,180 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-11-03 01:21:26,180 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-11-03 01:21:26,180 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-11-03 01:21:26,180 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-11-03 01:21:26,180 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-11-03 01:21:26,180 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-11-03 01:21:26,180 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-11-03 01:21:26,181 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-11-03 01:21:26,181 TADA INFO test ovis_json_test ended
2023-11-03 01:21:37 INFO: ----------------------------------------------
2023-11-03 01:21:38 INFO: ======== updtr_add_test ========
2023-11-03 01:21:38 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_add_test
2023-11-03 01:21:38,787 __main__ INFO -- Get or create the cluster --
2023-11-03 01:21:38,787 TADA INFO starting test `updtr_add test`
2023-11-03 01:21:38,787 TADA INFO   test-id: 3fb6896080c2d3abf63b49b334c923e471f237320605ad5b4cb90e1c10686fe2
2023-11-03 01:21:38,787 TADA INFO   test-suite: LDMSD
2023-11-03 01:21:38,788 TADA INFO   test-name: updtr_add test
2023-11-03 01:21:38,788 TADA INFO   test-user: narate
2023-11-03 01:21:38,788 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:21:54,906 __main__ INFO -- Start daemons --
2023-11-03 01:22:10,386 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:22:10,689 __main__ INFO All LDMSDs are up.
2023-11-03 01:22:11,921 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:22:13,157 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:22:14,373 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:22:15,594 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:22:16,821 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:22:19,253 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:22:21,704 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:22:22,914 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-11-03 01:22:22,914 __main__ INFO --- done ---
2023-11-03 01:22:22,915 TADA INFO test updtr_add test ended
2023-11-03 01:22:35 INFO: ----------------------------------------------
2023-11-03 01:22:36 INFO: ======== updtr_del_test ========
2023-11-03 01:22:36 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_del_test
2023-11-03 01:22:36,868 __main__ INFO -- Get or create the cluster --
2023-11-03 01:22:36,868 TADA INFO starting test `updtr_add test`
2023-11-03 01:22:36,869 TADA INFO   test-id: 219416591c0972c90edc2d20fce9a61538827540dc1cc6a922b1d897da7cdc62
2023-11-03 01:22:36,869 TADA INFO   test-suite: LDMSD
2023-11-03 01:22:36,869 TADA INFO   test-name: updtr_add test
2023-11-03 01:22:36,869 TADA INFO   test-user: narate
2023-11-03 01:22:36,869 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:22:53,315 __main__ INFO -- Start daemons --
2023-11-03 01:23:08,738 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:23:09,052 __main__ INFO All LDMSDs are up.
2023-11-03 01:23:10,264 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:23:11,474 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-03 01:23:12,689 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:23:13,925 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:23:13,925 __main__ INFO --- done ---
2023-11-03 01:23:13,926 TADA INFO test updtr_add test ended
2023-11-03 01:23:26 INFO: ----------------------------------------------
2023-11-03 01:23:27 INFO: ======== updtr_match_add_test ========
2023-11-03 01:23:27 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_match_add_test
2023-11-03 01:23:28,057 __main__ INFO -- Get or create the cluster --
2023-11-03 01:23:28,058 TADA INFO starting test `updtr_add test`
2023-11-03 01:23:28,058 TADA INFO   test-id: 82a2695cb28f1ae3777d46cd8e253d75a64b4fbe393d949246e15e09e5465108
2023-11-03 01:23:28,058 TADA INFO   test-suite: LDMSD
2023-11-03 01:23:28,058 TADA INFO   test-name: updtr_add test
2023-11-03 01:23:28,058 TADA INFO   test-user: narate
2023-11-03 01:23:28,058 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:23:44,290 __main__ INFO -- Start daemons --
2023-11-03 01:23:59,742 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:24:00,052 __main__ INFO All LDMSDs are up.
2023-11-03 01:24:01,265 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:24:02,484 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:24:03,701 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:24:04,918 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:24:06,142 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-03 01:24:06,142 __main__ INFO --- done ---
2023-11-03 01:24:06,142 TADA INFO test updtr_add test ended
2023-11-03 01:24:18 INFO: ----------------------------------------------
2023-11-03 01:24:19 INFO: ======== updtr_match_del_test ========
2023-11-03 01:24:19 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_match_del_test
2023-11-03 01:24:20,071 __main__ INFO -- Get or create the cluster --
2023-11-03 01:24:20,071 TADA INFO starting test `updtr_add test`
2023-11-03 01:24:20,071 TADA INFO   test-id: a47452d5fe171c09930607c93c8fbd57b48501a97c65ad37f56ae02b2ab235f9
2023-11-03 01:24:20,071 TADA INFO   test-suite: LDMSD
2023-11-03 01:24:20,071 TADA INFO   test-name: updtr_add test
2023-11-03 01:24:20,071 TADA INFO   test-user: narate
2023-11-03 01:24:20,071 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:24:36,269 __main__ INFO -- Start daemons --
2023-11-03 01:24:51,729 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:24:52,046 __main__ INFO All LDMSDs are up.
2023-11-03 01:24:53,261 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-11-03 01:24:54,481 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:24:55,718 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:24:56,930 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:24:58,169 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:24:59,388 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:25:00,596 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:25:00,597 __main__ INFO --- done ---
2023-11-03 01:25:00,597 TADA INFO test updtr_add test ended
2023-11-03 01:25:13 INFO: ----------------------------------------------
2023-11-03 01:25:13 INFO: ======== updtr_prdcr_add_test ========
2023-11-03 01:25:13 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_prdcr_add_test
2023-11-03 01:25:14,640 __main__ INFO -- Get or create the cluster --
2023-11-03 01:25:14,641 TADA INFO starting test `updtr_add test`
2023-11-03 01:25:14,641 TADA INFO   test-id: e210c675984eca574642585c9460cd821fd8221b2bc50a4e897b134cd825ae48
2023-11-03 01:25:14,641 TADA INFO   test-suite: LDMSD
2023-11-03 01:25:14,641 TADA INFO   test-name: updtr_add test
2023-11-03 01:25:14,641 TADA INFO   test-user: narate
2023-11-03 01:25:14,641 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:25:30,919 __main__ INFO -- Start daemons --
2023-11-03 01:25:46,351 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:25:46,664 __main__ INFO All LDMSDs are up.
2023-11-03 01:25:47,882 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:25:50,327 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:25:52,791 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:25:54,018 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-11-03 01:25:55,237 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:25:55,237 __main__ INFO --- done ---
2023-11-03 01:25:55,237 TADA INFO test updtr_add test ended
2023-11-03 01:26:07 INFO: ----------------------------------------------
2023-11-03 01:26:08 INFO: ======== updtr_prdcr_del_test ========
2023-11-03 01:26:08 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_prdcr_del_test
2023-11-03 01:26:09,199 __main__ INFO -- Get or create the cluster --
2023-11-03 01:26:09,199 TADA INFO starting test `updtr_add test`
2023-11-03 01:26:09,199 TADA INFO   test-id: 47deb083c18d34e7e1e1af8a4d9f3007b2672aa0959528b862deb5bdca211332
2023-11-03 01:26:09,199 TADA INFO   test-suite: LDMSD
2023-11-03 01:26:09,199 TADA INFO   test-name: updtr_add test
2023-11-03 01:26:09,199 TADA INFO   test-user: narate
2023-11-03 01:26:09,200 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:26:25,440 __main__ INFO -- Start daemons --
2023-11-03 01:26:40,968 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:26:41,296 __main__ INFO All LDMSDs are up.
2023-11-03 01:26:42,499 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:26:43,715 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-03 01:26:44,915 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:26:47,357 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:26:47,357 __main__ INFO --- done ---
2023-11-03 01:26:47,357 TADA INFO test updtr_add test ended
2023-11-03 01:26:59 INFO: ----------------------------------------------
2023-11-03 01:27:00 INFO: ======== updtr_start_test ========
2023-11-03 01:27:00 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_start_test
2023-11-03 01:27:01,320 __main__ INFO -- Get or create the cluster --
2023-11-03 01:27:01,321 TADA INFO starting test `updtr_add test`
2023-11-03 01:27:01,321 TADA INFO   test-id: 94dac87ec35d42419b706c0d9bf89514c87975a34b7f5214f1a9ae51c8a1de1d
2023-11-03 01:27:01,321 TADA INFO   test-suite: LDMSD
2023-11-03 01:27:01,321 TADA INFO   test-name: updtr_add test
2023-11-03 01:27:01,321 TADA INFO   test-user: narate
2023-11-03 01:27:01,321 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:27:17,509 __main__ INFO -- Start daemons --
2023-11-03 01:27:32,957 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:27:33,274 __main__ INFO All LDMSDs are up.
2023-11-03 01:27:34,487 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:27:35,713 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:27:36,933 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:27:38,174 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:27:39,401 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-11-03 01:27:41,865 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:27:43,093 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:27:45,551 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:27:48,011 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:27:50,446 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:27:51,672 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-11-03 01:27:51,672 __main__ INFO --- done ---
2023-11-03 01:27:51,672 TADA INFO test updtr_add test ended
2023-11-03 01:28:04 INFO: ----------------------------------------------
2023-11-03 01:28:04 INFO: ======== updtr_status_test ========
2023-11-03 01:28:04 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_status_test
2023-11-03 01:28:05,666 __main__ INFO -- Get or create the cluster --
2023-11-03 01:28:05,667 TADA INFO starting test `updtr_status test`
2023-11-03 01:28:05,667 TADA INFO   test-id: bc97be72f8d99d0ca8c983c57439dac6a1c7e9710f6dfa381f710c880b9cc629
2023-11-03 01:28:05,667 TADA INFO   test-suite: LDMSD
2023-11-03 01:28:05,667 TADA INFO   test-name: updtr_status test
2023-11-03 01:28:05,667 TADA INFO   test-user: narate
2023-11-03 01:28:05,667 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:28:25,033 __main__ INFO -- Start daemons --
2023-11-03 01:28:45,702 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:28:46,146 __main__ INFO All LDMSDs are up.
2023-11-03 01:28:47,365 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-11-03 01:28:48,591 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-11-03 01:28:49,818 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:28:51,030 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:28:52,261 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-11-03 01:28:52,261 __main__ INFO --- done ---
2023-11-03 01:28:52,261 TADA INFO test updtr_status test ended
2023-11-03 01:29:05 INFO: ----------------------------------------------
2023-11-03 01:29:06 INFO: ======== updtr_stop_test ========
2023-11-03 01:29:06 INFO: CMD: python3 updtr_stop_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/updtr_stop_test
2023-11-03 01:29:07,099 __main__ INFO -- Get or create the cluster --
2023-11-03 01:29:07,099 TADA INFO starting test `updtr_add test`
2023-11-03 01:29:07,099 TADA INFO   test-id: 402255f27625e95166a7defb112b136c0b54024ee1fd502e3369f0c58d26d910
2023-11-03 01:29:07,099 TADA INFO   test-suite: LDMSD
2023-11-03 01:29:07,099 TADA INFO   test-name: updtr_add test
2023-11-03 01:29:07,099 TADA INFO   test-user: narate
2023-11-03 01:29:07,099 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:29:23,071 __main__ INFO -- Start daemons --
2023-11-03 01:29:38,489 __main__ INFO Waiting ... for all LDMSDs to start
2023-11-03 01:29:38,823 __main__ INFO All LDMSDs are up.
2023-11-03 01:29:40,034 TADA INFO assertion 1, Send updtr_stop for a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-11-03 01:29:42,510 TADA INFO assertion 2, Send updtr_stop to a running updater: report(rc = 0, status = [{'name': 'running', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'running', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-11-03 01:29:43,739 TADA INFO assertion 3, Send updtr_stop to a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-11-03 01:29:43,739 __main__ INFO --- done ---
2023-11-03 01:29:43,739 TADA INFO test updtr_add test ended
2023-11-03 01:29:56 INFO: ----------------------------------------------
2023-11-03 01:29:56 INFO: ======== ldmsd_flex_decomp_test ========
2023-11-03 01:29:56 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_flex_decomp_test
2023-11-03 01:29:57,648 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-11-03 01:29:57,648 TADA INFO   test-id: f24c874dd669224eb94c9e332184075d60b1ddd504dd8fe65409f09db432b72a
2023-11-03 01:29:57,649 TADA INFO   test-suite: LDMSD
2023-11-03 01:29:57,649 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-11-03 01:29:57,649 TADA INFO   test-user: narate
2023-11-03 01:29:57,649 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:29:57,650 __main__ INFO -- Get or create the cluster --
2023-11-03 01:30:26,177 __main__ INFO -- Start daemons --
2023-11-03 01:30:55,880 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:31:45,139 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-11-03 01:31:45,140 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-11-03 01:31:45,140 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-11-03 01:31:45,140 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-11-03 01:31:45,140 TADA INFO assertion 5, record sos schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 10, record csv schema check: OK, passed
2023-11-03 01:31:45,141 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-11-03 01:31:45,142 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-11-03 01:31:45,142 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-11-03 01:31:45,142 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-11-03 01:31:45,142 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-11-03 01:31:45,143 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-11-03 01:31:45,219 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-11-03 01:31:45,222 TADA INFO assertion 18, fill sos data check: OK, passed
2023-11-03 01:31:45,224 TADA INFO assertion 19, filter sos data check: OK, passed
2023-11-03 01:31:45,233 TADA INFO assertion 20, record sos data check: OK, passed
2023-11-03 01:31:45,235 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-11-03 01:31:45,312 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-11-03 01:31:45,315 TADA INFO assertion 23, fill csv data check: OK, passed
2023-11-03 01:31:45,317 TADA INFO assertion 24, filter csv data check: OK, passed
2023-11-03 01:31:45,326 TADA INFO assertion 25, record csv data check: OK, passed
2023-11-03 01:31:45,327 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-11-03 01:31:45,351 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-11-03 01:31:45,352 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-11-03 01:31:45,353 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-11-03 01:31:45,357 TADA INFO assertion 30, record kafka data check: OK, passed
2023-11-03 01:31:45,357 TADA INFO test ldmsd_flex_decomp_test ended
2023-11-03 01:31:45,357 TADA INFO test ldmsd_flex_decomp_test ended
2023-11-03 01:32:00 INFO: ----------------------------------------------
2023-11-03 01:32:01 INFO: ======== ldms_set_info_test ========
2023-11-03 01:32:01 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_set_info_test
2023-11-03 01:32:19,277 TADA INFO starting test `ldms_set_info_test`
2023-11-03 01:32:19,277 TADA INFO   test-id: cd6494321acb0fde5c2383fb047f4314efca0d72ae6ed7e8ca6c759a8be04fa8
2023-11-03 01:32:19,277 TADA INFO   test-suite: LDMSD
2023-11-03 01:32:19,277 TADA INFO   test-name: ldms_set_info_test
2023-11-03 01:32:19,277 TADA INFO   test-user: narate
2023-11-03 01:32:19,277 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:32:19,278 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-11-03 01:32:19,278 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-11-03 01:32:19,278 TADA INFO assertion 3, Get a value : -, passed
2023-11-03 01:32:19,278 TADA INFO assertion 4, Unset a pair : -, passed
2023-11-03 01:32:19,278 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 7, Server resetting a key : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 8, Server unset a key : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 9, Server add a key : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 10, Adding a key : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-11-03 01:32:19,279 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-11-03 01:32:19,280 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-11-03 01:32:19,280 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-11-03 01:32:19,280 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-11-03 01:32:19,280 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-11-03 01:32:19,280 TADA INFO test ldms_set_info_test ended
2023-11-03 01:32:30 INFO: ----------------------------------------------
2023-11-03 01:32:31 INFO: ======== slurm_sampler2_test ========
2023-11-03 01:32:31 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/slurm_sampler2_test
2023-11-03 01:32:31,830 TADA INFO starting test `slurm_sampler2_test`
2023-11-03 01:32:31,830 TADA INFO   test-id: 2d0e3c7532a6893684bded8a5d2b67e1975cc75e3fdd1e75a597ee53840ec7a6
2023-11-03 01:32:31,830 TADA INFO   test-suite: LDMSD
2023-11-03 01:32:31,830 TADA INFO   test-name: slurm_sampler2_test
2023-11-03 01:32:31,830 TADA INFO   test-user: narate
2023-11-03 01:32:31,830 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:32:31,831 __main__ INFO -- Get or create the cluster --
2023-11-03 01:32:57,173 __main__ INFO -- Add users --
2023-11-03 01:33:02,913 __main__ INFO -- Preparing job script & programs --
2023-11-03 01:33:03,662 __main__ INFO -- Start daemons --
2023-11-03 01:33:45,362 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-11-03 01:33:50,092 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-11-03 01:33:52,837 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-11-03 01:33:55,587 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-11-03 01:33:58,347 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:01,108 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:05,833 TADA INFO assertion 3.1, Expanding the set heap -- job_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:08,591 TADA INFO assertion 3.2, Expanding the set heap -- step_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:12,647 TADA INFO assertion 3.3, Expanding the set heap -- task_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:16,653 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:19,421 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:25,783 TADA INFO assertion 4.1, Multi-tenant -- job_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:27,504 TADA INFO assertion 4.2, Multi-tenant -- step_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:30,192 TADA INFO assertion 4.3, Multi-tenant -- task_init: The metric values are as expected on all nodes., passed
2023-11-03 01:34:32,795 TADA INFO assertion 4.4, Multi-tenant -- task_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:34,529 TADA INFO assertion 4.5, Multi-tenant -- job_exit: The metric values are as expected on all nodes., passed
2023-11-03 01:34:34,529 TADA INFO test slurm_sampler2_test ended
2023-11-03 01:34:49 INFO: ----------------------------------------------
2023-11-03 01:34:49 INFO: ======== libovis_log_test ========
2023-11-03 01:34:49 INFO: CMD: python3 libovis_log_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/libovis_log_test
2023-11-03 01:34:50,701 TADA INFO starting test `libovis_log_test`
2023-11-03 01:34:50,701 TADA INFO   test-id: f17cbd1ed193d41b310a527423f737ab15d84f3ce550e23179f440955163208f
2023-11-03 01:34:50,701 TADA INFO   test-suite: LDMSD
2023-11-03 01:34:50,701 TADA INFO   test-name: libovis_log_test
2023-11-03 01:34:50,701 TADA INFO   test-user: narate
2023-11-03 01:34:50,701 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:34:50,702 __main__ INFO -- Create the cluster -- 
2023-11-03 01:35:02,598 __main__ INFO -- Start daemons --
2023-11-03 01:35:04,818 TADA INFO assertion 1, Call ovis_log_init() with valid arguments: 'return_code=0' and 'liovis_log_test' in 'Fri Nov 03 01:35:03 2023:         : libovis_log_test: return_code=0
', passed
2023-11-03 01:35:05,942 TADA INFO assertion 2, Call ovis_log_init() with name = NULL: ('return_code=0' and ': :') in 'Fri Nov 03 01:35:04 2023:         : : return_code=0
', passed
2023-11-03 01:35:07,061 TADA INFO assertion 3, Call ovis_log_init() with an invalid level: 'return_code=22' in 'Fri Nov 03 01:35:06 2023:         : : return_code=22
', passed
2023-11-03 01:35:08,186 TADA INFO assertion 4, Call ovis_log_init() with an invalid mode: 'return_code=22' in 'Fri Nov 03 01:35:07 2023:         : : return_code=22
', passed
Traceback (most recent call last):
  File "libovis_log_test", line 504, in <module>
    raise RuntimeError("Cannot parse the output")
RuntimeError: Cannot parse the output
2023-11-03 01:35:08,848 TADA INFO assertion 7, Open the log file at a non-existing path: skipped
2023-11-03 01:35:08,848 TADA INFO assertion 6, Log messages to a file: skipped
2023-11-03 01:35:08,848 TADA INFO assertion 5, Log messages to stdout: skipped
2023-11-03 01:35:08,848 TADA INFO assertion 8, Reopen the log file at another path: skipped
2023-11-03 01:35:08,848 TADA INFO assertion 9, Convert 'DEBUG,INFO' integer to a string: skipped
2023-11-03 01:35:08,848 TADA INFO assertion 10, Convert 'DEBUG,WARNING' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 11, Convert 'DEBUG,ERROR' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 12, Convert 'DEBUG,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 13, Convert 'INFO,WARNING' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 14, Convert 'INFO,ERROR' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 15, Convert 'INFO,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 16, Convert 'WARNING,ERROR' integer to a string: skipped
2023-11-03 01:35:08,849 TADA INFO assertion 17, Convert 'WARNING,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 18, Convert 'ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 19, Convert 'DEBUG,INFO,WARNING' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 20, Convert 'DEBUG,INFO,ERROR' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 21, Convert 'DEBUG,INFO,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 22, Convert 'DEBUG,WARNING,ERROR' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 23, Convert 'DEBUG,WARNING,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,850 TADA INFO assertion 24, Convert 'DEBUG,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 25, Convert 'INFO,WARNING,ERROR' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 26, Convert 'INFO,WARNING,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 27, Convert 'INFO,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 28, Convert 'WARNING,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 29, Convert 'DEBUG,INFO,WARNING,ERROR' integer to a string: skipped
2023-11-03 01:35:08,851 TADA INFO assertion 30, Convert 'DEBUG,INFO,WARNING,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 31, Convert 'DEBUG,INFO,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 32, Convert 'DEBUG,WARNING,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 33, Convert 'INFO,WARNING,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 34, Convert 'DEBUG,INFO,WARNING,ERROR,CRITICAL' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 35, Convert 'DEBUG,' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 36, Convert 'INFO,' integer to a string: skipped
2023-11-03 01:35:08,852 TADA INFO assertion 37, Convert 'WARNING,' integer to a string: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 38, Convert 'ERROR,' integer to a string: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 39, Convert 'CRITICAL,' integer to a string: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 40, Convert an invalid integer to a level string: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 41, Convert the 'DEBUG,INFO' to an integer: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 42, Convert the 'DEBUG,WARNING' to an integer: skipped
2023-11-03 01:35:08,853 TADA INFO assertion 43, Convert the 'DEBUG,ERROR' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 44, Convert the 'DEBUG,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 45, Convert the 'INFO,WARNING' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 46, Convert the 'INFO,ERROR' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 47, Convert the 'INFO,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 48, Convert the 'WARNING,ERROR' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 49, Convert the 'WARNING,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,854 TADA INFO assertion 50, Convert the 'ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 51, Convert the 'DEBUG,INFO,WARNING' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 52, Convert the 'DEBUG,INFO,ERROR' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 53, Convert the 'DEBUG,INFO,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 54, Convert the 'DEBUG,WARNING,ERROR' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 55, Convert the 'DEBUG,WARNING,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 56, Convert the 'DEBUG,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,855 TADA INFO assertion 57, Convert the 'INFO,WARNING,ERROR' to an integer: skipped
2023-11-03 01:35:08,856 TADA INFO assertion 58, Convert the 'INFO,WARNING,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,856 TADA INFO assertion 59, Convert the 'INFO,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,856 TADA INFO assertion 60, Convert the 'WARNING,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,856 TADA INFO assertion 61, Convert the 'DEBUG,INFO,WARNING,ERROR' to an integer: skipped
2023-11-03 01:35:08,856 TADA INFO assertion 62, Convert the 'DEBUG,INFO,WARNING,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 63, Convert the 'DEBUG,INFO,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 64, Convert the 'DEBUG,WARNING,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 65, Convert the 'INFO,WARNING,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 66, Convert the 'DEBUG,INFO,WARNING,ERROR,CRITICAL' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 67, Convert the 'DEBUG,' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 68, Convert the 'INFO,' to an integer: skipped
2023-11-03 01:35:08,857 TADA INFO assertion 69, Convert the 'WARNING,' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 70, Convert the 'ERROR,' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 71, Convert the 'CRITICAL,' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 72, Convert the 'DEBUG' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 73, Convert the 'INFO' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 74, Convert the 'WARNING' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 75, Convert the 'ERROR' to an integer: skipped
2023-11-03 01:35:08,858 TADA INFO assertion 76, Convert the 'CRITICAL' to an integer: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 77, Convert an invalid level string to an integer: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 78, Verify that no messages were printed when the level is QUIET.: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 79, Verify that messages of DEBUG,INFO were reported.: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 80, Verify that messages of DEBUG,WARNING were reported.: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 81, Verify that messages of DEBUG,ERROR were reported.: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 82, Verify that messages of DEBUG,CRITICAL were reported.: skipped
2023-11-03 01:35:08,859 TADA INFO assertion 83, Verify that messages of INFO,WARNING were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 84, Verify that messages of INFO,ERROR were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 85, Verify that messages of INFO,CRITICAL were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 86, Verify that messages of WARNING,ERROR were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 87, Verify that messages of WARNING,CRITICAL were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 88, Verify that messages of ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,860 TADA INFO assertion 89, Verify that messages of DEBUG,INFO,WARNING were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 90, Verify that messages of DEBUG,INFO,ERROR were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 91, Verify that messages of DEBUG,INFO,CRITICAL were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 92, Verify that messages of DEBUG,WARNING,ERROR were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 93, Verify that messages of DEBUG,WARNING,CRITICAL were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 94, Verify that messages of DEBUG,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 95, Verify that messages of INFO,WARNING,ERROR were reported.: skipped
2023-11-03 01:35:08,861 TADA INFO assertion 96, Verify that messages of INFO,WARNING,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 97, Verify that messages of INFO,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 98, Verify that messages of WARNING,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 99, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 100, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 101, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 102, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,862 TADA INFO assertion 103, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 104, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 105, Verify that messages of DEBUG, were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 106, Verify that messages of INFO, were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 107, Verify that messages of WARNING, were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 108, Verify that messages of ERROR, were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 109, Verify that messages of CRITICAL, were reported.: skipped
2023-11-03 01:35:08,863 TADA INFO assertion 110, Verify that messages of DEBUG were reported.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 111, Verify that messages of INFO were reported.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 112, Verify that messages of WARNING were reported.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 113, Verify that messages of ERROR were reported.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 114, Verify that messages of CRITICAL were reported.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 116, Verify that ovis_log_close() works properly: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 115, Verify that applications can open, rename, and reopen log files to perform log rotation.: skipped
2023-11-03 01:35:08,864 TADA INFO assertion 117, Test a ovis_log_register() call with valid arguments: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 118, Test a ovis_log_register() call with NULL name: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 119, Test a ovis_log_register() call with NULL desc: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 120, Test a ovis_log_register() call with an existing subsystem: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 122, Verify that messages of DEBUG,INFO were reported from a subsystem.: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 123, Verify that messages of DEBUG,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 124, Verify that messages of DEBUG,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,865 TADA INFO assertion 125, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 126, Verify that messages of INFO,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 127, Verify that messages of INFO,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 128, Verify that messages of INFO,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 129, Verify that messages of WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 130, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,866 TADA INFO assertion 131, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 132, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 133, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 134, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 135, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 136, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 137, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,867 TADA INFO assertion 138, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 139, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 140, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 141, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 142, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 143, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,868 TADA INFO assertion 144, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 145, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 146, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 147, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 148, Verify that messages of DEBUG, were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 149, Verify that messages of INFO, were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 150, Verify that messages of WARNING, were reported from a subsystem.: skipped
2023-11-03 01:35:08,869 TADA INFO assertion 151, Verify that messages of ERROR, were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 152, Verify that messages of CRITICAL, were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 153, Verify that messages of DEBUG were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 154, Verify that messages of INFO were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 155, Verify that messages of WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 156, Verify that messages of ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 157, Verify that messages of CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,870 TADA INFO assertion 158, Verify that messages of DEBUG,INFO were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 159, Verify that messages of DEBUG,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 160, Verify that messages of DEBUG,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 161, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 162, Verify that messages of INFO,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 163, Verify that messages of INFO,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 164, Verify that messages of INFO,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,871 TADA INFO assertion 165, Verify that messages of WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 166, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 167, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 168, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 169, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 170, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 171, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,872 TADA INFO assertion 172, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 173, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 174, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 175, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 176, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 177, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 178, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,873 TADA INFO assertion 179, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 180, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 181, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 182, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 183, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 184, Verify that messages of DEBUG, were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 185, Verify that messages of INFO, were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 186, Verify that messages of WARNING, were reported from a subsystem.: skipped
2023-11-03 01:35:08,874 TADA INFO assertion 187, Verify that messages of ERROR, were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 188, Verify that messages of CRITICAL, were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 189, Verify that messages of DEBUG were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 190, Verify that messages of INFO were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 191, Verify that messages of WARNING were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 192, Verify that messages of ERROR were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 193, Verify that messages of CRITICAL were reported from a subsystem.: skipped
2023-11-03 01:35:08,875 TADA INFO assertion 195, Verify that ovis_log_set_level_by_regex() returns an error when the given regular expression string is invalid.: skipped
2023-11-03 01:35:08,876 TADA INFO assertion 194, Verify that ovis_log_set_level_by_regex() returns ENOENT when the given regular expression string doesn't match any logs.: skipped
2023-11-03 01:35:08,876 TADA INFO assertion 196, Verify that ovis_log_set_level_by_regex() sets the level of the matched log subsystems to the given value.: skipped
2023-11-03 01:35:08,876 TADA INFO assertion 197, Verify that ovis_log_list() works correctly.: skipped
2023-11-03 01:35:08,876 TADA INFO test libovis_log_test ended
2023-11-03 01:35:19 INFO: ----------------------------------------------
2023-11-03 01:35:20 INFO: ======== ldmsd_long_config_test ========
2023-11-03 01:35:20 INFO: CMD: python3 ldmsd_long_config_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_long_config_test
2023-11-03 01:35:21,486 TADA INFO starting test `ldmsd_long_config_line`
2023-11-03 01:35:21,486 TADA INFO   test-id: 3335c90fd3eb569becb2a7bf7a2e177a2a9b8ac54e350bc4778f64cf6d665f15
2023-11-03 01:35:21,486 TADA INFO   test-suite: LDMSD
2023-11-03 01:35:21,486 TADA INFO   test-name: ldmsd_long_config_line
2023-11-03 01:35:21,486 TADA INFO   test-user: narate
2023-11-03 01:35:21,486 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:35:21,487 __main__ INFO ---Get or create the cluster --
2023-11-03 01:35:38,293 __main__ INFO --- Start daemons ---
2023-11-03 01:35:55,011 TADA INFO assertion 1, LDMSD correctly processes a config line in a config file: LDMSD processed the long config line in the config file correctly., passed
2023-11-03 01:35:55,544 TADA INFO assertion 2, LDMSD correctly handle a config line from ldmsd_controller: LDMSD receives the correct message from ldmsd_controller., passed
2023-11-03 01:35:56,189 TADA INFO assertion 3, LDMSD correctly handle a config line from ldmsctl: LDMSD receives the correct message from ldmsctl., failed
Traceback (most recent call last):
  File "ldmsd_long_config_test", line 202, in <module>
    "LDMSD receives the correct message from ldmsctl.")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test the code path that handles long config lines that larger than the max of the transport message size, LDMSD receives the correct message from ldmsctl.: FAILED
2023-11-03 01:35:56,189 TADA INFO test ldmsd_long_config_line ended
2023-11-03 01:36:08 INFO: ----------------------------------------------
2023-11-03 01:36:09 INFO: ======== ldms_rail_test ========
2023-11-03 01:36:09 INFO: CMD: python3 ldms_rail_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_rail_test
2023-11-03 01:36:10,141 TADA INFO starting test `ldms_rail_test`
2023-11-03 01:36:10,141 TADA INFO   test-id: 47c47895ab649a986653b951882ca50e2eca282d43fc69ab531f5d7838737003
2023-11-03 01:36:10,141 TADA INFO   test-suite: LDMSD
2023-11-03 01:36:10,141 TADA INFO   test-name: ldms_rail_test
2023-11-03 01:36:10,141 TADA INFO   test-user: narate
2023-11-03 01:36:10,141 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:36:10,142 __main__ INFO -- Get or create the cluster --
2023-11-03 01:36:26,367 __main__ INFO -- Start daemons --
2023-11-03 01:36:31,216 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:36:33,218 __main__ INFO start ldms_rail_server.py and ldms_rail_client.py interactive sessions
2023-11-03 01:36:36,236 TADA INFO assertion 1, Start interactive LDMS server: OK, passed
2023-11-03 01:36:39,255 TADA INFO assertion 2, Start interactive LDMS client: OK, passed
2023-11-03 01:36:42,860 TADA INFO assertion 3, Client rail has 8 endpoints on 8 thread pools: OK, passed
2023-11-03 01:36:46,465 TADA INFO assertion 4, Server rail has 8 endpoints on 8 thread pools: OK, passed
2023-11-03 01:36:50,070 TADA INFO assertion 5, Sets on client are processed by different threads: OK, passed
2023-11-03 01:36:53,675 TADA INFO assertion 6, Verify sets on the client: OK, passed
2023-11-03 01:36:56,693 TADA INFO assertion 7, Start interactive LDMS client2: OK, passed
2023-11-03 01:37:00,298 TADA INFO assertion 8, Client2 rail has 8 endpoints on 4 thread pools: OK, passed
2023-11-03 01:37:03,317 TADA INFO assertion 9, Client3 (wrong auth) cannot connect: OK, passed
2023-11-03 01:37:06,334 TADA INFO assertion 10, Start interactive client4 (for push mode): OK, passed
2023-11-03 01:37:06,335 __main__ INFO waiting push ...
2023-11-03 01:37:08,337 __main__ INFO server: sampling new data (2)
2023-11-03 01:37:12,943 __main__ INFO client4: set pushes received
2023-11-03 01:37:12,943 __main__ INFO client4: verifying data in sets
2023-11-03 01:37:16,547 __main__ INFO client4: verifying threads-sets-endpoints spread
2023-11-03 01:37:27,360 TADA INFO assertion 11, Client4 got push callback from the corresponding thread: OK, passed
2023-11-03 01:37:30,381 TADA INFO assertion 12, Client5 started (for clean-up path test): OK, passed
2023-11-03 01:37:30,381 __main__ INFO xprt close by client1
2023-11-03 01:37:43,197 TADA INFO assertion 13, Active-side close: client1 clean up: OK, passed
2023-11-03 01:37:46,801 TADA INFO assertion 14, Active-side close: server-side clean up: OK, passed
2023-11-03 01:38:03,221 TADA INFO assertion 15, Passive-side close: client2 clean up: OK, passed
2023-11-03 01:38:03,222 TADA INFO assertion 16, Passive-side close: server-side clean up: OK, passed
2023-11-03 01:38:08,829 TADA INFO assertion 17, Active-side term: server-side clean up: OK, passed
2023-11-03 01:38:16,038 TADA INFO assertion 18, Passive-side term: client5 clean up: OK, passed
2023-11-03 01:38:36,492 TADA INFO assertion 19, server -> client overspending send: error message verified, passed
2023-11-03 01:38:47,305 TADA INFO assertion 20, client -> server overspending send: error message verified, passed
2023-11-03 01:38:50,910 TADA INFO assertion 21, verify send credits on the server: OK, passed
2023-11-03 01:38:54,515 TADA INFO assertion 22, verify send credits on the client: OK, passed
2023-11-03 01:39:02,625 TADA INFO assertion 23, server unblock, verify recv data: recv data verified, passed
2023-11-03 01:39:10,736 TADA INFO assertion 24, client unblock, verify recv data: recv data verified, passed
2023-11-03 01:39:14,340 TADA INFO assertion 25, verify send credits on the server: OK, passed
2023-11-03 01:39:17,945 TADA INFO assertion 26, verify send credits on the client: OK, passed
2023-11-03 01:39:21,550 TADA INFO assertion 27, server -> client send after credited back: OK, passed
2023-11-03 01:39:25,155 TADA INFO assertion 28, client -> server send after credited back: OK, passed
2023-11-03 01:39:28,760 TADA INFO assertion 29, verify send credits on the server: OK, passed
2023-11-03 01:39:32,365 TADA INFO assertion 30, verify send credits on the client: OK, passed
2023-11-03 01:39:35,970 TADA INFO assertion 31, server unblock, verify recv data: OK, passed
2023-11-03 01:39:39,574 TADA INFO assertion 32, client unblock, verify recv data: OK, passed
2023-11-03 01:39:43,179 TADA INFO assertion 33, verify send credits on the server: OK, passed
2023-11-03 01:39:46,784 TADA INFO assertion 34, verify send credits on the client: OK, passed
2023-11-03 01:39:50,389 TADA INFO assertion 35, verify send-credit deposits on the server: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-11-03 01:39:53,994 TADA INFO assertion 36, verify send-credit deposits on the client: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-11-03 01:39:53,994 TADA INFO test ldms_rail_test ended
2023-11-03 01:40:06 INFO: ----------------------------------------------
2023-11-03 01:40:07 INFO: ======== ldms_stream_test ========
2023-11-03 01:40:07 INFO: CMD: python3 ldms_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_stream_test
2023-11-03 01:40:07,889 TADA INFO starting test `ldms_stream_test`
2023-11-03 01:40:07,890 TADA INFO   test-id: 3ad18022b5f2c4a39a4ae4841a29ee5be1a55ef5d9843196e4c06eaea0ee6176
2023-11-03 01:40:07,890 TADA INFO   test-suite: LDMSD
2023-11-03 01:40:07,890 TADA INFO   test-name: ldms_stream_test
2023-11-03 01:40:07,890 TADA INFO   test-user: narate
2023-11-03 01:40:07,890 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:40:07,890 __main__ INFO -- Get or create the cluster --
2023-11-03 01:40:39,637 __main__ INFO -- Adding 'foo' and 'bar' users --
2023-11-03 01:40:49,597 __main__ INFO -- Start daemons --
2023-11-03 01:41:02,373 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:41:04,376 __main__ INFO start interactive stream servers
2023-11-03 01:41:04,376 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-7 
2023-11-03 01:41:07,392 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-6 
2023-11-03 01:41:10,410 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-5 
2023-11-03 01:41:13,428 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-4 
2023-11-03 01:41:16,447 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-3 
2023-11-03 01:41:19,469 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-2 
2023-11-03 01:41:22,488 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-1 
2023-11-03 01:41:25,504 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 
2023-11-03 01:41:29,025 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-5 
2023-11-03 01:41:32,544 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-6 
2023-11-03 01:41:36,062 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-7 
2023-11-03 01:41:39,581 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 as foo
2023-11-03 01:41:43,100 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-32d5252-node-4 as bar
2023-11-03 01:41:46,621 __main__ INFO starting /tada-src/python/ldms_stream_client.py on narate-ldms_stream_test-32d5252-node-8 as foo
2023-11-03 01:41:50,140 TADA INFO assertion 1, Publishing oversize data results in an error: checking..., passed
2023-11-03 01:41:50,652 __main__ INFO getting data from srv1
2023-11-03 01:41:53,158 __main__ INFO getting data from srv2
2023-11-03 01:41:55,665 __main__ INFO getting data from srv3
2023-11-03 01:41:58,171 __main__ INFO getting data from srv4
2023-11-03 01:42:00,678 __main__ INFO getting data from srv5
2023-11-03 01:42:03,183 __main__ INFO getting data from srv6
2023-11-03 01:42:05,689 __main__ INFO getting data from srv7
2023-11-03 01:42:08,194 __main__ INFO getting data from cli8foo
2023-11-03 01:42:10,701 TADA INFO assertion 2, JSON support (l3-stream): client data verified, passed
2023-11-03 01:42:10,701 __main__ INFO publishing 'four' on l3-stream by pub4
2023-11-03 01:42:11,204 __main__ INFO publishing 'five' on l3-stream by pub5
2023-11-03 01:42:11,706 __main__ INFO publishing 'six' on l3-stream by pub6
2023-11-03 01:42:12,209 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-11-03 01:42:12,712 TADA INFO assertion 301, send-credit taken: credits: [114, 128, 128, 128], passed
2023-11-03 01:42:12,712 __main__ INFO obtaining all client data (0)
2023-11-03 01:42:12,712 __main__ INFO getting data from srv1
2023-11-03 01:42:15,219 __main__ INFO getting data from srv2
2023-11-03 01:42:17,725 __main__ INFO getting data from srv3
2023-11-03 01:42:20,231 __main__ INFO getting data from srv4
2023-11-03 01:42:22,738 __main__ INFO getting data from srv5
2023-11-03 01:42:25,244 __main__ INFO getting data from srv6
2023-11-03 01:42:27,750 __main__ INFO getting data from srv7
2023-11-03 01:42:30,257 __main__ INFO getting data from cli8foo
2023-11-03 01:42:32,763 __main__ INFO obtaining all client data (1)
2023-11-03 01:42:32,763 __main__ INFO getting data from srv1
2023-11-03 01:42:35,269 __main__ INFO getting data from srv2
2023-11-03 01:42:37,776 __main__ INFO getting data from srv3
2023-11-03 01:42:40,282 __main__ INFO getting data from srv4
2023-11-03 01:42:42,788 __main__ INFO getting data from srv5
2023-11-03 01:42:45,293 __main__ INFO getting data from srv6
2023-11-03 01:42:47,799 __main__ INFO getting data from srv7
2023-11-03 01:42:50,305 __main__ INFO getting data from cli8foo
2023-11-03 01:42:52,811 __main__ INFO obtaining all client data (2)
2023-11-03 01:42:52,811 __main__ INFO getting data from srv1
2023-11-03 01:42:55,318 __main__ INFO getting data from srv2
2023-11-03 01:42:57,823 __main__ INFO getting data from srv3
2023-11-03 01:43:00,329 __main__ INFO getting data from srv4
2023-11-03 01:43:02,835 __main__ INFO getting data from srv5
2023-11-03 01:43:05,340 __main__ INFO getting data from srv6
2023-11-03 01:43:07,846 __main__ INFO getting data from srv7
2023-11-03 01:43:10,351 __main__ INFO getting data from cli8foo
2023-11-03 01:43:12,858 __main__ INFO obtaining all client data (3)
2023-11-03 01:43:12,858 __main__ INFO getting data from srv1
2023-11-03 01:43:15,364 __main__ INFO getting data from srv2
2023-11-03 01:43:17,869 __main__ INFO getting data from srv3
2023-11-03 01:43:20,375 __main__ INFO getting data from srv4
2023-11-03 01:43:22,881 __main__ INFO getting data from srv5
2023-11-03 01:43:25,386 __main__ INFO getting data from srv6
2023-11-03 01:43:27,892 __main__ INFO getting data from srv7
2023-11-03 01:43:30,397 __main__ INFO getting data from cli8foo
2023-11-03 01:43:33,405 TADA INFO assertion 302, send-credit returned: credits: [128, 128, 128, 128], passed
2023-11-03 01:43:33,406 TADA INFO assertion 303, stream delivery spread among rails: tids: {204, 205, 206, 207}, passed
2023-11-03 01:43:33,407 TADA INFO assertion 3, l3-stream delivery: client data verified, passed
2023-11-03 01:43:33,407 __main__ INFO publishing 'four' on l2-stream by pub4
2023-11-03 01:43:33,909 __main__ INFO publishing 'five' on l2-stream by pub5
2023-11-03 01:43:34,411 __main__ INFO publishing 'six' on l2-stream by pub6
2023-11-03 01:43:34,914 __main__ INFO publishing 'seven' on l2-stream by pub7
2023-11-03 01:43:35,416 __main__ INFO obtaining all client data (0)
2023-11-03 01:43:35,416 __main__ INFO getting data from srv1
2023-11-03 01:43:37,923 __main__ INFO getting data from srv2
2023-11-03 01:43:40,429 __main__ INFO getting data from srv3
2023-11-03 01:43:42,935 __main__ INFO getting data from srv4
2023-11-03 01:43:45,442 __main__ INFO getting data from srv5
2023-11-03 01:43:47,948 __main__ INFO getting data from srv6
2023-11-03 01:43:50,454 __main__ INFO getting data from srv7
2023-11-03 01:43:52,961 __main__ INFO getting data from cli8foo
2023-11-03 01:43:55,467 __main__ INFO obtaining all client data (1)
2023-11-03 01:43:55,467 __main__ INFO getting data from srv1
2023-11-03 01:43:57,974 __main__ INFO getting data from srv2
2023-11-03 01:44:00,480 __main__ INFO getting data from srv3
2023-11-03 01:44:02,987 __main__ INFO getting data from srv4
2023-11-03 01:44:05,492 __main__ INFO getting data from srv5
2023-11-03 01:44:07,998 __main__ INFO getting data from srv6
2023-11-03 01:44:10,503 __main__ INFO getting data from srv7
2023-11-03 01:44:13,009 __main__ INFO getting data from cli8foo
2023-11-03 01:44:15,516 TADA INFO assertion 4, l2-stream delivery: client data verified, passed
2023-11-03 01:44:15,516 __main__ INFO publishing 'four' on l1-stream by pub4
2023-11-03 01:44:16,018 __main__ INFO publishing 'five' on l1-stream by pub5
2023-11-03 01:44:16,519 __main__ INFO publishing 'six' on l1-stream by pub6
2023-11-03 01:44:17,022 __main__ INFO publishing 'seven' on l1-stream by pub7
2023-11-03 01:44:17,524 __main__ INFO obtaining all client data (0)
2023-11-03 01:44:17,525 __main__ INFO getting data from srv1
2023-11-03 01:44:20,031 __main__ INFO getting data from srv2
2023-11-03 01:44:22,537 __main__ INFO getting data from srv3
2023-11-03 01:44:25,043 __main__ INFO getting data from srv4
2023-11-03 01:44:27,550 __main__ INFO getting data from srv5
2023-11-03 01:44:30,056 __main__ INFO getting data from srv6
2023-11-03 01:44:32,562 __main__ INFO getting data from srv7
2023-11-03 01:44:35,069 __main__ INFO getting data from cli8foo
2023-11-03 01:44:37,575 __main__ INFO obtaining all client data (1)
2023-11-03 01:44:37,575 __main__ INFO getting data from srv1
2023-11-03 01:44:40,081 __main__ INFO getting data from srv2
2023-11-03 01:44:42,587 __main__ INFO getting data from srv3
2023-11-03 01:44:45,093 __main__ INFO getting data from srv4
2023-11-03 01:44:47,599 __main__ INFO getting data from srv5
2023-11-03 01:44:50,105 __main__ INFO getting data from srv6
2023-11-03 01:44:52,610 __main__ INFO getting data from srv7
2023-11-03 01:44:55,116 __main__ INFO getting data from cli8foo
2023-11-03 01:44:57,623 TADA INFO assertion 5, l1-stream delivery: client data verified, passed
2023-11-03 01:44:57,623 __main__ INFO publishing 'four' on x-stream by pub4
2023-11-03 01:44:58,125 __main__ INFO publishing 'five' on x-stream by pub5
2023-11-03 01:44:58,626 __main__ INFO publishing 'six' on x-stream by pub6
2023-11-03 01:44:59,128 __main__ INFO publishing 'seven' on x-stream by pub7
2023-11-03 01:44:59,631 __main__ INFO obtaining all client data (0)
2023-11-03 01:44:59,631 __main__ INFO getting data from srv1
2023-11-03 01:45:02,137 __main__ INFO getting data from srv2
2023-11-03 01:45:04,643 __main__ INFO getting data from srv3
2023-11-03 01:45:07,149 __main__ INFO getting data from srv4
2023-11-03 01:45:09,656 __main__ INFO getting data from srv5
2023-11-03 01:45:12,162 __main__ INFO getting data from srv6
2023-11-03 01:45:14,668 __main__ INFO getting data from srv7
2023-11-03 01:45:17,174 __main__ INFO getting data from cli8foo
2023-11-03 01:45:19,680 __main__ INFO obtaining all client data (1)
2023-11-03 01:45:19,680 __main__ INFO getting data from srv1
2023-11-03 01:45:22,187 __main__ INFO getting data from srv2
2023-11-03 01:45:24,692 __main__ INFO getting data from srv3
2023-11-03 01:45:27,198 __main__ INFO getting data from srv4
2023-11-03 01:45:29,704 __main__ INFO getting data from srv5
2023-11-03 01:45:32,210 __main__ INFO getting data from srv6
2023-11-03 01:45:34,715 __main__ INFO getting data from srv7
2023-11-03 01:45:37,221 __main__ INFO getting data from cli8foo
2023-11-03 01:45:39,727 TADA INFO assertion 6, x-stream delivery: client data verified, passed
2023-11-03 01:45:39,728 __main__ INFO publishing 'four' on nada by pub4
2023-11-03 01:45:40,229 __main__ INFO publishing 'five' on nada by pub5
2023-11-03 01:45:40,731 __main__ INFO publishing 'six' on nada by pub6
2023-11-03 01:45:41,232 __main__ INFO publishing 'seven' on nada by pub7
2023-11-03 01:45:41,734 __main__ INFO obtaining all client data (0)
2023-11-03 01:45:41,734 __main__ INFO getting data from srv1
2023-11-03 01:45:44,240 __main__ INFO getting data from srv2
2023-11-03 01:45:46,745 __main__ INFO getting data from srv3
2023-11-03 01:45:49,251 __main__ INFO getting data from srv4
2023-11-03 01:45:51,756 __main__ INFO getting data from srv5
2023-11-03 01:45:54,262 __main__ INFO getting data from srv6
2023-11-03 01:45:56,768 __main__ INFO getting data from srv7
2023-11-03 01:45:59,274 __main__ INFO getting data from cli8foo
2023-11-03 01:46:01,779 __main__ INFO obtaining all client data (1)
2023-11-03 01:46:01,780 __main__ INFO getting data from srv1
2023-11-03 01:46:04,285 __main__ INFO getting data from srv2
2023-11-03 01:46:06,791 __main__ INFO getting data from srv3
2023-11-03 01:46:09,296 __main__ INFO getting data from srv4
2023-11-03 01:46:11,802 __main__ INFO getting data from srv5
2023-11-03 01:46:14,307 __main__ INFO getting data from srv6
2023-11-03 01:46:16,813 __main__ INFO getting data from srv7
2023-11-03 01:46:19,318 __main__ INFO getting data from cli8foo
2023-11-03 01:46:21,824 TADA INFO assertion 7, nada delivery: client data verified, passed
2023-11-03 01:46:21,825 __main__ INFO publishing 'four' on l3-stream by pub4 (0400)
2023-11-03 01:46:22,327 __main__ INFO publishing 'five' on l3-stream by pub5 (0400)
2023-11-03 01:46:22,830 __main__ INFO publishing 'six' on l3-stream by pub6 (0400)
2023-11-03 01:46:23,332 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400)
2023-11-03 01:46:23,835 __main__ INFO obtaining all client data (0)
2023-11-03 01:46:23,835 __main__ INFO getting data from srv1
2023-11-03 01:46:26,341 __main__ INFO getting data from srv2
2023-11-03 01:46:28,847 __main__ INFO getting data from srv3
2023-11-03 01:46:31,354 __main__ INFO getting data from srv4
2023-11-03 01:46:33,860 __main__ INFO getting data from srv5
2023-11-03 01:46:36,366 __main__ INFO getting data from srv6
2023-11-03 01:46:38,872 __main__ INFO getting data from srv7
2023-11-03 01:46:41,379 __main__ INFO getting data from cli8foo
2023-11-03 01:46:43,884 __main__ INFO obtaining all client data (1)
2023-11-03 01:46:43,884 __main__ INFO getting data from srv1
2023-11-03 01:46:46,390 __main__ INFO getting data from srv2
2023-11-03 01:46:48,897 __main__ INFO getting data from srv3
2023-11-03 01:46:51,403 __main__ INFO getting data from srv4
2023-11-03 01:46:53,909 __main__ INFO getting data from srv5
2023-11-03 01:46:56,414 __main__ INFO getting data from srv6
2023-11-03 01:46:58,920 __main__ INFO getting data from srv7
2023-11-03 01:47:01,426 __main__ INFO getting data from cli8foo
2023-11-03 01:47:03,931 __main__ INFO obtaining all client data (2)
2023-11-03 01:47:03,931 __main__ INFO getting data from srv1
2023-11-03 01:47:06,438 __main__ INFO getting data from srv2
2023-11-03 01:47:08,943 __main__ INFO getting data from srv3
2023-11-03 01:47:11,449 __main__ INFO getting data from srv4
2023-11-03 01:47:13,954 __main__ INFO getting data from srv5
2023-11-03 01:47:16,460 __main__ INFO getting data from srv6
2023-11-03 01:47:18,965 __main__ INFO getting data from srv7
2023-11-03 01:47:21,471 __main__ INFO getting data from cli8foo
2023-11-03 01:47:23,977 __main__ INFO obtaining all client data (3)
2023-11-03 01:47:23,977 __main__ INFO getting data from srv1
2023-11-03 01:47:26,483 __main__ INFO getting data from srv2
2023-11-03 01:47:28,989 __main__ INFO getting data from srv3
2023-11-03 01:47:31,494 __main__ INFO getting data from srv4
2023-11-03 01:47:34,000 __main__ INFO getting data from srv5
2023-11-03 01:47:36,506 __main__ INFO getting data from srv6
2023-11-03 01:47:39,011 __main__ INFO getting data from srv7
2023-11-03 01:47:41,517 __main__ INFO getting data from cli8foo
2023-11-03 01:47:44,024 TADA INFO assertion 8, l3-stream by 'root' with 0400 permission: client data verified, passed
2023-11-03 01:47:44,024 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as foo
2023-11-03 01:47:44,526 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as foo
2023-11-03 01:47:45,029 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as foo
2023-11-03 01:47:45,531 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as foo
2023-11-03 01:47:46,034 __main__ INFO obtaining all client data (0)
2023-11-03 01:47:46,034 __main__ INFO getting data from srv1
2023-11-03 01:47:48,541 __main__ INFO getting data from srv2
2023-11-03 01:47:51,047 __main__ INFO getting data from srv3
2023-11-03 01:47:53,553 __main__ INFO getting data from srv4
2023-11-03 01:47:56,060 __main__ INFO getting data from srv5
2023-11-03 01:47:58,566 __main__ INFO getting data from srv6
2023-11-03 01:48:01,072 __main__ INFO getting data from srv7
2023-11-03 01:48:03,579 __main__ INFO getting data from cli8foo
2023-11-03 01:48:06,085 __main__ INFO obtaining all client data (1)
2023-11-03 01:48:06,085 __main__ INFO getting data from srv1
2023-11-03 01:48:08,591 __main__ INFO getting data from srv2
2023-11-03 01:48:11,097 __main__ INFO getting data from srv3
2023-11-03 01:48:13,604 __main__ INFO getting data from srv4
2023-11-03 01:48:16,109 __main__ INFO getting data from srv5
2023-11-03 01:48:18,615 __main__ INFO getting data from srv6
2023-11-03 01:48:21,120 __main__ INFO getting data from srv7
2023-11-03 01:48:23,626 __main__ INFO getting data from cli8foo
2023-11-03 01:48:26,132 __main__ INFO obtaining all client data (2)
2023-11-03 01:48:26,132 __main__ INFO getting data from srv1
2023-11-03 01:48:28,638 __main__ INFO getting data from srv2
2023-11-03 01:48:31,144 __main__ INFO getting data from srv3
2023-11-03 01:48:33,649 __main__ INFO getting data from srv4
2023-11-03 01:48:36,155 __main__ INFO getting data from srv5
2023-11-03 01:48:38,660 __main__ INFO getting data from srv6
2023-11-03 01:48:41,166 __main__ INFO getting data from srv7
2023-11-03 01:48:43,671 __main__ INFO getting data from cli8foo
2023-11-03 01:48:46,178 __main__ INFO obtaining all client data (3)
2023-11-03 01:48:46,178 __main__ INFO getting data from srv1
2023-11-03 01:48:48,684 __main__ INFO getting data from srv2
2023-11-03 01:48:51,190 __main__ INFO getting data from srv3
2023-11-03 01:48:53,695 __main__ INFO getting data from srv4
2023-11-03 01:48:56,201 __main__ INFO getting data from srv5
2023-11-03 01:48:58,706 __main__ INFO getting data from srv6
2023-11-03 01:49:01,212 __main__ INFO getting data from srv7
2023-11-03 01:49:03,718 __main__ INFO getting data from cli8foo
2023-11-03 01:49:06,225 TADA INFO assertion 9, l3-stream by 'root' as 'foo' with 0400 permission: client data verified, passed
2023-11-03 01:49:06,225 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as bar
2023-11-03 01:49:06,728 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as bar
2023-11-03 01:49:07,230 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as bar
2023-11-03 01:49:07,733 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as bar
2023-11-03 01:49:08,235 __main__ INFO obtaining all client data (0)
2023-11-03 01:49:08,235 __main__ INFO getting data from srv1
2023-11-03 01:49:10,742 __main__ INFO getting data from srv2
2023-11-03 01:49:13,248 __main__ INFO getting data from srv3
2023-11-03 01:49:15,754 __main__ INFO getting data from srv4
2023-11-03 01:49:18,261 __main__ INFO getting data from srv5
2023-11-03 01:49:20,767 __main__ INFO getting data from srv6
2023-11-03 01:49:23,273 __main__ INFO getting data from srv7
2023-11-03 01:49:25,780 __main__ INFO getting data from cli8foo
2023-11-03 01:49:28,285 __main__ INFO obtaining all client data (1)
2023-11-03 01:49:28,286 __main__ INFO getting data from srv1
2023-11-03 01:49:30,792 __main__ INFO getting data from srv2
2023-11-03 01:49:33,298 __main__ INFO getting data from srv3
2023-11-03 01:49:35,805 __main__ INFO getting data from srv4
2023-11-03 01:49:38,310 __main__ INFO getting data from srv5
2023-11-03 01:49:40,816 __main__ INFO getting data from srv6
2023-11-03 01:49:43,322 __main__ INFO getting data from srv7
2023-11-03 01:49:45,827 __main__ INFO getting data from cli8foo
2023-11-03 01:49:48,333 __main__ INFO obtaining all client data (2)
2023-11-03 01:49:48,333 __main__ INFO getting data from srv1
2023-11-03 01:49:50,839 __main__ INFO getting data from srv2
2023-11-03 01:49:53,345 __main__ INFO getting data from srv3
2023-11-03 01:49:55,850 __main__ INFO getting data from srv4
2023-11-03 01:49:58,356 __main__ INFO getting data from srv5
2023-11-03 01:50:00,862 __main__ INFO getting data from srv6
2023-11-03 01:50:03,367 __main__ INFO getting data from srv7
2023-11-03 01:50:05,873 __main__ INFO getting data from cli8foo
2023-11-03 01:50:08,378 __main__ INFO obtaining all client data (3)
2023-11-03 01:50:08,378 __main__ INFO getting data from srv1
2023-11-03 01:50:10,884 __main__ INFO getting data from srv2
2023-11-03 01:50:13,390 __main__ INFO getting data from srv3
2023-11-03 01:50:15,895 __main__ INFO getting data from srv4
2023-11-03 01:50:18,401 __main__ INFO getting data from srv5
2023-11-03 01:50:20,907 __main__ INFO getting data from srv6
2023-11-03 01:50:23,412 __main__ INFO getting data from srv7
2023-11-03 01:50:25,918 __main__ INFO getting data from cli8foo
2023-11-03 01:50:28,424 TADA INFO assertion 10, l3-stream by 'root' as 'bar' with 0400 permission: client data verified, passed
2023-11-03 01:50:28,425 __main__ INFO publishing 'four' on l3-stream by pub4 (0440) root as bar
2023-11-03 01:50:28,927 __main__ INFO publishing 'five' on l3-stream by pub5 (0440) root as bar
2023-11-03 01:50:29,430 __main__ INFO publishing 'six' on l3-stream by pub6 (0440) root as bar
2023-11-03 01:50:29,932 __main__ INFO publishing 'seven' on l3-stream by pub7 (0440) root as bar
2023-11-03 01:50:30,435 __main__ INFO obtaining all client data (0)
2023-11-03 01:50:30,435 __main__ INFO getting data from srv1
2023-11-03 01:50:32,941 __main__ INFO getting data from srv2
2023-11-03 01:50:35,447 __main__ INFO getting data from srv3
2023-11-03 01:50:37,954 __main__ INFO getting data from srv4
2023-11-03 01:50:40,460 __main__ INFO getting data from srv5
2023-11-03 01:50:42,966 __main__ INFO getting data from srv6
2023-11-03 01:50:45,472 __main__ INFO getting data from srv7
2023-11-03 01:50:47,979 __main__ INFO getting data from cli8foo
2023-11-03 01:50:50,484 __main__ INFO obtaining all client data (1)
2023-11-03 01:50:50,485 __main__ INFO getting data from srv1
2023-11-03 01:50:52,991 __main__ INFO getting data from srv2
2023-11-03 01:50:55,497 __main__ INFO getting data from srv3
2023-11-03 01:50:58,003 __main__ INFO getting data from srv4
2023-11-03 01:51:00,509 __main__ INFO getting data from srv5
2023-11-03 01:51:03,015 __main__ INFO getting data from srv6
2023-11-03 01:51:05,520 __main__ INFO getting data from srv7
2023-11-03 01:51:08,026 __main__ INFO getting data from cli8foo
2023-11-03 01:51:10,531 __main__ INFO obtaining all client data (2)
2023-11-03 01:51:10,532 __main__ INFO getting data from srv1
2023-11-03 01:51:13,038 __main__ INFO getting data from srv2
2023-11-03 01:51:15,543 __main__ INFO getting data from srv3
2023-11-03 01:51:18,049 __main__ INFO getting data from srv4
2023-11-03 01:51:20,554 __main__ INFO getting data from srv5
2023-11-03 01:51:23,060 __main__ INFO getting data from srv6
2023-11-03 01:51:25,565 __main__ INFO getting data from srv7
2023-11-03 01:51:28,071 __main__ INFO getting data from cli8foo
2023-11-03 01:51:30,577 __main__ INFO obtaining all client data (3)
2023-11-03 01:51:30,577 __main__ INFO getting data from srv1
2023-11-03 01:51:33,083 __main__ INFO getting data from srv2
2023-11-03 01:51:35,589 __main__ INFO getting data from srv3
2023-11-03 01:51:38,094 __main__ INFO getting data from srv4
2023-11-03 01:51:40,600 __main__ INFO getting data from srv5
2023-11-03 01:51:43,105 __main__ INFO getting data from srv6
2023-11-03 01:51:45,611 __main__ INFO getting data from srv7
2023-11-03 01:51:48,116 __main__ INFO getting data from cli8foo
2023-11-03 01:51:50,623 TADA INFO assertion 11, l3-stream by 'root' as 'bar' with 0440 permission: client data verified, passed
2023-11-03 01:51:51,125 TADA INFO assertion 12, l3-stream by 'foo' as 'bar' results in an error: checking..., passed
2023-11-03 01:51:51,125 __main__ INFO publishing 'four' on l3-stream by pub4foo (0440)
2023-11-03 01:51:51,628 __main__ INFO obtaining all client data (0)
2023-11-03 01:51:51,628 __main__ INFO getting data from srv1
2023-11-03 01:51:54,134 __main__ INFO getting data from srv2
2023-11-03 01:51:56,641 __main__ INFO getting data from srv3
2023-11-03 01:51:59,146 __main__ INFO getting data from srv4
2023-11-03 01:52:01,653 __main__ INFO getting data from srv5
2023-11-03 01:52:04,158 __main__ INFO getting data from srv6
2023-11-03 01:52:06,664 __main__ INFO getting data from srv7
2023-11-03 01:52:09,169 __main__ INFO getting data from cli8foo
2023-11-03 01:52:11,676 TADA INFO assertion 13, l3-stream by 'foo' with 0440 permission: client data verified, passed
2023-11-03 01:52:11,676 __main__ INFO publishing 'four' on l3-stream by pub4bar (0440)
2023-11-03 01:52:12,179 __main__ INFO obtaining all client data (0)
2023-11-03 01:52:12,179 __main__ INFO getting data from srv1
2023-11-03 01:52:14,685 __main__ INFO getting data from srv2
2023-11-03 01:52:17,191 __main__ INFO getting data from srv3
2023-11-03 01:52:19,697 __main__ INFO getting data from srv4
2023-11-03 01:52:22,204 __main__ INFO getting data from srv5
2023-11-03 01:52:24,709 __main__ INFO getting data from srv6
2023-11-03 01:52:27,215 __main__ INFO getting data from srv7
2023-11-03 01:52:29,720 __main__ INFO getting data from cli8foo
2023-11-03 01:52:32,226 TADA INFO assertion 14, l3-stream by 'bar' with 0440 permission: client data verified, passed
2023-11-03 01:52:35,734 TADA INFO assertion 15, Blocking client and asynchronous client have the same data: verified, passed
2023-11-03 01:52:37,238 __main__ INFO publishing 'four' on l3-stream by srv4
2023-11-03 01:52:37,740 __main__ INFO obtaining all client data (0)
2023-11-03 01:52:37,740 __main__ INFO getting data from srv1
2023-11-03 01:52:40,247 __main__ INFO getting data from srv2
2023-11-03 01:52:42,753 __main__ INFO getting data from srv3
2023-11-03 01:52:45,259 __main__ INFO getting data from srv4
2023-11-03 01:52:47,765 __main__ INFO getting data from srv5
2023-11-03 01:52:50,271 __main__ INFO getting data from srv6
2023-11-03 01:52:52,776 __main__ INFO getting data from srv7
2023-11-03 01:52:55,282 __main__ INFO getting data from cli8foo
2023-11-03 01:52:57,788 TADA INFO assertion 20, l3-stream publish from L1 (srv4): client data verified, passed
2023-11-03 01:52:57,789 __main__ INFO publishing 'four' on nada by srv4
2023-11-03 01:52:58,290 __main__ INFO obtaining all client data (0)
2023-11-03 01:52:58,290 __main__ INFO getting data from srv1
2023-11-03 01:53:00,796 __main__ INFO getting data from srv2
2023-11-03 01:53:03,301 __main__ INFO getting data from srv3
2023-11-03 01:53:05,807 __main__ INFO getting data from srv4
2023-11-03 01:53:08,313 __main__ INFO getting data from srv5
2023-11-03 01:53:10,818 __main__ INFO getting data from srv6
2023-11-03 01:53:13,324 __main__ INFO getting data from srv7
2023-11-03 01:53:15,829 __main__ INFO getting data from cli8foo
2023-11-03 01:53:18,335 TADA INFO assertion 21, nada publish from L1 (srv4): client data verified, passed
2023-11-03 01:53:21,927 TADA INFO assertion 22, Check stream stats in each process: verified, passed
2023-11-03 01:53:25,535 TADA INFO assertion 23, Check stream client stats in each process: verified, passed
2023-11-03 01:53:28,040 TADA INFO assertion 16, srv-6 clean up properly after srv-3 exited: checking..., passed
2023-11-03 01:53:28,041 TADA INFO assertion 17, srv-7 clean up properly after srv-3 exited: checking..., passed
2023-11-03 01:53:28,041 TADA INFO assertion 18, srv-1 clean up properly after srv-3 exited: checking..., passed
2023-11-03 01:53:28,041 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-32d5252-node-3 
2023-11-03 01:53:31,560 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-11-03 01:53:32,063 __main__ INFO obtaining all client data (0)
2023-11-03 01:53:32,063 __main__ INFO getting data from srv1
2023-11-03 01:53:34,569 __main__ INFO getting data from srv2
2023-11-03 01:53:37,075 __main__ INFO getting data from srv3
2023-11-03 01:53:39,581 __main__ INFO getting data from srv4
2023-11-03 01:53:42,087 __main__ INFO getting data from srv5
2023-11-03 01:53:44,592 __main__ INFO getting data from srv6
2023-11-03 01:53:47,098 __main__ INFO getting data from srv7
2023-11-03 01:53:49,604 __main__ INFO getting data from cli8foo
2023-11-03 01:53:52,111 TADA INFO assertion 19, l3-stream successfully delivered after srv-3 restarted: client data verified, passed
2023-11-03 01:53:52,112 TADA INFO test ldms_stream_test ended
2023-11-03 01:54:07 INFO: ----------------------------------------------
2023-11-03 01:54:08 INFO: ======== set_sec_mod_test ========
2023-11-03 01:54:08 INFO: CMD: python3 set_sec_mod_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/set_sec_mod_test
2023-11-03 01:54:09,577 TADA INFO starting test `set_sec_test`
2023-11-03 01:54:09,577 TADA INFO   test-id: ef5a83de173c964d8e09243dac5f3f70eefb5b6059d8275028026d515a9d6dee
2023-11-03 01:54:09,577 TADA INFO   test-suite: LDMSD
2023-11-03 01:54:09,577 TADA INFO   test-name: set_sec_test
2023-11-03 01:54:09,577 TADA INFO   test-user: narate
2023-11-03 01:54:09,578 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:54:09,578 __main__ INFO ---Get or create the cluster ---
2023-11-03 01:54:22,591 __main__ INFO --- Start daemons ---
2023-11-03 01:54:36,272 TADA INFO assertion 1, Change UID to an existing username: {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-03 01:54:37,489 TADA INFO assertion 2, Change UID to a not-existing username: errcode (22) == expected (22), passed
2023-11-03 01:54:38,931 TADA INFO assertion 3, Change UID to a valid UID: {'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}} == {'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-03 01:54:40,157 TADA INFO assertion 4, Change UID to an invalid UID: errcode (22) == expected (22), passed
2023-11-03 01:54:41,597 TADA INFO assertion 5, Change GID to an existing groupname: {'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}} == {'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}}, passed
2023-11-03 01:54:42,820 TADA INFO assertion 6, Change GID to a not-existing groupname: errcode (22) == expected (22), passed
2023-11-03 01:54:44,276 TADA INFO assertion 7, Change GID to a valid GID: {'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}} == {'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}}, passed
2023-11-03 01:54:45,506 TADA INFO assertion 8, Change GID to an invalid GID: errcode (22) == expected (22), passed
2023-11-03 01:54:46,954 TADA INFO assertion 9, Change permission bits to a valid permission value: {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}} == {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}}, passed
2023-11-03 01:54:48,177 TADA INFO assertion 10, Change permission bits to an invalid permission value: errcode (22) == expected (22), passed
2023-11-03 01:54:48,397 TADA INFO assertion 11, Verify that the aggregator got sets' new security info: {'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_5': {'uid': 1000, 'gid': 1111, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_7': {'uid': 1000, 'gid': 2222, 'perm': '0440'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_9': {'uid': 1000, 'gid': 1000, 'perm': '0400'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-03 01:54:48,618 TADA INFO assertion 12.1, Clients with different UID and the same GID cannot access 0400 sets.: {'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}} == {'set_1': {'uid': 1111, 'gid': 1000, 'perm': '0440'}, 'set_2': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_3': {'uid': 2222, 'gid': 1000, 'perm': '0440'}, 'set_4': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_6': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_8': {'uid': 1000, 'gid': 1000, 'perm': '0440'}, 'set_10': {'uid': 1000, 'gid': 1000, 'perm': '0440'}}, passed
2023-11-03 01:54:48,828 TADA INFO assertion 12.2, Clients with different UID and GID cannot access 04## sets.: {} == {}, passed
2023-11-03 01:54:48,828 __main__ INFO --- done ---
2023-11-03 01:54:48,828 TADA INFO test set_sec_test ended
2023-11-03 01:55:00 INFO: ----------------------------------------------
2023-11-03 01:55:01 INFO: ======== dump_cfg_test ========
2023-11-03 01:55:01 INFO: CMD: python3 dump_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/dump_cfg_test
2023-11-03 01:55:02,084 TADA INFO starting test `dump_cfg_test`
2023-11-03 01:55:02,084 TADA INFO   test-id: a638b2fd1f6a2d38d4692c9e4331453c8dc21afee8545dbdc06222aeb8b2c6af
2023-11-03 01:55:02,084 TADA INFO   test-suite: LDMSD
2023-11-03 01:55:02,084 TADA INFO   test-name: dump_cfg_test
2023-11-03 01:55:02,085 TADA INFO   test-user: narate
2023-11-03 01:55:02,085 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:55:02,085 __main__ INFO -- Get or create the cluster --
2023-11-03 01:55:21,386 __main__ INFO -- Start daemons --
2023-11-03 01:55:44,497 __main__ INFO -- Begin the test --
2023-11-03 01:55:50,080 TADA INFO assertion 1.1, Specify the command-line options: The generated configuration is as expected., passed
2023-11-03 01:55:55,781 TADA INFO assertion 1.2, Specify host at the command-line: The generated configuration is as expected., passed
2023-11-03 01:56:01,507 TADA INFO assertion 1.3, Specify auth_opt at the command-line: The generated configuration is as expected., passed
2023-11-03 01:56:07,326 TADA INFO assertion 2.1, Specify the command-line options in a configuration file: The generated configuration is as expected., passed
2023-11-03 01:56:13,147 TADA INFO assertion 3.1, Sampler configuration commands: The generated configuration is as expected., passed
2023-11-03 01:56:18,981 TADA INFO assertion 3.2, Sampler configuration commands with plugin-specific attributes: The generated configuration is as expected., passed
2023-11-03 01:56:24,933 TADA INFO assertion 4.1, Simple aggregator configuration commands: The generated configuration is as expected., passed
2023-11-03 01:56:30,736 TADA INFO assertion 5.1, prdcr_subscribe configuration commands: The generated configuration is as expected., passed
2023-11-03 01:56:30,854 __main__ INFO --- done ---
2023-11-03 01:56:30,854 TADA INFO test dump_cfg_test ended
2023-11-03 01:56:44 INFO: ----------------------------------------------
2023-11-03 01:56:45 INFO: ======== ldmsd_stream_rate_test ========
2023-11-03 01:56:45 INFO: CMD: python3 ldmsd_stream_rate_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldmsd_stream_rate_test
2023-11-03 01:56:45,814 TADA INFO starting test `ldmsd_stream_rate_test`
2023-11-03 01:56:45,814 TADA INFO   test-id: 2f2b2efcf74a828280f8288fa14972b3a88ddf9e4ad8eaffc510b310a2cd4ecf
2023-11-03 01:56:45,814 TADA INFO   test-suite: LDMSD
2023-11-03 01:56:45,814 TADA INFO   test-name: ldmsd_stream_rate_test
2023-11-03 01:56:45,815 TADA INFO   test-user: narate
2023-11-03 01:56:45,815 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:56:45,815 __main__ INFO -- Get or create the cluster --
2023-11-03 01:56:58,897 __main__ INFO -- Start daemons --
2023-11-03 01:57:09,628 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:57:11,630 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_rate_test-32d5252-samp 
2023-11-03 01:57:14,647 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_rate_test-32d5252-agg-1 
2023-11-03 01:57:28,178 TADA INFO assertion 1, Sampler cannot publish all data (prdcr rate limit): received stream data 34 is limited by prdcr rx_rate, passed
2023-11-03 01:57:28,178 TADA INFO assertion 2, After the wait, the sampler can publish: stream data received, passed
2023-11-03 01:57:28,178 TADA INFO assertion 3, Sampler cannot publish all data (stream rate limit): received stream data 34 is limited by stream rx_rate, passed
2023-11-03 01:57:28,179 TADA INFO assertion 4, After the wait, the sampler can publish: stream data received, passed
2023-11-03 01:57:28,179 TADA INFO test ldmsd_stream_rate_test ended
2023-11-03 01:57:39 INFO: ----------------------------------------------
2023-11-03 01:57:40 INFO: ======== ldms_rate_test ========
2023-11-03 01:57:40 INFO: CMD: python3 ldms_rate_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_rate_test
2023-11-03 01:57:41,346 TADA INFO starting test `ldms_rate_test`
2023-11-03 01:57:41,346 TADA INFO   test-id: 2292b89bb381b78a794b38eacfd6a100ecb532eb9647a8a1f4a8557164bdc2db
2023-11-03 01:57:41,346 TADA INFO   test-suite: LDMSD
2023-11-03 01:57:41,346 TADA INFO   test-name: ldms_rate_test
2023-11-03 01:57:41,347 TADA INFO   test-user: narate
2023-11-03 01:57:41,347 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:57:41,347 __main__ INFO -- Get or create the cluster --
2023-11-03 01:57:51,530 __main__ INFO -- Start daemons --
2023-11-03 01:57:52,417 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:57:54,419 root INFO starting /tada-src/python/ldms_rate.py on narate-ldms_rate_test-32d5252-node-1 
2023-11-03 01:57:57,437 root INFO starting /tada-src/python/ldms_rate.py on narate-ldms_rate_test-32d5252-node-1 
2023-11-03 01:58:32,415 TADA INFO assertion 1, Publisher cannot publish all data (rail rate limit): received stream data is limited, passed
2023-11-03 01:58:32,917 TADA INFO assertion 2, Publisher get a rate limit error: publisher got rate limit errors, passed
2023-11-03 01:58:32,918 TADA INFO assertion 3, After the wait, the publisher can publish: stream data received, passed
2023-11-03 01:58:32,918 TADA INFO assertion 4, Publisher cannot publish all data (stream rate limit): received stream data is limited, passed
2023-11-03 01:58:33,420 TADA INFO assertion 5, Publisher get a rate limit error (by stream): publisher got rate limit errors, passed
2023-11-03 01:58:33,420 TADA INFO assertion 6, After the wait, the publisher can publish (by stream): stream data received, passed
2023-11-03 01:58:33,421 TADA INFO test ldms_rate_test ended
2023-11-03 01:58:44 INFO: ----------------------------------------------
2023-11-03 01:58:45 INFO: ======== ldms_ipv6_test ========
2023-11-03 01:58:45 INFO: CMD: python3 ldms_ipv6_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-11-03-003001/data/ldms_ipv6_test
2023-11-03 01:58:45,957 TADA INFO starting test `ldms_ipv6_test`
2023-11-03 01:58:45,957 TADA INFO   test-id: 10c5561f3cea07e678539eb293c53c3eed4925754b25099eb2358fc217b5846a
2023-11-03 01:58:45,957 TADA INFO   test-suite: LDMSD
2023-11-03 01:58:45,957 TADA INFO   test-name: ldms_ipv6_test
2023-11-03 01:58:45,957 TADA INFO   test-user: narate
2023-11-03 01:58:45,958 TADA INFO   commit-id: 32d5252f2776353a2e7fc9be6a15930f6c1f575b
2023-11-03 01:58:45,958 __main__ INFO -- Get or create the cluster --
2023-11-03 01:59:25,947 __main__ INFO -- Start daemons --
2023-11-03 01:59:52,724 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-11-03 01:59:54,727 root INFO starting /tada-src/python/pypubsub.py on narate-ldms_ipv6_test-32d5252-samp 
2023-11-03 01:59:57,744 root INFO starting /tada-src/python/pypubsub.py on narate-ldms_ipv6_test-32d5252-agg-2 
2023-11-03 02:00:02,885 TADA INFO assertion 1, ldms_ls to samp using IPv6: expecting ['fd00:0:0:1::5'], got ['fd00:0:0:1::5'], passed
2023-11-03 02:00:02,998 TADA INFO assertion 2, ldms_ls to agg-2 using IPv6: expecting ['fd00:0:0:1::5'], got ['fd00:0:0:1::5'], passed
2023-11-03 02:00:02,998 TADA INFO assertion 3, ldms_ls to agg-2 contains 'samp/meminfo': samp/meminfo found, passed
2023-11-03 02:00:06,002 TADA INFO assertion 4, python stream publish using IPv6: verified, passed
2023-11-03 02:00:09,006 TADA INFO assertion 5, python stream subscribe using IPv6: verified, passed
2023-11-03 02:00:09,508 TADA INFO assertion 6, steam data contain IPv6 addressing: expecting fd00:0:0:1::5, got fd00:0:0:1::5, passed
2023-11-03 02:00:09,791 TADA INFO assertion 7, stream stats reported IPv6 addresses: Expecting fd00:0:0:1::3, got fd00:0:0:1::3, passed
2023-11-03 02:00:09,791 TADA INFO test ldms_ipv6_test ended
2023-11-03 02:00:28 INFO: ----------------------------------------------
2023-11-03 02:00:29 INFO: ======== test-ldms ========
2023-11-03 02:00:29 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-11-03T02:00:29-05:00 INFO: starting test-samp-1
f1aa76038e8e1d1f9fb3c386c30aac4e94c28158b10a69edc15c424838009d46
2023-11-03T02:00:31-05:00 INFO: starting test-samp-2
9fb57f8ae31314fbf6742532b25a1c71e573060f74ae288db3f631b69265605e
2023-11-03T02:00:33-05:00 INFO: starting test-samp-3
65930746f4908dfe0ed609731b972af823f07282635cdbee18d66a5158f9bb09
2023-11-03T02:00:35-05:00 INFO: starting test-samp-4
ee8328ad1e5e58c27a61ae4f64c9119d470a572ef6507c21b61c950a8c68c4a0
2023-11-03T02:00:36-05:00 INFO: test-samp-1 is running
2023-11-03T02:00:37-05:00 INFO: test-samp-2 is running
2023-11-03T02:00:37-05:00 INFO: test-samp-3 is running
2023-11-03T02:00:37-05:00 INFO: test-samp-4 is running
2023-11-03T02:00:37-05:00 INFO: starting test-agg-11
9af066b7149f463eb4351763267133f1024ddb24b42ec27038ca686c4bc4d8b0
2023-11-03T02:00:39-05:00 INFO: starting test-agg-12
6039390ed885de497d4a85b51305b08f9ae73ae806e0abee007a706b22811f1e
2023-11-03T02:00:40-05:00 INFO: test-agg-11 is running
2023-11-03T02:00:40-05:00 INFO: test-agg-12 is running
2023-11-03T02:00:40-05:00 INFO: starting test-agg-2
82dea5f09cdfd365c9ba8d912c0f434cf59ad0a25bec362ef1fddc62f49e47ba
2023-11-03T02:00:42-05:00 INFO: test-agg-2 is running
2023-11-03T02:00:42-05:00 INFO: Collecting data (into SOS)
2023-11-03T02:00:52-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-11-03T02:00:54-05:00 INFO: check rc: 0
2023-11-03T02:00:54-05:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-11-03T02:00:58-05:00 INFO: DONE
2023-11-03 02:01:08 INFO: ----------------------------------------------
2023-11-03 02:01:08 INFO: ======== test-maestro ========
2023-11-03 02:01:08 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-11-03T02:01:08-05:00 INFO: starting mtest-maestro
a85d1f7bb12a49b5951125d30a0f4f4a9470aa54c3c1b2db352178275cabf901
2023-11-03T02:01:10-05:00 INFO: starting mtest-samp-1
96c1d813d7b46b3866348ce1f1aa3da0b988ac552077f9cdd996f15321271e1d
2023-11-03T02:01:12-05:00 INFO: starting mtest-samp-2
5aaee349595de691ae8130eca2f50819ba5d6ace377e93fffc4cc4eb01976974
2023-11-03T02:01:14-05:00 INFO: starting mtest-samp-3
4b9c59f894b332c3d6d73e5631c3f77d7fe71e0f422db7811e8ad25ca157ad8f
2023-11-03T02:01:15-05:00 INFO: starting mtest-samp-4
7308d1cceb33e64a256e1922e5bfe8796ca959c8a2fd2e52bb3a37126e2baca9
2023-11-03T02:01:16-05:00 INFO: mtest-samp-1 is running
2023-11-03T02:01:16-05:00 INFO: mtest-samp-2 is running
2023-11-03T02:01:16-05:00 INFO: mtest-samp-3 is running
2023-11-03T02:01:16-05:00 INFO: mtest-samp-4 is running
2023-11-03T02:01:16-05:00 INFO: starting mtest-agg-11
1bad1f465629a6db2140093637830087ca6048e435716180aaa816b1e2104e70
2023-11-03T02:01:18-05:00 INFO: starting mtest-agg-12
3fd819d6cd04ce97791dd63e5f994c493fe0028f134a40ae288908605dee52aa
2023-11-03T02:01:19-05:00 INFO: mtest-agg-11 is running
2023-11-03T02:01:19-05:00 INFO: mtest-agg-12 is running
2023-11-03T02:01:19-05:00 INFO: starting mtest-agg-2
05af383d2dca4837344bef135c2c1e856bc29990342bf2c4b945b939a1f5a6c5
2023-11-03T02:01:20-05:00 INFO: mtest-agg-2 is running
2023-11-03T02:01:20-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-03T02:03:21-05:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-03T02:03:23-05:00 INFO: sos check rc: 1
2023-11-03T02:03:23-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-03T02:03:27-05:00 INFO: DONE
2023-11-03 02:03:37 INFO: ----------------------------------------------
2023-11-03 02:03:37 INFO: ======== test-maestro-hostmunge ========
2023-11-03 02:03:37 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-11-03T02:03:37-05:00 INFO: Checking munge on localhost
2023-11-03T02:03:37-05:00 INFO: munge encode/decode successfully
2023-11-03T02:03:37-05:00 INFO: starting mtest-maestro
1ce499b3e430e6a6f4612939f94e9ed64d53ded45703af54a9ec5e8fb2ec1fa3
2023-11-03T02:03:39-05:00 INFO: starting mtest-samp-1
afd7f20213dd70af6c9db0e4a9f74be7551c23cf90b37293e5f9ce964ece9c79
2023-11-03T02:03:41-05:00 INFO: starting mtest-samp-2
cc151f766ffa33063c47f6b140e3977ff190eee01b3cd0410671fe2eca5c9846
2023-11-03T02:03:43-05:00 INFO: starting mtest-samp-3
4720fd9ec72601ce1f84f50d231ecbbae0c18487de7a9680779da12481e69396
2023-11-03T02:03:44-05:00 INFO: starting mtest-samp-4
b150f69507b80be9bfb88d9746fc38bfee00d488bf9f8a2faadb978ce802fad2
2023-11-03T02:03:46-05:00 INFO: mtest-samp-1 is running
2023-11-03T02:03:46-05:00 INFO: mtest-samp-2 is running
2023-11-03T02:03:46-05:00 INFO: mtest-samp-3 is running
2023-11-03T02:03:46-05:00 INFO: mtest-samp-4 is running
2023-11-03T02:03:46-05:00 INFO: starting mtest-agg-11
802a515c40176441bb57a766e5ec6ef5cdd6207868c5ddc95d827fb9669b87bc
2023-11-03T02:03:47-05:00 INFO: starting mtest-agg-12
8674657c50d386cc171329a81aef0a10221fd436db555211a02837bb2c0770a6
2023-11-03T02:03:49-05:00 INFO: mtest-agg-11 is running
2023-11-03T02:03:49-05:00 INFO: mtest-agg-12 is running
2023-11-03T02:03:49-05:00 INFO: starting mtest-agg-2
c2f65741630ec90943a66b6c6746d5cc243c492ea4b2fcb9eeb32afb277b5bc0
2023-11-03T02:03:50-05:00 INFO: mtest-agg-2 is running
2023-11-03T02:03:50-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-03T02:05:51-05:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-03T02:05:53-05:00 INFO: sos check rc: 1
2023-11-03T02:05:53-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-03T02:05:56-05:00 INFO: DONE
2023-11-03 02:06:06 INFO: ----------------------------------------------
2023-11-03 02:06:06 INFO: ======== test-maestro-munge ========
2023-11-03 02:06:06 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000362081 s, 11.3 MB/s
2023-11-03T02:06:07-05:00 INFO: starting mtest-maestro
4d7814b5b8fd45fa1da602c1eacdaf87ead841fe9cecfea8a50ada1accf351a0
2023-11-03T02:06:10-05:00 INFO: starting mtest-samp-1
28709b723d2b3b5ad42470c4b7db470c4abfdfc004df365d193f1e48f4cc3fb2
2023-11-03T02:06:11-05:00 INFO: starting mtest-samp-2
d7ddd0332858a371cc8ac716cb90f27dfe37c6b580e47990a81a6ca544b618c0
2023-11-03T02:06:13-05:00 INFO: starting mtest-samp-3
8a36ade15349a11c04e98f7513292d717e5aa83d1338d3cc0143e2eff09e3e06
2023-11-03T02:06:14-05:00 INFO: starting mtest-samp-4
aad7ae246bf1f265a719b1845645382aca7d53d72ea7b8160c321f001237c35c
2023-11-03T02:06:16-05:00 INFO: mtest-samp-1 is running
2023-11-03T02:06:16-05:00 INFO: mtest-samp-2 is running
2023-11-03T02:06:16-05:00 INFO: mtest-samp-3 is running
2023-11-03T02:06:16-05:00 INFO: mtest-samp-4 is running
2023-11-03T02:06:16-05:00 INFO: starting mtest-agg-11
616d06d5c279003b9d58dea59b22bfd3be48015c2263ccdc7ebc780075171e5a
2023-11-03T02:06:17-05:00 INFO: starting mtest-agg-12
858dbe9587e27a963c4f0acf806773e9d69c54f6f57765838875d65c87e461f2
2023-11-03T02:06:18-05:00 INFO: mtest-agg-11 is running
2023-11-03T02:06:18-05:00 INFO: mtest-agg-12 is running
2023-11-03T02:06:18-05:00 INFO: starting mtest-agg-2
8b1932baedde2884901ab24e35d1036e312a57cee1d43ac222525aabe6872f31
2023-11-03T02:06:20-05:00 INFO: mtest-agg-2 is running
2023-11-03T02:06:20-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-11-03T02:08:20-05:00 INFO: Checking SOS data
Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "Sos.pyx", line 1136, in python.Sos.Container.open
  File "Sos.pyx", line 231, in python.Sos.SosObject.abort
Exception: No such file or directory
2023-11-03T02:08:22-05:00 INFO: sos check rc: 1
2023-11-03T02:08:22-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
Error response from daemon: No such container: mtest-ui
Error response from daemon: No such container: mtest-grafana
2023-11-03T02:08:26-05:00 INFO: DONE
2023-11-03 02:08:36 INFO: ----------------------------------------------
2023-11-03 02:08:36 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;32mPASSED[0m
papi_store_test: [01;32mPASSED[0m
updtr_status_test: [01;32mPASSED[0m
ldms_stream_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;31mFAILED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;32mPASSED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;31mFAILED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;32mPASSED[0m
agg_slurm_test: [01;31mFAILED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;32mPASSED[0m
ldmsd_auth_test: [01;32mPASSED[0m
updtr_stop_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;31mFAILED[0m
cont-test-ldms: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;31mFAILED[0m
ldmsd_long_config_test: [01;31mFAILED[0m
maestro_raft_test: [01;31mFAILED[0m
dump_cfg_test: [01;32mPASSED[0m
ldmsd_stream_test2: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
libovis_log_test: [01;31mFAILED[0m
set_sec_mod_test: [01;32mPASSED[0m
cont-test-maestro: [01;31mFAILED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_stream_status_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
ldms_ipv6_test: [01;32mPASSED[0m
ldmsd_stream_rate_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
ldms_rail_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
ldms_rate_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;32mPASSED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 48/57
------------------------------------------
