2023-08-10 15:58:33 INFO: WORK_DIR: /mnt/300G/data/2023-08-10-155832
2023-08-10 15:58:33 INFO: LOG: /mnt/300G/data/2023-08-10-155832/cygnus-weekly.log
2023-08-10 15:58:33 INFO: OVIS_NEW_GIT_SHA: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:58:33 INFO: OVIS_OLD_GIT_SHA: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:58:33 INFO: CONT_GIT_SHA: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:58:33 INFO: -----------------------------------------------
2023-08-10 15:58:33 INFO: LDMS_TEST_REPO: https://github.com/ovis-hpc/ldms-test
2023-08-10 15:58:33 INFO: LDMS_TEST_BRANCH: master
2023-08-10 15:58:33 INFO: LDMS_TEST_NEW_GIT_SHA: cf438e7f587e5635ca848a17f73a04fec8150785
2023-08-10 15:58:33 INFO: LDMS_TEST_OLD_GIT_SHA: fb5e44882f90ae264f47566353d7fc4b1897caac
~/cron/ldms-test ~/cron/ldms-test
/mnt/300G/data/2023-08-10-155832 ~/cron/ldms-test ~/cron/ldms-test
2023-08-10 15:58:34 INFO: Skip building on host because GIT SHA has not changed: 
3b206c99fefb698222470a58fbc89d5c9df5ac75
OVIS_LDMS_OVIS_GIT_LONG "3b206c99fefb698222470a58fbc89d5c9df5ac75"
2023-08-10 15:58:34 INFO: Skip building containerized binary because GIT SHA has not changed: 
2023-08-10 15:58:34 INFO: -- Installation process succeeded --
2023-08-10 15:58:34 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-08-10-155832
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-08-10-155832
HEAD is now at 1c90b38 2023-07-29-104830
[master d987122] 2023-08-10-155832
 2 files changed, 24 insertions(+), 2856 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   1c90b38..d987122  master -> master
~/cron/ldms-test /mnt/300G/data/2023-08-10-155832
2023-08-10 15:58:36 INFO: ==== OVIS+SOS Installation Completed ====
2023-08-10 15:58:36 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-08-10-155832 ~/cron/ldms-test ~/cron/ldms-test
2023-08-10 15:58:36 INFO: ======== direct_ldms_ls_conn_test ========
2023-08-10 15:58:36 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/direct_ldms_ls_conn_test
2023-08-10 15:58:36,938 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-08-10 15:58:36,938 TADA INFO   test-id: 2e6726f73179c633e94c92c8cc1bcd61368213c736f257a694da6888e590f146
2023-08-10 15:58:36,939 TADA INFO   test-suite: LDMSD
2023-08-10 15:58:36,939 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-08-10 15:58:36,939 TADA INFO   test-user: narate
2023-08-10 15:58:36,939 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:58:37,380 __main__ INFO starting munged on cygnus-01-iw
2023-08-10 15:58:37,718 __main__ INFO starting munged on localhost
2023-08-10 15:58:37,955 __main__ INFO starting ldmsd on cygnus-01-iw
2023-08-10 15:58:38,252 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-08-10 15:58:43,468 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-08-10 15:58:43,468 __main__ INFO Stopping sampler daemon ...
2023-08-10 15:58:48,883 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-08-10 15:58:48,922 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-08-10 15:58:48,960 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-08-10 15:58:48,961 TADA INFO test direct_ldms_ls_conn_test ended
2023-08-10 15:58:49,173 __main__ INFO stopping munged on cygnus-01-iw
2023-08-10 15:58:49,580 __main__ INFO stopping munged on localhost
2023-08-10 15:58:49 INFO: ----------------------------------------------
2023-08-10 15:58:49 INFO: ======== direct_prdcr_subscribe_test ========
2023-08-10 15:58:49 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/direct_prdcr_subscribe_test
2023-08-10 15:58:50,416 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-08-10 15:58:50,416 TADA INFO   test-id: 131761bab06f61f7a1cae17912b852e2d6b801863f8b5e9528f4ddb91045683e
2023-08-10 15:58:50,416 TADA INFO   test-suite: LDMSD
2023-08-10 15:58:50,416 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-08-10 15:58:50,417 TADA INFO   test-user: narate
2023-08-10 15:58:50,417 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:58:52,332 __main__ INFO starting munged on cygnus-01-iw
2023-08-10 15:58:53,152 __main__ INFO starting munged on cygnus-05-iw
2023-08-10 15:58:53,925 __main__ INFO starting munged on cygnus-03-iw
2023-08-10 15:58:54,668 __main__ INFO starting munged on cygnus-04-iw
2023-08-10 15:58:54,983 __main__ INFO starting munged on localhost
2023-08-10 15:58:55,211 __main__ INFO starting ldmsd on cygnus-01-iw
2023-08-10 15:58:55,784 __main__ INFO starting ldmsd on cygnus-05-iw
2023-08-10 15:58:56,317 __main__ INFO starting ldmsd on cygnus-03-iw
2023-08-10 15:58:56,819 __main__ INFO starting ldmsd on cygnus-04-iw
2023-08-10 15:59:03,831 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-08-10 15:59:03,832 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-08-10 15:59:03,832 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-08-10 15:59:03,832 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-08-10 15:59:03,833 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
2023-08-10 15:59:03,878 TADA INFO assertion 5, Stopping the producers succeeds: agg-1 producers stopped, passed
2023-08-10 15:59:04,880 TADA INFO assertion 6, Restarting the producers succeeds: agg-1 producers started, passed
2023-08-10 15:59:11,559 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-08-10 15:59:11,560 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-08-10 15:59:11,561 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-08-10 15:59:11,561 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-08-10 15:59:11,562 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verified, passed
2023-08-10 15:59:11,563 __main__ INFO stopping sampler-1
2023-08-10 15:59:13,012 TADA INFO assertion 12, stream-sampler-1 is not running: sampler-1 stopped, passed
2023-08-10 15:59:13,013 __main__ INFO starting sampler-1
2023-08-10 15:59:14,271 TADA INFO assertion 13, stream-sampler-1 has restarted: sampler-1 running, passed
2023-08-10 15:59:14,272 __main__ INFO allow some time for prdcr to reconnect ...
2023-08-10 15:59:20,236 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-08-10 15:59:20,237 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-08-10 15:59:20,238 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verified, passed
2023-08-10 15:59:20,239 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: unsubscribed, passed
2023-08-10 15:59:22,626 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-08-10 15:59:22,631 __main__ INFO stopping agg-1
2023-08-10 15:59:27,849 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-08-10 15:59:27,850 TADA INFO test direct_prdcr_subscribe_test ended
2023-08-10 15:59:28,061 __main__ INFO stopping munged on cygnus-01-iw
2023-08-10 15:59:28,521 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-08-10 15:59:29,011 __main__ INFO stopping munged on cygnus-05-iw
2023-08-10 15:59:29,485 __main__ INFO stopping ldmsd on cygnus-05-iw
2023-08-10 15:59:29,955 __main__ INFO stopping munged on cygnus-03-iw
2023-08-10 15:59:30,633 __main__ INFO stopping munged on cygnus-04-iw
2023-08-10 15:59:31,061 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-08-10 15:59:31,270 __main__ INFO stopping munged on localhost
2023-08-10 15:59:31 INFO: ----------------------------------------------
2023-08-10 15:59:31 INFO: ======== agg_slurm_test ========
2023-08-10 15:59:31 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/agg_slurm_test
2023-08-10 15:59:32,168 TADA INFO starting test `agg_slurm_test`
2023-08-10 15:59:32,168 TADA INFO   test-id: fb83f1bd735e849f3774d065184526640c3e52bf1e1a714a23b4e56cbdab98de
2023-08-10 15:59:32,169 TADA INFO   test-suite: LDMSD
2023-08-10 15:59:32,169 TADA INFO   test-name: agg_slurm_test
2023-08-10 15:59:32,169 TADA INFO   test-user: narate
2023-08-10 15:59:32,169 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 15:59:32,170 __main__ INFO -- Get or create the cluster --
2023-08-10 15:59:45,940 __main__ INFO -- Preparing syspapi JSON file --
2023-08-10 15:59:46,049 __main__ INFO -- Preparing jobpapi JSON file --
2023-08-10 15:59:46,145 __main__ INFO -- Preparing job script & programs --
2023-08-10 15:59:47,473 __main__ INFO -- Start daemons --
2023-08-10 16:00:19,217 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:00:24,220 __main__ INFO -- ldms_ls to agg-2 --
2023-08-10 16:00:24,356 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-08-10 16:00:24,505 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-08-10 16:00:29,508 __main__ INFO -- Submitting jobs --
2023-08-10 16:00:29,663 __main__ INFO job_one: 1
2023-08-10 16:00:29,779 __main__ INFO job_two: 2
2023-08-10 16:00:39,789 __main__ INFO -- Cancelling jobs --
2023-08-10 16:00:39,789 __main__ INFO job_one: 1
2023-08-10 16:00:39,910 __main__ INFO job_two: 2
2023-08-10 16:01:51,858 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-08-10 16:01:51,858 TADA INFO assertion 3, meminfo data verification: No data missing, failed
Traceback (most recent call last):
  File "agg_slurm_test", line 592, in <module>
    test.assert_test(3, len(meminfo) > 5 and missing_counts == 0, "No data missing")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: LDMSD 2-level agg with slurm, No data missing: FAILED
2023-08-10 16:01:51,859 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: skipped
2023-08-10 16:01:51,860 TADA INFO test agg_slurm_test ended
2023-08-10 16:02:06 INFO: ----------------------------------------------
2023-08-10 16:02:07 INFO: ======== papi_sampler_test ========
2023-08-10 16:02:07 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/papi_sampler_test
2023-08-10 16:02:08,059 TADA INFO starting test `papi_sampler_test`
2023-08-10 16:02:08,059 TADA INFO   test-id: e2ecde1e02814ad622f36a2ff20cc67005b197646c47ff5bee5f13a818b9994b
2023-08-10 16:02:08,059 TADA INFO   test-suite: LDMSD
2023-08-10 16:02:08,059 TADA INFO   test-name: papi_sampler_test
2023-08-10 16:02:08,059 TADA INFO   test-user: narate
2023-08-10 16:02:08,059 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:02:08,060 __main__ INFO -- Get or create the cluster --
2023-08-10 16:02:13,256 __main__ INFO -- Start daemons --
2023-08-10 16:02:26,942 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-08-10 16:02:27,186 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-08-10 16:02:32,320 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-08-10 16:02:32,500 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-08-10 16:02:32,500 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-08-10 16:02:46,352 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-08-10 16:02:46,352 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-08-10 16:02:46,352 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-08-10 16:02:46,352 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-08-10 16:02:46,561 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-08-10 16:02:52,418 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-08-10 16:02:52,418 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-08-10 16:02:52,418 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_BR_MSP', 'PAPI_TOT_INS'} == {'PAPI_BR_MSP', 'PAPI_TOT_INS'}, passed
2023-08-10 16:02:52,418 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-08-10 16:02:52,649 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-08-10 16:02:52,649 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/papi1/3.0', 'node-1/meminfo', 'node-1/papi0/2.0'}), passed
2023-08-10 16:03:03,287 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-08-10 16:03:43,637 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-08-10 16:03:45,998 TADA INFO assertion 8, Missing config file attribute is logged: : sampler.papi_sampler: papi_sampler[515]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-08-10 16:03:51,451 TADA INFO assertion 9, Bad config file is logged: : sampler.papi_sampler: configuration file syntax error., passed
2023-08-10 16:03:51,452 __main__ INFO -- Finishing Test --
2023-08-10 16:03:51,452 TADA INFO test papi_sampler_test ended
2023-08-10 16:03:51,452 __main__ INFO -- Cleaning up files --
2023-08-10 16:03:51,453 __main__ INFO -- Removing the virtual cluster --
2023-08-10 16:04:02 INFO: ----------------------------------------------
2023-08-10 16:04:03 INFO: ======== papi_store_test ========
2023-08-10 16:04:03 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/papi_store_test
2023-08-10 16:04:04,421 TADA INFO starting test `papi_store_test`
2023-08-10 16:04:04,421 TADA INFO   test-id: 595312b53deeb89a5a1fb6ab00e08039a13cdc6c40a50fc4a001612aa5abdf1e
2023-08-10 16:04:04,421 TADA INFO   test-suite: LDMSD
2023-08-10 16:04:04,422 TADA INFO   test-name: papi_store_test
2023-08-10 16:04:04,422 TADA INFO   test-user: narate
2023-08-10 16:04:04,422 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:04:04,423 __main__ INFO -- Get or create the cluster --
2023-08-10 16:04:12,064 __main__ INFO -- Start daemons --
2023-08-10 16:04:53,385 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-08-10 16:04:53,385 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-08-10 16:04:53,385 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-08-10 16:04:53,385 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-08-10 16:04:53,386 TADA INFO test papi_store_test ended
2023-08-10 16:05:05 INFO: ----------------------------------------------
2023-08-10 16:05:06 INFO: ======== store_app_test ========
2023-08-10 16:05:06 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/store_app_test
2023-08-10 16:05:07,430 TADA INFO starting test `store_app_test`
2023-08-10 16:05:07,431 TADA INFO   test-id: 211b14527def6f63d9ee8e87267b4fb604e36889ff3ebf28bf1795a1e0729fbc
2023-08-10 16:05:07,431 TADA INFO   test-suite: LDMSD
2023-08-10 16:05:07,431 TADA INFO   test-name: store_app_test
2023-08-10 16:05:07,431 TADA INFO   test-user: narate
2023-08-10 16:05:07,431 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:05:07,432 __main__ INFO -- Get or create the cluster --
2023-08-10 16:05:21,799 __main__ INFO -- Preparing job script & programs --
2023-08-10 16:05:22,204 __main__ INFO -- Start daemons --
2023-08-10 16:05:54,102 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:05:59,109 __main__ INFO -- Submitting jobs --
2023-08-10 16:05:59,338 __main__ INFO job_one: 1
2023-08-10 16:06:04,494 __main__ INFO job_two: 2
2023-08-10 16:06:18,296 __main__ INFO Verifying data ...
2023-08-10 16:08:22,799 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-08-10 16:08:22,799 TADA INFO test store_app_test ended
2023-08-10 16:08:36 INFO: ----------------------------------------------
2023-08-10 16:08:37 INFO: ======== syspapi_test ========
2023-08-10 16:08:37 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/syspapi_test
2023-08-10 16:08:38,193 TADA INFO starting test `syspapi_test`
2023-08-10 16:08:38,194 TADA INFO   test-id: ebd19abe5f20361c87f5ef9c7950cb000146c279cf791c4d75eb18c770f1e3f3
2023-08-10 16:08:38,194 TADA INFO   test-suite: LDMSD
2023-08-10 16:08:38,194 TADA INFO   test-name: syspapi_test
2023-08-10 16:08:38,194 TADA INFO   test-user: narate
2023-08-10 16:08:38,194 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:08:38,195 __main__ INFO -- Get or create the cluster --
2023-08-10 16:08:49,815 __main__ INFO -- Write syspapi JSON config files --
2023-08-10 16:08:49,815 __main__ INFO    - db/syspapi-1.json
2023-08-10 16:08:49,815 __main__ INFO    - db/syspapi-bad.json
2023-08-10 16:08:49,817 __main__ INFO -- Start daemons --
2023-08-10 16:09:09,966 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:09:14,968 __main__ INFO -- Verifying --
2023-08-10 16:09:15,109 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-08-10 16:09:15,109 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-08-10 16:09:15,229 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-08-10 16:09:17,364 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-08-10 16:09:17,483 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-08-10 16:09:17,592 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-08-10 16:09:39,498 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-08-10 16:09:39,498 __main__ INFO  events succeeded: 77
2023-08-10 16:09:39,499 __main__ INFO  events failed: 114
2023-08-10 16:09:39,499 TADA INFO test syspapi_test ended
2023-08-10 16:09:52 INFO: ----------------------------------------------
2023-08-10 16:09:53 INFO: ======== agg_test ========
2023-08-10 16:09:53 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/agg_test
2023-08-10 16:09:54,416 TADA INFO starting test `agg_test`
2023-08-10 16:09:54,417 TADA INFO   test-id: 2625bd3156adc9746ec12677600223c2b319c558c04a7f23aed346e59b6bfe81
2023-08-10 16:09:54,417 TADA INFO   test-suite: LDMSD
2023-08-10 16:09:54,417 TADA INFO   test-name: agg_test
2023-08-10 16:09:54,417 TADA INFO   test-user: narate
2023-08-10 16:09:54,417 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:09:54,418 __main__ INFO -- Get or create the cluster --
2023-08-10 16:10:11,989 __main__ INFO -- Start daemons --
2023-08-10 16:10:48,860 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:10:53,865 __main__ INFO -- ldms_ls to agg-2 --
2023-08-10 16:10:53,988 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-08-10 16:10:54,824 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-08-10 16:10:54,824 __main__ INFO -- Terminating ldmsd on node-1 --
2023-08-10 16:10:57,198 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-08-10 16:10:57,417 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-08-10 16:10:57,417 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-08-10 16:11:07,015 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-08-10 16:11:07,123 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-3/meminfo'}), passed
2023-08-10 16:11:07,124 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-08-10 16:11:09,487 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-08-10 16:11:09,604 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-08-10 16:11:09,714 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-08-10 16:11:09,714 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-08-10 16:11:19,352 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-2/meminfo', 'node-4/meminfo', 'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-3/meminfo'}), passed
2023-08-10 16:11:19,353 TADA INFO test agg_test ended
2023-08-10 16:11:34 INFO: ----------------------------------------------
2023-08-10 16:11:35 INFO: ======== failover_test ========
2023-08-10 16:11:35 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/failover_test
2023-08-10 16:11:36,347 TADA INFO starting test `failover_test`
2023-08-10 16:11:36,347 TADA INFO   test-id: 85508f7911582783eb23a711a41a10e62ebc84def83be6d2e52a0a3f8f708bae
2023-08-10 16:11:36,347 TADA INFO   test-suite: LDMSD
2023-08-10 16:11:36,347 TADA INFO   test-name: failover_test
2023-08-10 16:11:36,347 TADA INFO   test-user: narate
2023-08-10 16:11:36,348 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:11:36,348 __main__ INFO -- Get or create the cluster --
2023-08-10 16:11:54,044 __main__ INFO -- Start daemons --
2023-08-10 16:12:30,787 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:12:45,802 __main__ INFO -- ldms_ls to agg-2 --
2023-08-10 16:12:45,924 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-08-10 16:12:46,735 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-08-10 16:12:46,735 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-08-10 16:12:52,104 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:12:52,207 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:12:52,319 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-08-10 16:12:52,433 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-08-10 16:12:52,433 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-08-10 16:13:17,014 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:13:17,128 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:13:17,128 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-08-10 16:13:22,488 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:13:22,600 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:13:22,712 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-08-10 16:13:22,831 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-08-10 16:13:22,831 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-08-10 16:13:47,402 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-08-10 16:13:47,519 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-2/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo', 'node-4/meminfo', 'node-2/meminfo'}), passed
2023-08-10 16:13:47,519 TADA INFO test failover_test ended
2023-08-10 16:14:03 INFO: ----------------------------------------------
2023-08-10 16:14:04 INFO: ======== ldmsd_auth_ovis_test ========
2023-08-10 16:14:04 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_auth_ovis_test
2023-08-10 16:14:04,892 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-08-10 16:14:04,892 TADA INFO   test-id: f4462d3c9feda66e305b3476c436e6cc97aa5c39cae8a86782a12e6f820827d7
2023-08-10 16:14:04,892 TADA INFO   test-suite: LDMSD
2023-08-10 16:14:04,892 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-08-10 16:14:04,892 TADA INFO   test-user: narate
2023-08-10 16:14:04,892 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:14:04,893 __main__ INFO -- Get or create the cluster --
2023-08-10 16:14:09,916 __main__ INFO -- Start daemons --
2023-08-10 16:14:15,795 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:14:20,934 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-08-10 16:14:21,060 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-08-10 16:14:21,203 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-08-10 16:14:21,500 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-08-10 16:14:21,500 TADA INFO test ldmsd_auth_ovis_test ended
2023-08-10 16:14:32 INFO: ----------------------------------------------
2023-08-10 16:14:33 INFO: ======== ldmsd_auth_test ========
2023-08-10 16:14:33 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_auth_test
2023-08-10 16:14:34,526 TADA INFO starting test `ldmsd_auth_test`
2023-08-10 16:14:34,527 TADA INFO   test-id: b20a56fb1d44f2efb91048a7b6366c9b9f1f912bc4cb70a3fc16321b62033bb0
2023-08-10 16:14:34,527 TADA INFO   test-suite: LDMSD
2023-08-10 16:14:34,527 TADA INFO   test-name: ldmsd_auth_test
2023-08-10 16:14:34,527 TADA INFO   test-user: narate
2023-08-10 16:14:34,527 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:14:34,528 __main__ INFO -- Get or create the cluster --
2023-08-10 16:14:52,180 __main__ INFO -- Start daemons --
2023-08-10 16:15:38,893 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:15:44,030 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-08-10 16:15:44,170 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-08-10 16:15:44,287 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-08-10 16:15:44,397 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-08-10 16:15:44,515 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-08-10 16:15:44,515 TADA INFO test ldmsd_auth_test ended
2023-08-10 16:16:00 INFO: ----------------------------------------------
2023-08-10 16:16:01 INFO: ======== ldmsd_ctrl_test ========
2023-08-10 16:16:01 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_ctrl_test
2023-08-10 16:16:01,807 TADA INFO starting test `ldmsd_ctrl_test`
2023-08-10 16:16:01,808 TADA INFO   test-id: fd0582cc8df4cda70e6b897b86fed9aa034e6a863593bd58a2df0d41c5930923
2023-08-10 16:16:01,808 TADA INFO   test-suite: LDMSD
2023-08-10 16:16:01,808 TADA INFO   test-name: ldmsd_ctrl_test
2023-08-10 16:16:01,808 TADA INFO   test-user: narate
2023-08-10 16:16:01,808 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:16:01,809 __main__ INFO -- Get or create the cluster --
2023-08-10 16:16:10,888 __main__ INFO -- Start daemons --
2023-08-10 16:16:27,100 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:16:33,221 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-08-10 16:16:34,337 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-08-10 16:16:34,938 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-08-10 16:16:35,540 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-08-10 16:16:36,141 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-08-10 16:16:36,743 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-08-10 16:16:37,344 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-08-10 16:16:37,946 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-08-10 16:16:55,152 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-08-10 16:17:12,349 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-08-10 16:17:12,349 TADA INFO test ldmsd_ctrl_test ended
2023-08-10 16:17:25 INFO: ----------------------------------------------
2023-08-10 16:17:26 INFO: ======== ldmsd_stream_test2 ========
2023-08-10 16:17:26 INFO: CMD: python3 ldmsd_stream_test2 --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_stream_test2
2023-08-10 16:17:26,759 TADA INFO starting test `ldmsd_stream_test`
2023-08-10 16:17:26,759 TADA INFO   test-id: 864e38e952e4d019b37b9a039fd9d65759a18415b515b970979e6dc77fdaffa7
2023-08-10 16:17:26,759 TADA INFO   test-suite: LDMSD
2023-08-10 16:17:26,759 TADA INFO   test-name: ldmsd_stream_test
2023-08-10 16:17:26,760 TADA INFO   test-user: narate
2023-08-10 16:17:26,760 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:17:26,760 __main__ INFO -- Get or create the cluster --
2023-08-10 16:17:36,304 __main__ INFO -- Start daemons --
2023-08-10 16:17:53,930 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:17:55,932 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-3b206c9-new 
2023-08-10 16:17:58,951 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-3b206c9-agg-2 
2023-08-10 16:18:09,378 TADA INFO assertion 1, Check data from old ldmsd_stream at agg-1: , passed
2023-08-10 16:18:09,378 TADA INFO assertion 2, Check data from old ldmsd_stream at agg-2: , passed
2023-08-10 16:18:09,379 TADA INFO assertion 3, Check data from old ldmsd_stream at the last subscriber: , passed
2023-08-10 16:18:09,379 TADA INFO assertion 4, Check data from the matching new ldms stream at agg-1: , passed
2023-08-10 16:18:09,379 TADA INFO assertion 5, Check data from the matching new ldms stream at agg-2: , passed
2023-08-10 16:18:09,379 TADA INFO assertion 6, Check data from the matching new ldms stream at the last subscriber: , passed
2023-08-10 16:18:09,380 TADA INFO assertion 7, Check data from the non-matching new ldms stream at agg-1: , passed
2023-08-10 16:18:09,380 TADA INFO assertion 8, Check data from the non-matching new ldms stream at agg-2: , passed
2023-08-10 16:18:09,380 TADA INFO assertion 9, Check data from the non-matching new ldms stream at last subscriber: , passed
2023-08-10 16:18:09,942 TADA INFO assertion 10, Check stream_stats before stream data transfer: , passed
2023-08-10 16:18:09,942 TADA INFO assertion 11, Check stream_client_stats before stream data transfer: , passed
2023-08-10 16:18:09,942 TADA INFO assertion 12, Check stream_stats after stream data transfer: , passed
2023-08-10 16:18:09,943 TADA INFO assertion 13, Check stream_client_stats after stream data transfer: , passed
2023-08-10 16:18:09,943 TADA INFO test ldmsd_stream_test ended
2023-08-10 16:18:22 INFO: ----------------------------------------------
2023-08-10 16:18:23 INFO: ======== maestro_cfg_test ========
2023-08-10 16:18:23 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/maestro_cfg_test
2023-08-10 16:18:24,355 TADA INFO starting test `maestro_cfg_test`
2023-08-10 16:18:24,355 TADA INFO   test-id: 5c533b78a0412e6b79e57d2ca95da587fb40182d8e92e12d73a8fc80316bd2ff
2023-08-10 16:18:24,355 TADA INFO   test-suite: LDMSD
2023-08-10 16:18:24,355 TADA INFO   test-name: maestro_cfg_test
2023-08-10 16:18:24,356 TADA INFO   test-user: narate
2023-08-10 16:18:24,356 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:18:34,368 __main__ INFO -- Get or create cluster --
2023-08-10 16:19:00,438 __main__ INFO -- Start daemons --
2023-08-10 16:20:02,517 __main__ INFO ... make sure ldmsd's are up
2023-08-10 16:20:09,970 TADA INFO assertion 1, load maestro etcd cluster: etcd cluster loaded successfully, passed
2023-08-10 16:20:50,006 TADA INFO assertion 2, config ldmsd cluster with maestro: Maestro ldmsd configuration successful, passed
---Wait for config to write to file---
Traceback (most recent call last):
  File "maestro_cfg_test", line 382, in <module>
    obj = ldms_ls(f"node-{i}")
  File "maestro_cfg_test", line 376, in ldms_ls
    raise RuntimeError(f"ldms_ls error {rc}, out: {out}")
RuntimeError: ldms_ls error 1, out: Traceback (most recent call last):
  File "/tada-src/python/ldms_ls.py", line 128, in <module>
    x.connect(host = args.host, port = args.port)
  File "ldms.pyx", line 3233, in python.ldms.Xprt.connect
ConnectionError: [Errno 103] Connect error: ECONNABORTED(103)

2023-08-10 16:20:51,056 TADA INFO assertion 3, verify sampler daemons: skipped
2023-08-10 16:20:51,056 TADA INFO assertion 4, verify L1 aggregator daemons: skipped
2023-08-10 16:20:51,056 TADA INFO assertion 5, verify L2 aggregator daemon: skipped
2023-08-10 16:20:51,056 TADA INFO assertion 6, verify data storage: skipped
2023-08-10 16:20:51,056 TADA INFO test maestro_cfg_test ended
2023-08-10 16:21:09 INFO: ----------------------------------------------
2023-08-10 16:21:10 INFO: ======== mt-slurm-test ========
2023-08-10 16:21:10 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1691702527.928668', '1,1691702527.928668', '2,1691702527.928668', '3,1691702527.928668', '4,1691702527.928668', '5,1691702527.928668', '6,1691702527.928668', '7,1691702528.931150', '8,1691702528.931150', '9,1691702529.974030', '10,1691702529.974030', '11,1691702529.974030', '12,1691702529.974030', '13,1691702529.974030', '14,1691702530.922099', '15,1691702530.922099', '16,1691702530.922099', '17,1691702530.922099', '18,1691702531.930191', '19,1691702531.930191', '20,1691702532.970361', '21,1691702532.970361', '22,1691702532.970361', '23,1691702532.970361', '24,1691702532.970361', '25,1691702532.970361', '26,1691702532.970361', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-08-10 16:22:47 INFO: ----------------------------------------------
2023-08-10 16:22:48 INFO: ======== ovis_ev_test ========
2023-08-10 16:22:48 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ovis_ev_test
2023-08-10 16:22:49,521 __main__ INFO -- Create the cluster -- 
2023-08-10 16:22:58,931 TADA INFO starting test `ovis_ev_test`
2023-08-10 16:22:58,931 TADA INFO   test-id: 9e947f244ca1e259b053b6522552aa68d8a0f37840d9db0b6f16047a9644bf8b
2023-08-10 16:22:58,931 TADA INFO   test-suite: test_ovis_ev
2023-08-10 16:22:58,932 TADA INFO   test-name: ovis_ev_test
2023-08-10 16:22:58,932 TADA INFO   test-user: narate
2023-08-10 16:22:58,932 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:22:58,932 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-08-10 16:22:58,933 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-08-10 16:22:58,933 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-08-10 16:22:58,933 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-08-10 16:22:58,933 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-08-10 16:22:58,933 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-08-10 16:22:58,933 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-08-10 16:22:58,934 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-08-10 16:22:58,934 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-08-10 16:22:58,934 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-08-10 16:22:58,934 TADA INFO test ovis_ev_test ended
2023-08-10 16:23:09 INFO: ----------------------------------------------
2023-08-10 16:23:10 INFO: ======== prdcr_subscribe_test ========
2023-08-10 16:23:10 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/prdcr_subscribe_test
2023-08-10 16:23:11,359 TADA INFO starting test `prdcr_subscribe_test`
2023-08-10 16:23:11,360 TADA INFO   test-id: c02735915b3b4bbb9ae2572e65248f39f294cff5caba4dc2456ef68916eacaf7
2023-08-10 16:23:11,360 TADA INFO   test-suite: LDMSD
2023-08-10 16:23:11,360 TADA INFO   test-name: prdcr_subscribe_test
2023-08-10 16:23:11,360 TADA INFO   test-user: narate
2023-08-10 16:23:11,360 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:23:58,686 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-08-10 16:23:58,686 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-08-10 16:23:58,686 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-08-10 16:23:58,687 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-08-10 16:23:58,687 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-08-10 16:23:59,067 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-08-10 16:23:59,427 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-08-10 16:24:07,519 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-08-10 16:24:07,520 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-08-10 16:24:07,520 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-08-10 16:24:07,520 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-08-10 16:24:07,521 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-08-10 16:24:08,727 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-08-10 16:24:14,245 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-08-10 16:24:21,842 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-08-10 16:24:21,842 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-08-10 16:24:21,843 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-08-10 16:24:22,209 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-08-10 16:24:25,542 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-08-10 16:24:31,384 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: verified, passed
2023-08-10 16:24:31,385 TADA INFO test prdcr_subscribe_test ended
2023-08-10 16:24:44 INFO: ----------------------------------------------
2023-08-10 16:24:44 INFO: ======== set_array_test ========
2023-08-10 16:24:44 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/set_array_test
2023-08-10 16:24:45,613 TADA INFO starting test `set_array_test`
2023-08-10 16:24:45,613 TADA INFO   test-id: d7a9fa3dbd36a3295b633ef432084d9b3890e6ada9425541a22ab1f26d84a011
2023-08-10 16:24:45,613 TADA INFO   test-suite: LDMSD
2023-08-10 16:24:45,613 TADA INFO   test-name: set_array_test
2023-08-10 16:24:45,613 TADA INFO   test-user: narate
2023-08-10 16:24:45,613 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:24:45,614 __main__ INFO -- Get or create the cluster --
2023-08-10 16:24:50,691 __main__ INFO -- Start daemons --
2023-08-10 16:24:56,587 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:25:25,536 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 4 snapshots, passed
2023-08-10 16:25:25,536 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-08-10 16:25:25,536 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-08-10 16:25:25,537 TADA INFO test set_array_test ended
2023-08-10 16:25:36 INFO: ----------------------------------------------
2023-08-10 16:25:37 INFO: ======== setgroup_test ========
2023-08-10 16:25:37 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/setgroup_test
2023-08-10 16:25:38,591 TADA INFO starting test `setgroup_test`
2023-08-10 16:25:38,591 TADA INFO   test-id: b3a6a1291b9488a4a823b2a860a18190674236e0e4e69f176f9d6060d38f51a1
2023-08-10 16:25:38,591 TADA INFO   test-suite: LDMSD
2023-08-10 16:25:38,591 TADA INFO   test-name: setgroup_test
2023-08-10 16:25:38,592 TADA INFO   test-user: narate
2023-08-10 16:25:38,592 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:25:38,593 __main__ INFO -- Get or create the cluster --
2023-08-10 16:25:48,099 __main__ INFO -- Start daemons --
2023-08-10 16:26:04,396 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:26:09,401 __main__ INFO -- ldms_ls to agg-2 --
2023-08-10 16:26:09,519 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-08-10 16:26:11,770 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-08-10 16:26:11,770 __main__ INFO -- Removing test_2 from grp --
2023-08-10 16:26:12,240 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:16,375 TADA INFO assertion 4, test_2 is removed from grp on agg-1: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:20,522 TADA INFO assertion 5, test_2 is removed from grp on agg-2: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:24,525 __main__ INFO -- Adding test_2 back into grp --
2023-08-10 16:26:25,044 TADA INFO assertion 6, test_2 is added back to grp on sampler: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:29,174 TADA INFO assertion 7, test_2 is added back to grp on agg-1: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:31,316 TADA INFO assertion 8, test_2 is added back to grp on agg-2: expect {'node-1/test_1', 'node-1/test_2', 'node-1/grp'}, got {'node-1/test_2', 'node-1/test_1', 'node-1/grp'}, passed
2023-08-10 16:26:33,319 TADA INFO test setgroup_test ended
2023-08-10 16:26:46 INFO: ----------------------------------------------
2023-08-10 16:26:47 INFO: ======== slurm_stream_test ========
2023-08-10 16:26:47 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/slurm_stream_test
2023-08-10 16:26:47,736 TADA INFO starting test `slurm_stream_test`
2023-08-10 16:26:47,736 TADA INFO   test-id: d38bdeefaf52fa386fb57bb3eed7046275b83f1adca3c45cdbf6fa6f01b3490b
2023-08-10 16:26:47,736 TADA INFO   test-suite: LDMSD
2023-08-10 16:26:47,736 TADA INFO   test-name: slurm_stream_test
2023-08-10 16:26:47,736 TADA INFO   test-user: narate
2023-08-10 16:26:47,737 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:26:47,737 __main__ INFO -- Get or create the cluster --
2023-08-10 16:26:54,581 __main__ INFO -- Start daemons --
2023-08-10 16:27:05,098 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:27:35,556 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:35,556 __main__ INFO 12345
2023-08-10 16:27:35,556 __main__ INFO 12345
2023-08-10 16:27:35,557 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,557 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-08-10 16:27:35,557 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,557 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,557 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,557 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,664 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:35,664 __main__ INFO 12345
2023-08-10 16:27:35,664 __main__ INFO 12345
2023-08-10 16:27:35,664 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,664 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-08-10 16:27:35,664 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,665 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,665 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,665 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-08-10 16:27:35,769 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:35,769 __main__ INFO 12346
2023-08-10 16:27:35,769 __main__ INFO 12346
2023-08-10 16:27:35,769 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,769 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-08-10 16:27:35,769 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,770 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,770 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,770 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,896 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:35,896 __main__ INFO 12346
2023-08-10 16:27:35,896 __main__ INFO 12346
2023-08-10 16:27:35,897 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,897 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-08-10 16:27:35,897 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,897 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,898 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:35,898 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-08-10 16:27:36,006 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,007 __main__ INFO 12347
2023-08-10 16:27:36,007 __main__ INFO 12347
2023-08-10 16:27:36,007 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,007 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-08-10 16:27:36,007 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,007 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,007 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,008 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,113 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,113 __main__ INFO 12347
2023-08-10 16:27:36,113 __main__ INFO 12347
2023-08-10 16:27:36,113 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,113 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-08-10 16:27:36,113 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,113 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,114 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,114 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-08-10 16:27:36,239 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,239 __main__ INFO 12348
2023-08-10 16:27:36,239 __main__ INFO 12348
2023-08-10 16:27:36,239 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,239 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-08-10 16:27:36,240 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,240 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,240 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,240 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,354 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,354 __main__ INFO 12348
2023-08-10 16:27:36,354 __main__ INFO 12348
2023-08-10 16:27:36,354 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,354 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-08-10 16:27:36,354 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,355 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,355 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,355 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-08-10 16:27:36,472 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,472 __main__ INFO 12355
2023-08-10 16:27:36,472 __main__ INFO 12355
2023-08-10 16:27:36,472 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,472 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,473 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,474 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,474 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,587 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,587 __main__ INFO 12355
2023-08-10 16:27:36,587 __main__ INFO 12355
2023-08-10 16:27:36,587 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,587 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-08-10 16:27:36,587 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,587 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,588 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-08-10 16:27:36,699 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,699 __main__ INFO 12356
2023-08-10 16:27:36,699 __main__ INFO 12356
2023-08-10 16:27:36,699 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,699 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-08-10 16:27:36,699 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,699 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,700 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,813 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,813 __main__ INFO 12356
2023-08-10 16:27:36,813 __main__ INFO 12356
2023-08-10 16:27:36,813 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,813 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,814 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,815 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-08-10 16:27:36,920 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:36,920 __main__ INFO 12357
2023-08-10 16:27:36,920 __main__ INFO 12357
2023-08-10 16:27:36,920 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,920 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-08-10 16:27:36,920 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,920 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:36,921 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,029 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:37,029 __main__ INFO 12357
2023-08-10 16:27:37,029 __main__ INFO 12357
2023-08-10 16:27:37,029 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,029 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-08-10 16:27:37,029 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,029 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,030 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-08-10 16:27:37,142 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:37,142 __main__ INFO 12358
2023-08-10 16:27:37,142 __main__ INFO 12358
2023-08-10 16:27:37,142 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,142 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,143 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,144 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,264 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-08-10 16:27:37,264 __main__ INFO 12358
2023-08-10 16:27:37,264 __main__ INFO 12358
2023-08-10 16:27:37,264 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,264 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-08-10 16:27:37,264 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:37,265 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-08-10 16:27:39,422 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-08-10 16:27:39,422 __main__ INFO 12353
2023-08-10 16:27:39,422 __main__ INFO 12353
2023-08-10 16:27:39,422 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,422 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,423 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,424 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,424 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-08-10 16:27:39,424 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-08-10 16:27:39,424 TADA INFO test slurm_stream_test ended
2023-08-10 16:27:51 INFO: ----------------------------------------------
2023-08-10 16:27:52 INFO: ======== spank_notifier_test ========
2023-08-10 16:27:52 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/spank_notifier_test
2023-08-10 16:27:52,740 TADA INFO starting test `spank_notifier_test`
2023-08-10 16:27:52,740 TADA INFO   test-id: 8fc2ea20170473c17e008c22343248e7a731b03394cd46974db5111ded0d7e91
2023-08-10 16:27:52,740 TADA INFO   test-suite: Slurm_Plugins
2023-08-10 16:27:52,741 TADA INFO   test-name: spank_notifier_test
2023-08-10 16:27:52,741 TADA INFO   test-user: narate
2023-08-10 16:27:52,741 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:27:52,741 __main__ INFO -- Create the cluster --
2023-08-10 16:28:17,868 __main__ INFO -- Cleanup output --
2023-08-10 16:28:18,255 __main__ INFO -- Test bad plugstack config --
2023-08-10 16:28:18,255 __main__ INFO Starting slurm ...
2023-08-10 16:28:32,226 __main__ INFO Starting slurm ... OK
2023-08-10 16:28:52,700 __main__ INFO -- Submitting job with num_tasks 4 --
2023-08-10 16:28:52,824 __main__ INFO   jobid = 1
2023-08-10 16:28:53,049 __main__ INFO -- Submitting job with num_tasks 4 --
2023-08-10 16:28:53,170 __main__ INFO   jobid = 2
2023-08-10 16:28:53,382 __main__ INFO -- Submitting job with num_tasks 4 --
2023-08-10 16:28:53,497 __main__ INFO   jobid = 3
2023-08-10 16:28:53,741 __main__ INFO -- Submitting job with num_tasks 4 --
2023-08-10 16:28:53,866 __main__ INFO   jobid = 4
2023-08-10 16:29:03,571 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-08-10 16:29:03,571 __main__ INFO Killin slurm ...
2023-08-10 16:29:06,570 __main__ INFO Killin slurm ... OK
2023-08-10 16:29:26,591 __main__ INFO -- Start daemons --
2023-08-10 16:29:37,076 __main__ INFO Starting slurm ... OK
2023-08-10 16:29:57,335 __main__ INFO -- Submitting job with no stream listener --
2023-08-10 16:29:57,562 __main__ INFO -- Submitting job with num_tasks 8 --
2023-08-10 16:29:57,703 __main__ INFO   jobid = 5
2023-08-10 16:30:13,727 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-08-10 16:30:13,727 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-08-10 16:30:19,619 __main__ INFO -- Submitting job with listener --
2023-08-10 16:30:19,870 __main__ INFO -- Submitting job with num_tasks 1 --
2023-08-10 16:30:19,995 __main__ INFO   jobid = 6
2023-08-10 16:30:20,198 __main__ INFO -- Submitting job with num_tasks 2 --
2023-08-10 16:30:20,311 __main__ INFO   jobid = 7
2023-08-10 16:30:20,515 __main__ INFO -- Submitting job with num_tasks 4 --
2023-08-10 16:30:20,630 __main__ INFO   jobid = 8
2023-08-10 16:30:20,889 __main__ INFO -- Submitting job with num_tasks 8 --
2023-08-10 16:30:21,007 __main__ INFO   jobid = 9
2023-08-10 16:30:21,223 __main__ INFO -- Submitting job with num_tasks 27 --
2023-08-10 16:30:21,337 __main__ INFO   jobid = 10
2023-08-10 16:30:43,149 __main__ INFO -- Verifying Events --
2023-08-10 16:30:43,150 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-08-10 16:30:43,150 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-08-10 16:30:43,150 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-08-10 16:30:43,150 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-08-10 16:30:43,150 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-08-10 16:30:43,151 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-08-10 16:30:43,151 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-08-10 16:30:43,151 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-08-10 16:30:43,151 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-08-10 16:30:43,151 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-08-10 16:30:43,152 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-08-10 16:30:43,152 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-08-10 16:30:43,152 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-08-10 16:30:43,152 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-08-10 16:30:43,152 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-08-10 16:30:43,153 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-08-10 16:30:43,153 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-08-10 16:30:43,153 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-08-10 16:30:43,153 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-08-10 16:30:43,153 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-08-10 16:30:43,154 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-08-10 16:30:43,154 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-08-10 16:30:43,154 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-08-10 16:30:43,154 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-08-10 16:30:43,154 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-08-10 16:30:43,155 __main__ INFO job 7 multi-tenant with dict_keys([6])
2023-08-10 16:30:43,155 __main__ INFO job 10 multi-tenant with dict_keys([6, 7])
2023-08-10 16:30:43,155 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-08-10 16:30:43,155 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-08-10 16:30:43,155 __main__ INFO job 9 multi-tenant with dict_keys([10])
2023-08-10 16:30:43,155 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-08-10 16:30:43,362 __main__ INFO -- Submitting job that crashes listener --
2023-08-10 16:30:43,478 __main__ INFO   jobid = 11
2023-08-10 16:30:53,711 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-08-10 16:30:53,822 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-08-10 16:30:53,823 TADA INFO test spank_notifier_test ended
2023-08-10 16:31:10 INFO: ----------------------------------------------
2023-08-10 16:31:11 INFO: ======== ldms_list_test ========
2023-08-10 16:31:11 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_list_test
2023-08-10 16:31:11,853 TADA INFO starting test `ldms_list_test`
2023-08-10 16:31:11,853 TADA INFO   test-id: c04fcfe08ec02c780470cc77b746a73c38bc64e5e4be7fbcdeb1079ec001e2d8
2023-08-10 16:31:11,853 TADA INFO   test-suite: LDMSD
2023-08-10 16:31:11,853 TADA INFO   test-name: ldms_list_test
2023-08-10 16:31:11,853 TADA INFO   test-user: narate
2023-08-10 16:31:11,853 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:31:11,854 __main__ INFO -- Get or create the cluster --
2023-08-10 16:31:15,118 __main__ INFO -- Start daemons --
2023-08-10 16:31:23,580 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:31:25,583 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-08-10 16:31:31,618 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-08-10 16:31:31,618 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-08-10 16:31:31,618 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-08-10 16:31:31,619 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-08-10 16:31:31,619 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-08-10 16:31:31,619 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-08-10 16:31:31,619 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-08-10 16:31:31,620 __main__ INFO 2nd sampling on the sampler...
2023-08-10 16:31:38,829 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-08-10 16:31:38,829 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-08-10 16:31:38,830 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-08-10 16:31:38,830 __main__ INFO 2nd update on the aggregator...
2023-08-10 16:31:46,039 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-08-10 16:31:46,039 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-08-10 16:31:46,040 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-08-10 16:31:46,040 __main__ INFO 3rd sampling on the sampler...
2023-08-10 16:31:53,249 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-08-10 16:31:53,249 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-08-10 16:31:53,250 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-08-10 16:31:53,250 __main__ INFO 3rd update on the aggregator...
2023-08-10 16:32:00,459 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-08-10 16:32:00,460 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-08-10 16:32:00,460 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-08-10 16:32:00,460 __main__ INFO 4th sampling on the sampler...
2023-08-10 16:32:07,669 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-08-10 16:32:07,670 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-08-10 16:32:07,670 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-08-10 16:32:07,670 __main__ INFO 4th update on the aggregator...
2023-08-10 16:32:14,879 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-08-10 16:32:14,880 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-08-10 16:32:14,880 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-08-10 16:32:14,880 __main__ INFO 5th sampling on the sampler...
2023-08-10 16:32:22,088 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-08-10 16:32:22,088 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-08-10 16:32:22,089 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-08-10 16:32:22,089 __main__ INFO 5th update on the aggregator...
2023-08-10 16:32:29,298 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-08-10 16:32:29,298 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-08-10 16:32:29,299 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-08-10 16:32:29,299 __main__ INFO 6th sampling on the sampler...
2023-08-10 16:32:36,508 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-08-10 16:32:36,508 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-08-10 16:32:36,509 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-08-10 16:32:36,509 __main__ INFO 6th update on the updator...
2023-08-10 16:32:43,718 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-08-10 16:32:43,719 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-08-10 16:32:43,719 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-08-10 16:32:43,719 TADA INFO test ldms_list_test ended
2023-08-10 16:32:54 INFO: ----------------------------------------------
2023-08-10 16:32:55 INFO: ======== quick_set_add_rm_test ========
2023-08-10 16:32:55 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/quick_set_add_rm_test
2023-08-10 16:32:56,088 TADA INFO starting test `quick_set_add_rm_test`
2023-08-10 16:32:56,089 TADA INFO   test-id: bf1984593211f9ea3ae6e8e4a6e5b6e066a6446425299a6eb449b82d83e4daab
2023-08-10 16:32:56,089 TADA INFO   test-suite: LDMSD
2023-08-10 16:32:56,089 TADA INFO   test-name: quick_set_add_rm_test
2023-08-10 16:32:56,089 TADA INFO   test-user: narate
2023-08-10 16:32:56,089 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:32:56,090 __main__ INFO -- Get or create the cluster --
2023-08-10 16:33:03,444 __main__ INFO -- Start samp.py --
2023-08-10 16:33:08,556 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-08-10 16:33:08,557 __main__ INFO -- Start daemons --
2023-08-10 16:33:18,397 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:33:24,035 TADA INFO assertion 2, verify data: verified, passed
2023-08-10 16:33:28,680 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-08-10 16:33:33,290 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-08-10 16:33:37,913 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-08-10 16:33:43,038 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-08-10 16:33:43,038 TADA INFO test quick_set_add_rm_test ended
2023-08-10 16:33:55 INFO: ----------------------------------------------
2023-08-10 16:33:56 INFO: ======== set_array_hang_test ========
2023-08-10 16:33:56 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/set_array_hang_test
2023-08-10 16:33:56,912 TADA INFO starting test `set_array_hang_test`
2023-08-10 16:33:56,912 TADA INFO   test-id: 49afef83a45496ea129b701c4902f6cf3c9cd93bb51f59ba4283531be907aeb8
2023-08-10 16:33:56,912 TADA INFO   test-suite: LDMSD
2023-08-10 16:33:56,913 TADA INFO   test-name: set_array_hang_test
2023-08-10 16:33:56,913 TADA INFO   test-user: narate
2023-08-10 16:33:56,913 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:33:56,913 __main__ INFO -- Get or create the cluster --
2023-08-10 16:34:00,021 __main__ INFO -- Start processes --
2023-08-10 16:34:00,021 __main__ INFO starting interactive set_array_samp.py
2023-08-10 16:34:03,036 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-08-10 16:34:06,052 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-08-10 16:34:13,261 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-08-10 16:34:20,471 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-08-10 16:34:24,076 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-08-10 16:34:31,286 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-08-10 16:34:31,286 TADA INFO test set_array_hang_test ended
2023-08-10 16:34:41 INFO: ----------------------------------------------
2023-08-10 16:34:42 INFO: ======== ldmsd_autointerval_test ========
2023-08-10 16:34:42 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_autointerval_test
2023-08-10 16:34:43,588 TADA INFO starting test `ldmsd_autointerval_test`
2023-08-10 16:34:43,588 TADA INFO   test-id: 6843ce2b8afc61ca783d2423ed7cbc2c1f73e15c98a048109d6b26895c33ae3a
2023-08-10 16:34:43,588 TADA INFO   test-suite: LDMSD
2023-08-10 16:34:43,588 TADA INFO   test-name: ldmsd_autointerval_test
2023-08-10 16:34:43,588 TADA INFO   test-user: narate
2023-08-10 16:34:43,588 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:34:43,589 __main__ INFO -- Get or create the cluster --
2023-08-10 16:34:50,932 __main__ INFO -- Start daemons --
2023-08-10 16:35:06,486 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:35:13,009 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-08-10 16:35:15,250 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-08-10 16:35:15,250 __main__ INFO Let them run for a while to collect data ...
2023-08-10 16:35:25,261 __main__ INFO Setting sample interval to 1000000 ...
2023-08-10 16:35:33,521 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-08-10 16:35:33,521 __main__ INFO Let them run for a while to collect data ...
2023-08-10 16:35:43,531 __main__ INFO Setting sample interval to 2000000 ...
2023-08-10 16:35:51,767 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-08-10 16:35:51,768 __main__ INFO Let them run for a while to collect data ...
2023-08-10 16:36:02,008 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-08-10 16:36:02,123 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-08-10 16:36:02,124 TADA INFO test ldmsd_autointerval_test ended
2023-08-10 16:36:14 INFO: ----------------------------------------------
2023-08-10 16:36:15 INFO: ======== ldms_record_test ========
2023-08-10 16:36:15 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_record_test
2023-08-10 16:36:15,856 TADA INFO starting test `ldms_record_test`
2023-08-10 16:36:15,856 TADA INFO   test-id: ac3054e4325ba0e80b1473e78ce9843716d1948575cfaf301efc66cb9c6e98e8
2023-08-10 16:36:15,857 TADA INFO   test-suite: LDMSD
2023-08-10 16:36:15,857 TADA INFO   test-name: ldms_record_test
2023-08-10 16:36:15,857 TADA INFO   test-user: narate
2023-08-10 16:36:15,857 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:36:15,857 __main__ INFO -- Get or create the cluster --
2023-08-10 16:36:18,868 __main__ INFO -- Start daemons --
2023-08-10 16:36:27,241 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:36:29,243 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-08-10 16:36:35,278 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-08-10 16:36:35,278 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-08-10 16:36:35,278 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-08-10 16:36:35,279 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-08-10 16:36:35,279 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-08-10 16:36:35,279 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-08-10 16:36:35,280 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-08-10 16:36:35,280 __main__ INFO 2nd sampling on the sampler...
2023-08-10 16:36:42,488 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-08-10 16:36:42,489 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-08-10 16:36:42,489 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-08-10 16:36:42,489 __main__ INFO 2nd update on the aggregator...
2023-08-10 16:36:49,699 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-08-10 16:36:49,699 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-08-10 16:36:49,699 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-08-10 16:36:49,699 __main__ INFO 3rd sampling on the sampler...
2023-08-10 16:36:56,909 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-08-10 16:36:56,909 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-08-10 16:36:56,909 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-08-10 16:36:56,910 __main__ INFO 3rd update on the aggregator...
2023-08-10 16:37:04,119 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-08-10 16:37:04,119 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-08-10 16:37:04,119 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-08-10 16:37:04,120 __main__ INFO 4th sampling on the sampler...
2023-08-10 16:37:11,329 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-08-10 16:37:11,329 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-08-10 16:37:11,330 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-08-10 16:37:11,330 __main__ INFO 4th update on the aggregator...
2023-08-10 16:37:18,539 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-08-10 16:37:18,540 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-08-10 16:37:18,540 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-08-10 16:37:18,540 __main__ INFO 5th sampling on the sampler...
2023-08-10 16:37:25,749 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-08-10 16:37:25,750 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-08-10 16:37:25,750 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-08-10 16:37:25,750 __main__ INFO 5th update on the aggregator...
2023-08-10 16:37:32,960 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-08-10 16:37:32,960 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-08-10 16:37:32,960 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-08-10 16:37:32,960 __main__ INFO 6th sampling on the sampler...
2023-08-10 16:37:40,169 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-08-10 16:37:40,170 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-08-10 16:37:40,170 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-08-10 16:37:40,170 __main__ INFO 6th update on the updator...
2023-08-10 16:37:47,379 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-08-10 16:37:47,380 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-08-10 16:37:47,380 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-08-10 16:37:47,381 TADA INFO test ldms_record_test ended
2023-08-10 16:37:58 INFO: ----------------------------------------------
2023-08-10 16:37:58 INFO: ======== ldms_schema_digest_test ========
2023-08-10 16:37:58 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_schema_digest_test
2023-08-10 16:37:59,665 TADA INFO starting test `ldms_schema_digest_test`
2023-08-10 16:37:59,665 TADA INFO   test-id: cca8fe4c5a9d0a0249ba406e0d3f426b214fb8f725de676227ab93a93d9cd289
2023-08-10 16:37:59,665 TADA INFO   test-suite: LDMSD
2023-08-10 16:37:59,665 TADA INFO   test-name: ldms_schema_digest_test
2023-08-10 16:37:59,665 TADA INFO   test-user: narate
2023-08-10 16:37:59,665 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:37:59,666 __main__ INFO -- Get or create the cluster --
2023-08-10 16:38:06,762 __main__ INFO -- Start daemons --
2023-08-10 16:38:17,870 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:38:22,997 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-08-10 16:38:23,125 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-08-10 16:38:23,227 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-08-10 16:38:23,400 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-08-10 16:38:23,401 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-08-10 16:38:23,401 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-08-10 16:38:25,826 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-08-10 16:38:25,827 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-08-10 16:38:25,827 TADA INFO test ldms_schema_digest_test ended
2023-08-10 16:38:38 INFO: ----------------------------------------------
2023-08-10 16:38:39 INFO: ======== ldmsd_decomp_test ========
2023-08-10 16:38:39 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_decomp_test
2023-08-10 16:38:39,878 TADA INFO starting test `ldmsd_decomp_test`
2023-08-10 16:38:39,879 TADA INFO   test-id: 160436b55dc2f0836e47f15f1b2dfd9e0fb24615498ddbe411a4960e73606a8a
2023-08-10 16:38:39,879 TADA INFO   test-suite: LDMSD
2023-08-10 16:38:39,879 TADA INFO   test-name: ldmsd_decomp_test
2023-08-10 16:38:39,879 TADA INFO   test-user: narate
2023-08-10 16:38:39,879 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:38:39,880 __main__ INFO -- Get or create the cluster --
2023-08-10 16:38:55,121 __main__ INFO -- Start daemons --
2023-08-10 16:39:24,872 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:40:19,641 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-08-10 16:40:19,641 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-08-10 16:40:19,641 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-08-10 16:40:19,641 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-08-10 16:40:19,642 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-08-10 16:40:19,643 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-08-10 16:40:19,644 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-08-10 16:40:19,645 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-08-10 16:40:19,647 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-08-10 16:40:19,727 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-08-10 16:40:19,732 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-08-10 16:40:19,735 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-08-10 16:40:19,745 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-08-10 16:40:19,747 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-08-10 16:40:19,748 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-08-10 16:40:19,824 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-08-10 16:40:19,829 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-08-10 16:40:19,832 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-08-10 16:40:19,842 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-08-10 16:40:19,842 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-08-10 16:40:19,843 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-08-10 16:40:19,872 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-08-10 16:40:19,874 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-08-10 16:40:19,876 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-08-10 16:40:19,880 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-08-10 16:40:19,880 TADA INFO test ldmsd_decomp_test ended
2023-08-10 16:40:19,881 TADA INFO test ldmsd_decomp_test ended
2023-08-10 16:40:35 INFO: ----------------------------------------------
2023-08-10 16:40:36 INFO: ======== ldmsd_stream_status_test ========
2023-08-10 16:40:36 INFO: CMD: python3 ldmsd_stream_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_stream_status_test
2023-08-10 16:40:36,745 __main__ INFO -- Get or create the cluster --
2023-08-10 16:40:36,745 TADA INFO starting test `ldmsd_stream_status`
2023-08-10 16:40:36,745 TADA INFO   test-id: be2884659ba2a92957887462d49d8db2364340ee1816d03a2bd6b8674b2bcdca
2023-08-10 16:40:36,745 TADA INFO   test-suite: LDMSD
2023-08-10 16:40:36,745 TADA INFO   test-name: ldmsd_stream_status
2023-08-10 16:40:36,745 TADA INFO   test-user: narate
2023-08-10 16:40:36,745 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:40:45,054 __main__ INFO -- Start daemons --
2023-08-10 16:41:00,526 __main__ INFO waiting ... for all LDMSDs to start
2023-08-10 16:41:00,855 __main__ INFO All LDMSDs are up.
2023-08-10 16:41:02,086 TADA INFO assertion 1, No Stream data: [] == [], passed
2023-08-10 16:41:03,422 TADA INFO assertion 2, stream_status -- one stream message: [{'name': 'foo', 'rx': {'bytes': 6, 'count': 1, 'first_ts': 1691703662.2028599, 'last_ts': 1691703662.2028599}, 'sources': {'0.0.0.0:0': {'bytes': 6, 'count': 1, 'first_ts': 1691703662.2028599, 'last_ts': 1691703662.2028599}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 1, 'bytes': 6, 'first_ts': 1691703662.2028599, 'last_ts': 1691703662.2028599}, 'sources': {'0.0.0.0:0': {'count': 1, 'bytes': 6, 'first_ts': 1691703662.2028599, 'last_ts': 1691703662.2028599}}, 'clients': []}], passed
2023-08-10 16:41:05,899 TADA INFO assertion 3, stream_status --  multiple stream messages: [{'name': 'foo', 'rx': {'bytes': 18, 'count': 3, 'first_ts': 1691703662.2028599, 'last_ts': 1691703664.6684003}, 'sources': {'0.0.0.0:0': {'bytes': 18, 'count': 3, 'first_ts': 1691703662.2028599, 'last_ts': 1691703664.6684003}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 3, 'bytes': 18, 'first_ts': 1691703662.2028599, 'last_ts': 1691703664.6684003}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 18, 'first_ts': 1691703662.2028599, 'last_ts': 1691703664.6684003}}, 'clients': []}], passed
2023-08-10 16:41:09,714 TADA INFO assertion 4, stream_status -- mulitple streams: [{'name': 'bar', 'rx': {'bytes': 48, 'count': 3, 'first_ts': 1691703667.2572432, 'last_ts': 1691703668.4758377}, 'sources': {'0.0.0.0:0': {'bytes': 48, 'count': 3, 'first_ts': 1691703667.2572432, 'last_ts': 1691703668.4758377}}, 'clients': []}, {'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1691703666.013722, 'last_ts': 1691703667.1297314}, 'sources': {'0.0.0.0:0': {'bytes': 12, 'count': 2, 'first_ts': 1691703666.013722, 'last_ts': 1691703667.1297314}}, 'clients': []}] == [{'name': 'bar', 'rx': {'count': 3, 'bytes': 48, 'first_ts': 1691703667.2572432, 'last_ts': 1691703668.4758377}, 'sources': {'0.0.0.0:0': {'count': 3, 'bytes': 48, 'first_ts': 1691703667.2572432, 'last_ts': 1691703668.4758377}}, 'clients': []}, {'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1691703666.013722, 'last_ts': 1691703667.1297314}, 'sources': {'0.0.0.0:0': {'count': 2, 'bytes': 12, 'first_ts': 1691703666.013722, 'last_ts': 1691703667.1297314}}, 'clients': []}], passed
2023-08-10 16:41:13,399 TADA INFO assertion 5, stream_status to agg after one producer republished stream: [{'name': 'foo', 'rx': {'bytes': 12, 'count': 2, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}, 'sources': {'10.0.255.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 2, 'bytes': 12, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}, 'sources': {'10.0.255.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}}, 'clients': []}], passed
2023-08-10 16:41:14,955 TADA INFO assertion 6, stream_status to agg after two producers republished stream: [{'name': 'foo', 'rx': {'bytes': 30, 'count': 5, 'first_ts': 1691703671.0319226, 'last_ts': 1691703673.7424617}, 'sources': {'10.0.255.2:10001': {'bytes': 12, 'count': 2, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}, '10.0.255.4:10001': {'bytes': 18, 'count': 3, 'first_ts': 1691703673.5124724, 'last_ts': 1691703673.7424617}}, 'clients': []}] == [{'name': 'foo', 'rx': {'count': 5, 'bytes': 30, 'first_ts': 1691703671.0319226, 'last_ts': 1691703673.7424617}, 'sources': {'10.0.255.2:10001': {'count': 2, 'bytes': 12, 'first_ts': 1691703671.0319226, 'last_ts': 1691703672.1659257}, '10.0.255.4:10001': {'count': 3, 'bytes': 18, 'first_ts': 1691703673.5124724, 'last_ts': 1691703673.7424617}}, 'clients': []}], passed
2023-08-10 16:41:14,955 TADA INFO test ldmsd_stream_status ended
2023-08-10 16:41:27 INFO: ----------------------------------------------
2023-08-10 16:41:28 INFO: ======== store_list_record_test ========
2023-08-10 16:41:28 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/store_list_record_test
2023-08-10 16:41:28,787 __main__ INFO -- Get or create the cluster --
2023-08-10 16:41:28,787 TADA INFO starting test `store_sos_lists_test`
2023-08-10 16:41:28,787 TADA INFO   test-id: 75c61a00caf4872904e04bc8a535f5c33526ce36cf4c3101c58114694d221e54
2023-08-10 16:41:28,787 TADA INFO   test-suite: LDMSD
2023-08-10 16:41:28,787 TADA INFO   test-name: store_sos_lists_test
2023-08-10 16:41:28,787 TADA INFO   test-user: narate
2023-08-10 16:41:28,787 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:41:36,550 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:41:52,201 __main__ INFO All sampler daemons are up.
2023-08-10 16:41:52,307 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-08-10 16:41:52,419 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-08-10 16:41:53,800 TADA INFO assertion 3, store_sos is storing data.: file_exists(a) for a in supported_schema, passed
2023-08-10 16:41:54,518 TADA INFO assertion 4, store_sos stores data correctly.: verify_data(db) for db in all_db, passed
2023-08-10 16:42:03,429 TADA INFO assertion 5, store_sos stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-08-10 16:42:04,108 TADA INFO assertion 6, store_sos reports multiple list errror messages resulted by the config file.: store_sos reported the multiple list error messages., passed
2023-08-10 16:42:08,949 TADA INFO assertion 7, store_sos reports multiple list errror messages resulted by ldmsd_controller.: store_sos reported the multiple list error messages., passed
2023-08-10 16:42:09,308 TADA INFO assertion 8, store_csv is storing data.: file_exists(a) for a in supported_schema, passed
2023-08-10 16:42:15,757 TADA INFO assertion 9, store_csv stores data correctly.: verify_data(db) for db in all_db, passed
2023-08-10 16:42:25,470 TADA INFO assertion 10, store_csv stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-08-10 16:42:26,131 TADA INFO assertion 11, store_csv reports multiple list errror messages resulted by the config file.: store_csv reported the multiple list error messages., passed
2023-08-10 16:42:31,012 TADA INFO assertion 12, store_csv reports multiple list errror messages resulted by ldmsd_controller.: store_csv reported the multiple list error messages., passed
2023-08-10 16:42:31,013 TADA INFO test store_sos_lists_test ended
2023-08-10 16:42:43 INFO: ----------------------------------------------
2023-08-10 16:42:44 INFO: ======== maestro_raft_test ========
2023-08-10 16:42:44 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/maestro_raft_test
2023-08-10 16:42:45,346 TADA INFO starting test `maestro_raft_test`
2023-08-10 16:42:45,346 TADA INFO   test-id: 370ae4fd91ec16d78d5e6c85838f64eb36883dcc5d734b4b79537a642baa0a04
2023-08-10 16:42:45,346 TADA INFO   test-suite: LDMSD
2023-08-10 16:42:45,346 TADA INFO   test-name: maestro_raft_test
2023-08-10 16:42:45,346 TADA INFO   test-user: narate
2023-08-10 16:42:45,346 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:42:55,358 __main__ INFO -- Get or create cluster --
2023-08-10 16:43:29,767 __main__ INFO -- Start daemons --
2023-08-10 16:45:05,743 __main__ INFO -- making known hosts (ssh) --
2023-08-10 16:45:12,864 __main__ INFO ... make sure ldmsd's are up
2023-08-10 16:45:31,624 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: [('FOLLOWER', 2), ('LEADER', 1)], passed
2023-08-10 16:45:44,146 TADA INFO assertion 2, All ldmsds are up and configured: sets verified, passed
2023-08-10 16:45:44,386 TADA INFO assertion 3, Data are being stored: data check, passed
2023-08-10 16:45:49,320 TADA INFO assertion 4, New leader elected: checked, passed
2023-08-10 16:46:24,346 TADA INFO assertion 5, Restarted ldmsd is configured: sets verified, passed
2023-08-10 16:46:24,629 TADA INFO assertion 6, New data are presented in the store: data check, passed
2023-08-10 16:46:35,611 TADA INFO assertion 7, The restarted maestro becomes a follower: checked, passed
---Wait for config to write to file---
2023-08-10 16:46:35,612 TADA INFO test maestro_raft_test ended
2023-08-10 16:46:56 INFO: ----------------------------------------------
2023-08-10 16:46:57 INFO: ======== ovis_json_test ========
2023-08-10 16:46:57 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ovis_json_test
2023-08-10 16:46:58,399 __main__ INFO -- Create the cluster -- 
2023-08-10 16:47:03,869 TADA INFO starting test `ovis_json_test`
2023-08-10 16:47:03,869 TADA INFO   test-id: bea90d3c3e74308d471436f945c332827d51423b0c9ff43afcec748b48ff2626
2023-08-10 16:47:03,870 TADA INFO   test-suite: OVIS-LIB
2023-08-10 16:47:03,870 TADA INFO   test-name: ovis_json_test
2023-08-10 16:47:03,870 TADA INFO   test-user: narate
2023-08-10 16:47:03,870 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:47:03,871 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-08-10 16:47:03,871 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-08-10 16:47:03,871 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-08-10 16:47:03,872 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-08-10 16:47:03,872 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-08-10 16:47:03,872 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-08-10 16:47:03,872 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-08-10 16:47:03,872 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-08-10 16:47:03,872 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,872 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,872 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-08-10 16:47:03,873 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-08-10 16:47:03,873 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-08-10 16:47:03,874 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-08-10 16:47:03,874 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-08-10 16:47:03,874 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-08-10 16:47:03,874 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-08-10 16:47:03,874 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-08-10 16:47:03,874 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-08-10 16:47:03,875 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-08-10 16:47:03,875 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-08-10 16:47:03,875 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-08-10 16:47:03,875 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,875 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,875 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-08-10 16:47:03,876 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-08-10 16:47:03,877 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-08-10 16:47:03,877 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-08-10 16:47:03,877 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-08-10 16:47:03,877 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-08-10 16:47:03,877 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-08-10 16:47:03,877 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-08-10 16:47:03,878 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-08-10 16:47:03,878 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-08-10 16:47:03,878 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-08-10 16:47:03,878 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-08-10 16:47:03,878 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-08-10 16:47:03,878 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-08-10 16:47:03,878 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-08-10 16:47:03,879 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-08-10 16:47:03,879 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-08-10 16:47:03,879 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-08-10 16:47:03,879 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-08-10 16:47:03,879 TADA INFO test ovis_json_test ended
2023-08-10 16:47:14 INFO: ----------------------------------------------
2023-08-10 16:47:15 INFO: ======== updtr_add_test ========
2023-08-10 16:47:15 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_add_test
2023-08-10 16:47:16,250 __main__ INFO -- Get or create the cluster --
2023-08-10 16:47:16,250 TADA INFO starting test `updtr_add test`
2023-08-10 16:47:16,251 TADA INFO   test-id: 4690f3bafc6bc11d76f5d5fe9d2ddf1fff2db89dd7610891caad4005f11d4855
2023-08-10 16:47:16,251 TADA INFO   test-suite: LDMSD
2023-08-10 16:47:16,251 TADA INFO   test-name: updtr_add test
2023-08-10 16:47:16,251 TADA INFO   test-user: narate
2023-08-10 16:47:16,251 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:47:24,131 __main__ INFO -- Start daemons --
2023-08-10 16:47:39,633 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:47:39,950 __main__ INFO All LDMSDs are up.
2023-08-10 16:47:41,167 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:47:42,392 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:47:43,615 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:47:44,834 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:47:46,065 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:47:48,514 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:47:50,961 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:47:52,188 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-08-10 16:47:52,188 __main__ INFO --- done ---
2023-08-10 16:47:52,188 TADA INFO test updtr_add test ended
2023-08-10 16:48:04 INFO: ----------------------------------------------
2023-08-10 16:48:05 INFO: ======== updtr_del_test ========
2023-08-10 16:48:05 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_del_test
2023-08-10 16:48:06,108 __main__ INFO -- Get or create the cluster --
2023-08-10 16:48:06,109 TADA INFO starting test `updtr_add test`
2023-08-10 16:48:06,109 TADA INFO   test-id: 490bdefe168b093bcc3a4a866460bf56487b5400099b4e465f3eae71480d74db
2023-08-10 16:48:06,109 TADA INFO   test-suite: LDMSD
2023-08-10 16:48:06,109 TADA INFO   test-name: updtr_add test
2023-08-10 16:48:06,109 TADA INFO   test-user: narate
2023-08-10 16:48:06,109 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:48:14,101 __main__ INFO -- Start daemons --
2023-08-10 16:48:29,557 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:48:29,875 __main__ INFO All LDMSDs are up.
2023-08-10 16:48:31,095 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:48:32,310 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-08-10 16:48:33,543 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:48:34,781 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:48:34,781 __main__ INFO --- done ---
2023-08-10 16:48:34,781 TADA INFO test updtr_add test ended
2023-08-10 16:48:47 INFO: ----------------------------------------------
2023-08-10 16:48:47 INFO: ======== updtr_match_add_test ========
2023-08-10 16:48:47 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_match_add_test
2023-08-10 16:48:48,659 __main__ INFO -- Get or create the cluster --
2023-08-10 16:48:48,659 TADA INFO starting test `updtr_add test`
2023-08-10 16:48:48,659 TADA INFO   test-id: 7ed4a703cffad69b44301d9663a0550b48e52472b60331d8d60c0325a55def62
2023-08-10 16:48:48,659 TADA INFO   test-suite: LDMSD
2023-08-10 16:48:48,659 TADA INFO   test-name: updtr_add test
2023-08-10 16:48:48,659 TADA INFO   test-user: narate
2023-08-10 16:48:48,659 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:48:56,389 __main__ INFO -- Start daemons --
2023-08-10 16:49:11,881 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:49:12,225 __main__ INFO All LDMSDs are up.
2023-08-10 16:49:13,440 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:49:14,673 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:49:15,895 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:49:17,103 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:49:18,335 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-08-10 16:49:18,335 __main__ INFO --- done ---
2023-08-10 16:49:18,335 TADA INFO test updtr_add test ended
2023-08-10 16:49:30 INFO: ----------------------------------------------
2023-08-10 16:49:31 INFO: ======== updtr_match_del_test ========
2023-08-10 16:49:31 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_match_del_test
2023-08-10 16:49:32,261 __main__ INFO -- Get or create the cluster --
2023-08-10 16:49:32,261 TADA INFO starting test `updtr_add test`
2023-08-10 16:49:32,261 TADA INFO   test-id: a39e4d352bb68089f463c3786138e5c97b47f2719928a5718281f629b285a51e
2023-08-10 16:49:32,261 TADA INFO   test-suite: LDMSD
2023-08-10 16:49:32,261 TADA INFO   test-name: updtr_add test
2023-08-10 16:49:32,261 TADA INFO   test-user: narate
2023-08-10 16:49:32,261 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:49:40,051 __main__ INFO -- Start daemons --
2023-08-10 16:49:55,574 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:49:55,908 __main__ INFO All LDMSDs are up.
2023-08-10 16:49:57,126 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-08-10 16:49:58,330 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:49:59,555 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:50:00,788 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:50:02,009 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:50:03,239 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:50:04,461 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:50:04,461 __main__ INFO --- done ---
2023-08-10 16:50:04,462 TADA INFO test updtr_add test ended
2023-08-10 16:50:16 INFO: ----------------------------------------------
2023-08-10 16:50:17 INFO: ======== updtr_prdcr_add_test ========
2023-08-10 16:50:17 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_prdcr_add_test
2023-08-10 16:50:18,207 __main__ INFO -- Get or create the cluster --
2023-08-10 16:50:18,207 TADA INFO starting test `updtr_add test`
2023-08-10 16:50:18,207 TADA INFO   test-id: ad1812e7c1d66a752e38aa90bd62e7a64b4d636eae5a9af38ccf068157f3b16f
2023-08-10 16:50:18,208 TADA INFO   test-suite: LDMSD
2023-08-10 16:50:18,208 TADA INFO   test-name: updtr_add test
2023-08-10 16:50:18,208 TADA INFO   test-user: narate
2023-08-10 16:50:18,208 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:50:25,930 __main__ INFO -- Start daemons --
2023-08-10 16:50:41,474 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:50:41,787 __main__ INFO All LDMSDs are up.
2023-08-10 16:50:42,995 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:50:45,412 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:50:47,853 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:50:49,075 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-08-10 16:50:50,301 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:50:50,301 __main__ INFO --- done ---
2023-08-10 16:50:50,301 TADA INFO test updtr_add test ended
2023-08-10 16:51:02 INFO: ----------------------------------------------
2023-08-10 16:51:03 INFO: ======== updtr_prdcr_del_test ========
2023-08-10 16:51:03 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_prdcr_del_test
2023-08-10 16:51:04,155 __main__ INFO -- Get or create the cluster --
2023-08-10 16:51:04,156 TADA INFO starting test `updtr_add test`
2023-08-10 16:51:04,156 TADA INFO   test-id: 7267e6999ed343e361588937b4a9209eb09a73701ba3cd633b8109145a1f87c9
2023-08-10 16:51:04,156 TADA INFO   test-suite: LDMSD
2023-08-10 16:51:04,156 TADA INFO   test-name: updtr_add test
2023-08-10 16:51:04,156 TADA INFO   test-user: narate
2023-08-10 16:51:04,156 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:51:11,965 __main__ INFO -- Start daemons --
2023-08-10 16:51:27,508 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:51:27,827 __main__ INFO All LDMSDs are up.
2023-08-10 16:51:29,040 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:51:30,272 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-08-10 16:51:31,510 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:51:33,981 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-08-10 16:51:33,981 __main__ INFO --- done ---
2023-08-10 16:51:33,982 TADA INFO test updtr_add test ended
2023-08-10 16:51:46 INFO: ----------------------------------------------
2023-08-10 16:51:47 INFO: ======== updtr_start_test ========
2023-08-10 16:51:47 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_start_test
2023-08-10 16:51:47,827 __main__ INFO -- Get or create the cluster --
2023-08-10 16:51:47,827 TADA INFO starting test `updtr_add test`
2023-08-10 16:51:47,828 TADA INFO   test-id: 88f1635e41712861d204d820d38c9b9efc90a939ddfb01814d7df4d7ed13b177
2023-08-10 16:51:47,828 TADA INFO   test-suite: LDMSD
2023-08-10 16:51:47,828 TADA INFO   test-name: updtr_add test
2023-08-10 16:51:47,828 TADA INFO   test-user: narate
2023-08-10 16:51:47,828 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:51:55,523 __main__ INFO -- Start daemons --
2023-08-10 16:52:11,126 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:52:11,430 __main__ INFO All LDMSDs are up.
2023-08-10 16:52:12,643 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:52:13,861 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:52:15,094 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-08-10 16:52:16,316 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:52:17,543 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-08-10 16:52:19,977 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-08-10 16:52:21,207 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-08-10 16:52:23,664 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-08-10 16:52:26,127 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-08-10 16:52:28,576 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-08-10 16:52:29,792 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-08-10 16:52:29,793 __main__ INFO --- done ---
2023-08-10 16:52:29,793 TADA INFO test updtr_add test ended
2023-08-10 16:52:42 INFO: ----------------------------------------------
2023-08-10 16:52:42 INFO: ======== updtr_status_test ========
2023-08-10 16:52:42 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/updtr_status_test
2023-08-10 16:52:43,627 __main__ INFO -- Get or create the cluster --
2023-08-10 16:52:43,628 TADA INFO starting test `updtr_status test`
2023-08-10 16:52:43,628 TADA INFO   test-id: 96c1b94028fa5f5dbc2184c369ee06c88c79702693a223cf3395d7f4bf60d3b7
2023-08-10 16:52:43,628 TADA INFO   test-suite: LDMSD
2023-08-10 16:52:43,628 TADA INFO   test-name: updtr_status test
2023-08-10 16:52:43,628 TADA INFO   test-user: narate
2023-08-10 16:52:43,628 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:52:53,976 __main__ INFO -- Start daemons --
2023-08-10 16:53:14,561 __main__ INFO Waiting ... for all LDMSDs to start
2023-08-10 16:53:14,998 __main__ INFO All LDMSDs are up.
2023-08-10 16:53:16,229 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-08-10 16:53:17,447 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-08-10 16:53:18,673 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:53:19,885 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:53:21,110 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-08-10 16:53:21,111 __main__ INFO --- done ---
2023-08-10 16:53:21,111 TADA INFO test updtr_status test ended
2023-08-10 16:53:34 INFO: ----------------------------------------------
2023-08-10 16:53:35 INFO: ======== ldmsd_flex_decomp_test ========
2023-08-10 16:53:35 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_flex_decomp_test
2023-08-10 16:53:35,756 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-08-10 16:53:35,756 TADA INFO   test-id: ab643838d6dbfdb421c402fb3e8b3d37626ea525ae8de1db829f52120c0c0a59
2023-08-10 16:53:35,756 TADA INFO   test-suite: LDMSD
2023-08-10 16:53:35,756 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-08-10 16:53:35,756 TADA INFO   test-user: narate
2023-08-10 16:53:35,756 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:53:35,757 __main__ INFO -- Get or create the cluster --
2023-08-10 16:53:51,333 __main__ INFO -- Start daemons --
2023-08-10 16:54:20,957 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 16:55:10,080 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-08-10 16:55:10,080 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-08-10 16:55:10,080 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-08-10 16:55:10,080 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 5, record sos schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-08-10 16:55:10,081 TADA INFO assertion 10, record csv schema check: OK, passed
2023-08-10 16:55:10,082 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-08-10 16:55:10,082 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-08-10 16:55:10,082 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-08-10 16:55:10,082 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-08-10 16:55:10,082 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-08-10 16:55:10,084 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-08-10 16:55:10,160 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-08-10 16:55:10,163 TADA INFO assertion 18, fill sos data check: OK, passed
2023-08-10 16:55:10,165 TADA INFO assertion 19, filter sos data check: OK, passed
2023-08-10 16:55:10,173 TADA INFO assertion 20, record sos data check: OK, passed
2023-08-10 16:55:10,175 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-08-10 16:55:10,250 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-08-10 16:55:10,253 TADA INFO assertion 23, fill csv data check: OK, passed
2023-08-10 16:55:10,254 TADA INFO assertion 24, filter csv data check: OK, passed
2023-08-10 16:55:10,263 TADA INFO assertion 25, record csv data check: OK, passed
2023-08-10 16:55:10,264 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-08-10 16:55:10,287 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-08-10 16:55:10,288 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-08-10 16:55:10,289 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-08-10 16:55:10,293 TADA INFO assertion 30, record kafka data check: OK, passed
2023-08-10 16:55:10,293 TADA INFO test ldmsd_flex_decomp_test ended
2023-08-10 16:55:10,294 TADA INFO test ldmsd_flex_decomp_test ended
2023-08-10 16:55:25 INFO: ----------------------------------------------
2023-08-10 16:55:26 INFO: ======== ldms_set_info_test ========
2023-08-10 16:55:26 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_set_info_test
2023-08-10 16:55:36,751 TADA INFO starting test `ldms_set_info_test`
2023-08-10 16:55:36,751 TADA INFO   test-id: ee344a01ff40122a5e4acc3bb586fe716b4c1bd049bfa50f0bbdac45c1b56078
2023-08-10 16:55:36,752 TADA INFO   test-suite: LDMSD
2023-08-10 16:55:36,752 TADA INFO   test-name: ldms_set_info_test
2023-08-10 16:55:36,752 TADA INFO   test-user: narate
2023-08-10 16:55:36,752 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:55:36,752 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 3, Get a value : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 4, Unset a pair : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 7, Server resetting a key : -, passed
2023-08-10 16:55:36,753 TADA INFO assertion 8, Server unset a key : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 9, Server add a key : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 10, Adding a key : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-08-10 16:55:36,754 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-08-10 16:55:36,755 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-08-10 16:55:36,755 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-08-10 16:55:36,755 TADA INFO test ldms_set_info_test ended
2023-08-10 16:55:47 INFO: ----------------------------------------------
2023-08-10 16:55:48 INFO: ======== slurm_sampler2_test ========
2023-08-10 16:55:48 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/slurm_sampler2_test
2023-08-10 16:55:49,054 TADA INFO starting test `slurm_sampler2_test`
2023-08-10 16:55:49,054 TADA INFO   test-id: 586b40d57f9f7fb8db21262e55574d2d2be2fbfe16ecfbb4703337c9c5e97e53
2023-08-10 16:55:49,054 TADA INFO   test-suite: LDMSD
2023-08-10 16:55:49,055 TADA INFO   test-name: slurm_sampler2_test
2023-08-10 16:55:49,055 TADA INFO   test-user: narate
2023-08-10 16:55:49,055 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:55:49,055 __main__ INFO -- Get or create the cluster --
2023-08-10 16:56:02,646 __main__ INFO -- Add users --
2023-08-10 16:56:07,822 __main__ INFO -- Preparing job script & programs --
2023-08-10 16:56:08,540 __main__ INFO -- Start daemons --
2023-08-10 16:56:50,051 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-08-10 16:56:54,739 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-08-10 16:56:57,418 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:00,135 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:02,855 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:05,570 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:10,250 TADA INFO assertion 3.1, Expanding the set heap -- job_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:12,959 TADA INFO assertion 3.2, Expanding the set heap -- step_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:16,974 TADA INFO assertion 3.3, Expanding the set heap -- task_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:20,923 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:23,624 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:29,975 TADA INFO assertion 4.1, Multi-tenant -- job_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:31,646 TADA INFO assertion 4.2, Multi-tenant -- step_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:34,243 TADA INFO assertion 4.3, Multi-tenant -- task_init: The metric values are as expected on all nodes., passed
2023-08-10 16:57:36,772 TADA INFO assertion 4.4, Multi-tenant -- task_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:38,472 TADA INFO assertion 4.5, Multi-tenant -- job_exit: The metric values are as expected on all nodes., passed
2023-08-10 16:57:38,472 TADA INFO test slurm_sampler2_test ended
2023-08-10 16:57:52 INFO: ----------------------------------------------
2023-08-10 16:57:53 INFO: ======== run_inside_cont_test.py ========
2023-08-10 16:57:53 INFO: CMD: python3 run_inside_cont_test.py --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/run_inside_cont_test.py
2023-08-10 16:57:54,476 inside_cont_test INFO ===========================================================
2023-08-10 16:57:54,476 inside_cont_test INFO plugin_config_cmd: Start testing plugin_config_cmd
2023-08-10 16:57:54,478 TADA INFO starting test `plugin_config_cmd`
2023-08-10 16:57:54,478 TADA INFO   test-id: 7917cde2913c18cc5f95039ee89927a2828ce7dbc742bff12bbfbdbb192f9cd9
2023-08-10 16:57:54,478 TADA INFO   test-suite: LDMSD
2023-08-10 16:57:54,479 TADA INFO   test-name: plugin_config_cmd
2023-08-10 16:57:54,479 TADA INFO   test-user: narate
2023-08-10 16:57:54,479 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:57:54,479 inside_cont_test INFO plugin_config_cmd: Preparing the containers
2023-08-10 16:58:03,972 inside_cont_test INFO plugin_config_cmd: Running the test script
2023-08-10 16:58:07,467 TADA INFO assertion status-1, Get the plugin statuses: status is as expected, passed
2023-08-10 16:58:07,468 TADA INFO assertion load-1, Load a non-existing plugin: resp['errcode'] (4294967295) != 0, passed
2023-08-10 16:58:07,468 TADA INFO assertion load-2, load a plugin: resp['errcode'] (0) == 0, passed
2023-08-10 16:58:07,468 TADA INFO assertion load-3, load a loadded plugin: resp['errcode'] (17) == 17, passed
2023-08-10 16:58:07,468 TADA INFO assertion config-1, Configure a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-08-10 16:58:07,468 TADA INFO assertion config-2, Misconfigure a loadded plugin: resp['errcode'] (22) != 0, passed
2023-08-10 16:58:07,469 TADA INFO assertion config-3, Correctly configure a loaded plugin: resp['errcode'] (0) == 0, passed
2023-08-10 16:58:07,469 TADA INFO assertion start-1, Start a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-08-10 16:58:07,469 TADA INFO assertion start-2, Start a store plugin: resp['errcode'] (22) == 22, passed
2023-08-10 16:58:07,469 TADA INFO assertion start-4, Start a sampler plugin using a negative interval: resp['errcode'] (0) == 22, failed
2023-08-10 16:58:08,115 inside_cont_test ERROR Verify that the config commands related to plugins work as expected, resp['errcode'] (0) == 22: FAILED
2023-08-10 16:58:08,119 inside_cont_test INFO ===========================================================
2023-08-10 16:58:08,119 inside_cont_test INFO prdcr_config_cmd: Start testing prdcr_config_cmd
2023-08-10 16:58:08,123 TADA INFO starting test `prdcr_config_cmd`
2023-08-10 16:58:08,123 TADA INFO   test-id: 1445db7148d4a49d3f4143427706939d1770f230e180ea9d6ab0a5b74b59bb3a
2023-08-10 16:58:08,123 TADA INFO   test-suite: LDMSD
2023-08-10 16:58:08,123 TADA INFO   test-name: prdcr_config_cmd
2023-08-10 16:58:08,123 TADA INFO   test-user: narate
2023-08-10 16:58:08,123 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:58:08,124 inside_cont_test INFO prdcr_config_cmd: Preparing the containers
2023-08-10 16:59:15,193 inside_cont_test INFO prdcr_config_cmd: Running the test script
2023-08-10 16:59:23,774 TADA INFO assertion status-1, LDMSD has no producers.: len(status) == 0, passed
2023-08-10 16:59:23,774 TADA INFO assertion status-2, Get prdcr_status of a non-existing producer.: resp['errcode'] (2) == 2, passed
2023-08-10 16:59:23,774 TADA INFO assertion status-3, Get the result of a single producer: status is as expected, passed
2023-08-10 16:59:23,775 TADA INFO assertion status-4, Get the result of a single producer with sets: status is as expected, passed
2023-08-10 16:59:23,775 TADA INFO assertion status-5, Get the result of a passive producer: status is as expected, passed
2023-08-10 16:59:23,775 TADA INFO assertion status-6, Get the results of two producers: status is as expected, passed
2023-08-10 16:59:23,775 TADA INFO assertion add-1, prdcr_add an active producer: resp['errcode'] (0) == 0, passed
2023-08-10 16:59:23,775 TADA INFO assertion add-2, prdcr_add a passive producer: resp['errcode'] (0) == 0, passed
2023-08-10 16:59:23,775 TADA INFO assertion add-3, prdcr_add with a string interval: resp['errcode'] (22) == 22, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-4, prdcr_add with a negative reconnect: resp['errcode'] (22) == 22, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-5, prdcr_add with zero reconnect: resp['errcode'] (22) == 22, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-6, prdcr_add with an invalid type: resp['errcode'] (22) == 22, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-7, prdcr_add with a negative port: resp['errcode'] (22) == 22, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-8, prdcr_add with a non-existing host: resp['errcode'] (97) == 97, passed
2023-08-10 16:59:23,776 TADA INFO assertion add-9, prdcr_add an existing producer: resp['errcode'] (17) == 17, passed
2023-08-10 16:59:23,777 TADA INFO assertion add-10, prdcr_add using the interval attribute: resp['errcode'] (0) == 0, passed
2023-08-10 16:59:23,777 TADA INFO assertion start-1, prdcr_start a non-existing producer: resp['errcode'] (2) == 2, passed
2023-08-10 16:59:23,777 TADA INFO assertion start-2.1, prdcr_start a stopped producer -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 16:59:23,777 TADA INFO assertion start-2.2, prdcdr_start a stopped producer -- checking the status: status is as expected, passed
2023-08-10 16:59:23,777 TADA INFO assertion start-3.1, prdcr_start a running producer -- checking the errcode: resp['errcode'] (16) == 16, passed
2023-08-10 16:59:23,777 TADA INFO assertion start-3.2, prdcr_start a running producer -- checking the status: status is as expected, passed
2023-08-10 16:59:23,778 TADA INFO assertion start_regex-1, prdcr_start_regex using an invalid regex: resp['errcode'] (2) != 0, passed
2023-08-10 16:59:23,778 TADA INFO assertion start_regex-2.1, prdcr_start_regex matching no producers -- checking the errcode: resp['errcode'] (1) == 0, failed
2023-08-10 16:59:30,356 inside_cont_test ERROR Verify that the handlers of the producer config commands work correctly, resp['errcode'] (1) == 0: FAILED
2023-08-10 16:59:30,361 inside_cont_test INFO ===========================================================
2023-08-10 16:59:30,362 inside_cont_test INFO strgp_config_cmd: Start testing strgp_config_cmd
2023-08-10 16:59:30,365 TADA INFO starting test `strgp_config_cmd`
2023-08-10 16:59:30,365 TADA INFO   test-id: db3428c175aefafa8630d4b06d2979d619426613a05c37c002a78fe208486586
2023-08-10 16:59:30,365 TADA INFO   test-suite: LDMSD
2023-08-10 16:59:30,365 TADA INFO   test-name: strgp_config_cmd
2023-08-10 16:59:30,365 TADA INFO   test-user: narate
2023-08-10 16:59:30,365 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 16:59:30,365 inside_cont_test INFO strgp_config_cmd: Preparing the containers
2023-08-10 17:00:51,478 inside_cont_test INFO strgp_config_cmd: Running the test script
2023-08-10 17:00:54,978 TADA INFO assertion status-1, LDMSD has no storage policies: len(status) == 0, passed
2023-08-10 17:00:54,979 TADA INFO assertion status-2, Get the status of a non-existing storage policy: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,979 TADA INFO assertion status-3, Get the status of a storage policy with a single producer: status is as expected, passed
2023-08-10 17:00:54,979 TADA INFO assertion status-4, Get the status of a storage policy with a single metric: status is as expected, passed
2023-08-10 17:00:54,979 TADA INFO assertion status-5, Sending strgp_status with no attributes: status is as expected, passed
2023-08-10 17:00:54,979 TADA INFO assertion status-6, Get the status of a stopped storage policy: status is as expected, passed
2023-08-10 17:00:54,979 TADA INFO assertion add-1.1, Add a new strgp -- checking the error code: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,980 TADA INFO assertion add-1.2, Add a new strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,980 TADA INFO assertion add-2, Add an existing strgp: resp['errcode'] (17) == 17, passed
2023-08-10 17:00:54,980 TADA INFO assertion prdcr_add-1, strgp_prdcr_add with an invalid regex: resp['errcode'] (2) != 0, passed
2023-08-10 17:00:54,980 TADA INFO assertion prdcr_add-2, strgp_prdcr_add to a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,980 TADA INFO assertion prdcr_add-3, strgp_prdcr_add to a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,981 TADA INFO assertion prdcr_add-4.1, strgp_prdcr_add to a strgp -- checking the error code: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,981 TADA INFO assertion prdcr_add-4.2, strgp_prdcr_add to a strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,981 TADA INFO assertion metric_add-1, strgp_metric_add to a non existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,981 TADA INFO assertion metric_add-2, strgp_metric_add to a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,981 TADA INFO assertion metric_add-3.1, strgp_metric_add to a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,981 TADA INFO assertion metric_add-3.2, strgp_metric_add to a stopped strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-1, strgp_start a non existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-2, strgp_start a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-3, strgp_start a strgp with a non-configured plugin: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-4.1, strgp_start a strgp with a producer filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-4.2, strgp_start a strgp with a producer filter -- checking the status: status is as expected, passed
2023-08-10 17:00:54,982 TADA INFO assertion start-5.1, strgp_start a strgp with a metric filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,983 TADA INFO assertion start-5.2, strgp_start a strgp with a metric filter -- checking the status: status is as expected, passed
2023-08-10 17:00:54,983 TADA INFO assertion start-6.1, strgp_start a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,983 TADA INFO assertion start-6.2, strgp_start a stopped strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,983 TADA INFO assertion start-6.3, strgp_start a stopped strgp -- checking the database: Database is not empty., passed
2023-08-10 17:00:54,983 TADA INFO assertion prdcr_del-1, strgp_prdcr_del a non existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,983 TADA INFO assertion prdcr_del-2, strgp_prdcr_del a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,983 TADA INFO assertion prdcr_del-3, strgp_prdcr_del a strgp that doesn't have the prdcr regex: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,984 TADA INFO assertion prdcr_del-4.1, strgp_prdcr_del a strgp with a producer filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,984 TADA INFO assertion prdcr_del-4.2, strgp_prdcr_del a strgp with a producer filter -- checking the status: status is as expected, passed
2023-08-10 17:00:54,984 TADA INFO assertion metric_del-1, strgp_metric_del a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,984 TADA INFO assertion metric_del-2, strgp_metric_del a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,984 TADA INFO assertion metric_del-3, strgp_metric_del a strgp that doesn't contain the metric name: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,985 TADA INFO assertion metric_del-4.1, strgp_metric_del from a strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,985 TADA INFO assertion metric_del-4.2, strgp_metric_del from a strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,985 TADA INFO assertion stop-1, strgp_stop a non existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,985 TADA INFO assertion stop-2, strgp_stop a stopped strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,985 TADA INFO assertion stop-3.1, strgp_stop a running strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,985 TADA INFO assertion stop-3.2, strgp_stop a running strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,986 TADA INFO assertion del-1, Delete a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-08-10 17:00:54,986 TADA INFO assertion del-2, Delete a running strgp: resp['errcode'] (16) == 16, passed
2023-08-10 17:00:54,986 TADA INFO assertion del-3.1, Delete a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-08-10 17:00:54,986 TADA INFO assertion del-3.2, Delete a stopped strgp -- checking the status: status is as expected, passed
2023-08-10 17:00:54,986 TADA INFO test strgp_config_cmd ended
2023-08-10 17:00:54,986 inside_cont_test INFO strgp_config_cmd: done
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/usr/lib/python3.6/site-packages/docker/api/client.py", line 261, in _raise_for_status
    response.raise_for_status()
  File "/usr/lib/python3.6/site-packages/requests/models.py", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker://localhost/v1.35/containers/ea734796d08d3d397fe5c4d9ed6317fa1f71f31077e1aaccdcf24ec1c6570a1a/exec

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.6/site-packages/docker/api/client.py", line 263, in _raise_for_status
    raise create_api_error_from_http_exception(e)
  File "/usr/lib/python3.6/site-packages/docker/errors.py", line 31, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation)
docker.errors.NotFound: 404 Client Error: Not Found ("No such container: ea734796d08d3d397fe5c4d9ed6317fa1f71f31077e1aaccdcf24ec1c6570a1a")
2023-08-10 17:01:13 INFO: ----------------------------------------------
2023-08-10 17:01:14 INFO: ======== libovis_log_test ========
2023-08-10 17:01:14 INFO: CMD: python3 libovis_log_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/libovis_log_test
2023-08-10 17:01:15,134 TADA INFO starting test `libovis_log_test`
2023-08-10 17:01:15,134 TADA INFO   test-id: 3132d4ba75c78b479cfdf180165b461a98b35e296f93fef5bd78dd57ad0fc9a7
2023-08-10 17:01:15,135 TADA INFO   test-suite: LDMSD
2023-08-10 17:01:15,135 TADA INFO   test-name: libovis_log_test
2023-08-10 17:01:15,135 TADA INFO   test-user: narate
2023-08-10 17:01:15,135 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 17:01:15,135 __main__ INFO -- Create the cluster -- 
2023-08-10 17:01:19,971 __main__ INFO -- Start daemons --
2023-08-10 17:01:22,191 TADA INFO assertion 1, Call ovis_log_init() with valid arguments: 'return_code=0' and 'liovis_log_test' in 'Thu Aug 10 17:01:21 2023:         : libovis_log_test: return_code=0
', passed
2023-08-10 17:01:23,314 TADA INFO assertion 2, Call ovis_log_init() with name = NULL: ('return_code=0' and ': :') in 'Thu Aug 10 17:01:22 2023:         : : return_code=0
', passed
2023-08-10 17:01:24,426 TADA INFO assertion 3, Call ovis_log_init() with an invalid level: 'return_code=22' in 'Thu Aug 10 17:01:23 2023:         : : return_code=22
', passed
2023-08-10 17:01:25,532 TADA INFO assertion 4, Call ovis_log_init() with an invalid mode: 'return_code=22' in 'Thu Aug 10 17:01:24 2023:         : : return_code=22
', passed
2023-08-10 17:01:26,197 TADA INFO assertion 6, Log messages to a file: 0 == ovis_log_open(/var/log/6.log) # (0), passed
2023-08-10 17:01:27,303 TADA INFO assertion 5, Log messages to stdout: 'return_code=0' in 'Thu Aug 10 17:01:26 2023:         : : return_code=0
', passed
2023-08-10 17:01:27,420 TADA INFO assertion 7, Open the log file at a non-existing path: 'Could not open the log file' in 'Thu Aug 10 17:01:27 2023:         : test: result=0
Thu Aug 10 17:01:27 2023:    ERROR: test: Could not open the log file named '/data/log/foo/7.log'
Thu Aug 10 17:01:27 2023:    ERROR: test: Failed to open the log file at /data/log/foo/7.log. Error 22
', passed
2023-08-10 17:01:27,930 TADA INFO assertion 8, Reopen the log file at another path: ovis_log_open() closes and opens the second path successfully, passed
2023-08-10 17:01:28,408 TADA INFO assertion 9, Convert 'DEBUG,INFO' integer to a string: DEBUG,INFO == DEBUG,INFO (expected), passed
2023-08-10 17:01:28,521 TADA INFO assertion 10, Convert 'DEBUG,WARNING' integer to a string: DEBUG,WARNING == DEBUG,WARNING (expected), passed
2023-08-10 17:01:28,620 TADA INFO assertion 11, Convert 'DEBUG,ERROR' integer to a string: DEBUG,ERROR == DEBUG,ERROR (expected), passed
2023-08-10 17:01:28,735 TADA INFO assertion 12, Convert 'DEBUG,CRITICAL' integer to a string: DEBUG,CRITICAL == DEBUG,CRITICAL (expected), passed
2023-08-10 17:01:28,855 TADA INFO assertion 13, Convert 'INFO,WARNING' integer to a string: INFO,WARNING == INFO,WARNING (expected), passed
2023-08-10 17:01:28,970 TADA INFO assertion 14, Convert 'INFO,ERROR' integer to a string: INFO,ERROR == INFO,ERROR (expected), passed
2023-08-10 17:01:29,076 TADA INFO assertion 15, Convert 'INFO,CRITICAL' integer to a string: INFO,CRITICAL == INFO,CRITICAL (expected), passed
2023-08-10 17:01:29,169 TADA INFO assertion 16, Convert 'WARNING,ERROR' integer to a string: WARNING,ERROR == WARNING,ERROR (expected), passed
2023-08-10 17:01:29,271 TADA INFO assertion 17, Convert 'WARNING,CRITICAL' integer to a string: WARNING,CRITICAL == WARNING,CRITICAL (expected), passed
2023-08-10 17:01:29,385 TADA INFO assertion 18, Convert 'ERROR,CRITICAL' integer to a string: ERROR,CRITICAL == ERROR,CRITICAL (expected), passed
2023-08-10 17:01:29,490 TADA INFO assertion 19, Convert 'DEBUG,INFO,WARNING' integer to a string: DEBUG,INFO,WARNING == DEBUG,INFO,WARNING (expected), passed
2023-08-10 17:01:29,596 TADA INFO assertion 20, Convert 'DEBUG,INFO,ERROR' integer to a string: DEBUG,INFO,ERROR == DEBUG,INFO,ERROR (expected), passed
2023-08-10 17:01:29,698 TADA INFO assertion 21, Convert 'DEBUG,INFO,CRITICAL' integer to a string: DEBUG,INFO,CRITICAL == DEBUG,INFO,CRITICAL (expected), passed
2023-08-10 17:01:29,801 TADA INFO assertion 22, Convert 'DEBUG,WARNING,ERROR' integer to a string: DEBUG,WARNING,ERROR == DEBUG,WARNING,ERROR (expected), passed
2023-08-10 17:01:29,915 TADA INFO assertion 23, Convert 'DEBUG,WARNING,CRITICAL' integer to a string: DEBUG,WARNING,CRITICAL == DEBUG,WARNING,CRITICAL (expected), passed
2023-08-10 17:01:30,018 TADA INFO assertion 24, Convert 'DEBUG,ERROR,CRITICAL' integer to a string: DEBUG,ERROR,CRITICAL == DEBUG,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:30,137 TADA INFO assertion 25, Convert 'INFO,WARNING,ERROR' integer to a string: INFO,WARNING,ERROR == INFO,WARNING,ERROR (expected), passed
2023-08-10 17:01:30,249 TADA INFO assertion 26, Convert 'INFO,WARNING,CRITICAL' integer to a string: INFO,WARNING,CRITICAL == INFO,WARNING,CRITICAL (expected), passed
2023-08-10 17:01:30,367 TADA INFO assertion 27, Convert 'INFO,ERROR,CRITICAL' integer to a string: INFO,ERROR,CRITICAL == INFO,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:30,486 TADA INFO assertion 28, Convert 'WARNING,ERROR,CRITICAL' integer to a string: WARNING,ERROR,CRITICAL == WARNING,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:30,605 TADA INFO assertion 29, Convert 'DEBUG,INFO,WARNING,ERROR' integer to a string: DEBUG,INFO,WARNING,ERROR == DEBUG,INFO,WARNING,ERROR (expected), passed
2023-08-10 17:01:30,718 TADA INFO assertion 30, Convert 'DEBUG,INFO,WARNING,CRITICAL' integer to a string: DEBUG,INFO,WARNING,CRITICAL == DEBUG,INFO,WARNING,CRITICAL (expected), passed
2023-08-10 17:01:30,818 TADA INFO assertion 31, Convert 'DEBUG,INFO,ERROR,CRITICAL' integer to a string: DEBUG,INFO,ERROR,CRITICAL == DEBUG,INFO,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:30,934 TADA INFO assertion 32, Convert 'DEBUG,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,WARNING,ERROR,CRITICAL == DEBUG,WARNING,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:31,038 TADA INFO assertion 33, Convert 'INFO,WARNING,ERROR,CRITICAL' integer to a string: INFO,WARNING,ERROR,CRITICAL == INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:31,142 TADA INFO assertion 34, Convert 'DEBUG,INFO,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,INFO,WARNING,ERROR,CRITICAL == DEBUG,INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-08-10 17:01:31,245 TADA INFO assertion 35, Convert 'DEBUG,' integer to a string: DEBUG, == DEBUG, (expected), passed
2023-08-10 17:01:31,362 TADA INFO assertion 36, Convert 'INFO,' integer to a string: INFO, == INFO, (expected), passed
2023-08-10 17:01:31,470 TADA INFO assertion 37, Convert 'WARNING,' integer to a string: WARNING, == WARNING, (expected), passed
2023-08-10 17:01:31,566 TADA INFO assertion 38, Convert 'ERROR,' integer to a string: ERROR, == ERROR, (expected), passed
2023-08-10 17:01:31,676 TADA INFO assertion 39, Convert 'CRITICAL,' integer to a string: CRITICAL, == CRITICAL, (expected), passed
2023-08-10 17:01:31,784 TADA INFO assertion 40, Convert an invalid integer to a level string: (null) == (null) (expected), passed
2023-08-10 17:01:31,910 TADA INFO assertion 41, Convert the 'DEBUG,INFO' to an integer: 3 == 3 (expected), passed
2023-08-10 17:01:32,026 TADA INFO assertion 42, Convert the 'DEBUG,WARNING' to an integer: 5 == 5 (expected), passed
2023-08-10 17:01:32,133 TADA INFO assertion 43, Convert the 'DEBUG,ERROR' to an integer: 9 == 9 (expected), passed
2023-08-10 17:01:32,244 TADA INFO assertion 44, Convert the 'DEBUG,CRITICAL' to an integer: 17 == 17 (expected), passed
2023-08-10 17:01:32,358 TADA INFO assertion 45, Convert the 'INFO,WARNING' to an integer: 6 == 6 (expected), passed
2023-08-10 17:01:32,466 TADA INFO assertion 46, Convert the 'INFO,ERROR' to an integer: 10 == 10 (expected), passed
2023-08-10 17:01:32,581 TADA INFO assertion 47, Convert the 'INFO,CRITICAL' to an integer: 18 == 18 (expected), passed
2023-08-10 17:01:32,691 TADA INFO assertion 48, Convert the 'WARNING,ERROR' to an integer: 12 == 12 (expected), passed
2023-08-10 17:01:32,809 TADA INFO assertion 49, Convert the 'WARNING,CRITICAL' to an integer: 20 == 20 (expected), passed
2023-08-10 17:01:32,927 TADA INFO assertion 50, Convert the 'ERROR,CRITICAL' to an integer: 24 == 24 (expected), passed
2023-08-10 17:01:33,023 TADA INFO assertion 51, Convert the 'DEBUG,INFO,WARNING' to an integer: 7 == 7 (expected), passed
2023-08-10 17:01:33,135 TADA INFO assertion 52, Convert the 'DEBUG,INFO,ERROR' to an integer: 11 == 11 (expected), passed
2023-08-10 17:01:33,241 TADA INFO assertion 53, Convert the 'DEBUG,INFO,CRITICAL' to an integer: 19 == 19 (expected), passed
2023-08-10 17:01:33,347 TADA INFO assertion 54, Convert the 'DEBUG,WARNING,ERROR' to an integer: 13 == 13 (expected), passed
2023-08-10 17:01:33,455 TADA INFO assertion 55, Convert the 'DEBUG,WARNING,CRITICAL' to an integer: 21 == 21 (expected), passed
2023-08-10 17:01:33,563 TADA INFO assertion 56, Convert the 'DEBUG,ERROR,CRITICAL' to an integer: 25 == 25 (expected), passed
2023-08-10 17:01:33,673 TADA INFO assertion 57, Convert the 'INFO,WARNING,ERROR' to an integer: 14 == 14 (expected), passed
2023-08-10 17:01:33,790 TADA INFO assertion 58, Convert the 'INFO,WARNING,CRITICAL' to an integer: 22 == 22 (expected), passed
2023-08-10 17:01:33,889 TADA INFO assertion 59, Convert the 'INFO,ERROR,CRITICAL' to an integer: 26 == 26 (expected), passed
2023-08-10 17:01:33,994 TADA INFO assertion 60, Convert the 'WARNING,ERROR,CRITICAL' to an integer: 28 == 28 (expected), passed
2023-08-10 17:01:34,101 TADA INFO assertion 61, Convert the 'DEBUG,INFO,WARNING,ERROR' to an integer: 15 == 15 (expected), passed
2023-08-10 17:01:34,211 TADA INFO assertion 62, Convert the 'DEBUG,INFO,WARNING,CRITICAL' to an integer: 23 == 23 (expected), passed
2023-08-10 17:01:34,319 TADA INFO assertion 63, Convert the 'DEBUG,INFO,ERROR,CRITICAL' to an integer: 27 == 27 (expected), passed
2023-08-10 17:01:34,425 TADA INFO assertion 64, Convert the 'DEBUG,WARNING,ERROR,CRITICAL' to an integer: 29 == 29 (expected), passed
2023-08-10 17:01:34,529 TADA INFO assertion 65, Convert the 'INFO,WARNING,ERROR,CRITICAL' to an integer: 30 == 30 (expected), passed
2023-08-10 17:01:34,646 TADA INFO assertion 66, Convert the 'DEBUG,INFO,WARNING,ERROR,CRITICAL' to an integer: 31 == 31 (expected), passed
2023-08-10 17:01:34,765 TADA INFO assertion 67, Convert the 'DEBUG,' to an integer: 1 == 1 (expected), passed
2023-08-10 17:01:34,879 TADA INFO assertion 68, Convert the 'INFO,' to an integer: 2 == 2 (expected), passed
2023-08-10 17:01:34,988 TADA INFO assertion 69, Convert the 'WARNING,' to an integer: 4 == 4 (expected), passed
2023-08-10 17:01:35,086 TADA INFO assertion 70, Convert the 'ERROR,' to an integer: 8 == 8 (expected), passed
2023-08-10 17:01:35,190 TADA INFO assertion 71, Convert the 'CRITICAL,' to an integer: 16 == 16 (expected), passed
2023-08-10 17:01:35,297 TADA INFO assertion 72, Convert the 'DEBUG' to an integer: 31 == 31 (expected), passed
2023-08-10 17:01:35,397 TADA INFO assertion 73, Convert the 'INFO' to an integer: 30 == 30 (expected), passed
2023-08-10 17:01:35,510 TADA INFO assertion 74, Convert the 'WARNING' to an integer: 28 == 28 (expected), passed
2023-08-10 17:01:35,627 TADA INFO assertion 75, Convert the 'ERROR' to an integer: 24 == 24 (expected), passed
2023-08-10 17:01:35,746 TADA INFO assertion 76, Convert the 'CRITICAL' to an integer: 16 == 16 (expected), passed
2023-08-10 17:01:35,865 TADA INFO assertion 77, Convert an invalid level string to an integer: -22 == -22 (expected), passed
2023-08-10 17:01:36,518 TADA INFO assertion 78, Verify that no messages were printed when the level is QUIET.: No messages were printed., passed
2023-08-10 17:01:36,840 TADA INFO assertion 79, Verify that messages of DEBUG,INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:37,161 TADA INFO assertion 80, Verify that messages of DEBUG,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:37,467 TADA INFO assertion 81, Verify that messages of DEBUG,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:37,792 TADA INFO assertion 82, Verify that messages of DEBUG,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:38,102 TADA INFO assertion 83, Verify that messages of INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:38,425 TADA INFO assertion 84, Verify that messages of INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:38,775 TADA INFO assertion 85, Verify that messages of INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:39,109 TADA INFO assertion 86, Verify that messages of WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:39,472 TADA INFO assertion 87, Verify that messages of WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:39,816 TADA INFO assertion 88, Verify that messages of ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:40,150 TADA INFO assertion 89, Verify that messages of DEBUG,INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:40,468 TADA INFO assertion 90, Verify that messages of DEBUG,INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:40,827 TADA INFO assertion 91, Verify that messages of DEBUG,INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:41,153 TADA INFO assertion 92, Verify that messages of DEBUG,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:41,453 TADA INFO assertion 93, Verify that messages of DEBUG,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:41,778 TADA INFO assertion 94, Verify that messages of DEBUG,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:42,122 TADA INFO assertion 95, Verify that messages of INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:42,454 TADA INFO assertion 96, Verify that messages of INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:42,787 TADA INFO assertion 97, Verify that messages of INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:43,111 TADA INFO assertion 98, Verify that messages of WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:43,436 TADA INFO assertion 99, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:43,755 TADA INFO assertion 100, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:44,079 TADA INFO assertion 101, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:44,390 TADA INFO assertion 102, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:44,702 TADA INFO assertion 103, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:45,030 TADA INFO assertion 104, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:45,334 TADA INFO assertion 105, Verify that messages of DEBUG, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:45,674 TADA INFO assertion 106, Verify that messages of INFO, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:45,990 TADA INFO assertion 107, Verify that messages of WARNING, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:46,338 TADA INFO assertion 108, Verify that messages of ERROR, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:46,678 TADA INFO assertion 109, Verify that messages of CRITICAL, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:46,977 TADA INFO assertion 110, Verify that messages of DEBUG were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:47,305 TADA INFO assertion 111, Verify that messages of INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:47,619 TADA INFO assertion 112, Verify that messages of WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:47,958 TADA INFO assertion 113, Verify that messages of ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:48,284 TADA INFO assertion 114, Verify that messages of CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:48,942 TADA INFO assertion 116, Verify that ovis_log_close() works properly: ovis_log_close() works properly., passed
2023-08-10 17:01:50,833 TADA INFO assertion 115, Verify that applications can open, rename, and reopen log files to perform log rotation.: ovis_log supports open, rename (external), and reopen., passed
2023-08-10 17:01:51,274 TADA INFO assertion 117, Test a ovis_log_register() call with valid arguments: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}], passed
2023-08-10 17:01:51,387 TADA INFO assertion 118, Test a ovis_log_register() call with NULL name: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-08-10 17:01:51,484 TADA INFO assertion 119, Test a ovis_log_register() call with NULL desc: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-08-10 17:01:51,586 TADA INFO assertion 120, Test a ovis_log_register() call with an existing subsystem: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}], passed
2023-08-10 17:01:52,234 TADA INFO assertion 122, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:52,560 TADA INFO assertion 123, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:52,884 TADA INFO assertion 124, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:53,213 TADA INFO assertion 125, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:53,529 TADA INFO assertion 126, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:53,846 TADA INFO assertion 127, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:54,162 TADA INFO assertion 128, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:54,480 TADA INFO assertion 129, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:54,811 TADA INFO assertion 130, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:55,144 TADA INFO assertion 131, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:55,472 TADA INFO assertion 132, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:55,795 TADA INFO assertion 133, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:56,141 TADA INFO assertion 134, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:56,460 TADA INFO assertion 135, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:56,800 TADA INFO assertion 136, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:57,135 TADA INFO assertion 137, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:57,454 TADA INFO assertion 138, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:57,801 TADA INFO assertion 139, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:58,143 TADA INFO assertion 140, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:58,476 TADA INFO assertion 141, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:58,807 TADA INFO assertion 142, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:59,130 TADA INFO assertion 143, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:59,430 TADA INFO assertion 144, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:01:59,746 TADA INFO assertion 145, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:00,074 TADA INFO assertion 146, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:00,401 TADA INFO assertion 147, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:00,722 TADA INFO assertion 148, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:01,043 TADA INFO assertion 149, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:01,398 TADA INFO assertion 150, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:01,721 TADA INFO assertion 151, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:02,038 TADA INFO assertion 152, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:02,358 TADA INFO assertion 153, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:02,675 TADA INFO assertion 154, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:02,995 TADA INFO assertion 155, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:03,308 TADA INFO assertion 156, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:03,617 TADA INFO assertion 157, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:03,968 TADA INFO assertion 158, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:04,281 TADA INFO assertion 159, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:04,583 TADA INFO assertion 160, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:04,933 TADA INFO assertion 161, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:05,268 TADA INFO assertion 162, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:05,585 TADA INFO assertion 163, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:05,923 TADA INFO assertion 164, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:06,248 TADA INFO assertion 165, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:06,572 TADA INFO assertion 166, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:06,903 TADA INFO assertion 167, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:07,225 TADA INFO assertion 168, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:07,544 TADA INFO assertion 169, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:07,895 TADA INFO assertion 170, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:08,207 TADA INFO assertion 171, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:08,539 TADA INFO assertion 172, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:08,884 TADA INFO assertion 173, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:09,206 TADA INFO assertion 174, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:09,510 TADA INFO assertion 175, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:09,848 TADA INFO assertion 176, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:10,164 TADA INFO assertion 177, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:10,487 TADA INFO assertion 178, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:10,834 TADA INFO assertion 179, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:11,132 TADA INFO assertion 180, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:11,453 TADA INFO assertion 181, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:11,801 TADA INFO assertion 182, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:12,122 TADA INFO assertion 183, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:12,433 TADA INFO assertion 184, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:12,764 TADA INFO assertion 185, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:13,098 TADA INFO assertion 186, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:13,413 TADA INFO assertion 187, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:13,722 TADA INFO assertion 188, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:14,058 TADA INFO assertion 189, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:14,400 TADA INFO assertion 190, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:14,722 TADA INFO assertion 191, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:15,061 TADA INFO assertion 192, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:15,392 TADA INFO assertion 193, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-08-10 17:02:16,052 TADA INFO assertion 195, Verify that ovis_log_set_level_by_regex() returns an error when the given regular expression string is invalid.: 'result=22' in 'Thu Aug 10 17:02:15 2023:         : test: result=22
', passed
2023-08-10 17:02:16,367 TADA INFO assertion 194, Verify that ovis_log_set_level_by_regex() returns ENOENT when the given regular expression string doesn't match any logs.: 'result=2' in '', passed
2023-08-10 17:02:16,695 TADA INFO assertion 196, Verify that ovis_log_set_level_by_regex() sets the level of the matched log subsystems to the given value.: ('config:' in 'Thu Aug 10 17:02:16 2023:         : config: ALWAYS' and (('CRITICAL' in 'Thu Aug 10 17:02:16 2023:         : config: ALWAYS') or ('ALWAYS' in Thu Aug 10 17:02:16 2023:         : config: ALWAYS)), passed
2023-08-10 17:02:17,331 TADA INFO assertion 197, Verify that ovis_log_list() works correctly.: '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]' == '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]', passed
2023-08-10 17:02:17,331 TADA INFO test libovis_log_test ended
2023-08-10 17:02:28 INFO: ----------------------------------------------
2023-08-10 17:02:29 INFO: ======== ldmsd_long_config_test ========
2023-08-10 17:02:29 INFO: CMD: python3 ldmsd_long_config_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldmsd_long_config_test
2023-08-10 17:02:29,787 TADA INFO starting test `ldmsd_long_config_line`
2023-08-10 17:02:29,787 TADA INFO   test-id: 06fdcf42bace1e2ef8806c8944c00af5e104b5a7a939ba1c523792d250c97bc7
2023-08-10 17:02:29,787 TADA INFO   test-suite: LDMSD
2023-08-10 17:02:29,787 TADA INFO   test-name: ldmsd_long_config_line
2023-08-10 17:02:29,787 TADA INFO   test-user: narate
2023-08-10 17:02:29,787 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 17:02:29,788 __main__ INFO ---Get or create the cluster --
2023-08-10 17:02:37,615 __main__ INFO --- Start daemons ---
2023-08-10 17:02:54,316 TADA INFO assertion 1, LDMSD correctly processes a config line in a config file: LDMSD processed the long config line in the config file correctly., passed
2023-08-10 17:02:54,834 TADA INFO assertion 2, LDMSD correctly handle a config line from ldmsd_controller: LDMSD receives the correct message from ldmsd_controller., passed
2023-08-10 17:02:55,514 TADA INFO assertion 3, LDMSD correctly handle a config line from ldmsctl: LDMSD receives the correct message from ldmsctl., passed
2023-08-10 17:02:55,515 TADA INFO test ldmsd_long_config_line ended
2023-08-10 17:03:07 INFO: ----------------------------------------------
2023-08-10 17:03:08 INFO: ======== ldms_rail_test ========
2023-08-10 17:03:08 INFO: CMD: python3 ldms_rail_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_rail_test
2023-08-10 17:03:09,132 TADA INFO starting test `ldms_rail_test`
2023-08-10 17:03:09,132 TADA INFO   test-id: 95e00495ccfb26e1379399a6d6e86843274c0afcc6473a4325cf235416c4cbba
2023-08-10 17:03:09,132 TADA INFO   test-suite: LDMSD
2023-08-10 17:03:09,132 TADA INFO   test-name: ldms_rail_test
2023-08-10 17:03:09,132 TADA INFO   test-user: narate
2023-08-10 17:03:09,132 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 17:03:09,133 __main__ INFO -- Get or create the cluster --
2023-08-10 17:03:16,328 __main__ INFO -- Start daemons --
2023-08-10 17:03:21,155 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 17:03:23,157 __main__ INFO start ldms_rail_server.py and ldms_rail_client.py interactive sessions
2023-08-10 17:03:26,176 TADA INFO assertion 1, Start interactive LDMS server: OK, passed
2023-08-10 17:03:29,195 TADA INFO assertion 2, Start interactive LDMS client: OK, passed
2023-08-10 17:03:32,800 TADA INFO assertion 3, Client rail has 8 endpoints on 8 thread pools: OK, passed
2023-08-10 17:03:36,405 TADA INFO assertion 4, Server rail has 8 endpoints on 8 thread pools: OK, passed
2023-08-10 17:03:40,010 TADA INFO assertion 5, Sets on client are processed by different threads: OK, passed
2023-08-10 17:03:43,615 TADA INFO assertion 6, Verify sets on the client: OK, passed
2023-08-10 17:03:46,633 TADA INFO assertion 7, Start interactive LDMS client2: OK, passed
2023-08-10 17:03:50,237 TADA INFO assertion 8, Client2 rail has 8 endpoints on 4 thread pools: OK, passed
2023-08-10 17:03:53,256 TADA INFO assertion 9, Client3 (wrong auth) cannot connect: OK, passed
2023-08-10 17:03:56,274 TADA INFO assertion 10, Start interactive client4 (for push mode): OK, passed
2023-08-10 17:03:56,275 __main__ INFO waiting push ...
2023-08-10 17:03:58,277 __main__ INFO server: sampling new data (2)
2023-08-10 17:04:02,883 __main__ INFO client4: set pushes received
2023-08-10 17:04:02,883 __main__ INFO client4: verifying data in sets
2023-08-10 17:04:06,488 __main__ INFO client4: verifying threads-sets-endpoints spread
2023-08-10 17:04:17,302 TADA INFO assertion 11, Client4 got push callback from the corresponding thread: OK, passed
2023-08-10 17:04:20,320 TADA INFO assertion 12, Client5 started (for clean-up path test): OK, passed
2023-08-10 17:04:20,320 __main__ INFO xprt close by client1
2023-08-10 17:04:33,136 TADA INFO assertion 13, Active-side close: client1 clean up: OK, passed
2023-08-10 17:04:36,741 TADA INFO assertion 14, Active-side close: server-side clean up: OK, passed
2023-08-10 17:04:53,161 TADA INFO assertion 15, Passive-side close: client2 clean up: OK, passed
2023-08-10 17:04:53,161 TADA INFO assertion 16, Passive-side close: server-side clean up: OK, passed
2023-08-10 17:04:58,768 TADA INFO assertion 17, Active-side term: server-side clean up: OK, passed
2023-08-10 17:05:05,977 TADA INFO assertion 18, Passive-side term: client5 clean up: OK, passed
2023-08-10 17:05:26,444 TADA INFO assertion 19, server -> client overspending send: error message verified, passed
2023-08-10 17:05:37,257 TADA INFO assertion 20, client -> server overspending send: error message verified, passed
2023-08-10 17:05:40,861 TADA INFO assertion 21, verify send credits on the server: OK, passed
2023-08-10 17:05:44,466 TADA INFO assertion 22, verify send credits on the client: OK, passed
2023-08-10 17:05:52,577 TADA INFO assertion 23, server unblock, verify recv data: recv data verified, passed
2023-08-10 17:06:00,686 TADA INFO assertion 24, client unblock, verify recv data: recv data verified, passed
2023-08-10 17:06:04,291 TADA INFO assertion 25, verify send credits on the server: OK, passed
2023-08-10 17:06:07,896 TADA INFO assertion 26, verify send credits on the client: OK, passed
2023-08-10 17:06:11,501 TADA INFO assertion 27, server -> client send after credited back: OK, passed
2023-08-10 17:06:15,106 TADA INFO assertion 28, client -> server send after credited back: OK, passed
2023-08-10 17:06:18,711 TADA INFO assertion 29, verify send credits on the server: OK, passed
2023-08-10 17:06:22,316 TADA INFO assertion 30, verify send credits on the client: OK, passed
2023-08-10 17:06:25,921 TADA INFO assertion 31, server unblock, verify recv data: OK, passed
2023-08-10 17:06:29,526 TADA INFO assertion 32, client unblock, verify recv data: OK, passed
2023-08-10 17:06:33,131 TADA INFO assertion 33, verify send credits on the server: OK, passed
2023-08-10 17:06:36,736 TADA INFO assertion 34, verify send credits on the client: OK, passed
2023-08-10 17:06:40,341 TADA INFO assertion 35, verify send-credit deposits on the server: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-08-10 17:06:43,946 TADA INFO assertion 36, verify send-credit deposits on the client: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-08-10 17:06:43,947 TADA INFO test ldms_rail_test ended
2023-08-10 17:06:56 INFO: ----------------------------------------------
2023-08-10 17:06:56 INFO: ======== ldms_stream_test ========
2023-08-10 17:06:56 INFO: CMD: python3 ldms_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-08-10-155832/data/ldms_stream_test
2023-08-10 17:06:57,612 TADA INFO starting test `ldms_stream_test`
2023-08-10 17:06:57,613 TADA INFO   test-id: f0fcdc58f80f4ec2a4af87f3027b9cdb0dd5e68746ff127c3260600832388285
2023-08-10 17:06:57,613 TADA INFO   test-suite: LDMSD
2023-08-10 17:06:57,613 TADA INFO   test-name: ldms_stream_test
2023-08-10 17:06:57,613 TADA INFO   test-user: narate
2023-08-10 17:06:57,613 TADA INFO   commit-id: 3b206c99fefb698222470a58fbc89d5c9df5ac75
2023-08-10 17:06:57,613 __main__ INFO -- Get or create the cluster --
2023-08-10 17:07:15,510 __main__ INFO -- Adding 'foo' and 'bar' users --
2023-08-10 17:07:25,371 __main__ INFO -- Start daemons --
2023-08-10 17:07:37,860 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-08-10 17:07:39,862 __main__ INFO start interactive stream servers
2023-08-10 17:07:39,863 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-7 
2023-08-10 17:07:42,881 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-6 
2023-08-10 17:07:45,899 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-5 
2023-08-10 17:07:48,920 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-4 
2023-08-10 17:07:51,938 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-3 
2023-08-10 17:07:54,963 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-2 
2023-08-10 17:07:57,985 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-1 
2023-08-10 17:08:01,005 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-4 
2023-08-10 17:08:04,525 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-5 
2023-08-10 17:08:08,045 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-6 
2023-08-10 17:08:11,564 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-7 
2023-08-10 17:08:15,084 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-4 as foo
2023-08-10 17:08:18,605 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-3b206c9-node-4 as bar
2023-08-10 17:08:22,125 __main__ INFO starting /tada-src/python/ldms_stream_client.py on narate-ldms_stream_test-3b206c9-node-8 as foo
2023-08-10 17:08:25,644 TADA INFO assertion 1, Publishing oversize data results in an error: checking..., passed
2023-08-10 17:08:26,156 __main__ INFO getting data from srv1
2023-08-10 17:08:28,662 __main__ INFO getting data from srv2
2023-08-10 17:08:31,169 __main__ INFO getting data from srv3
2023-08-10 17:08:33,675 __main__ INFO getting data from srv4
2023-08-10 17:08:36,181 __main__ INFO getting data from srv5
2023-08-10 17:08:38,687 __main__ INFO getting data from srv6
2023-08-10 17:08:41,193 __main__ INFO getting data from srv7
2023-08-10 17:08:43,698 __main__ INFO getting data from cli8foo
2023-08-10 17:08:46,205 TADA INFO assertion 2, JSON support (l3-stream): client data verified, passed
2023-08-10 17:08:46,205 __main__ INFO publishing 'four' on l3-stream by pub4
2023-08-10 17:08:46,708 __main__ INFO publishing 'five' on l3-stream by pub5
2023-08-10 17:08:47,210 __main__ INFO publishing 'six' on l3-stream by pub6
2023-08-10 17:08:47,713 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-08-10 17:08:48,216 TADA INFO assertion 301, send-credit taken: credits: [114, 128, 128, 128], passed
2023-08-10 17:08:48,216 __main__ INFO obtaining all client data (0)
2023-08-10 17:08:48,216 __main__ INFO getting data from srv1
2023-08-10 17:08:50,723 __main__ INFO getting data from srv2
2023-08-10 17:08:53,229 __main__ INFO getting data from srv3
2023-08-10 17:08:55,735 __main__ INFO getting data from srv4
2023-08-10 17:08:58,242 __main__ INFO getting data from srv5
2023-08-10 17:09:00,748 __main__ INFO getting data from srv6
2023-08-10 17:09:03,255 __main__ INFO getting data from srv7
2023-08-10 17:09:05,761 __main__ INFO getting data from cli8foo
2023-08-10 17:09:08,267 __main__ INFO obtaining all client data (1)
2023-08-10 17:09:08,268 __main__ INFO getting data from srv1
2023-08-10 17:09:10,774 __main__ INFO getting data from srv2
2023-08-10 17:09:13,280 __main__ INFO getting data from srv3
2023-08-10 17:09:15,786 __main__ INFO getting data from srv4
2023-08-10 17:09:18,292 __main__ INFO getting data from srv5
2023-08-10 17:09:20,798 __main__ INFO getting data from srv6
2023-08-10 17:09:23,303 __main__ INFO getting data from srv7
2023-08-10 17:09:25,809 __main__ INFO getting data from cli8foo
2023-08-10 17:09:28,316 __main__ INFO obtaining all client data (2)
2023-08-10 17:09:28,316 __main__ INFO getting data from srv1
2023-08-10 17:09:30,822 __main__ INFO getting data from srv2
2023-08-10 17:09:33,328 __main__ INFO getting data from srv3
2023-08-10 17:09:35,834 __main__ INFO getting data from srv4
2023-08-10 17:09:38,339 __main__ INFO getting data from srv5
2023-08-10 17:09:40,845 __main__ INFO getting data from srv6
2023-08-10 17:09:43,351 __main__ INFO getting data from srv7
2023-08-10 17:09:45,856 __main__ INFO getting data from cli8foo
2023-08-10 17:09:48,362 __main__ INFO obtaining all client data (3)
2023-08-10 17:09:48,363 __main__ INFO getting data from srv1
2023-08-10 17:09:50,869 __main__ INFO getting data from srv2
2023-08-10 17:09:53,375 __main__ INFO getting data from srv3
2023-08-10 17:09:55,880 __main__ INFO getting data from srv4
2023-08-10 17:09:58,386 __main__ INFO getting data from srv5
2023-08-10 17:10:00,891 __main__ INFO getting data from srv6
2023-08-10 17:10:03,397 __main__ INFO getting data from srv7
2023-08-10 17:10:05,903 __main__ INFO getting data from cli8foo
2023-08-10 17:10:08,911 TADA INFO assertion 302, send-credit returned: credits: [128, 128, 128, 128], passed
2023-08-10 17:10:08,911 TADA INFO assertion 303, stream delivery spread among rails: tids: {208, 209, 206, 207}, passed
2023-08-10 17:10:08,913 TADA INFO assertion 3, l3-stream delivery: client data verified, passed
2023-08-10 17:10:08,913 __main__ INFO publishing 'four' on l2-stream by pub4
2023-08-10 17:10:09,415 __main__ INFO publishing 'five' on l2-stream by pub5
2023-08-10 17:10:09,917 __main__ INFO publishing 'six' on l2-stream by pub6
2023-08-10 17:10:10,419 __main__ INFO publishing 'seven' on l2-stream by pub7
2023-08-10 17:10:10,922 __main__ INFO obtaining all client data (0)
2023-08-10 17:10:10,922 __main__ INFO getting data from srv1
2023-08-10 17:10:13,428 __main__ INFO getting data from srv2
2023-08-10 17:10:15,935 __main__ INFO getting data from srv3
2023-08-10 17:10:18,441 __main__ INFO getting data from srv4
2023-08-10 17:10:20,947 __main__ INFO getting data from srv5
2023-08-10 17:10:23,454 __main__ INFO getting data from srv6
2023-08-10 17:10:25,960 __main__ INFO getting data from srv7
2023-08-10 17:10:28,466 __main__ INFO getting data from cli8foo
2023-08-10 17:10:30,973 __main__ INFO obtaining all client data (1)
2023-08-10 17:10:30,973 __main__ INFO getting data from srv1
2023-08-10 17:10:33,479 __main__ INFO getting data from srv2
2023-08-10 17:10:35,986 __main__ INFO getting data from srv3
2023-08-10 17:10:38,492 __main__ INFO getting data from srv4
2023-08-10 17:10:40,998 __main__ INFO getting data from srv5
2023-08-10 17:10:43,504 __main__ INFO getting data from srv6
2023-08-10 17:10:46,009 __main__ INFO getting data from srv7
2023-08-10 17:10:48,515 __main__ INFO getting data from cli8foo
2023-08-10 17:10:51,023 TADA INFO assertion 4, l2-stream delivery: client data verified, passed
2023-08-10 17:10:51,023 __main__ INFO publishing 'four' on l1-stream by pub4
2023-08-10 17:10:51,524 __main__ INFO publishing 'five' on l1-stream by pub5
2023-08-10 17:10:52,026 __main__ INFO publishing 'six' on l1-stream by pub6
2023-08-10 17:10:52,529 __main__ INFO publishing 'seven' on l1-stream by pub7
2023-08-10 17:10:53,031 __main__ INFO obtaining all client data (0)
2023-08-10 17:10:53,032 __main__ INFO getting data from srv1
2023-08-10 17:10:55,538 __main__ INFO getting data from srv2
2023-08-10 17:10:58,044 __main__ INFO getting data from srv3
2023-08-10 17:11:00,550 __main__ INFO getting data from srv4
2023-08-10 17:11:03,057 __main__ INFO getting data from srv5
2023-08-10 17:11:05,563 __main__ INFO getting data from srv6
2023-08-10 17:11:08,069 __main__ INFO getting data from srv7
2023-08-10 17:11:10,576 __main__ INFO getting data from cli8foo
2023-08-10 17:11:13,082 __main__ INFO obtaining all client data (1)
2023-08-10 17:11:13,082 __main__ INFO getting data from srv1
2023-08-10 17:11:15,588 __main__ INFO getting data from srv2
2023-08-10 17:11:18,094 __main__ INFO getting data from srv3
2023-08-10 17:11:20,600 __main__ INFO getting data from srv4
2023-08-10 17:11:23,106 __main__ INFO getting data from srv5
2023-08-10 17:11:25,611 __main__ INFO getting data from srv6
2023-08-10 17:11:28,117 __main__ INFO getting data from srv7
2023-08-10 17:11:30,623 __main__ INFO getting data from cli8foo
2023-08-10 17:11:33,130 TADA INFO assertion 5, l1-stream delivery: client data verified, passed
2023-08-10 17:11:33,130 __main__ INFO publishing 'four' on x-stream by pub4
2023-08-10 17:11:33,632 __main__ INFO publishing 'five' on x-stream by pub5
2023-08-10 17:11:34,133 __main__ INFO publishing 'six' on x-stream by pub6
2023-08-10 17:11:34,636 __main__ INFO publishing 'seven' on x-stream by pub7
2023-08-10 17:11:35,138 __main__ INFO obtaining all client data (0)
2023-08-10 17:11:35,138 __main__ INFO getting data from srv1
2023-08-10 17:11:37,644 __main__ INFO getting data from srv2
2023-08-10 17:11:40,150 __main__ INFO getting data from srv3
2023-08-10 17:11:42,656 __main__ INFO getting data from srv4
2023-08-10 17:11:45,162 __main__ INFO getting data from srv5
2023-08-10 17:11:47,668 __main__ INFO getting data from srv6
2023-08-10 17:11:50,175 __main__ INFO getting data from srv7
2023-08-10 17:11:52,681 __main__ INFO getting data from cli8foo
2023-08-10 17:11:55,187 __main__ INFO obtaining all client data (1)
2023-08-10 17:11:55,187 __main__ INFO getting data from srv1
2023-08-10 17:11:57,693 __main__ INFO getting data from srv2
2023-08-10 17:12:00,199 __main__ INFO getting data from srv3
2023-08-10 17:12:02,705 __main__ INFO getting data from srv4
2023-08-10 17:12:05,211 __main__ INFO getting data from srv5
2023-08-10 17:12:07,717 __main__ INFO getting data from srv6
2023-08-10 17:12:10,223 __main__ INFO getting data from srv7
2023-08-10 17:12:12,729 __main__ INFO getting data from cli8foo
2023-08-10 17:12:15,235 TADA INFO assertion 6, x-stream delivery: client data verified, passed
2023-08-10 17:12:15,236 __main__ INFO publishing 'four' on nada by pub4
2023-08-10 17:12:15,737 __main__ INFO publishing 'five' on nada by pub5
2023-08-10 17:12:16,239 __main__ INFO publishing 'six' on nada by pub6
2023-08-10 17:12:16,740 __main__ INFO publishing 'seven' on nada by pub7
2023-08-10 17:12:17,242 __main__ INFO obtaining all client data (0)
2023-08-10 17:12:17,242 __main__ INFO getting data from srv1
2023-08-10 17:12:19,748 __main__ INFO getting data from srv2
2023-08-10 17:12:22,253 __main__ INFO getting data from srv3
2023-08-10 17:12:24,759 __main__ INFO getting data from srv4
2023-08-10 17:12:27,266 __main__ INFO getting data from srv5
2023-08-10 17:12:29,772 __main__ INFO getting data from srv6
2023-08-10 17:12:32,278 __main__ INFO getting data from srv7
2023-08-10 17:12:34,784 __main__ INFO getting data from cli8foo
2023-08-10 17:12:37,290 __main__ INFO obtaining all client data (1)
2023-08-10 17:12:37,290 __main__ INFO getting data from srv1
2023-08-10 17:12:39,796 __main__ INFO getting data from srv2
2023-08-10 17:12:42,301 __main__ INFO getting data from srv3
2023-08-10 17:12:44,807 __main__ INFO getting data from srv4
2023-08-10 17:12:47,313 __main__ INFO getting data from srv5
2023-08-10 17:12:49,818 __main__ INFO getting data from srv6
2023-08-10 17:12:52,324 __main__ INFO getting data from srv7
2023-08-10 17:12:54,830 __main__ INFO getting data from cli8foo
2023-08-10 17:12:57,336 TADA INFO assertion 7, nada delivery: client data verified, passed
2023-08-10 17:12:57,337 __main__ INFO publishing 'four' on l3-stream by pub4 (0400)
2023-08-10 17:12:57,839 __main__ INFO publishing 'five' on l3-stream by pub5 (0400)
2023-08-10 17:12:58,341 __main__ INFO publishing 'six' on l3-stream by pub6 (0400)
2023-08-10 17:12:58,844 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400)
2023-08-10 17:12:59,347 __main__ INFO obtaining all client data (0)
2023-08-10 17:12:59,347 __main__ INFO getting data from srv1
2023-08-10 17:13:01,853 __main__ INFO getting data from srv2
2023-08-10 17:13:04,360 __main__ INFO getting data from srv3
2023-08-10 17:13:06,866 __main__ INFO getting data from srv4
2023-08-10 17:13:09,372 __main__ INFO getting data from srv5
2023-08-10 17:13:11,879 __main__ INFO getting data from srv6
2023-08-10 17:13:14,385 __main__ INFO getting data from srv7
2023-08-10 17:13:16,891 __main__ INFO getting data from cli8foo
2023-08-10 17:13:19,397 __main__ INFO obtaining all client data (1)
2023-08-10 17:13:19,397 __main__ INFO getting data from srv1
2023-08-10 17:13:21,903 __main__ INFO getting data from srv2
2023-08-10 17:13:24,409 __main__ INFO getting data from srv3
2023-08-10 17:13:26,916 __main__ INFO getting data from srv4
2023-08-10 17:13:29,421 __main__ INFO getting data from srv5
2023-08-10 17:13:31,927 __main__ INFO getting data from srv6
2023-08-10 17:13:34,433 __main__ INFO getting data from srv7
2023-08-10 17:13:36,939 __main__ INFO getting data from cli8foo
2023-08-10 17:13:39,445 __main__ INFO obtaining all client data (2)
2023-08-10 17:13:39,445 __main__ INFO getting data from srv1
2023-08-10 17:13:41,951 __main__ INFO getting data from srv2
2023-08-10 17:13:44,457 __main__ INFO getting data from srv3
2023-08-10 17:13:46,962 __main__ INFO getting data from srv4
2023-08-10 17:13:49,468 __main__ INFO getting data from srv5
2023-08-10 17:13:51,974 __main__ INFO getting data from srv6
2023-08-10 17:13:54,479 __main__ INFO getting data from srv7
2023-08-10 17:13:56,985 __main__ INFO getting data from cli8foo
2023-08-10 17:13:59,491 __main__ INFO obtaining all client data (3)
2023-08-10 17:13:59,491 __main__ INFO getting data from srv1
2023-08-10 17:14:01,997 __main__ INFO getting data from srv2
2023-08-10 17:14:04,503 __main__ INFO getting data from srv3
2023-08-10 17:14:07,009 __main__ INFO getting data from srv4
2023-08-10 17:14:09,515 __main__ INFO getting data from srv5
2023-08-10 17:14:12,021 __main__ INFO getting data from srv6
2023-08-10 17:14:14,526 __main__ INFO getting data from srv7
2023-08-10 17:14:17,032 __main__ INFO getting data from cli8foo
2023-08-10 17:14:19,539 TADA INFO assertion 8, l3-stream by 'root' with 0400 permission: client data verified, passed
2023-08-10 17:14:19,539 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as foo
2023-08-10 17:14:20,042 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as foo
2023-08-10 17:14:20,544 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as foo
2023-08-10 17:14:21,047 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as foo
2023-08-10 17:14:21,550 __main__ INFO obtaining all client data (0)
2023-08-10 17:14:21,550 __main__ INFO getting data from srv1
2023-08-10 17:14:24,056 __main__ INFO getting data from srv2
2023-08-10 17:14:26,562 __main__ INFO getting data from srv3
2023-08-10 17:14:29,069 __main__ INFO getting data from srv4
2023-08-10 17:14:31,575 __main__ INFO getting data from srv5
2023-08-10 17:14:34,082 __main__ INFO getting data from srv6
2023-08-10 17:14:36,588 __main__ INFO getting data from srv7
2023-08-10 17:14:39,094 __main__ INFO getting data from cli8foo
2023-08-10 17:14:41,601 __main__ INFO obtaining all client data (1)
2023-08-10 17:14:41,601 __main__ INFO getting data from srv1
2023-08-10 17:14:44,107 __main__ INFO getting data from srv2
2023-08-10 17:14:46,613 __main__ INFO getting data from srv3
2023-08-10 17:14:49,119 __main__ INFO getting data from srv4
2023-08-10 17:14:51,625 __main__ INFO getting data from srv5
2023-08-10 17:14:54,131 __main__ INFO getting data from srv6
2023-08-10 17:14:56,636 __main__ INFO getting data from srv7
2023-08-10 17:14:59,142 __main__ INFO getting data from cli8foo
2023-08-10 17:15:01,648 __main__ INFO obtaining all client data (2)
2023-08-10 17:15:01,649 __main__ INFO getting data from srv1
2023-08-10 17:15:04,155 __main__ INFO getting data from srv2
2023-08-10 17:15:06,661 __main__ INFO getting data from srv3
2023-08-10 17:15:09,166 __main__ INFO getting data from srv4
2023-08-10 17:15:11,672 __main__ INFO getting data from srv5
2023-08-10 17:15:14,178 __main__ INFO getting data from srv6
2023-08-10 17:15:16,684 __main__ INFO getting data from srv7
2023-08-10 17:15:19,189 __main__ INFO getting data from cli8foo
2023-08-10 17:15:21,695 __main__ INFO obtaining all client data (3)
2023-08-10 17:15:21,695 __main__ INFO getting data from srv1
2023-08-10 17:15:24,202 __main__ INFO getting data from srv2
2023-08-10 17:15:26,707 __main__ INFO getting data from srv3
2023-08-10 17:15:29,213 __main__ INFO getting data from srv4
2023-08-10 17:15:31,719 __main__ INFO getting data from srv5
2023-08-10 17:15:34,224 __main__ INFO getting data from srv6
2023-08-10 17:15:36,730 __main__ INFO getting data from srv7
2023-08-10 17:15:39,236 __main__ INFO getting data from cli8foo
2023-08-10 17:15:41,743 TADA INFO assertion 9, l3-stream by 'root' as 'foo' with 0400 permission: client data verified, passed
2023-08-10 17:15:41,743 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as bar
2023-08-10 17:15:42,246 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as bar
2023-08-10 17:15:42,748 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as bar
2023-08-10 17:15:43,251 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as bar
2023-08-10 17:15:43,753 __main__ INFO obtaining all client data (0)
2023-08-10 17:15:43,753 __main__ INFO getting data from srv1
2023-08-10 17:15:46,260 __main__ INFO getting data from srv2
2023-08-10 17:15:48,766 __main__ INFO getting data from srv3
2023-08-10 17:15:51,272 __main__ INFO getting data from srv4
2023-08-10 17:15:53,779 __main__ INFO getting data from srv5
2023-08-10 17:15:56,285 __main__ INFO getting data from srv6
2023-08-10 17:15:58,791 __main__ INFO getting data from srv7
2023-08-10 17:16:01,297 __main__ INFO getting data from cli8foo
2023-08-10 17:16:03,803 __main__ INFO obtaining all client data (1)
2023-08-10 17:16:03,803 __main__ INFO getting data from srv1
2023-08-10 17:16:06,309 __main__ INFO getting data from srv2
2023-08-10 17:16:08,816 __main__ INFO getting data from srv3
2023-08-10 17:16:11,322 __main__ INFO getting data from srv4
2023-08-10 17:16:13,828 __main__ INFO getting data from srv5
2023-08-10 17:16:16,334 __main__ INFO getting data from srv6
2023-08-10 17:16:18,839 __main__ INFO getting data from srv7
2023-08-10 17:16:21,345 __main__ INFO getting data from cli8foo
2023-08-10 17:16:23,851 __main__ INFO obtaining all client data (2)
2023-08-10 17:16:23,851 __main__ INFO getting data from srv1
2023-08-10 17:16:26,357 __main__ INFO getting data from srv2
2023-08-10 17:16:28,863 __main__ INFO getting data from srv3
2023-08-10 17:16:31,369 __main__ INFO getting data from srv4
2023-08-10 17:16:33,874 __main__ INFO getting data from srv5
2023-08-10 17:16:36,380 __main__ INFO getting data from srv6
2023-08-10 17:16:38,886 __main__ INFO getting data from srv7
2023-08-10 17:16:41,391 __main__ INFO getting data from cli8foo
2023-08-10 17:16:43,897 __main__ INFO obtaining all client data (3)
2023-08-10 17:16:43,897 __main__ INFO getting data from srv1
2023-08-10 17:16:46,403 __main__ INFO getting data from srv2
2023-08-10 17:16:48,909 __main__ INFO getting data from srv3
2023-08-10 17:16:51,414 __main__ INFO getting data from srv4
2023-08-10 17:16:53,920 __main__ INFO getting data from srv5
2023-08-10 17:16:56,426 __main__ INFO getting data from srv6
2023-08-10 17:16:58,931 __main__ INFO getting data from srv7
2023-08-10 17:17:01,438 __main__ INFO getting data from cli8foo
2023-08-10 17:17:03,945 TADA INFO assertion 10, l3-stream by 'root' as 'bar' with 0400 permission: client data verified, passed
2023-08-10 17:17:03,945 __main__ INFO publishing 'four' on l3-stream by pub4 (0440) root as bar
2023-08-10 17:17:04,447 __main__ INFO publishing 'five' on l3-stream by pub5 (0440) root as bar
2023-08-10 17:17:04,950 __main__ INFO publishing 'six' on l3-stream by pub6 (0440) root as bar
2023-08-10 17:17:05,452 __main__ INFO publishing 'seven' on l3-stream by pub7 (0440) root as bar
2023-08-10 17:17:05,955 __main__ INFO obtaining all client data (0)
2023-08-10 17:17:05,955 __main__ INFO getting data from srv1
2023-08-10 17:17:08,462 __main__ INFO getting data from srv2
2023-08-10 17:17:10,968 __main__ INFO getting data from srv3
2023-08-10 17:17:13,474 __main__ INFO getting data from srv4
2023-08-10 17:17:15,981 __main__ INFO getting data from srv5
2023-08-10 17:17:18,487 __main__ INFO getting data from srv6
2023-08-10 17:17:20,993 __main__ INFO getting data from srv7
2023-08-10 17:17:23,500 __main__ INFO getting data from cli8foo
2023-08-10 17:17:26,005 __main__ INFO obtaining all client data (1)
2023-08-10 17:17:26,006 __main__ INFO getting data from srv1
2023-08-10 17:17:28,512 __main__ INFO getting data from srv2
2023-08-10 17:17:31,018 __main__ INFO getting data from srv3
2023-08-10 17:17:33,524 __main__ INFO getting data from srv4
2023-08-10 17:17:36,030 __main__ INFO getting data from srv5
2023-08-10 17:17:38,536 __main__ INFO getting data from srv6
2023-08-10 17:17:41,041 __main__ INFO getting data from srv7
2023-08-10 17:17:43,547 __main__ INFO getting data from cli8foo
2023-08-10 17:17:46,052 __main__ INFO obtaining all client data (2)
2023-08-10 17:17:46,053 __main__ INFO getting data from srv1
2023-08-10 17:17:48,559 __main__ INFO getting data from srv2
2023-08-10 17:17:51,065 __main__ INFO getting data from srv3
2023-08-10 17:17:53,570 __main__ INFO getting data from srv4
2023-08-10 17:17:56,076 __main__ INFO getting data from srv5
2023-08-10 17:17:58,581 __main__ INFO getting data from srv6
2023-08-10 17:18:01,087 __main__ INFO getting data from srv7
2023-08-10 17:18:03,593 __main__ INFO getting data from cli8foo
2023-08-10 17:18:06,099 __main__ INFO obtaining all client data (3)
2023-08-10 17:18:06,099 __main__ INFO getting data from srv1
2023-08-10 17:18:08,605 __main__ INFO getting data from srv2
2023-08-10 17:18:11,111 __main__ INFO getting data from srv3
2023-08-10 17:18:13,617 __main__ INFO getting data from srv4
2023-08-10 17:18:16,123 __main__ INFO getting data from srv5
2023-08-10 17:18:18,628 __main__ INFO getting data from srv6
2023-08-10 17:18:21,134 __main__ INFO getting data from srv7
2023-08-10 17:18:23,639 __main__ INFO getting data from cli8foo
2023-08-10 17:18:26,146 TADA INFO assertion 11, l3-stream by 'root' as 'bar' with 0440 permission: client data verified, passed
2023-08-10 17:18:26,648 TADA INFO assertion 12, l3-stream by 'foo' as 'bar' results in an error: checking..., passed
2023-08-10 17:18:26,648 __main__ INFO publishing 'four' on l3-stream by pub4foo (0440)
2023-08-10 17:18:27,150 __main__ INFO obtaining all client data (0)
2023-08-10 17:18:27,151 __main__ INFO getting data from srv1
2023-08-10 17:18:29,657 __main__ INFO getting data from srv2
2023-08-10 17:18:32,163 __main__ INFO getting data from srv3
2023-08-10 17:18:34,669 __main__ INFO getting data from srv4
2023-08-10 17:18:37,175 __main__ INFO getting data from srv5
2023-08-10 17:18:39,681 __main__ INFO getting data from srv6
2023-08-10 17:18:42,187 __main__ INFO getting data from srv7
2023-08-10 17:18:44,692 __main__ INFO getting data from cli8foo
2023-08-10 17:18:47,199 TADA INFO assertion 13, l3-stream by 'foo' with 0440 permission: client data verified, passed
2023-08-10 17:18:47,199 __main__ INFO publishing 'four' on l3-stream by pub4bar (0440)
2023-08-10 17:18:47,702 __main__ INFO obtaining all client data (0)
2023-08-10 17:18:47,702 __main__ INFO getting data from srv1
2023-08-10 17:18:50,208 __main__ INFO getting data from srv2
2023-08-10 17:18:52,715 __main__ INFO getting data from srv3
2023-08-10 17:18:55,220 __main__ INFO getting data from srv4
2023-08-10 17:18:57,727 __main__ INFO getting data from srv5
2023-08-10 17:19:00,233 __main__ INFO getting data from srv6
2023-08-10 17:19:02,738 __main__ INFO getting data from srv7
2023-08-10 17:19:05,244 __main__ INFO getting data from cli8foo
2023-08-10 17:19:07,750 TADA INFO assertion 14, l3-stream by 'bar' with 0440 permission: client data verified, passed
2023-08-10 17:19:11,259 TADA INFO assertion 15, Blocking client and asynchronous client have the same data: verified, passed
2023-08-10 17:19:12,762 __main__ INFO publishing 'four' on l3-stream by srv4
2023-08-10 17:19:13,265 __main__ INFO obtaining all client data (0)
2023-08-10 17:19:13,265 __main__ INFO getting data from srv1
2023-08-10 17:19:15,771 __main__ INFO getting data from srv2
2023-08-10 17:19:18,278 __main__ INFO getting data from srv3
2023-08-10 17:19:20,783 __main__ INFO getting data from srv4
2023-08-10 17:19:23,290 __main__ INFO getting data from srv5
2023-08-10 17:19:25,795 __main__ INFO getting data from srv6
2023-08-10 17:19:28,301 __main__ INFO getting data from srv7
2023-08-10 17:19:30,807 __main__ INFO getting data from cli8foo
2023-08-10 17:19:33,314 TADA INFO assertion 20, l3-stream publish from L1 (srv4): client data verified, passed
2023-08-10 17:19:33,314 __main__ INFO publishing 'four' on nada by srv4
2023-08-10 17:19:33,815 __main__ INFO obtaining all client data (0)
2023-08-10 17:19:33,816 __main__ INFO getting data from srv1
2023-08-10 17:19:36,321 __main__ INFO getting data from srv2
2023-08-10 17:19:38,827 __main__ INFO getting data from srv3
2023-08-10 17:19:41,333 __main__ INFO getting data from srv4
2023-08-10 17:19:43,839 __main__ INFO getting data from srv5
2023-08-10 17:19:46,344 __main__ INFO getting data from srv6
2023-08-10 17:19:48,850 __main__ INFO getting data from srv7
2023-08-10 17:19:51,356 __main__ INFO getting data from cli8foo
2023-08-10 17:19:53,862 TADA INFO assertion 21, nada publish from L1 (srv4): client data verified, passed
2023-08-10 17:19:57,463 TADA INFO assertion 22, Check stream stats in each process: verified, passed
2023-08-10 17:20:01,073 TADA INFO assertion 23, Check stream client stats in each process: verified, passed
2023-08-10 17:20:03,578 TADA INFO assertion 16, srv-6 clean up properly after srv-3 exited: checking..., passed
2023-08-10 17:20:03,578 TADA INFO assertion 17, srv-7 clean up properly after srv-3 exited: checking..., passed
2023-08-10 17:20:03,578 TADA INFO assertion 18, srv-1 clean up properly after srv-3 exited: checking..., passed
2023-08-10 17:20:03,579 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-3b206c9-node-3 
2023-08-10 17:20:07,098 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-08-10 17:20:07,601 __main__ INFO obtaining all client data (0)
2023-08-10 17:20:07,601 __main__ INFO getting data from srv1
2023-08-10 17:20:10,107 __main__ INFO getting data from srv2
2023-08-10 17:20:12,613 __main__ INFO getting data from srv3
2023-08-10 17:20:15,120 __main__ INFO getting data from srv4
2023-08-10 17:20:17,625 __main__ INFO getting data from srv5
2023-08-10 17:20:20,131 __main__ INFO getting data from srv6
2023-08-10 17:20:22,637 __main__ INFO getting data from srv7
2023-08-10 17:20:25,143 __main__ INFO getting data from cli8foo
2023-08-10 17:20:27,649 TADA INFO assertion 19, l3-stream successfully delivered after srv-3 restarted: client data verified, passed
2023-08-10 17:20:27,650 TADA INFO test ldms_stream_test ended
2023-08-10 17:20:43 INFO: ----------------------------------------------
2023-08-10 17:20:43 INFO: ======== test-ldms ========
2023-08-10 17:20:43 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-08-10T17:20:44-05:00 INFO: starting test-samp-1
795be76894b83d09cc7967ae406f773c89e074e75c9d431e518323135f4b58f8
2023-08-10T17:20:46-05:00 INFO: starting test-samp-2
a18d6c16bad8bdbc574e130864e82faff1d4489d643e72cc8811553b4c7d123f
2023-08-10T17:20:48-05:00 INFO: starting test-samp-3
c1085b49871ef5c2546d22d277a52cace1ad02af2ddc91f36910f37207dabc5b
2023-08-10T17:20:49-05:00 INFO: starting test-samp-4
3f7302f14eb406533edf2c1843a1b95814cb9f7892261d6cabf09e103cd5cacc
2023-08-10T17:20:51-05:00 INFO: test-samp-1 is running
2023-08-10T17:20:51-05:00 INFO: test-samp-2 is running
2023-08-10T17:20:51-05:00 INFO: test-samp-3 is running
2023-08-10T17:20:51-05:00 INFO: test-samp-4 is running
2023-08-10T17:20:51-05:00 INFO: starting test-agg-11
7822dfe7604d88ab087cbbeb4f9ef8a987551a0f2afbc2a5cc80cc2fefc6fbad
2023-08-10T17:20:53-05:00 INFO: starting test-agg-12
ea11c9cf25048e8fed93bd07f714ddc6c280a7035cd782e542143d76e0dbee1f
2023-08-10T17:20:55-05:00 INFO: test-agg-11 is running
2023-08-10T17:20:55-05:00 INFO: test-agg-12 is running
2023-08-10T17:20:55-05:00 INFO: starting test-agg-2
f4623e5da6415d74ac964e73627e31d404791468f924485ac09a07574e32da86
2023-08-10T17:20:56-05:00 INFO: test-agg-2 is running
2023-08-10T17:20:56-05:00 INFO: Collecting data (into SOS)
2023-08-10T17:21:06-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-08-10T17:21:08-05:00 INFO: check rc: 0
2023-08-10T17:21:08-05:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-08-10T17:21:13-05:00 INFO: DONE
2023-08-10 17:21:23 INFO: ----------------------------------------------
2023-08-10 17:21:23 INFO: ======== test-maestro ========
2023-08-10 17:21:23 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-08-10T17:21:23-05:00 INFO: starting mtest-maestro
603f57486985660734a2ba32201f9021f3bc0cbbf0700d4f2d55d624551afe04
2023-08-10T17:21:25-05:00 INFO: starting mtest-samp-1
fcef59eed0d3583048b48570c5556547ef321a728e4b2c49ae18f59ee818c5ff
2023-08-10T17:21:26-05:00 INFO: starting mtest-samp-2
fd6234288ff6a68a2f17599c24dcfafe5fc629d4bbd97b2a750fc7838855287c
2023-08-10T17:21:29-05:00 INFO: starting mtest-samp-3
7f3d3e75a5993410ed4b675add0285e066fcd5b28dd81a6bb851b85321328580
2023-08-10T17:21:31-05:00 INFO: starting mtest-samp-4
702bf8847b63a79be41ac090b40ae19b5142ba486d706482ed01c079f417d4ca
2023-08-10T17:21:32-05:00 INFO: mtest-samp-1 is running
2023-08-10T17:21:32-05:00 INFO: mtest-samp-2 is running
2023-08-10T17:21:32-05:00 INFO: mtest-samp-3 is running
2023-08-10T17:21:32-05:00 INFO: mtest-samp-4 is running
2023-08-10T17:21:32-05:00 INFO: starting mtest-agg-11
7461c404f0e5792efb5bf73db87ad78107eb5e7262f98ee0feb572823e889dbf
2023-08-10T17:21:34-05:00 INFO: starting mtest-agg-12
b020bd2078c8bcdcf17e34c6e6c51f0e1bd53241f099f907fce14da02198feef
2023-08-10T17:21:35-05:00 INFO: mtest-agg-11 is running
2023-08-10T17:21:35-05:00 INFO: mtest-agg-12 is running
2023-08-10T17:21:35-05:00 INFO: starting mtest-agg-2
93554fe43915aee4f139fd7ede0b471869f812869c2823ccf2bf6c6224623f72
2023-08-10T17:21:36-05:00 INFO: mtest-agg-2 is running
2023-08-10T17:21:36-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-08-10T17:23:38-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-08-10T17:23:40-05:00 INFO: sos check rc: 0
2023-08-10T17:23:41-05:00 INFO: starting mtest-ui
6336f23b416889f49e034501a4290d809bcf10c5b435b80482c4839b1011dbbe
2023-08-10T17:23:47-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[3511152, 1691706116001.8628], [3511420, 1691706117001.996], [3511420, 1691706117002.009], [3511420, 1691706117002.017], [3511420, 1691706117002.018], [3511524, 1691706118000.9338], [3511524, 1691706118001.137], [3511524, 1691706118001.173], [3511524, 1691706118002.135], [3511524, 1691706119001.293], [3511524, 1691706119001.296], [3511524, 1691706119001.303], [3511524, 1691706119001.3152], [3511524, 1691706120001.423], [3511524, 1691706120001.427], [3511524, 1691706120001.428], [3511524, 1691706120001.434], [3511524, 1691706121001.565], [3511524, 1691706121001.569], [3511524, 1691706121001.57], [3511524, 1691706121001.572], [3511524, 1691706122001.7249], [3511524, 1691706122001.728], [3511524, 1691706122001.729], [3511524, 1691706122001.738], [3511524, 1691706123001.855], [3511524, 1691706123001.8618], [3511524, 1691706123001.8628], [3511524, 1691706123001.865], [3511524, 1691706124002.007], [3511524, 1691706124002.011], [3511524, 1691706124002.013], [3511524, 1691706124002.013], [3511524, 1691706125000.199], [3511524, 1691706125001.13], [3511524, 1691706125001.1572], [3511524, 1691706125001.158], [3511524, 1691706126001.271], [3511524, 1691706126001.275], [3511524, 1691706126001.28], [3511524, 1691706126001.2869], [3511524, 1691706127001.433], [3511524, 1691706127001.439], [3511524, 1691706127001.44], [3511524, 1691706127001.444], [3511524, 1691706128001.557], [3511524, 1691706128001.559], [3511524, 1691706128001.564], [3511524, 1691706128001.567], [3511524, 1691706129001.71], [3511524, 1691706129001.7112], [3511524, 1691706129001.716], [3511524, 1691706129001.716], [3511524, 1691706130001.8372], [3511524, 1691706130001.839], [3511524, 1691706130001.8408], [3511524, 1691706130001.844], [3511524, 1691706131001.966], [3511524, 1691706131001.978], [3511524, 1691706131001.981], [3511524, 1691706131001.985], [3511524, 1691706132001.9468], [3511524, 1691706132002.112], [3511524, 1691706132002.135], [3511524, 1691706132002.14], [3511524, 1691706133001.258], [3511524, 1691706133001.259], [3511524, 1691706133001.26], [3511524, 1691706133001.262], [3511524, 1691706134001.377], [3511524, 1691706134001.3792], [3511524, 1691706134001.396], [3511524, 1691706134001.396], [3511524, 1691706135000.747], [3511524, 1691706135001.496], [3511524, 1691706135001.514], [3511524, 1691706135001.522], [3511524, 1691706136000.8489], [3511524, 1691706136001.633], [3511524, 1691706136001.646], [3511524, 1691706136001.874], [3511524, 1691706137001.7568], [3511524, 1691706137001.7642], [3511524, 1691706137001.769], [3511524, 1691706137002.001], [3511524, 1691706138001.16], [3511524, 1691706138001.163], [3511524, 1691706138001.878], [3511524, 1691706138001.91], [3511524, 1691706139001.3079], [3511524, 1691706139001.3171], [3511524, 1691706139001.321], [3511524, 1691706139002.052], [3511524, 1691706140001.1892], [3511524, 1691706140001.438], [3511524, 1691706140001.444], [3511524, 1691706140001.447], [3511524, 1691706141000.784], [3511524, 1691706141001.554], [3511524, 1691706141001.562], [3511524, 1691706141001.573], [3511524, 1691706142001.6829], [3511524, 1691706142001.6929], [3511524, 1691706142001.6929], [3511524, 1691706142001.695], [3511524, 1691706143001.243], [3511524, 1691706143001.8052], [3511524, 1691706143001.844], [3511524, 1691706143001.844], [3511524, 1691706144001.3801], [3511524, 1691706144001.386], [3511524, 1691706144001.956], [3511524, 1691706144001.9788], [3511524, 1691706145001.533], [3511524, 1691706145001.541], [3511524, 1691706145001.544], [3511524, 1691706145002.125], [3511524, 1691706146001.262], [3511524, 1691706146001.667], [3511524, 1691706146001.674], [3511524, 1691706146001.675], [3511524, 1691706147001.4102], [3511524, 1691706147001.815], [3511524, 1691706147001.8198], [3511524, 1691706147001.823], [3511524, 1691706148001.025], [3511524, 1691706148001.565], [3511524, 1691706148001.9468], [3511524, 1691706148001.976], [3511524, 1691706149001.187], [3511524, 1691706149001.187], [3511524, 1691706149001.7148], [3511524, 1691706149002.094], [3511524, 1691706150000.276], [3511524, 1691706150000.3608], [3511524, 1691706150001.2842], [3511524, 1691706150001.834], [3511524, 1691706151001.408], [3511524, 1691706151001.416], [3511524, 1691706151001.493], [3511524, 1691706151001.494], [3511524, 1691706152001.558], [3511524, 1691706152001.565], [3511524, 1691706152001.624], [3511524, 1691706152001.634], [3511524, 1691706153001.7148], [3511524, 1691706153001.716], [3511524, 1691706153001.769], [3511524, 1691706153001.781], [3511524, 1691706154001.845], [3511524, 1691706154001.882], [3511524, 1691706154001.8938], [3511524, 1691706154001.916], [3511524, 1691706155001.977], [3511524, 1691706155002.003], [3511524, 1691706155002.018], [3511524, 1691706155002.037], [3511524, 1691706156001.141], [3511524, 1691706156001.146], [3511524, 1691706156001.159], [3511524, 1691706156002.114], [3511524, 1691706157001.075], [3511524, 1691706157001.248], [3511524, 1691706157001.2642], [3511524, 1691706157001.2659], [3511524, 1691706158001.236], [3511524, 1691706158001.4011], [3511524, 1691706158001.407], [3511524, 1691706158001.4148], [3511524, 1691706159001.3682], [3511524, 1691706159001.528], [3511524, 1691706159001.534], [3511524, 1691706159001.534], [3511524, 1691706160001.512], [3511524, 1691706160001.655], [3511524, 1691706160001.664], [3511524, 1691706160001.666], [3511524, 1691706161001.649], [3511524, 1691706161001.7678], [3511524, 1691706161001.769], [3511524, 1691706161001.78], [3511524, 1691706162001.7888], [3511524, 1691706162001.8938], [3511524, 1691706162001.902], [3511524, 1691706162001.908], [3511528, 1691706163001.99], [3511528, 1691706163002.067], [3511528, 1691706163002.0679], [3511528, 1691706163002.076], [3511524, 1691706164001.2002], [3511524, 1691706164001.206], [3511524, 1691706164001.206], [3511524, 1691706164002.1272], [3511524, 1691706165001.2659], [3511524, 1691706165001.31], [3511524, 1691706165001.313], [3511524, 1691706165001.3281], [3511524, 1691706166001.4028], [3511524, 1691706166001.4238], [3511524, 1691706166001.4421], [3511524, 1691706166001.451], [3511524, 1691706167001.5261], [3511524, 1691706167001.529], [3511524, 1691706167001.55], [3511524, 1691706167001.564], [3511524, 1691706168001.656], [3511524, 1691706168001.659], [3511524, 1691706168001.67], [3511524, 1691706168001.691], [3511524, 1691706169000.827], [3511524, 1691706169001.7139], [3511524, 1691706169001.7778], [3511524, 1691706169001.801], [3511524, 1691706170001.868], [3511524, 1691706170001.934], [3511524, 1691706170001.934], [3511524, 1691706170001.946], [3511524, 1691706171001.085], [3511524, 1691706171002.005], [3511524, 1691706171002.038], [3511524, 1691706171002.0679], [3511524, 1691706172000.45], [3511524, 1691706172001.171], [3511524, 1691706172001.1829], [3511524, 1691706172001.187], [3511524, 1691706173001.124], [3511524, 1691706173001.2842], [3511524, 1691706173001.2942], [3511524, 1691706173001.2979], [3511524, 1691706174001.286], [3511524, 1691706174001.2878], [3511524, 1691706174001.419], [3511524, 1691706174001.43], [3511524, 1691706175001.426], [3511524, 1691706175001.429], [3511524, 1691706175001.437], [3511524, 1691706175001.575], [3511524, 1691706176001.56], [3511524, 1691706176001.561], [3511524, 1691706176001.568], [3511524, 1691706176001.685], [3511524, 1691706177001.697], [3511524, 1691706177001.7039], [3511524, 1691706177001.706], [3511524, 1691706177001.8052], [3511524, 1691706178001.834], [3511524, 1691706178001.842], [3511524, 1691706178001.845], [3511524, 1691706178001.965], [3511524, 1691706179001.994], [3511524, 1691706179001.996], [3511524, 1691706179001.997], [3511524, 1691706179002.103], [3511524, 1691706180001.13], [3511524, 1691706180001.223], [3511524, 1691706180002.123], [3511524, 1691706180002.1272], [3511524, 1691706181001.255], [3511524, 1691706181001.2632], [3511524, 1691706181001.2659], [3511524, 1691706181001.3508], [3511524, 1691706182001.392], [3511524, 1691706182001.398], [3511524, 1691706182001.408], [3511524, 1691706182001.485], [3511524, 1691706183001.5308], [3511524, 1691706183001.533], [3511524, 1691706183001.533], [3511524, 1691706183001.604], [3511524, 1691706184001.688], [3511524, 1691706184001.6902], [3511524, 1691706184001.691], [3511524, 1691706184001.751], [3511524, 1691706185001.823], [3511524, 1691706185001.829], [3511524, 1691706185001.8308], [3511524, 1691706185001.875], [3511524, 1691706186001.966], [3511524, 1691706186001.976], [3511524, 1691706186001.976], [3511524, 1691706186001.998], [3511524, 1691706187001.141], [3511524, 1691706187002.105], [3511524, 1691706187002.108], [3511524, 1691706187002.109], [3511524, 1691706188001.244], [3511524, 1691706188001.244], [3511524, 1691706188001.2458], [3511524, 1691706188001.258], [3511524, 1691706189001.405], [3511524, 1691706189001.407], [3511524, 1691706189001.4111], [3511524, 1691706189001.412], [3511524, 1691706190001.532], [3511524, 1691706190001.534], [3511524, 1691706190001.538], [3511524, 1691706190001.541], [3511524, 1691706191001.665], [3511524, 1691706191001.6682], [3511524, 1691706191001.6692], [3511524, 1691706191001.67], [3511524, 1691706192001.8162], [3511524, 1691706192001.818], [3511524, 1691706192001.8198], [3511524, 1691706192001.821], [3511524, 1691706193001.948], [3511524, 1691706193001.95], [3511524, 1691706193001.952], [3511524, 1691706193001.9531], [3511524, 1691706194001.8828], [3511524, 1691706194001.906], [3511524, 1691706194002.041], [3511524, 1691706194002.088], [3511524, 1691706195001.144], [3511524, 1691706195001.18], [3511524, 1691706195001.23], [3511524, 1691706195002.019], [3511524, 1691706196001.152], [3511524, 1691706196001.258], [3511524, 1691706196001.291], [3511524, 1691706196001.346], [3511524, 1691706197001.2769], [3511524, 1691706197001.375], [3511524, 1691706197001.407], [3511524, 1691706197001.462], [3511524, 1691706198001.4001], [3511524, 1691706198001.49], [3511524, 1691706198001.523], [3511524, 1691706198001.582], [3511524, 1691706199000.645], [3511524, 1691706199001.413], [3511524, 1691706199001.5308], [3511524, 1691706199001.567], [3511524, 1691706200000.9768], [3511524, 1691706200001.535], [3511524, 1691706200001.67], [3511524, 1691706200001.708], [3511524, 1691706201001.685], [3511524, 1691706201001.687], [3511524, 1691706201001.801], [3511524, 1691706201001.8372], [3511524, 1691706202001.818], [3511524, 1691706202001.825], [3511524, 1691706202001.922], [3511524, 1691706202001.958], [3511524, 1691706203001.938], [3511524, 1691706203001.944], [3511524, 1691706203002.03], [3511524, 1691706203002.069], [3511524, 1691706204001.177], [3511524, 1691706204001.194], [3511524, 1691706204002.073], [3511524, 1691706204002.076], [3511524, 1691706205001.23], [3511524, 1691706205001.232], [3511524, 1691706205001.3262], [3511524, 1691706205001.3389], [3511524, 1691706206001.363], [3511524, 1691706206001.3691], [3511524, 1691706206001.448], [3511524, 1691706206001.4568], [3511524, 1691706207001.511], [3511524, 1691706207001.514], [3511524, 1691706207001.5842], [3511524, 1691706207001.595], [3511524, 1691706208001.6582], [3511524, 1691706208001.659], [3511524, 1691706208001.717], [3511524, 1691706208001.723], [3511524, 1691706209001.812], [3511524, 1691706209001.8162], [3511524, 1691706209001.857], [3511524, 1691706209001.865], [3511524, 1691706210001.972], [3511524, 1691706210001.975], [3511524, 1691706210002.014], [3511524, 1691706210002.016], [3511524, 1691706211001.159], [3511524, 1691706211001.161], [3511524, 1691706211002.112], [3511524, 1691706211002.1172], [3511524, 1691706212001.243], [3511524, 1691706212001.244], [3511524, 1691706212001.283], [3511524, 1691706212001.2842], [3511524, 1691706213001.392], [3511524, 1691706213001.392], [3511524, 1691706213001.417], [3511524, 1691706213001.42], [3511524, 1691706214001.536], [3511524, 1691706214001.545], [3511524, 1691706214001.559], [3511524, 1691706214001.56], [3511524, 1691706215001.7012], [3511524, 1691706215001.703], [3511524, 1691706215001.705], [3511524, 1691706215001.71], [3511524, 1691706216001.833], [3511524, 1691706216001.8372], [3511524, 1691706216001.8372], [3511524, 1691706216001.84]]}, {"target": "component_id", "datapoints": [[1, 1691706116001.8628], [2, 1691706117001.996], [4, 1691706117002.009], [1, 1691706117002.017], [3, 1691706117002.018], [4, 1691706118000.9338], [1, 1691706118001.137], [3, 1691706118001.173], [2, 1691706118002.135], [2, 1691706119001.293], [4, 1691706119001.296], [1, 1691706119001.303], [3, 1691706119001.3152], [4, 1691706120001.423], [2, 1691706120001.427], [1, 1691706120001.428], [3, 1691706120001.434], [3, 1691706121001.565], [4, 1691706121001.569], [1, 1691706121001.57], [2, 1691706121001.572], [1, 1691706122001.7249], [3, 1691706122001.728], [4, 1691706122001.729], [2, 1691706122001.738], [2, 1691706123001.855], [4, 1691706123001.8618], [3, 1691706123001.8628], [1, 1691706123001.865], [3, 1691706124002.007], [4, 1691706124002.011], [1, 1691706124002.013], [2, 1691706124002.013], [3, 1691706125000.199], [2, 1691706125001.13], [1, 1691706125001.1572], [4, 1691706125001.158], [3, 1691706126001.271], [2, 1691706126001.275], [1, 1691706126001.28], [4, 1691706126001.2869], [1, 1691706127001.433], [3, 1691706127001.439], [4, 1691706127001.44], [2, 1691706127001.444], [3, 1691706128001.557], [1, 1691706128001.559], [2, 1691706128001.564], [4, 1691706128001.567], [4, 1691706129001.71], [1, 1691706129001.7112], [2, 1691706129001.716], [3, 1691706129001.716], [2, 1691706130001.8372], [3, 1691706130001.839], [4, 1691706130001.8408], [1, 1691706130001.844], [1, 1691706131001.966], [3, 1691706131001.978], [4, 1691706131001.981], [2, 1691706131001.985], [1, 1691706132001.9468], [3, 1691706132002.112], [2, 1691706132002.135], [4, 1691706132002.14], [3, 1691706133001.258], [2, 1691706133001.259], [1, 1691706133001.26], [4, 1691706133001.262], [3, 1691706134001.377], [1, 1691706134001.3792], [2, 1691706134001.396], [4, 1691706134001.396], [1, 1691706135000.747], [3, 1691706135001.496], [4, 1691706135001.514], [2, 1691706135001.522], [3, 1691706136000.8489], [2, 1691706136001.633], [4, 1691706136001.646], [1, 1691706136001.874], [4, 1691706137001.7568], [3, 1691706137001.7642], [2, 1691706137001.769], [1, 1691706137002.001], [1, 1691706138001.16], [3, 1691706138001.163], [2, 1691706138001.878], [4, 1691706138001.91], [1, 1691706139001.3079], [3, 1691706139001.3171], [2, 1691706139001.321], [4, 1691706139002.052], [4, 1691706140001.1892], [1, 1691706140001.438], [2, 1691706140001.444], [3, 1691706140001.447], [4, 1691706141000.784], [2, 1691706141001.554], [1, 1691706141001.562], [3, 1691706141001.573], [1, 1691706142001.6829], [2, 1691706142001.6929], [3, 1691706142001.6929], [4, 1691706142001.695], [2, 1691706143001.243], [3, 1691706143001.8052], [1, 1691706143001.844], [4, 1691706143001.844], [3, 1691706144001.3801], [2, 1691706144001.386], [1, 1691706144001.956], [4, 1691706144001.9788], [2, 1691706145001.533], [1, 1691706145001.541], [3, 1691706145001.544], [4, 1691706145002.125], [4, 1691706146001.262], [1, 1691706146001.667], [3, 1691706146001.674], [2, 1691706146001.675], [4, 1691706147001.4102], [3, 1691706147001.815], [2, 1691706147001.8198], [1, 1691706147001.823], [3, 1691706148001.025], [4, 1691706148001.565], [2, 1691706148001.9468], [1, 1691706148001.976], [2, 1691706149001.187], [3, 1691706149001.187], [4, 1691706149001.7148], [1, 1691706149002.094], [1, 1691706150000.276], [2, 1691706150000.3608], [3, 1691706150001.2842], [4, 1691706150001.834], [1, 1691706151001.408], [3, 1691706151001.416], [4, 1691706151001.493], [2, 1691706151001.494], [1, 1691706152001.558], [3, 1691706152001.565], [4, 1691706152001.624], [2, 1691706152001.634], [1, 1691706153001.7148], [3, 1691706153001.716], [4, 1691706153001.769], [2, 1691706153001.781], [1, 1691706154001.845], [3, 1691706154001.882], [4, 1691706154001.8938], [2, 1691706154001.916], [1, 1691706155001.977], [3, 1691706155002.003], [4, 1691706155002.018], [2, 1691706155002.037], [3, 1691706156001.141], [4, 1691706156001.146], [2, 1691706156001.159], [1, 1691706156002.114], [2, 1691706157001.075], [1, 1691706157001.248], [3, 1691706157001.2642], [4, 1691706157001.2659], [2, 1691706158001.236], [1, 1691706158001.4011], [4, 1691706158001.407], [3, 1691706158001.4148], [2, 1691706159001.3682], [4, 1691706159001.528], [1, 1691706159001.534], [3, 1691706159001.534], [2, 1691706160001.512], [4, 1691706160001.655], [3, 1691706160001.664], [1, 1691706160001.666], [2, 1691706161001.649], [4, 1691706161001.7678], [3, 1691706161001.769], [1, 1691706161001.78], [2, 1691706162001.7888], [4, 1691706162001.8938], [3, 1691706162001.902], [1, 1691706162001.908], [2, 1691706163001.99], [4, 1691706163002.067], [3, 1691706163002.0679], [1, 1691706163002.076], [1, 1691706164001.2002], [3, 1691706164001.206], [4, 1691706164001.206], [2, 1691706164002.1272], [2, 1691706165001.2659], [1, 1691706165001.31], [3, 1691706165001.313], [4, 1691706165001.3281], [2, 1691706166001.4028], [1, 1691706166001.4238], [3, 1691706166001.4421], [4, 1691706166001.451], [2, 1691706167001.5261], [1, 1691706167001.529], [3, 1691706167001.55], [4, 1691706167001.564], [1, 1691706168001.656], [2, 1691706168001.659], [3, 1691706168001.67], [4, 1691706168001.691], [3, 1691706169000.827], [4, 1691706169001.7139], [1, 1691706169001.7778], [2, 1691706169001.801], [4, 1691706170001.868], [1, 1691706170001.934], [3, 1691706170001.934], [2, 1691706170001.946], [3, 1691706171001.085], [4, 1691706171002.005], [1, 1691706171002.038], [2, 1691706171002.0679], [4, 1691706172000.45], [2, 1691706172001.171], [3, 1691706172001.1829], [1, 1691706172001.187], [1, 1691706173001.124], [2, 1691706173001.2842], [3, 1691706173001.2942], [4, 1691706173001.2979], [2, 1691706174001.286], [1, 1691706174001.2878], [4, 1691706174001.419], [3, 1691706174001.43], [1, 1691706175001.426], [2, 1691706175001.429], [4, 1691706175001.437], [3, 1691706175001.575], [4, 1691706176001.56], [2, 1691706176001.561], [1, 1691706176001.568], [3, 1691706176001.685], [2, 1691706177001.697], [4, 1691706177001.7039], [1, 1691706177001.706], [3, 1691706177001.8052], [1, 1691706178001.834], [4, 1691706178001.842], [2, 1691706178001.845], [3, 1691706178001.965], [1, 1691706179001.994], [4, 1691706179001.996], [2, 1691706179001.997], [3, 1691706179002.103], [2, 1691706180001.13], [3, 1691706180001.223], [1, 1691706180002.123], [4, 1691706180002.1272], [1, 1691706181001.255], [2, 1691706181001.2632], [4, 1691706181001.2659], [3, 1691706181001.3508], [1, 1691706182001.392], [2, 1691706182001.398], [4, 1691706182001.408], [3, 1691706182001.485], [1, 1691706183001.5308], [2, 1691706183001.533], [4, 1691706183001.533], [3, 1691706183001.604], [4, 1691706184001.688], [2, 1691706184001.6902], [1, 1691706184001.691], [3, 1691706184001.751], [2, 1691706185001.823], [1, 1691706185001.829], [4, 1691706185001.8308], [3, 1691706185001.875], [2, 1691706186001.966], [1, 1691706186001.976], [4, 1691706186001.976], [3, 1691706186001.998], [3, 1691706187001.141], [1, 1691706187002.105], [4, 1691706187002.108], [2, 1691706187002.109], [1, 1691706188001.244], [2, 1691706188001.244], [4, 1691706188001.2458], [3, 1691706188001.258], [3, 1691706189001.405], [1, 1691706189001.407], [2, 1691706189001.4111], [4, 1691706189001.412], [2, 1691706190001.532], [4, 1691706190001.534], [1, 1691706190001.538], [3, 1691706190001.541], [3, 1691706191001.665], [1, 1691706191001.6682], [4, 1691706191001.6692], [2, 1691706191001.67], [1, 1691706192001.8162], [4, 1691706192001.818], [3, 1691706192001.8198], [2, 1691706192001.821], [4, 1691706193001.948], [3, 1691706193001.95], [1, 1691706193001.952], [2, 1691706193001.9531], [2, 1691706194001.8828], [3, 1691706194001.906], [1, 1691706194002.041], [4, 1691706194002.088], [3, 1691706195001.144], [1, 1691706195001.18], [4, 1691706195001.23], [2, 1691706195002.019], [2, 1691706196001.152], [3, 1691706196001.258], [1, 1691706196001.291], [4, 1691706196001.346], [2, 1691706197001.2769], [3, 1691706197001.375], [1, 1691706197001.407], [4, 1691706197001.462], [2, 1691706198001.4001], [3, 1691706198001.49], [1, 1691706198001.523], [4, 1691706198001.582], [3, 1691706199000.645], [1, 1691706199001.413], [2, 1691706199001.5308], [4, 1691706199001.567], [3, 1691706200000.9768], [1, 1691706200001.535], [2, 1691706200001.67], [4, 1691706200001.708], [3, 1691706201001.685], [1, 1691706201001.687], [2, 1691706201001.801], [4, 1691706201001.8372], [3, 1691706202001.818], [1, 1691706202001.825], [2, 1691706202001.922], [4, 1691706202001.958], [3, 1691706203001.938], [1, 1691706203001.944], [2, 1691706203002.03], [4, 1691706203002.069], [2, 1691706204001.177], [4, 1691706204001.194], [3, 1691706204002.073], [1, 1691706204002.076], [3, 1691706205001.23], [1, 1691706205001.232], [2, 1691706205001.3262], [4, 1691706205001.3389], [1, 1691706206001.363], [3, 1691706206001.3691], [2, 1691706206001.448], [4, 1691706206001.4568], [1, 1691706207001.511], [3, 1691706207001.514], [2, 1691706207001.5842], [4, 1691706207001.595], [1, 1691706208001.6582], [3, 1691706208001.659], [2, 1691706208001.717], [4, 1691706208001.723], [1, 1691706209001.812], [3, 1691706209001.8162], [2, 1691706209001.857], [4, 1691706209001.865], [3, 1691706210001.972], [1, 1691706210001.975], [4, 1691706210002.014], [2, 1691706210002.016], [2, 1691706211001.159], [4, 1691706211001.161], [3, 1691706211002.112], [1, 1691706211002.1172], [3, 1691706212001.243], [1, 1691706212001.244], [4, 1691706212001.283], [2, 1691706212001.2842], [1, 1691706213001.392], [3, 1691706213001.392], [4, 1691706213001.417], [2, 1691706213001.42], [3, 1691706214001.536], [1, 1691706214001.545], [4, 1691706214001.559], [2, 1691706214001.56], [1, 1691706215001.7012], [3, 1691706215001.703], [2, 1691706215001.705], [4, 1691706215001.71], [4, 1691706216001.833], [1, 1691706216001.8372], [3, 1691706216001.8372], [2, 1691706216001.84]]}, {"target": "job_id", "datapoints": [[0, 1691706116001.8628], [0, 1691706117001.996], [0, 1691706117002.009], [0, 1691706117002.017], [0, 1691706117002.018], [0, 1691706118000.9338], [0, 1691706118001.137], [0, 1691706118001.173], [0, 1691706118002.135], [0, 1691706119001.293], [0, 1691706119001.296], [0, 1691706119001.303], [0, 1691706119001.3152], [0, 1691706120001.423], [0, 1691706120001.427], [0, 1691706120001.428], [0, 1691706120001.434], [0, 1691706121001.565], [0, 1691706121001.569], [0, 1691706121001.57], [0, 1691706121001.572], [0, 1691706122001.7249], [0, 1691706122001.728], [0, 1691706122001.729], [0, 1691706122001.738], [0, 1691706123001.855], [0, 1691706123001.8618], [0, 1691706123001.8628], [0, 1691706123001.865], [0, 1691706124002.007], [0, 1691706124002.011], [0, 1691706124002.013], [0, 1691706124002.013], [0, 1691706125000.199], [0, 1691706125001.13], [0, 1691706125001.1572], [0, 1691706125001.158], [0, 1691706126001.271], [0, 1691706126001.275], [0, 1691706126001.28], [0, 1691706126001.2869], [0, 1691706127001.433], [0, 1691706127001.439], [0, 1691706127001.44], [0, 1691706127001.444], [0, 1691706128001.557], [0, 1691706128001.559], [0, 1691706128001.564], [0, 1691706128001.567], [0, 1691706129001.71], [0, 1691706129001.7112], [0, 1691706129001.716], [0, 1691706129001.716], [0, 1691706130001.8372], [0, 1691706130001.839], [0, 1691706130001.8408], [0, 1691706130001.844], [0, 1691706131001.966], [0, 1691706131001.978], [0, 1691706131001.981], [0, 1691706131001.985], [0, 1691706132001.9468], [0, 1691706132002.112], [0, 1691706132002.135], [0, 1691706132002.14], [0, 1691706133001.258], [0, 1691706133001.259], [0, 1691706133001.26], [0, 1691706133001.262], [0, 1691706134001.377], [0, 1691706134001.3792], [0, 1691706134001.396], [0, 1691706134001.396], [0, 1691706135000.747], [0, 1691706135001.496], [0, 1691706135001.514], [0, 1691706135001.522], [0, 1691706136000.8489], [0, 1691706136001.633], [0, 1691706136001.646], [0, 1691706136001.874], [0, 1691706137001.7568], [0, 1691706137001.7642], [0, 1691706137001.769], [0, 1691706137002.001], [0, 1691706138001.16], [0, 1691706138001.163], [0, 1691706138001.878], [0, 1691706138001.91], [0, 1691706139001.3079], [0, 1691706139001.3171], [0, 1691706139001.321], [0, 1691706139002.052], [0, 1691706140001.1892], [0, 1691706140001.438], [0, 1691706140001.444], [0, 1691706140001.447], [0, 1691706141000.784], [0, 1691706141001.554], [0, 1691706141001.562], [0, 1691706141001.573], [0, 1691706142001.6829], [0, 1691706142001.6929], [0, 1691706142001.6929], [0, 1691706142001.695], [0, 1691706143001.243], [0, 1691706143001.8052], [0, 1691706143001.844], [0, 1691706143001.844], [0, 1691706144001.3801], [0, 1691706144001.386], [0, 1691706144001.956], [0, 1691706144001.9788], [0, 1691706145001.533], [0, 1691706145001.541], [0, 1691706145001.544], [0, 1691706145002.125], [0, 1691706146001.262], [0, 1691706146001.667], [0, 1691706146001.674], [0, 1691706146001.675], [0, 1691706147001.4102], [0, 1691706147001.815], [0, 1691706147001.8198], [0, 1691706147001.823], [0, 1691706148001.025], [0, 1691706148001.565], [0, 1691706148001.9468], [0, 1691706148001.976], [0, 1691706149001.187], [0, 1691706149001.187], [0, 1691706149001.7148], [0, 1691706149002.094], [0, 1691706150000.276], [0, 1691706150000.3608], [0, 1691706150001.2842], [0, 1691706150001.834], [0, 1691706151001.408], [0, 1691706151001.416], [0, 1691706151001.493], [0, 1691706151001.494], [0, 1691706152001.558], [0, 1691706152001.565], [0, 1691706152001.624], [0, 1691706152001.634], [0, 1691706153001.7148], [0, 1691706153001.716], [0, 1691706153001.769], [0, 1691706153001.781], [0, 1691706154001.845], [0, 1691706154001.882], [0, 1691706154001.8938], [0, 1691706154001.916], [0, 1691706155001.977], [0, 1691706155002.003], [0, 1691706155002.018], [0, 1691706155002.037], [0, 1691706156001.141], [0, 1691706156001.146], [0, 1691706156001.159], [0, 1691706156002.114], [0, 1691706157001.075], [0, 1691706157001.248], [0, 1691706157001.2642], [0, 1691706157001.2659], [0, 1691706158001.236], [0, 1691706158001.4011], [0, 1691706158001.407], [0, 1691706158001.4148], [0, 1691706159001.3682], [0, 1691706159001.528], [0, 1691706159001.534], [0, 1691706159001.534], [0, 1691706160001.512], [0, 1691706160001.655], [0, 1691706160001.664], [0, 1691706160001.666], [0, 1691706161001.649], [0, 1691706161001.7678], [0, 1691706161001.769], [0, 1691706161001.78], [0, 1691706162001.7888], [0, 1691706162001.8938], [0, 1691706162001.902], [0, 1691706162001.908], [0, 1691706163001.99], [0, 1691706163002.067], [0, 1691706163002.0679], [0, 1691706163002.076], [0, 1691706164001.2002], [0, 1691706164001.206], [0, 1691706164001.206], [0, 1691706164002.1272], [0, 1691706165001.2659], [0, 1691706165001.31], [0, 1691706165001.313], [0, 1691706165001.3281], [0, 1691706166001.4028], [0, 1691706166001.4238], [0, 1691706166001.4421], [0, 1691706166001.451], [0, 1691706167001.5261], [0, 1691706167001.529], [0, 1691706167001.55], [0, 1691706167001.564], [0, 1691706168001.656], [0, 1691706168001.659], [0, 1691706168001.67], [0, 1691706168001.691], [0, 1691706169000.827], [0, 1691706169001.7139], [0, 1691706169001.7778], [0, 1691706169001.801], [0, 1691706170001.868], [0, 1691706170001.934], [0, 1691706170001.934], [0, 1691706170001.946], [0, 1691706171001.085], [0, 1691706171002.005], [0, 1691706171002.038], [0, 1691706171002.0679], [0, 1691706172000.45], [0, 1691706172001.171], [0, 1691706172001.1829], [0, 1691706172001.187], [0, 1691706173001.124], [0, 1691706173001.2842], [0, 1691706173001.2942], [0, 1691706173001.2979], [0, 1691706174001.286], [0, 1691706174001.2878], [0, 1691706174001.419], [0, 1691706174001.43], [0, 1691706175001.426], [0, 1691706175001.429], [0, 1691706175001.437], [0, 1691706175001.575], [0, 1691706176001.56], [0, 1691706176001.561], [0, 1691706176001.568], [0, 1691706176001.685], [0, 1691706177001.697], [0, 1691706177001.7039], [0, 1691706177001.706], [0, 1691706177001.8052], [0, 1691706178001.834], [0, 1691706178001.842], [0, 1691706178001.845], [0, 1691706178001.965], [0, 1691706179001.994], [0, 1691706179001.996], [0, 1691706179001.997], [0, 1691706179002.103], [0, 1691706180001.13], [0, 1691706180001.223], [0, 1691706180002.123], [0, 1691706180002.1272], [0, 1691706181001.255], [0, 1691706181001.2632], [0, 1691706181001.2659], [0, 1691706181001.3508], [0, 1691706182001.392], [0, 1691706182001.398], [0, 1691706182001.408], [0, 1691706182001.485], [0, 1691706183001.5308], [0, 1691706183001.533], [0, 1691706183001.533], [0, 1691706183001.604], [0, 1691706184001.688], [0, 1691706184001.6902], [0, 1691706184001.691], [0, 1691706184001.751], [0, 1691706185001.823], [0, 1691706185001.829], [0, 1691706185001.8308], [0, 1691706185001.875], [0, 1691706186001.966], [0, 1691706186001.976], [0, 1691706186001.976], [0, 1691706186001.998], [0, 1691706187001.141], [0, 1691706187002.105], [0, 1691706187002.108], [0, 1691706187002.109], [0, 1691706188001.244], [0, 1691706188001.244], [0, 1691706188001.2458], [0, 1691706188001.258], [0, 1691706189001.405], [0, 1691706189001.407], [0, 1691706189001.4111], [0, 1691706189001.412], [0, 1691706190001.532], [0, 1691706190001.534], [0, 1691706190001.538], [0, 1691706190001.541], [0, 1691706191001.665], [0, 1691706191001.6682], [0, 1691706191001.6692], [0, 1691706191001.67], [0, 1691706192001.8162], [0, 1691706192001.818], [0, 1691706192001.8198], [0, 1691706192001.821], [0, 1691706193001.948], [0, 1691706193001.95], [0, 1691706193001.952], [0, 1691706193001.9531], [0, 1691706194001.8828], [0, 1691706194001.906], [0, 1691706194002.041], [0, 1691706194002.088], [0, 1691706195001.144], [0, 1691706195001.18], [0, 1691706195001.23], [0, 1691706195002.019], [0, 1691706196001.152], [0, 1691706196001.258], [0, 1691706196001.291], [0, 1691706196001.346], [0, 1691706197001.2769], [0, 1691706197001.375], [0, 1691706197001.407], [0, 1691706197001.462], [0, 1691706198001.4001], [0, 1691706198001.49], [0, 1691706198001.523], [0, 1691706198001.582], [0, 1691706199000.645], [0, 1691706199001.413], [0, 1691706199001.5308], [0, 1691706199001.567], [0, 1691706200000.9768], [0, 1691706200001.535], [0, 1691706200001.67], [0, 1691706200001.708], [0, 1691706201001.685], [0, 1691706201001.687], [0, 1691706201001.801], [0, 1691706201001.8372], [0, 1691706202001.818], [0, 1691706202001.825], [0, 1691706202001.922], [0, 1691706202001.958], [0, 1691706203001.938], [0, 1691706203001.944], [0, 1691706203002.03], [0, 1691706203002.069], [0, 1691706204001.177], [0, 1691706204001.194], [0, 1691706204002.073], [0, 1691706204002.076], [0, 1691706205001.23], [0, 1691706205001.232], [0, 1691706205001.3262], [0, 1691706205001.3389], [0, 1691706206001.363], [0, 1691706206001.3691], [0, 1691706206001.448], [0, 1691706206001.4568], [0, 1691706207001.511], [0, 1691706207001.514], [0, 1691706207001.5842], [0, 1691706207001.595], [0, 1691706208001.6582], [0, 1691706208001.659], [0, 1691706208001.717], [0, 1691706208001.723], [0, 1691706209001.812], [0, 1691706209001.8162], [0, 1691706209001.857], [0, 1691706209001.865], [0, 1691706210001.972], [0, 1691706210001.975], [0, 1691706210002.014], [0, 1691706210002.016], [0, 1691706211001.159], [0, 1691706211001.161], [0, 1691706211002.112], [0, 1691706211002.1172], [0, 1691706212001.243], [0, 1691706212001.244], [0, 1691706212001.283], [0, 1691706212001.2842], [0, 1691706213001.392], [0, 1691706213001.392], [0, 1691706213001.417], [0, 1691706213001.42], [0, 1691706214001.536], [0, 1691706214001.545], [0, 1691706214001.559], [0, 1691706214001.56], [0, 1691706215001.7012], [0, 1691706215001.703], [0, 1691706215001.705], [0, 1691706215001.71], [0, 1691706216001.833], [0, 1691706216001.8372], [0, 1691706216001.8372], [0, 1691706216001.84]]}]'
comp_ids:{1, 2, 3, 4}
2023-08-10T17:23:49-05:00 INFO: query check RC: 0
609d1e410a80e3d06470f70981c5c8f416761e20a33336baf3b77b6d8fe8573e
2023-08-10T17:24:21-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2324    717 --:--:-- --:--:-- --:--:--  3050
{"datasource":{"id":1,"uid":"UqNmJa64k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-08-10T17:24:22-05:00 INFO: Checking grafana data
2023-08-10T17:24:22-05:00 INFO: Grafana data check, rc: 0
2023-08-10T17:24:22-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-08-10T17:24:27-05:00 INFO: DONE
2023-08-10 17:24:37 INFO: ----------------------------------------------
2023-08-10 17:24:37 INFO: ======== test-maestro-hostmunge ========
2023-08-10 17:24:37 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-08-10T17:24:37-05:00 INFO: Checking munge on localhost
2023-08-10T17:24:37-05:00 INFO: munge encode/decode successfully
2023-08-10T17:24:37-05:00 INFO: starting mtest-maestro
62a71304f6110e55fe4ac6fcbe955a382d29794dc2037a0b0cc1686f67526348
2023-08-10T17:24:40-05:00 INFO: starting mtest-samp-1
edece3a26a28d555dba71c08d5b8296eae01126d789f7a0df92536c33793d5a4
2023-08-10T17:24:41-05:00 INFO: starting mtest-samp-2
c9c887aa3818d784971a6a5ea93e2d54ccd315704cbebea778ad30613953fc26
2023-08-10T17:24:43-05:00 INFO: starting mtest-samp-3
99ff2a3243d777b5ac2a8176052373313128c5e451684d90fbf16a6ee2fa38cc
2023-08-10T17:24:44-05:00 INFO: starting mtest-samp-4
a0844ba37b35dc8fe3128d7bf51a1a6ec71108139ec3c18f47f52c1e2fe7c38f
2023-08-10T17:24:46-05:00 INFO: mtest-samp-1 is running
2023-08-10T17:24:46-05:00 INFO: mtest-samp-2 is running
2023-08-10T17:24:46-05:00 INFO: mtest-samp-3 is running
2023-08-10T17:24:46-05:00 INFO: mtest-samp-4 is running
2023-08-10T17:24:46-05:00 INFO: starting mtest-agg-11
1877079a9bb828631b776b55b8971db84127fcc909e2963a23206773e2037d8d
2023-08-10T17:24:47-05:00 INFO: starting mtest-agg-12
61e9943216530d2481bb3f346c570006303e6e464f6f6b9714b7264eaaf6f8ee
2023-08-10T17:24:49-05:00 INFO: mtest-agg-11 is running
2023-08-10T17:24:49-05:00 INFO: mtest-agg-12 is running
2023-08-10T17:24:49-05:00 INFO: starting mtest-agg-2
4186f9006411c72777ae4550ddca5b516af2f639e0d4f672b74eec00d4ae64ac
2023-08-10T17:24:50-05:00 INFO: mtest-agg-2 is running
2023-08-10T17:24:50-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-08-10T17:26:51-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-08-10T17:26:53-05:00 INFO: sos check rc: 0
2023-08-10T17:26:54-05:00 INFO: starting mtest-ui
29acfc932ac271dcb715e07ee53a59f09c6c0a12e8748b00be12af2295e4f430
2023-08-10T17:26:56-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[3513404, 1691706294001.474], [3513404, 1691706294001.7358], [3513404, 1691706294001.7432], [3513404, 1691706294001.7842], [3513776, 1691706295001.623], [3513776, 1691706295001.865], [3513776, 1691706295001.886], [3513776, 1691706295001.925], [3513776, 1691706296001.78], [3513776, 1691706296002.025], [3513776, 1691706296002.031], [3513776, 1691706296002.071], [3513776, 1691706297001.152], [3513776, 1691706297001.161], [3513776, 1691706297001.186], [3513776, 1691706297001.917], [3513776, 1691706298000.4941], [3513776, 1691706298000.809], [3513776, 1691706298000.908], [3513776, 1691706298002.065], [3513776, 1691706299001.212], [3513776, 1691706299001.634], [3513776, 1691706299001.956], [3513776, 1691706299002.038], [3513776, 1691706300001.17], [3513776, 1691706300001.349], [3513776, 1691706300001.7668], [3513776, 1691706300002.088], [3513776, 1691706301001.244], [3513776, 1691706301001.3079], [3513776, 1691706301001.502], [3513776, 1691706301001.917], [3513776, 1691706302001.378], [3513776, 1691706302001.422], [3513776, 1691706302001.64], [3513776, 1691706302002.04], [3513776, 1691706303001.184], [3513776, 1691706303001.521], [3513776, 1691706303001.548], [3513776, 1691706303001.781], [3513776, 1691706304001.3179], [3513776, 1691706304001.646], [3513776, 1691706304001.66], [3513776, 1691706304001.912], [3513776, 1691706305001.4568], [3513776, 1691706305001.845], [3513776, 1691706305001.847], [3513776, 1691706305002.05], [3513776, 1691706306001.203], [3513776, 1691706306001.614], [3513776, 1691706306002.004], [3513776, 1691706306002.006], [3513776, 1691706307001.126], [3513776, 1691706307001.134], [3513776, 1691706307001.331], [3513776, 1691706307001.7432], [3513776, 1691706308001.2659], [3513776, 1691706308001.269], [3513776, 1691706308001.4778], [3513776, 1691706308001.8838], [3513776, 1691706309001.408], [3513776, 1691706309001.417], [3513776, 1691706309001.6262], [3513776, 1691706309002.034], [3513776, 1691706310001.1892], [3513776, 1691706310001.565], [3513776, 1691706310001.566], [3513776, 1691706310001.779], [3513776, 1691706311001.3472], [3513776, 1691706311001.7249], [3513776, 1691706311001.728], [3513776, 1691706311001.9358], [3513776, 1691706312000.874], [3513776, 1691706312001.639], [3513776, 1691706312001.847], [3513776, 1691706312002.066], [3513776, 1691706313001.216], [3513776, 1691706313001.79], [3513776, 1691706313001.982], [3513776, 1691706313002.007], [3513776, 1691706314001.145], [3513776, 1691706314001.355], [3513776, 1691706314001.922], [3513776, 1691706314001.938], [3513776, 1691706315000.995], [3513776, 1691706315001.276], [3513776, 1691706315001.29], [3513776, 1691706315002.057], [3513776, 1691706316001.2239], [3513776, 1691706316001.226], [3513776, 1691706316001.227], [3513776, 1691706316001.4138], [3513776, 1691706317001.384], [3513776, 1691706317001.386], [3513776, 1691706317001.391], [3513776, 1691706317001.3928], [3513776, 1691706318001.533], [3513776, 1691706318001.538], [3513776, 1691706318001.541], [3513776, 1691706318001.543], [3513776, 1691706319001.6892], [3513776, 1691706319001.6929], [3513776, 1691706319001.6938], [3513776, 1691706319001.696], [3513776, 1691706320001.8188], [3513776, 1691706320001.822], [3513776, 1691706320001.826], [3513776, 1691706320001.829], [3513776, 1691706321001.964], [3513776, 1691706321001.967], [3513776, 1691706321001.972], [3513776, 1691706321001.974], [3513776, 1691706322002.038], [3513776, 1691706322002.112], [3513776, 1691706322002.115], [3513776, 1691706322002.122], [3513776, 1691706323001.179], [3513776, 1691706323001.236], [3513776, 1691706323001.239], [3513776, 1691706323001.249], [3513776, 1691706324001.31], [3513776, 1691706324001.354], [3513776, 1691706324001.3618], [3513776, 1691706324001.365], [3513776, 1691706325001.4631], [3513776, 1691706325001.495], [3513776, 1691706325001.496], [3513776, 1691706325001.506], [3513776, 1691706326001.601], [3513776, 1691706326001.623], [3513776, 1691706326001.623], [3513776, 1691706326001.628], [3513776, 1691706327001.521], [3513776, 1691706327001.7239], [3513776, 1691706327001.7678], [3513776, 1691706327001.769], [3513776, 1691706328001.675], [3513776, 1691706328001.677], [3513776, 1691706328001.897], [3513776, 1691706328001.9011], [3513776, 1691706329001.821], [3513776, 1691706329001.825], [3513776, 1691706329002.04], [3513776, 1691706329002.051], [3513776, 1691706330001.174], [3513776, 1691706330001.179], [3513776, 1691706330001.9531], [3513776, 1691706330001.958], [3513776, 1691706331001.3], [3513776, 1691706331001.302], [3513776, 1691706331002.076], [3513776, 1691706331002.081], [3513776, 1691706332001.226], [3513776, 1691706332001.233], [3513776, 1691706332001.46], [3513776, 1691706332001.46], [3513776, 1691706333001.366], [3513776, 1691706333001.367], [3513776, 1691706333001.597], [3513776, 1691706333001.603], [3513776, 1691706334001.4988], [3513776, 1691706334001.501], [3513776, 1691706334001.7322], [3513776, 1691706334001.7349], [3513776, 1691706335001.624], [3513776, 1691706335001.629], [3513776, 1691706335001.861], [3513776, 1691706335001.861], [3513776, 1691706336001.76], [3513776, 1691706336001.7642], [3513776, 1691706336001.9998], [3513776, 1691706336002.002], [3513776, 1691706337001.1619], [3513776, 1691706337001.1619], [3513776, 1691706337001.918], [3513776, 1691706337001.92], [3513776, 1691706338001.296], [3513776, 1691706338001.2979], [3513776, 1691706338002.049], [3513776, 1691706338002.051], [3513776, 1691706339001.218], [3513776, 1691706339001.244], [3513776, 1691706339001.3398], [3513776, 1691706339001.406], [3513776, 1691706340001.375], [3513776, 1691706340001.395], [3513776, 1691706340001.4011], [3513776, 1691706340001.48], [3513776, 1691706341001.513], [3513776, 1691706341001.515], [3513776, 1691706341001.523], [3513776, 1691706341001.606], [3513776, 1691706342001.6692], [3513776, 1691706342001.671], [3513776, 1691706342001.677], [3513776, 1691706342001.751], [3513776, 1691706343001.069], [3513776, 1691706343001.7961], [3513776, 1691706343001.811], [3513776, 1691706343001.814], [3513776, 1691706344001.2212], [3513776, 1691706344001.2239], [3513776, 1691706344001.925], [3513776, 1691706344001.96], [3513776, 1691706345001.3599], [3513776, 1691706345001.366], [3513776, 1691706345001.3691], [3513776, 1691706345002.091], [3513776, 1691706346001.222], [3513776, 1691706346001.4421], [3513776, 1691706346001.486], [3513776, 1691706346001.4878], [3513776, 1691706347000.8179], [3513776, 1691706347001.568], [3513776, 1691706347001.627], [3513776, 1691706347001.691], [3513776, 1691706348001.7239], [3513776, 1691706348001.7249], [3513776, 1691706348001.77], [3513776, 1691706348001.842], [3513776, 1691706349001.878], [3513776, 1691706349001.881], [3513776, 1691706349001.916], [3513776, 1691706349001.987], [3513776, 1691706350002.01], [3513776, 1691706350002.012], [3513776, 1691706350002.038], [3513776, 1691706350002.109], [3513776, 1691706351001.139], [3513776, 1691706351001.141], [3513776, 1691706351001.153], [3513776, 1691706351001.2312], [3513776, 1691706352001.2769], [3513776, 1691706352001.2852], [3513776, 1691706352001.2852], [3513776, 1691706352001.3582], [3513776, 1691706353001.4348], [3513776, 1691706353001.437], [3513776, 1691706353001.44], [3513776, 1691706353001.5051], [3513776, 1691706354001.564], [3513776, 1691706354001.567], [3513776, 1691706354001.572], [3513776, 1691706354001.627], [3513776, 1691706355001.7122], [3513776, 1691706355001.7139], [3513776, 1691706355001.7139], [3513776, 1691706355001.7642], [3513776, 1691706356001.857], [3513776, 1691706356001.86], [3513776, 1691706356001.864], [3513776, 1691706356001.8938], [3513776, 1691706357001.991], [3513776, 1691706357001.993], [3513776, 1691706357001.993], [3513776, 1691706357002.018], [3513776, 1691706358001.174], [3513776, 1691706358002.145], [3513776, 1691706358002.151], [3513776, 1691706358002.1519], [3513776, 1691706359001.268], [3513776, 1691706359001.2742], [3513776, 1691706359001.2769], [3513776, 1691706359001.29], [3513776, 1691706360001.3938], [3513776, 1691706360001.405], [3513776, 1691706360001.408], [3513776, 1691706360001.419], [3513776, 1691706361001.523], [3513776, 1691706361001.537], [3513776, 1691706361001.542], [3513776, 1691706361001.5518], [3513776, 1691706362001.653], [3513776, 1691706362001.6582], [3513776, 1691706362001.665], [3513776, 1691706362001.671], [3513776, 1691706363001.815], [3513776, 1691706363001.8162], [3513776, 1691706363001.818], [3513776, 1691706363001.8188], [3513776, 1691706364001.957], [3513776, 1691706364001.958], [3513776, 1691706364001.959], [3513776, 1691706364001.961], [3513776, 1691706365002.0852], [3513776, 1691706365002.09], [3513776, 1691706365002.09], [3513776, 1691706365002.091], [3513776, 1691706366001.216], [3513776, 1691706366001.2249], [3513776, 1691706366001.226], [3513776, 1691706366001.228], [3513776, 1691706367001.3372], [3513776, 1691706367001.3472], [3513776, 1691706367001.3472], [3513776, 1691706367001.3481], [3513776, 1691706368001.496], [3513776, 1691706368001.502], [3513776, 1691706368001.504], [3513776, 1691706368001.507], [3513776, 1691706369000.8398], [3513776, 1691706369001.468], [3513776, 1691706369001.618], [3513776, 1691706369001.646], [3513776, 1691706370001.623], [3513776, 1691706370001.6262], [3513776, 1691706370001.7869], [3513776, 1691706370001.987], [3513776, 1691706371001.771], [3513776, 1691706371001.7761], [3513776, 1691706371001.931], [3513776, 1691706371002.1372], [3513580, 1691706372001.3281], [3513580, 1691706372001.916], [3513580, 1691706372001.917], [3513580, 1691706372002.072], [3513776, 1691706373001.232], [3513776, 1691706373001.486], [3513776, 1691706373002.073], [3513776, 1691706373002.084], [3513776, 1691706374001.201], [3513776, 1691706374001.208], [3513776, 1691706374001.363], [3513776, 1691706374001.623], [3513776, 1691706375001.3591], [3513776, 1691706375001.3591], [3513776, 1691706375001.494], [3513776, 1691706375001.772], [3513776, 1691706376001.496], [3513776, 1691706376001.508], [3513776, 1691706376001.51], [3513776, 1691706376001.8901], [3513776, 1691706377001.632], [3513776, 1691706377001.64], [3513776, 1691706377001.646], [3513776, 1691706377001.6482], [3513776, 1691706378000.8462], [3513776, 1691706378001.344], [3513776, 1691706378001.716], [3513776, 1691706378001.7358], [3513776, 1691706379001.5051], [3513776, 1691706379001.517], [3513776, 1691706379001.546], [3513776, 1691706379001.854], [3513776, 1691706380001.659], [3513776, 1691706380001.671], [3513776, 1691706380001.6729], [3513776, 1691706380001.691], [3513776, 1691706381001.8071], [3513776, 1691706381001.8098], [3513776, 1691706381001.811], [3513776, 1691706381001.829], [3513776, 1691706382001.933], [3513776, 1691706382001.9358], [3513776, 1691706382001.9468], [3513776, 1691706382001.9531], [3513776, 1691706383002.066], [3513776, 1691706383002.07], [3513776, 1691706383002.073], [3513776, 1691706383002.078], [3513776, 1691706384001.226], [3513776, 1691706384001.228], [3513776, 1691706384001.23], [3513776, 1691706384001.2312], [3513776, 1691706385001.364], [3513776, 1691706385001.3682], [3513776, 1691706385001.37], [3513776, 1691706385001.3708], [3513776, 1691706386001.5], [3513776, 1691706386001.5], [3513776, 1691706386001.504], [3513776, 1691706386001.5051], [3513776, 1691706387001.623], [3513776, 1691706387001.625], [3513776, 1691706387001.629], [3513776, 1691706387001.63], [3513776, 1691706388001.7432], [3513776, 1691706388001.7449], [3513776, 1691706388001.7532], [3513776, 1691706388001.755], [3513776, 1691706389001.098], [3513776, 1691706389001.896], [3513776, 1691706389001.903], [3513776, 1691706389001.904], [3513776, 1691706390000.926], [3513776, 1691706390002.025], [3513776, 1691706390002.051], [3513776, 1691706390002.055], [3513776, 1691706391001.175], [3513776, 1691706391001.175], [3513776, 1691706391001.176], [3513776, 1691706391001.181], [3513776, 1691706392001.201], [3513776, 1691706392001.2969], [3513776, 1691706392001.2988], [3513776, 1691706392001.321], [3513776, 1691706393001.3281], [3513776, 1691706393001.344], [3513776, 1691706393001.416], [3513776, 1691706393001.417], [3513776, 1691706394001.487], [3513776, 1691706394001.489], [3513776, 1691706394001.558], [3513776, 1691706394001.564], [3513776, 1691706395001.627], [3513776, 1691706395001.63], [3513776, 1691706395001.687], [3513776, 1691706395001.6892], [3513776, 1691706396001.772], [3513776, 1691706396001.773], [3513776, 1691706396001.8088], [3513776, 1691706396001.8198], [3513776, 1691706397001.9001], [3513776, 1691706397001.9048], [3513776, 1691706397001.9258], [3513776, 1691706397001.945], [3513776, 1691706398002.031], [3513776, 1691706398002.042], [3513776, 1691706398002.049], [3513776, 1691706398002.076], [3513776, 1691706399001.192], [3513776, 1691706399001.192], [3513776, 1691706399001.1929], [3513776, 1691706399001.227], [3513776, 1691706400001.3398], [3513776, 1691706400001.345], [3513776, 1691706400001.3499], [3513776, 1691706400001.363], [3513776, 1691706401001.487], [3513776, 1691706401001.491], [3513776, 1691706401001.491], [3513776, 1691706401001.504], [3513776, 1691706402001.613], [3513776, 1691706402001.6262], [3513776, 1691706402001.63], [3513776, 1691706402001.635], [3513776, 1691706403001.7751], [3513776, 1691706403001.7751], [3513776, 1691706403001.7761], [3513776, 1691706403001.78], [3513776, 1691706404001.933], [3513776, 1691706404001.94], [3513776, 1691706404001.941], [3513776, 1691706404001.943], [3513776, 1691706405002.077], [3513776, 1691706405002.078], [3513776, 1691706405002.08], [3513776, 1691706405002.081], [3513776, 1691706406001.229], [3513776, 1691706406001.234], [3513776, 1691706406001.236], [3513776, 1691706406001.237], [3513776, 1691706407001.3708], [3513776, 1691706407001.373], [3513776, 1691706407001.376], [3513776, 1691706407001.376], [3513776, 1691706408001.506], [3513776, 1691706408001.507], [3513776, 1691706408001.512], [3513776, 1691706408001.513], [3513776, 1691706409001.3828], [3513776, 1691706409001.544], [3513776, 1691706409001.66], [3513776, 1691706409001.675], [3513776, 1691706410001.542], [3513776, 1691706410001.545], [3513776, 1691706410001.7852], [3513776, 1691706410001.8088]]}, {"target": "component_id", "datapoints": [[2, 1691706294001.474], [4, 1691706294001.7358], [3, 1691706294001.7432], [1, 1691706294001.7842], [2, 1691706295001.623], [3, 1691706295001.865], [4, 1691706295001.886], [1, 1691706295001.925], [2, 1691706296001.78], [3, 1691706296002.025], [4, 1691706296002.031], [1, 1691706296002.071], [4, 1691706297001.152], [3, 1691706297001.161], [1, 1691706297001.186], [2, 1691706297001.917], [1, 1691706298000.4941], [3, 1691706298000.809], [4, 1691706298000.908], [2, 1691706298002.065], [2, 1691706299001.212], [1, 1691706299001.634], [3, 1691706299001.956], [4, 1691706299002.038], [4, 1691706300001.17], [2, 1691706300001.349], [1, 1691706300001.7668], [3, 1691706300002.088], [3, 1691706301001.244], [4, 1691706301001.3079], [2, 1691706301001.502], [1, 1691706301001.917], [3, 1691706302001.378], [4, 1691706302001.422], [2, 1691706302001.64], [1, 1691706302002.04], [1, 1691706303001.184], [3, 1691706303001.521], [4, 1691706303001.548], [2, 1691706303001.781], [1, 1691706304001.3179], [3, 1691706304001.646], [4, 1691706304001.66], [2, 1691706304001.912], [1, 1691706305001.4568], [3, 1691706305001.845], [4, 1691706305001.847], [2, 1691706305002.05], [2, 1691706306001.203], [1, 1691706306001.614], [3, 1691706306002.004], [4, 1691706306002.006], [4, 1691706307001.126], [3, 1691706307001.134], [2, 1691706307001.331], [1, 1691706307001.7432], [3, 1691706308001.2659], [4, 1691706308001.269], [2, 1691706308001.4778], [1, 1691706308001.8838], [3, 1691706309001.408], [4, 1691706309001.417], [2, 1691706309001.6262], [1, 1691706309002.034], [1, 1691706310001.1892], [4, 1691706310001.565], [3, 1691706310001.566], [2, 1691706310001.779], [1, 1691706311001.3472], [4, 1691706311001.7249], [3, 1691706311001.728], [2, 1691706311001.9358], [1, 1691706312000.874], [4, 1691706312001.639], [3, 1691706312001.847], [2, 1691706312002.066], [2, 1691706313001.216], [4, 1691706313001.79], [1, 1691706313001.982], [3, 1691706313002.007], [3, 1691706314001.145], [2, 1691706314001.355], [1, 1691706314001.922], [4, 1691706314001.938], [2, 1691706315000.995], [1, 1691706315001.276], [3, 1691706315001.29], [4, 1691706315002.057], [1, 1691706316001.2239], [4, 1691706316001.226], [2, 1691706316001.227], [3, 1691706316001.4138], [4, 1691706317001.384], [1, 1691706317001.386], [3, 1691706317001.391], [2, 1691706317001.3928], [1, 1691706318001.533], [2, 1691706318001.538], [4, 1691706318001.541], [3, 1691706318001.543], [2, 1691706319001.6892], [3, 1691706319001.6929], [4, 1691706319001.6938], [1, 1691706319001.696], [1, 1691706320001.8188], [2, 1691706320001.822], [3, 1691706320001.826], [4, 1691706320001.829], [3, 1691706321001.964], [1, 1691706321001.967], [4, 1691706321001.972], [2, 1691706321001.974], [4, 1691706322002.038], [3, 1691706322002.112], [1, 1691706322002.115], [2, 1691706322002.122], [4, 1691706323001.179], [3, 1691706323001.236], [1, 1691706323001.239], [2, 1691706323001.249], [4, 1691706324001.31], [3, 1691706324001.354], [1, 1691706324001.3618], [2, 1691706324001.365], [4, 1691706325001.4631], [3, 1691706325001.495], [1, 1691706325001.496], [2, 1691706325001.506], [4, 1691706326001.601], [1, 1691706326001.623], [3, 1691706326001.623], [2, 1691706326001.628], [2, 1691706327001.521], [4, 1691706327001.7239], [3, 1691706327001.7678], [1, 1691706327001.769], [4, 1691706328001.675], [2, 1691706328001.677], [3, 1691706328001.897], [1, 1691706328001.9011], [4, 1691706329001.821], [2, 1691706329001.825], [3, 1691706329002.04], [1, 1691706329002.051], [3, 1691706330001.174], [1, 1691706330001.179], [4, 1691706330001.9531], [2, 1691706330001.958], [3, 1691706331001.3], [1, 1691706331001.302], [2, 1691706331002.076], [4, 1691706331002.081], [4, 1691706332001.226], [2, 1691706332001.233], [1, 1691706332001.46], [3, 1691706332001.46], [4, 1691706333001.366], [2, 1691706333001.367], [3, 1691706333001.597], [1, 1691706333001.603], [4, 1691706334001.4988], [2, 1691706334001.501], [3, 1691706334001.7322], [1, 1691706334001.7349], [4, 1691706335001.624], [2, 1691706335001.629], [1, 1691706335001.861], [3, 1691706335001.861], [4, 1691706336001.76], [2, 1691706336001.7642], [3, 1691706336001.9998], [1, 1691706336002.002], [1, 1691706337001.1619], [3, 1691706337001.1619], [2, 1691706337001.918], [4, 1691706337001.92], [3, 1691706338001.296], [1, 1691706338001.2979], [2, 1691706338002.049], [4, 1691706338002.051], [2, 1691706339001.218], [3, 1691706339001.244], [4, 1691706339001.3398], [1, 1691706339001.406], [2, 1691706340001.375], [3, 1691706340001.395], [1, 1691706340001.4011], [4, 1691706340001.48], [2, 1691706341001.513], [3, 1691706341001.515], [1, 1691706341001.523], [4, 1691706341001.606], [1, 1691706342001.6692], [3, 1691706342001.671], [2, 1691706342001.677], [4, 1691706342001.751], [4, 1691706343001.069], [1, 1691706343001.7961], [3, 1691706343001.811], [2, 1691706343001.814], [1, 1691706344001.2212], [4, 1691706344001.2239], [3, 1691706344001.925], [2, 1691706344001.96], [4, 1691706345001.3599], [3, 1691706345001.366], [1, 1691706345001.3691], [2, 1691706345002.091], [2, 1691706346001.222], [1, 1691706346001.4421], [3, 1691706346001.486], [4, 1691706346001.4878], [2, 1691706347000.8179], [1, 1691706347001.568], [3, 1691706347001.627], [4, 1691706347001.691], [1, 1691706348001.7239], [2, 1691706348001.7249], [3, 1691706348001.77], [4, 1691706348001.842], [2, 1691706349001.878], [1, 1691706349001.881], [3, 1691706349001.916], [4, 1691706349001.987], [1, 1691706350002.01], [2, 1691706350002.012], [3, 1691706350002.038], [4, 1691706350002.109], [2, 1691706351001.139], [1, 1691706351001.141], [3, 1691706351001.153], [4, 1691706351001.2312], [1, 1691706352001.2769], [2, 1691706352001.2852], [3, 1691706352001.2852], [4, 1691706352001.3582], [2, 1691706353001.4348], [1, 1691706353001.437], [3, 1691706353001.44], [4, 1691706353001.5051], [3, 1691706354001.564], [2, 1691706354001.567], [1, 1691706354001.572], [4, 1691706354001.627], [2, 1691706355001.7122], [1, 1691706355001.7139], [3, 1691706355001.7139], [4, 1691706355001.7642], [3, 1691706356001.857], [1, 1691706356001.86], [2, 1691706356001.864], [4, 1691706356001.8938], [1, 1691706357001.991], [2, 1691706357001.993], [3, 1691706357001.993], [4, 1691706357002.018], [4, 1691706358001.174], [2, 1691706358002.145], [1, 1691706358002.151], [3, 1691706358002.1519], [1, 1691706359001.268], [2, 1691706359001.2742], [3, 1691706359001.2769], [4, 1691706359001.29], [1, 1691706360001.3938], [3, 1691706360001.405], [2, 1691706360001.408], [4, 1691706360001.419], [1, 1691706361001.523], [3, 1691706361001.537], [2, 1691706361001.542], [4, 1691706361001.5518], [3, 1691706362001.653], [1, 1691706362001.6582], [2, 1691706362001.665], [4, 1691706362001.671], [3, 1691706363001.815], [1, 1691706363001.8162], [4, 1691706363001.818], [2, 1691706363001.8188], [3, 1691706364001.957], [4, 1691706364001.958], [1, 1691706364001.959], [2, 1691706364001.961], [3, 1691706365002.0852], [2, 1691706365002.09], [4, 1691706365002.09], [1, 1691706365002.091], [1, 1691706366001.216], [3, 1691706366001.2249], [4, 1691706366001.226], [2, 1691706366001.228], [2, 1691706367001.3372], [3, 1691706367001.3472], [4, 1691706367001.3472], [1, 1691706367001.3481], [2, 1691706368001.496], [3, 1691706368001.502], [4, 1691706368001.504], [1, 1691706368001.507], [1, 1691706369000.8398], [3, 1691706369001.468], [4, 1691706369001.618], [2, 1691706369001.646], [4, 1691706370001.623], [3, 1691706370001.6262], [2, 1691706370001.7869], [1, 1691706370001.987], [4, 1691706371001.771], [3, 1691706371001.7761], [2, 1691706371001.931], [1, 1691706371002.1372], [1, 1691706372001.3281], [3, 1691706372001.916], [4, 1691706372001.917], [2, 1691706372002.072], [2, 1691706373001.232], [1, 1691706373001.486], [4, 1691706373002.073], [3, 1691706373002.084], [3, 1691706374001.201], [4, 1691706374001.208], [2, 1691706374001.363], [1, 1691706374001.623], [3, 1691706375001.3591], [4, 1691706375001.3591], [2, 1691706375001.494], [1, 1691706375001.772], [3, 1691706376001.496], [2, 1691706376001.508], [4, 1691706376001.51], [1, 1691706376001.8901], [4, 1691706377001.632], [3, 1691706377001.64], [2, 1691706377001.646], [1, 1691706377001.6482], [1, 1691706378000.8462], [3, 1691706378001.344], [2, 1691706378001.716], [4, 1691706378001.7358], [2, 1691706379001.5051], [3, 1691706379001.517], [1, 1691706379001.546], [4, 1691706379001.854], [3, 1691706380001.659], [4, 1691706380001.671], [2, 1691706380001.6729], [1, 1691706380001.691], [4, 1691706381001.8071], [3, 1691706381001.8098], [2, 1691706381001.811], [1, 1691706381001.829], [4, 1691706382001.933], [3, 1691706382001.9358], [2, 1691706382001.9468], [1, 1691706382001.9531], [4, 1691706383002.066], [3, 1691706383002.07], [2, 1691706383002.073], [1, 1691706383002.078], [1, 1691706384001.226], [3, 1691706384001.228], [2, 1691706384001.23], [4, 1691706384001.2312], [2, 1691706385001.364], [3, 1691706385001.3682], [1, 1691706385001.37], [4, 1691706385001.3708], [3, 1691706386001.5], [4, 1691706386001.5], [1, 1691706386001.504], [2, 1691706386001.5051], [3, 1691706387001.623], [4, 1691706387001.625], [2, 1691706387001.629], [1, 1691706387001.63], [3, 1691706388001.7432], [4, 1691706388001.7449], [1, 1691706388001.7532], [2, 1691706388001.755], [4, 1691706389001.098], [3, 1691706389001.896], [1, 1691706389001.903], [2, 1691706389001.904], [4, 1691706390000.926], [2, 1691706390002.025], [1, 1691706390002.051], [3, 1691706390002.055], [2, 1691706391001.175], [4, 1691706391001.175], [1, 1691706391001.176], [3, 1691706391001.181], [3, 1691706392001.201], [1, 1691706392001.2969], [2, 1691706392001.2988], [4, 1691706392001.321], [3, 1691706393001.3281], [4, 1691706393001.344], [1, 1691706393001.416], [2, 1691706393001.417], [3, 1691706394001.487], [4, 1691706394001.489], [1, 1691706394001.558], [2, 1691706394001.564], [4, 1691706395001.627], [3, 1691706395001.63], [2, 1691706395001.687], [1, 1691706395001.6892], [3, 1691706396001.772], [4, 1691706396001.773], [2, 1691706396001.8088], [1, 1691706396001.8198], [3, 1691706397001.9001], [4, 1691706397001.9048], [2, 1691706397001.9258], [1, 1691706397001.945], [4, 1691706398002.031], [3, 1691706398002.042], [2, 1691706398002.049], [1, 1691706398002.076], [2, 1691706399001.192], [3, 1691706399001.192], [4, 1691706399001.1929], [1, 1691706399001.227], [4, 1691706400001.3398], [2, 1691706400001.345], [3, 1691706400001.3499], [1, 1691706400001.363], [4, 1691706401001.487], [2, 1691706401001.491], [3, 1691706401001.491], [1, 1691706401001.504], [2, 1691706402001.613], [3, 1691706402001.6262], [4, 1691706402001.63], [1, 1691706402001.635], [3, 1691706403001.7751], [4, 1691706403001.7751], [2, 1691706403001.7761], [1, 1691706403001.78], [4, 1691706404001.933], [1, 1691706404001.94], [2, 1691706404001.941], [3, 1691706404001.943], [2, 1691706405002.077], [4, 1691706405002.078], [1, 1691706405002.08], [3, 1691706405002.081], [3, 1691706406001.229], [1, 1691706406001.234], [2, 1691706406001.236], [4, 1691706406001.237], [1, 1691706407001.3708], [4, 1691706407001.373], [2, 1691706407001.376], [3, 1691706407001.376], [2, 1691706408001.506], [3, 1691706408001.507], [1, 1691706408001.512], [4, 1691706408001.513], [1, 1691706409001.3828], [3, 1691706409001.544], [2, 1691706409001.66], [4, 1691706409001.675], [3, 1691706410001.542], [1, 1691706410001.545], [4, 1691706410001.7852], [2, 1691706410001.8088]]}, {"target": "job_id", "datapoints": [[0, 1691706294001.474], [0, 1691706294001.7358], [0, 1691706294001.7432], [0, 1691706294001.7842], [0, 1691706295001.623], [0, 1691706295001.865], [0, 1691706295001.886], [0, 1691706295001.925], [0, 1691706296001.78], [0, 1691706296002.025], [0, 1691706296002.031], [0, 1691706296002.071], [0, 1691706297001.152], [0, 1691706297001.161], [0, 1691706297001.186], [0, 1691706297001.917], [0, 1691706298000.4941], [0, 1691706298000.809], [0, 1691706298000.908], [0, 1691706298002.065], [0, 1691706299001.212], [0, 1691706299001.634], [0, 1691706299001.956], [0, 1691706299002.038], [0, 1691706300001.17], [0, 1691706300001.349], [0, 1691706300001.7668], [0, 1691706300002.088], [0, 1691706301001.244], [0, 1691706301001.3079], [0, 1691706301001.502], [0, 1691706301001.917], [0, 1691706302001.378], [0, 1691706302001.422], [0, 1691706302001.64], [0, 1691706302002.04], [0, 1691706303001.184], [0, 1691706303001.521], [0, 1691706303001.548], [0, 1691706303001.781], [0, 1691706304001.3179], [0, 1691706304001.646], [0, 1691706304001.66], [0, 1691706304001.912], [0, 1691706305001.4568], [0, 1691706305001.845], [0, 1691706305001.847], [0, 1691706305002.05], [0, 1691706306001.203], [0, 1691706306001.614], [0, 1691706306002.004], [0, 1691706306002.006], [0, 1691706307001.126], [0, 1691706307001.134], [0, 1691706307001.331], [0, 1691706307001.7432], [0, 1691706308001.2659], [0, 1691706308001.269], [0, 1691706308001.4778], [0, 1691706308001.8838], [0, 1691706309001.408], [0, 1691706309001.417], [0, 1691706309001.6262], [0, 1691706309002.034], [0, 1691706310001.1892], [0, 1691706310001.565], [0, 1691706310001.566], [0, 1691706310001.779], [0, 1691706311001.3472], [0, 1691706311001.7249], [0, 1691706311001.728], [0, 1691706311001.9358], [0, 1691706312000.874], [0, 1691706312001.639], [0, 1691706312001.847], [0, 1691706312002.066], [0, 1691706313001.216], [0, 1691706313001.79], [0, 1691706313001.982], [0, 1691706313002.007], [0, 1691706314001.145], [0, 1691706314001.355], [0, 1691706314001.922], [0, 1691706314001.938], [0, 1691706315000.995], [0, 1691706315001.276], [0, 1691706315001.29], [0, 1691706315002.057], [0, 1691706316001.2239], [0, 1691706316001.226], [0, 1691706316001.227], [0, 1691706316001.4138], [0, 1691706317001.384], [0, 1691706317001.386], [0, 1691706317001.391], [0, 1691706317001.3928], [0, 1691706318001.533], [0, 1691706318001.538], [0, 1691706318001.541], [0, 1691706318001.543], [0, 1691706319001.6892], [0, 1691706319001.6929], [0, 1691706319001.6938], [0, 1691706319001.696], [0, 1691706320001.8188], [0, 1691706320001.822], [0, 1691706320001.826], [0, 1691706320001.829], [0, 1691706321001.964], [0, 1691706321001.967], [0, 1691706321001.972], [0, 1691706321001.974], [0, 1691706322002.038], [0, 1691706322002.112], [0, 1691706322002.115], [0, 1691706322002.122], [0, 1691706323001.179], [0, 1691706323001.236], [0, 1691706323001.239], [0, 1691706323001.249], [0, 1691706324001.31], [0, 1691706324001.354], [0, 1691706324001.3618], [0, 1691706324001.365], [0, 1691706325001.4631], [0, 1691706325001.495], [0, 1691706325001.496], [0, 1691706325001.506], [0, 1691706326001.601], [0, 1691706326001.623], [0, 1691706326001.623], [0, 1691706326001.628], [0, 1691706327001.521], [0, 1691706327001.7239], [0, 1691706327001.7678], [0, 1691706327001.769], [0, 1691706328001.675], [0, 1691706328001.677], [0, 1691706328001.897], [0, 1691706328001.9011], [0, 1691706329001.821], [0, 1691706329001.825], [0, 1691706329002.04], [0, 1691706329002.051], [0, 1691706330001.174], [0, 1691706330001.179], [0, 1691706330001.9531], [0, 1691706330001.958], [0, 1691706331001.3], [0, 1691706331001.302], [0, 1691706331002.076], [0, 1691706331002.081], [0, 1691706332001.226], [0, 1691706332001.233], [0, 1691706332001.46], [0, 1691706332001.46], [0, 1691706333001.366], [0, 1691706333001.367], [0, 1691706333001.597], [0, 1691706333001.603], [0, 1691706334001.4988], [0, 1691706334001.501], [0, 1691706334001.7322], [0, 1691706334001.7349], [0, 1691706335001.624], [0, 1691706335001.629], [0, 1691706335001.861], [0, 1691706335001.861], [0, 1691706336001.76], [0, 1691706336001.7642], [0, 1691706336001.9998], [0, 1691706336002.002], [0, 1691706337001.1619], [0, 1691706337001.1619], [0, 1691706337001.918], [0, 1691706337001.92], [0, 1691706338001.296], [0, 1691706338001.2979], [0, 1691706338002.049], [0, 1691706338002.051], [0, 1691706339001.218], [0, 1691706339001.244], [0, 1691706339001.3398], [0, 1691706339001.406], [0, 1691706340001.375], [0, 1691706340001.395], [0, 1691706340001.4011], [0, 1691706340001.48], [0, 1691706341001.513], [0, 1691706341001.515], [0, 1691706341001.523], [0, 1691706341001.606], [0, 1691706342001.6692], [0, 1691706342001.671], [0, 1691706342001.677], [0, 1691706342001.751], [0, 1691706343001.069], [0, 1691706343001.7961], [0, 1691706343001.811], [0, 1691706343001.814], [0, 1691706344001.2212], [0, 1691706344001.2239], [0, 1691706344001.925], [0, 1691706344001.96], [0, 1691706345001.3599], [0, 1691706345001.366], [0, 1691706345001.3691], [0, 1691706345002.091], [0, 1691706346001.222], [0, 1691706346001.4421], [0, 1691706346001.486], [0, 1691706346001.4878], [0, 1691706347000.8179], [0, 1691706347001.568], [0, 1691706347001.627], [0, 1691706347001.691], [0, 1691706348001.7239], [0, 1691706348001.7249], [0, 1691706348001.77], [0, 1691706348001.842], [0, 1691706349001.878], [0, 1691706349001.881], [0, 1691706349001.916], [0, 1691706349001.987], [0, 1691706350002.01], [0, 1691706350002.012], [0, 1691706350002.038], [0, 1691706350002.109], [0, 1691706351001.139], [0, 1691706351001.141], [0, 1691706351001.153], [0, 1691706351001.2312], [0, 1691706352001.2769], [0, 1691706352001.2852], [0, 1691706352001.2852], [0, 1691706352001.3582], [0, 1691706353001.4348], [0, 1691706353001.437], [0, 1691706353001.44], [0, 1691706353001.5051], [0, 1691706354001.564], [0, 1691706354001.567], [0, 1691706354001.572], [0, 1691706354001.627], [0, 1691706355001.7122], [0, 1691706355001.7139], [0, 1691706355001.7139], [0, 1691706355001.7642], [0, 1691706356001.857], [0, 1691706356001.86], [0, 1691706356001.864], [0, 1691706356001.8938], [0, 1691706357001.991], [0, 1691706357001.993], [0, 1691706357001.993], [0, 1691706357002.018], [0, 1691706358001.174], [0, 1691706358002.145], [0, 1691706358002.151], [0, 1691706358002.1519], [0, 1691706359001.268], [0, 1691706359001.2742], [0, 1691706359001.2769], [0, 1691706359001.29], [0, 1691706360001.3938], [0, 1691706360001.405], [0, 1691706360001.408], [0, 1691706360001.419], [0, 1691706361001.523], [0, 1691706361001.537], [0, 1691706361001.542], [0, 1691706361001.5518], [0, 1691706362001.653], [0, 1691706362001.6582], [0, 1691706362001.665], [0, 1691706362001.671], [0, 1691706363001.815], [0, 1691706363001.8162], [0, 1691706363001.818], [0, 1691706363001.8188], [0, 1691706364001.957], [0, 1691706364001.958], [0, 1691706364001.959], [0, 1691706364001.961], [0, 1691706365002.0852], [0, 1691706365002.09], [0, 1691706365002.09], [0, 1691706365002.091], [0, 1691706366001.216], [0, 1691706366001.2249], [0, 1691706366001.226], [0, 1691706366001.228], [0, 1691706367001.3372], [0, 1691706367001.3472], [0, 1691706367001.3472], [0, 1691706367001.3481], [0, 1691706368001.496], [0, 1691706368001.502], [0, 1691706368001.504], [0, 1691706368001.507], [0, 1691706369000.8398], [0, 1691706369001.468], [0, 1691706369001.618], [0, 1691706369001.646], [0, 1691706370001.623], [0, 1691706370001.6262], [0, 1691706370001.7869], [0, 1691706370001.987], [0, 1691706371001.771], [0, 1691706371001.7761], [0, 1691706371001.931], [0, 1691706371002.1372], [0, 1691706372001.3281], [0, 1691706372001.916], [0, 1691706372001.917], [0, 1691706372002.072], [0, 1691706373001.232], [0, 1691706373001.486], [0, 1691706373002.073], [0, 1691706373002.084], [0, 1691706374001.201], [0, 1691706374001.208], [0, 1691706374001.363], [0, 1691706374001.623], [0, 1691706375001.3591], [0, 1691706375001.3591], [0, 1691706375001.494], [0, 1691706375001.772], [0, 1691706376001.496], [0, 1691706376001.508], [0, 1691706376001.51], [0, 1691706376001.8901], [0, 1691706377001.632], [0, 1691706377001.64], [0, 1691706377001.646], [0, 1691706377001.6482], [0, 1691706378000.8462], [0, 1691706378001.344], [0, 1691706378001.716], [0, 1691706378001.7358], [0, 1691706379001.5051], [0, 1691706379001.517], [0, 1691706379001.546], [0, 1691706379001.854], [0, 1691706380001.659], [0, 1691706380001.671], [0, 1691706380001.6729], [0, 1691706380001.691], [0, 1691706381001.8071], [0, 1691706381001.8098], [0, 1691706381001.811], [0, 1691706381001.829], [0, 1691706382001.933], [0, 1691706382001.9358], [0, 1691706382001.9468], [0, 1691706382001.9531], [0, 1691706383002.066], [0, 1691706383002.07], [0, 1691706383002.073], [0, 1691706383002.078], [0, 1691706384001.226], [0, 1691706384001.228], [0, 1691706384001.23], [0, 1691706384001.2312], [0, 1691706385001.364], [0, 1691706385001.3682], [0, 1691706385001.37], [0, 1691706385001.3708], [0, 1691706386001.5], [0, 1691706386001.5], [0, 1691706386001.504], [0, 1691706386001.5051], [0, 1691706387001.623], [0, 1691706387001.625], [0, 1691706387001.629], [0, 1691706387001.63], [0, 1691706388001.7432], [0, 1691706388001.7449], [0, 1691706388001.7532], [0, 1691706388001.755], [0, 1691706389001.098], [0, 1691706389001.896], [0, 1691706389001.903], [0, 1691706389001.904], [0, 1691706390000.926], [0, 1691706390002.025], [0, 1691706390002.051], [0, 1691706390002.055], [0, 1691706391001.175], [0, 1691706391001.175], [0, 1691706391001.176], [0, 1691706391001.181], [0, 1691706392001.201], [0, 1691706392001.2969], [0, 1691706392001.2988], [0, 1691706392001.321], [0, 1691706393001.3281], [0, 1691706393001.344], [0, 1691706393001.416], [0, 1691706393001.417], [0, 1691706394001.487], [0, 1691706394001.489], [0, 1691706394001.558], [0, 1691706394001.564], [0, 1691706395001.627], [0, 1691706395001.63], [0, 1691706395001.687], [0, 1691706395001.6892], [0, 1691706396001.772], [0, 1691706396001.773], [0, 1691706396001.8088], [0, 1691706396001.8198], [0, 1691706397001.9001], [0, 1691706397001.9048], [0, 1691706397001.9258], [0, 1691706397001.945], [0, 1691706398002.031], [0, 1691706398002.042], [0, 1691706398002.049], [0, 1691706398002.076], [0, 1691706399001.192], [0, 1691706399001.192], [0, 1691706399001.1929], [0, 1691706399001.227], [0, 1691706400001.3398], [0, 1691706400001.345], [0, 1691706400001.3499], [0, 1691706400001.363], [0, 1691706401001.487], [0, 1691706401001.491], [0, 1691706401001.491], [0, 1691706401001.504], [0, 1691706402001.613], [0, 1691706402001.6262], [0, 1691706402001.63], [0, 1691706402001.635], [0, 1691706403001.7751], [0, 1691706403001.7751], [0, 1691706403001.7761], [0, 1691706403001.78], [0, 1691706404001.933], [0, 1691706404001.94], [0, 1691706404001.941], [0, 1691706404001.943], [0, 1691706405002.077], [0, 1691706405002.078], [0, 1691706405002.08], [0, 1691706405002.081], [0, 1691706406001.229], [0, 1691706406001.234], [0, 1691706406001.236], [0, 1691706406001.237], [0, 1691706407001.3708], [0, 1691706407001.373], [0, 1691706407001.376], [0, 1691706407001.376], [0, 1691706408001.506], [0, 1691706408001.507], [0, 1691706408001.512], [0, 1691706408001.513], [0, 1691706409001.3828], [0, 1691706409001.544], [0, 1691706409001.66], [0, 1691706409001.675], [0, 1691706410001.542], [0, 1691706410001.545], [0, 1691706410001.7852], [0, 1691706410001.8088]]}]'
comp_ids:{1, 2, 3, 4}
2023-08-10T17:26:58-05:00 INFO: query check RC: 0
89067fe2685bda848d2dd94ba8a1b0c5f5d1915dfbdb85ea662ccc1e021ebea2
2023-08-10T17:27:30-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2457    758 --:--:-- --:--:-- --:--:--  3236
{"datasource":{"id":1,"uid":"nnzN1a64z","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-08-10T17:27:31-05:00 INFO: Checking grafana data
2023-08-10T17:27:31-05:00 INFO: Grafana data check, rc: 0
2023-08-10T17:27:31-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-08-10T17:27:36-05:00 INFO: DONE
2023-08-10 17:27:46 INFO: ----------------------------------------------
2023-08-10 17:27:46 INFO: ======== test-maestro-munge ========
2023-08-10 17:27:46 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000370024 s, 11.1 MB/s
2023-08-10T17:27:48-05:00 INFO: starting mtest-maestro
605163493ec7dd74a0b039fd5d1ea9f3516dc2341c6c836ff726e9ba9245d8e7
2023-08-10T17:27:50-05:00 INFO: starting mtest-samp-1
044904316b9598d3eb5d7a9ce9502b9af86c5dd8e3400a4e582d81e198d46bd8
2023-08-10T17:27:51-05:00 INFO: starting mtest-samp-2
cc88161727514d4866768a75c0f0c9c5fea11bf6364ae347f4dc92258a11a652
2023-08-10T17:27:53-05:00 INFO: starting mtest-samp-3
47508fed047c6adfac52bf6fed3c7141649f31556f064b27cc47797ae5b8c961
2023-08-10T17:27:55-05:00 INFO: starting mtest-samp-4
5790bc365733cefc0945bb831c26c5a3522c549bbf11949f711e0481e920b60a
2023-08-10T17:27:56-05:00 INFO: mtest-samp-1 is running
2023-08-10T17:27:56-05:00 INFO: mtest-samp-2 is running
2023-08-10T17:27:56-05:00 INFO: mtest-samp-3 is running
2023-08-10T17:27:56-05:00 INFO: mtest-samp-4 is running
2023-08-10T17:27:56-05:00 INFO: starting mtest-agg-11
1bf6e4c1cffe20c20d2cb201fd687ef5a85b6624694a9aa23bb294d94dccbca5
2023-08-10T17:27:58-05:00 INFO: starting mtest-agg-12
19e14d103fc78cf23e16a7371a273e599455d998777421b23282ef609eeafa40
2023-08-10T17:27:59-05:00 INFO: mtest-agg-11 is running
2023-08-10T17:27:59-05:00 INFO: mtest-agg-12 is running
2023-08-10T17:27:59-05:00 INFO: starting mtest-agg-2
a14ed2ebb069f2fd6e57d24a620cec698381f00ab44aab167b7b3f5dd3c7e8de
2023-08-10T17:28:00-05:00 INFO: mtest-agg-2 is running
2023-08-10T17:28:00-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-08-10T17:30:01-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-08-10T17:30:03-05:00 INFO: sos check rc: 0
2023-08-10T17:30:05-05:00 INFO: starting mtest-ui
ca64701c78754f63ab8b7db08f871dd06ca8043465f8797551cb9ba1295a785f
2023-08-10T17:30:06-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[3516508, 1691706501001.868], [3516508, 1691706501001.8718], [3516508, 1691706501001.8728], [3516508, 1691706501001.978], [3516848, 1691706502002.029], [3516848, 1691706502002.03], [3516848, 1691706502002.0322], [3516848, 1691706502002.073], [3516848, 1691706503001.1829], [3516848, 1691706503001.185], [3516848, 1691706503001.188], [3516848, 1691706503001.213], [3516848, 1691706504001.3198], [3516848, 1691706504001.322], [3516848, 1691706504001.325], [3516848, 1691706504001.3408], [3516848, 1691706505001.4631], [3516848, 1691706505001.464], [3516848, 1691706505001.47], [3516848, 1691706505001.47], [3516848, 1691706506001.597], [3516848, 1691706506001.5989], [3516848, 1691706506001.6], [3516848, 1691706506001.602], [3516848, 1691706507001.7532], [3516848, 1691706507001.755], [3516848, 1691706507001.758], [3516848, 1691706507001.759], [3516848, 1691706508001.907], [3516848, 1691706508001.91], [3516848, 1691706508001.9111], [3516848, 1691706508001.913], [3516848, 1691706509002.059], [3516848, 1691706509002.059], [3516848, 1691706509002.064], [3516848, 1691706509002.065], [3516848, 1691706510001.181], [3516848, 1691706510001.181], [3516848, 1691706510001.191], [3516848, 1691706510001.191], [3516848, 1691706511001.3298], [3516848, 1691706511001.333], [3516848, 1691706511001.3381], [3516848, 1691706511001.3381], [3516848, 1691706512001.491], [3516848, 1691706512001.495], [3516848, 1691706512001.497], [3516848, 1691706512001.498], [3516848, 1691706513001.459], [3516848, 1691706513001.612], [3516848, 1691706513001.6372], [3516848, 1691706513001.64], [3516848, 1691706514001.594], [3516848, 1691706514001.595], [3516848, 1691706514001.7642], [3516848, 1691706514001.7651], [3516848, 1691706515001.7148], [3516848, 1691706515001.7212], [3516848, 1691706515001.889], [3516848, 1691706515001.891], [3516848, 1691706516001.856], [3516848, 1691706516001.8591], [3516848, 1691706516002.025], [3516848, 1691706516002.027], [3516848, 1691706517001.1829], [3516848, 1691706517001.186], [3516848, 1691706517002.014], [3516848, 1691706517002.016], [3516848, 1691706518001.158], [3516848, 1691706518001.159], [3516848, 1691706518001.3079], [3516848, 1691706518001.31], [3516848, 1691706519001.2942], [3516848, 1691706519001.2952], [3516848, 1691706519001.426], [3516848, 1691706519001.4312], [3516848, 1691706520001.4348], [3516848, 1691706520001.438], [3516848, 1691706520001.551], [3516848, 1691706520001.561], [3516848, 1691706521001.563], [3516848, 1691706521001.565], [3516848, 1691706521001.666], [3516848, 1691706521001.678], [3516848, 1691706522001.729], [3516848, 1691706522001.7322], [3516848, 1691706522001.814], [3516848, 1691706522001.823], [3516848, 1691706523001.8801], [3516848, 1691706523001.8801], [3516848, 1691706523001.944], [3516848, 1691706523001.9468], [3516848, 1691706524002.003], [3516848, 1691706524002.011], [3516848, 1691706524002.056], [3516848, 1691706524002.062], [3516848, 1691706525001.107], [3516848, 1691706525001.115], [3516848, 1691706525001.17], [3516848, 1691706525001.177], [3516848, 1691706526001.248], [3516848, 1691706526001.257], [3516848, 1691706526001.3079], [3516848, 1691706526001.31], [3516848, 1691706527000.989], [3516848, 1691706527001.413], [3516848, 1691706527001.451], [3516848, 1691706527001.4521], [3516848, 1691706528001.555], [3516848, 1691706528001.578], [3516848, 1691706528001.579], [3516848, 1691706528002.129], [3516848, 1691706529001.2732], [3516848, 1691706529001.7012], [3516848, 1691706529001.709], [3516848, 1691706529001.713], [3516848, 1691706530001.428], [3516848, 1691706530001.8518], [3516848, 1691706530001.8591], [3516848, 1691706530001.8628], [3516848, 1691706531001.563], [3516848, 1691706531001.984], [3516848, 1691706531001.988], [3516848, 1691706531001.989], [3516848, 1691706532001.15], [3516848, 1691706532001.7012], [3516848, 1691706532002.103], [3516848, 1691706532002.107], [3516848, 1691706533001.261], [3516848, 1691706533001.2659], [3516848, 1691706533001.269], [3516848, 1691706533001.272], [3516848, 1691706534001.3938], [3516848, 1691706534001.399], [3516848, 1691706534001.399], [3516848, 1691706534001.4001], [3516848, 1691706535001.533], [3516848, 1691706535001.536], [3516848, 1691706535001.538], [3516848, 1691706535001.539], [3516848, 1691706536001.691], [3516848, 1691706536001.6938], [3516848, 1691706536001.695], [3516848, 1691706536001.698], [3516848, 1691706537001.821], [3516848, 1691706537001.825], [3516848, 1691706537001.826], [3516848, 1691706537001.8281], [3516848, 1691706538001.982], [3516848, 1691706538001.983], [3516848, 1691706538001.985], [3516848, 1691706538001.986], [3516848, 1691706539002.118], [3516848, 1691706539002.1199], [3516848, 1691706539002.1208], [3516848, 1691706539002.122], [3516848, 1691706540001.2532], [3516848, 1691706540001.26], [3516848, 1691706540001.261], [3516848, 1691706540001.262], [3516848, 1691706541001.386], [3516848, 1691706541001.3892], [3516848, 1691706541001.391], [3516848, 1691706541001.391], [3516848, 1691706542001.52], [3516848, 1691706542001.53], [3516848, 1691706542001.5308], [3516848, 1691706542001.532], [3516848, 1691706543001.682], [3516848, 1691706543001.684], [3516848, 1691706543001.687], [3516848, 1691706543001.687], [3516848, 1691706544001.8062], [3516848, 1691706544001.812], [3516848, 1691706544001.818], [3516848, 1691706544001.818], [3516848, 1691706545001.928], [3516848, 1691706545001.933], [3516848, 1691706545001.941], [3516848, 1691706545001.941], [3516848, 1691706546002.042], [3516848, 1691706546002.0469], [3516848, 1691706546002.049], [3516848, 1691706546002.056], [3516848, 1691706547000.701], [3516848, 1691706547001.174], [3516848, 1691706547001.174], [3516848, 1691706547001.19], [3516848, 1691706548001.3271], [3516848, 1691706548001.332], [3516848, 1691706548001.335], [3516848, 1691706548001.857], [3516848, 1691706549001.4841], [3516848, 1691706549001.4841], [3516848, 1691706549001.485], [3516848, 1691706549001.7852], [3516848, 1691706550001.63], [3516848, 1691706550001.635], [3516848, 1691706550001.642], [3516848, 1691706550001.642], [3516848, 1691706551001.7559], [3516848, 1691706551001.7568], [3516848, 1691706551001.76], [3516848, 1691706551001.7632], [3516848, 1691706552001.895], [3516848, 1691706552001.898], [3516848, 1691706552001.902], [3516848, 1691706552001.903], [3516848, 1691706553002.057], [3516848, 1691706553002.059], [3516848, 1691706553002.061], [3516848, 1691706553002.062], [3516848, 1691706554001.177], [3516848, 1691706554001.1829], [3516848, 1691706554001.187], [3516848, 1691706554001.19], [3516848, 1691706555001.3088], [3516848, 1691706555001.31], [3516848, 1691706555001.312], [3516848, 1691706555001.312], [3516848, 1691706556001.427], [3516848, 1691706556001.428], [3516848, 1691706556001.429], [3516848, 1691706556001.4312], [3516848, 1691706557001.561], [3516848, 1691706557001.564], [3516848, 1691706557001.564], [3516848, 1691706557001.567], [3516852, 1691706558001.3088], [3516852, 1691706558001.6902], [3516852, 1691706558001.7212], [3516852, 1691706558001.7212], [3516848, 1691706559001.458], [3516848, 1691706559001.4631], [3516848, 1691706559001.8618], [3516848, 1691706559001.864], [3516848, 1691706560001.596], [3516848, 1691706560001.6], [3516848, 1691706560002.002], [3516848, 1691706560002.007], [3516848, 1691706561001.142], [3516848, 1691706561001.149], [3516848, 1691706561001.7349], [3516848, 1691706561001.74], [3516848, 1691706562001.291], [3516848, 1691706562001.293], [3516848, 1691706562001.8792], [3516848, 1691706562001.881], [3516848, 1691706563001.391], [3516848, 1691706563001.4038], [3516848, 1691706563001.582], [3516848, 1691706563001.666], [3516848, 1691706564001.547], [3516848, 1691706564001.559], [3516848, 1691706564001.568], [3516848, 1691706564001.569], [3516848, 1691706565001.2212], [3516848, 1691706565001.682], [3516848, 1691706565001.6892], [3516848, 1691706565001.6938], [3516848, 1691706566001.373], [3516848, 1691706566001.373], [3516848, 1691706566001.8281], [3516848, 1691706566001.829], [3516848, 1691706567001.519], [3516848, 1691706567001.521], [3516848, 1691706567001.966], [3516848, 1691706567001.97], [3516848, 1691706568001.665], [3516848, 1691706568001.666], [3516848, 1691706568002.105], [3516848, 1691706568002.105], [3516848, 1691706569001.26], [3516848, 1691706569001.261], [3516848, 1691706569001.8188], [3516848, 1691706569001.821], [3516848, 1691706570001.117], [3516848, 1691706570001.398], [3516848, 1691706570001.943], [3516848, 1691706570001.949], [3516848, 1691706571001.272], [3516848, 1691706571001.2732], [3516848, 1691706571001.545], [3516848, 1691706571002.069], [3516848, 1691706572001.202], [3516848, 1691706572001.203], [3516848, 1691706572001.396], [3516848, 1691706572001.675], [3516848, 1691706573001.324], [3516848, 1691706573001.3408], [3516848, 1691706573001.342], [3516848, 1691706573001.7979], [3516848, 1691706574001.482], [3516848, 1691706574001.485], [3516848, 1691706574001.486], [3516848, 1691706574001.948], [3516848, 1691706575000.747], [3516848, 1691706575001.2979], [3516848, 1691706575001.6], [3516848, 1691706575001.613], [3516848, 1691706576001.434], [3516848, 1691706576001.7239], [3516848, 1691706576001.73], [3516848, 1691706576001.734], [3516848, 1691706577001.59], [3516848, 1691706577001.594], [3516848, 1691706577001.85], [3516848, 1691706577001.8718], [3516848, 1691706578001.7139], [3516848, 1691706578001.716], [3516848, 1691706578001.7239], [3516848, 1691706578002.004], [3516848, 1691706579001.158], [3516848, 1691706579001.8728], [3516848, 1691706579001.874], [3516848, 1691706579001.877], [3516848, 1691706580001.2942], [3516848, 1691706580002.011], [3516848, 1691706580002.011], [3516848, 1691706580002.013], [3516848, 1691706581001.15], [3516848, 1691706581001.151], [3516848, 1691706581001.152], [3516848, 1691706581001.434], [3516848, 1691706582001.271], [3516848, 1691706582001.275], [3516848, 1691706582001.2769], [3516848, 1691706582001.568], [3516848, 1691706583001.396], [3516848, 1691706583001.399], [3516848, 1691706583001.407], [3516848, 1691706583001.698], [3516848, 1691706584001.557], [3516848, 1691706584001.558], [3516848, 1691706584001.56], [3516848, 1691706584001.847], [3516848, 1691706585001.6829], [3516848, 1691706585001.692], [3516848, 1691706585001.6938], [3516848, 1691706585001.983], [3516848, 1691706586001.8098], [3516848, 1691706586001.8162], [3516848, 1691706586001.818], [3516848, 1691706586002.113], [3516848, 1691706587001.244], [3516848, 1691706587001.934], [3516848, 1691706587001.935], [3516848, 1691706587001.937], [3516848, 1691706588001.3691], [3516848, 1691706588002.0532], [3516848, 1691706588002.0532], [3516848, 1691706588002.054], [3516848, 1691706589001.215], [3516848, 1691706589001.216], [3516848, 1691706589001.2202], [3516848, 1691706589001.5308], [3516848, 1691706590001.354], [3516848, 1691706590001.354], [3516848, 1691706590001.3591], [3516848, 1691706590001.6719], [3516848, 1691706591001.4731], [3516848, 1691706591001.48], [3516848, 1691706591001.482], [3516848, 1691706591001.812], [3516848, 1691706592001.622], [3516848, 1691706592001.623], [3516848, 1691706592001.628], [3516848, 1691706592001.9531], [3516848, 1691706593001.77], [3516848, 1691706593001.771], [3516848, 1691706593001.7751], [3516848, 1691706593002.094], [3516848, 1691706594001.8828], [3516848, 1691706594001.961], [3516848, 1691706594002.03], [3516848, 1691706594002.697], [3516848, 1691706595001.192], [3516848, 1691706595001.1992], [3516848, 1691706595001.203], [3516848, 1691706595001.844], [3516848, 1691706596001.3198], [3516848, 1691706596001.325], [3516848, 1691706596001.3289], [3516848, 1691706596001.976], [3516848, 1691706597001.466], [3516848, 1691706597001.468], [3516848, 1691706597001.469], [3516848, 1691706597002.1172], [3516848, 1691706598001.259], [3516848, 1691706598001.608], [3516848, 1691706598001.6099], [3516848, 1691706598001.611], [3516848, 1691706599001.406], [3516848, 1691706599001.7642], [3516848, 1691706599001.7659], [3516848, 1691706599001.7678], [3516848, 1691706600001.565], [3516848, 1691706600001.962], [3516848, 1691706600001.968], [3516848, 1691706600001.968]]}, {"target": "component_id", "datapoints": [[1, 1691706501001.868], [2, 1691706501001.8718], [3, 1691706501001.8728], [4, 1691706501001.978], [1, 1691706502002.029], [2, 1691706502002.03], [3, 1691706502002.0322], [4, 1691706502002.073], [3, 1691706503001.1829], [1, 1691706503001.185], [2, 1691706503001.188], [4, 1691706503001.213], [1, 1691706504001.3198], [3, 1691706504001.322], [2, 1691706504001.325], [4, 1691706504001.3408], [3, 1691706505001.4631], [1, 1691706505001.464], [2, 1691706505001.47], [4, 1691706505001.47], [2, 1691706506001.597], [3, 1691706506001.5989], [4, 1691706506001.6], [1, 1691706506001.602], [3, 1691706507001.7532], [1, 1691706507001.755], [4, 1691706507001.758], [2, 1691706507001.759], [3, 1691706508001.907], [2, 1691706508001.91], [1, 1691706508001.9111], [4, 1691706508001.913], [1, 1691706509002.059], [4, 1691706509002.059], [3, 1691706509002.064], [2, 1691706509002.065], [2, 1691706510001.181], [3, 1691706510001.181], [1, 1691706510001.191], [4, 1691706510001.191], [1, 1691706511001.3298], [4, 1691706511001.333], [2, 1691706511001.3381], [3, 1691706511001.3381], [3, 1691706512001.491], [2, 1691706512001.495], [4, 1691706512001.497], [1, 1691706512001.498], [1, 1691706513001.459], [2, 1691706513001.612], [4, 1691706513001.6372], [3, 1691706513001.64], [2, 1691706514001.594], [1, 1691706514001.595], [3, 1691706514001.7642], [4, 1691706514001.7651], [2, 1691706515001.7148], [1, 1691706515001.7212], [3, 1691706515001.889], [4, 1691706515001.891], [1, 1691706516001.856], [2, 1691706516001.8591], [3, 1691706516002.025], [4, 1691706516002.027], [4, 1691706517001.1829], [3, 1691706517001.186], [2, 1691706517002.014], [1, 1691706517002.016], [1, 1691706518001.158], [2, 1691706518001.159], [4, 1691706518001.3079], [3, 1691706518001.31], [1, 1691706519001.2942], [2, 1691706519001.2952], [4, 1691706519001.426], [3, 1691706519001.4312], [2, 1691706520001.4348], [1, 1691706520001.438], [4, 1691706520001.551], [3, 1691706520001.561], [1, 1691706521001.563], [2, 1691706521001.565], [4, 1691706521001.666], [3, 1691706521001.678], [1, 1691706522001.729], [2, 1691706522001.7322], [4, 1691706522001.814], [3, 1691706522001.823], [1, 1691706523001.8801], [2, 1691706523001.8801], [4, 1691706523001.944], [3, 1691706523001.9468], [2, 1691706524002.003], [1, 1691706524002.011], [3, 1691706524002.056], [4, 1691706524002.062], [2, 1691706525001.107], [1, 1691706525001.115], [3, 1691706525001.17], [4, 1691706525001.177], [2, 1691706526001.248], [1, 1691706526001.257], [4, 1691706526001.3079], [3, 1691706526001.31], [2, 1691706527000.989], [1, 1691706527001.413], [4, 1691706527001.451], [3, 1691706527001.4521], [1, 1691706528001.555], [4, 1691706528001.578], [3, 1691706528001.579], [2, 1691706528002.129], [2, 1691706529001.2732], [1, 1691706529001.7012], [3, 1691706529001.709], [4, 1691706529001.713], [2, 1691706530001.428], [1, 1691706530001.8518], [4, 1691706530001.8591], [3, 1691706530001.8628], [2, 1691706531001.563], [1, 1691706531001.984], [3, 1691706531001.988], [4, 1691706531001.989], [3, 1691706532001.15], [2, 1691706532001.7012], [4, 1691706532002.103], [1, 1691706532002.107], [1, 1691706533001.261], [2, 1691706533001.2659], [3, 1691706533001.269], [4, 1691706533001.272], [2, 1691706534001.3938], [1, 1691706534001.399], [4, 1691706534001.399], [3, 1691706534001.4001], [2, 1691706535001.533], [3, 1691706535001.536], [1, 1691706535001.538], [4, 1691706535001.539], [4, 1691706536001.691], [1, 1691706536001.6938], [3, 1691706536001.695], [2, 1691706536001.698], [3, 1691706537001.821], [4, 1691706537001.825], [1, 1691706537001.826], [2, 1691706537001.8281], [1, 1691706538001.982], [4, 1691706538001.983], [3, 1691706538001.985], [2, 1691706538001.986], [1, 1691706539002.118], [4, 1691706539002.1199], [2, 1691706539002.1208], [3, 1691706539002.122], [4, 1691706540001.2532], [2, 1691706540001.26], [3, 1691706540001.261], [1, 1691706540001.262], [4, 1691706541001.386], [1, 1691706541001.3892], [2, 1691706541001.391], [3, 1691706541001.391], [4, 1691706542001.52], [3, 1691706542001.53], [1, 1691706542001.5308], [2, 1691706542001.532], [4, 1691706543001.682], [3, 1691706543001.684], [1, 1691706543001.687], [2, 1691706543001.687], [4, 1691706544001.8062], [1, 1691706544001.812], [2, 1691706544001.818], [3, 1691706544001.818], [4, 1691706545001.928], [2, 1691706545001.933], [1, 1691706545001.941], [3, 1691706545001.941], [4, 1691706546002.042], [2, 1691706546002.0469], [1, 1691706546002.049], [3, 1691706546002.056], [1, 1691706547000.701], [2, 1691706547001.174], [4, 1691706547001.174], [3, 1691706547001.19], [4, 1691706548001.3271], [3, 1691706548001.332], [2, 1691706548001.335], [1, 1691706548001.857], [3, 1691706549001.4841], [4, 1691706549001.4841], [2, 1691706549001.485], [1, 1691706549001.7852], [3, 1691706550001.63], [4, 1691706550001.635], [1, 1691706550001.642], [2, 1691706550001.642], [1, 1691706551001.7559], [3, 1691706551001.7568], [2, 1691706551001.76], [4, 1691706551001.7632], [4, 1691706552001.895], [3, 1691706552001.898], [2, 1691706552001.902], [1, 1691706552001.903], [1, 1691706553002.057], [2, 1691706553002.059], [3, 1691706553002.061], [4, 1691706553002.062], [3, 1691706554001.177], [2, 1691706554001.1829], [4, 1691706554001.187], [1, 1691706554001.19], [4, 1691706555001.3088], [3, 1691706555001.31], [1, 1691706555001.312], [2, 1691706555001.312], [1, 1691706556001.427], [4, 1691706556001.428], [2, 1691706556001.429], [3, 1691706556001.4312], [4, 1691706557001.561], [1, 1691706557001.564], [2, 1691706557001.564], [3, 1691706557001.567], [4, 1691706558001.3088], [3, 1691706558001.6902], [1, 1691706558001.7212], [2, 1691706558001.7212], [4, 1691706559001.458], [3, 1691706559001.4631], [2, 1691706559001.8618], [1, 1691706559001.864], [4, 1691706560001.596], [3, 1691706560001.6], [1, 1691706560002.002], [2, 1691706560002.007], [1, 1691706561001.142], [2, 1691706561001.149], [4, 1691706561001.7349], [3, 1691706561001.74], [1, 1691706562001.291], [2, 1691706562001.293], [3, 1691706562001.8792], [4, 1691706562001.881], [1, 1691706563001.391], [2, 1691706563001.4038], [3, 1691706563001.582], [4, 1691706563001.666], [2, 1691706564001.547], [3, 1691706564001.559], [4, 1691706564001.568], [1, 1691706564001.569], [1, 1691706565001.2212], [4, 1691706565001.682], [3, 1691706565001.6892], [2, 1691706565001.6938], [1, 1691706566001.373], [4, 1691706566001.373], [3, 1691706566001.8281], [2, 1691706566001.829], [4, 1691706567001.519], [1, 1691706567001.521], [2, 1691706567001.966], [3, 1691706567001.97], [4, 1691706568001.665], [1, 1691706568001.666], [2, 1691706568002.105], [3, 1691706568002.105], [2, 1691706569001.26], [3, 1691706569001.261], [4, 1691706569001.8188], [1, 1691706569001.821], [2, 1691706570001.117], [3, 1691706570001.398], [1, 1691706570001.943], [4, 1691706570001.949], [2, 1691706571001.272], [1, 1691706571001.2732], [3, 1691706571001.545], [4, 1691706571002.069], [4, 1691706572001.202], [1, 1691706572001.203], [2, 1691706572001.396], [3, 1691706572001.675], [4, 1691706573001.324], [1, 1691706573001.3408], [2, 1691706573001.342], [3, 1691706573001.7979], [2, 1691706574001.482], [1, 1691706574001.485], [4, 1691706574001.486], [3, 1691706574001.948], [4, 1691706575000.747], [3, 1691706575001.2979], [1, 1691706575001.6], [2, 1691706575001.613], [3, 1691706576001.434], [4, 1691706576001.7239], [2, 1691706576001.73], [1, 1691706576001.734], [4, 1691706577001.59], [3, 1691706577001.594], [1, 1691706577001.85], [2, 1691706577001.8718], [3, 1691706578001.7139], [4, 1691706578001.716], [1, 1691706578001.7239], [2, 1691706578002.004], [2, 1691706579001.158], [3, 1691706579001.8728], [1, 1691706579001.874], [4, 1691706579001.877], [2, 1691706580001.2942], [1, 1691706580002.011], [4, 1691706580002.011], [3, 1691706580002.013], [3, 1691706581001.15], [1, 1691706581001.151], [4, 1691706581001.152], [2, 1691706581001.434], [4, 1691706582001.271], [1, 1691706582001.275], [3, 1691706582001.2769], [2, 1691706582001.568], [4, 1691706583001.396], [1, 1691706583001.399], [3, 1691706583001.407], [2, 1691706583001.698], [3, 1691706584001.557], [4, 1691706584001.558], [1, 1691706584001.56], [2, 1691706584001.847], [4, 1691706585001.6829], [3, 1691706585001.692], [1, 1691706585001.6938], [2, 1691706585001.983], [4, 1691706586001.8098], [1, 1691706586001.8162], [3, 1691706586001.818], [2, 1691706586002.113], [2, 1691706587001.244], [4, 1691706587001.934], [1, 1691706587001.935], [3, 1691706587001.937], [2, 1691706588001.3691], [1, 1691706588002.0532], [4, 1691706588002.0532], [3, 1691706588002.054], [1, 1691706589001.215], [3, 1691706589001.216], [4, 1691706589001.2202], [2, 1691706589001.5308], [3, 1691706590001.354], [4, 1691706590001.354], [1, 1691706590001.3591], [2, 1691706590001.6719], [3, 1691706591001.4731], [4, 1691706591001.48], [1, 1691706591001.482], [2, 1691706591001.812], [3, 1691706592001.622], [4, 1691706592001.623], [1, 1691706592001.628], [2, 1691706592001.9531], [4, 1691706593001.77], [1, 1691706593001.771], [3, 1691706593001.7751], [2, 1691706593002.094], [1, 1691706594001.8828], [3, 1691706594001.961], [4, 1691706594002.03], [2, 1691706594002.697], [1, 1691706595001.192], [3, 1691706595001.1992], [4, 1691706595001.203], [2, 1691706595001.844], [4, 1691706596001.3198], [1, 1691706596001.325], [3, 1691706596001.3289], [2, 1691706596001.976], [3, 1691706597001.466], [4, 1691706597001.468], [1, 1691706597001.469], [2, 1691706597002.1172], [2, 1691706598001.259], [4, 1691706598001.608], [1, 1691706598001.6099], [3, 1691706598001.611], [2, 1691706599001.406], [1, 1691706599001.7642], [3, 1691706599001.7659], [4, 1691706599001.7678], [2, 1691706600001.565], [4, 1691706600001.962], [1, 1691706600001.968], [3, 1691706600001.968]]}, {"target": "job_id", "datapoints": [[0, 1691706501001.868], [0, 1691706501001.8718], [0, 1691706501001.8728], [0, 1691706501001.978], [0, 1691706502002.029], [0, 1691706502002.03], [0, 1691706502002.0322], [0, 1691706502002.073], [0, 1691706503001.1829], [0, 1691706503001.185], [0, 1691706503001.188], [0, 1691706503001.213], [0, 1691706504001.3198], [0, 1691706504001.322], [0, 1691706504001.325], [0, 1691706504001.3408], [0, 1691706505001.4631], [0, 1691706505001.464], [0, 1691706505001.47], [0, 1691706505001.47], [0, 1691706506001.597], [0, 1691706506001.5989], [0, 1691706506001.6], [0, 1691706506001.602], [0, 1691706507001.7532], [0, 1691706507001.755], [0, 1691706507001.758], [0, 1691706507001.759], [0, 1691706508001.907], [0, 1691706508001.91], [0, 1691706508001.9111], [0, 1691706508001.913], [0, 1691706509002.059], [0, 1691706509002.059], [0, 1691706509002.064], [0, 1691706509002.065], [0, 1691706510001.181], [0, 1691706510001.181], [0, 1691706510001.191], [0, 1691706510001.191], [0, 1691706511001.3298], [0, 1691706511001.333], [0, 1691706511001.3381], [0, 1691706511001.3381], [0, 1691706512001.491], [0, 1691706512001.495], [0, 1691706512001.497], [0, 1691706512001.498], [0, 1691706513001.459], [0, 1691706513001.612], [0, 1691706513001.6372], [0, 1691706513001.64], [0, 1691706514001.594], [0, 1691706514001.595], [0, 1691706514001.7642], [0, 1691706514001.7651], [0, 1691706515001.7148], [0, 1691706515001.7212], [0, 1691706515001.889], [0, 1691706515001.891], [0, 1691706516001.856], [0, 1691706516001.8591], [0, 1691706516002.025], [0, 1691706516002.027], [0, 1691706517001.1829], [0, 1691706517001.186], [0, 1691706517002.014], [0, 1691706517002.016], [0, 1691706518001.158], [0, 1691706518001.159], [0, 1691706518001.3079], [0, 1691706518001.31], [0, 1691706519001.2942], [0, 1691706519001.2952], [0, 1691706519001.426], [0, 1691706519001.4312], [0, 1691706520001.4348], [0, 1691706520001.438], [0, 1691706520001.551], [0, 1691706520001.561], [0, 1691706521001.563], [0, 1691706521001.565], [0, 1691706521001.666], [0, 1691706521001.678], [0, 1691706522001.729], [0, 1691706522001.7322], [0, 1691706522001.814], [0, 1691706522001.823], [0, 1691706523001.8801], [0, 1691706523001.8801], [0, 1691706523001.944], [0, 1691706523001.9468], [0, 1691706524002.003], [0, 1691706524002.011], [0, 1691706524002.056], [0, 1691706524002.062], [0, 1691706525001.107], [0, 1691706525001.115], [0, 1691706525001.17], [0, 1691706525001.177], [0, 1691706526001.248], [0, 1691706526001.257], [0, 1691706526001.3079], [0, 1691706526001.31], [0, 1691706527000.989], [0, 1691706527001.413], [0, 1691706527001.451], [0, 1691706527001.4521], [0, 1691706528001.555], [0, 1691706528001.578], [0, 1691706528001.579], [0, 1691706528002.129], [0, 1691706529001.2732], [0, 1691706529001.7012], [0, 1691706529001.709], [0, 1691706529001.713], [0, 1691706530001.428], [0, 1691706530001.8518], [0, 1691706530001.8591], [0, 1691706530001.8628], [0, 1691706531001.563], [0, 1691706531001.984], [0, 1691706531001.988], [0, 1691706531001.989], [0, 1691706532001.15], [0, 1691706532001.7012], [0, 1691706532002.103], [0, 1691706532002.107], [0, 1691706533001.261], [0, 1691706533001.2659], [0, 1691706533001.269], [0, 1691706533001.272], [0, 1691706534001.3938], [0, 1691706534001.399], [0, 1691706534001.399], [0, 1691706534001.4001], [0, 1691706535001.533], [0, 1691706535001.536], [0, 1691706535001.538], [0, 1691706535001.539], [0, 1691706536001.691], [0, 1691706536001.6938], [0, 1691706536001.695], [0, 1691706536001.698], [0, 1691706537001.821], [0, 1691706537001.825], [0, 1691706537001.826], [0, 1691706537001.8281], [0, 1691706538001.982], [0, 1691706538001.983], [0, 1691706538001.985], [0, 1691706538001.986], [0, 1691706539002.118], [0, 1691706539002.1199], [0, 1691706539002.1208], [0, 1691706539002.122], [0, 1691706540001.2532], [0, 1691706540001.26], [0, 1691706540001.261], [0, 1691706540001.262], [0, 1691706541001.386], [0, 1691706541001.3892], [0, 1691706541001.391], [0, 1691706541001.391], [0, 1691706542001.52], [0, 1691706542001.53], [0, 1691706542001.5308], [0, 1691706542001.532], [0, 1691706543001.682], [0, 1691706543001.684], [0, 1691706543001.687], [0, 1691706543001.687], [0, 1691706544001.8062], [0, 1691706544001.812], [0, 1691706544001.818], [0, 1691706544001.818], [0, 1691706545001.928], [0, 1691706545001.933], [0, 1691706545001.941], [0, 1691706545001.941], [0, 1691706546002.042], [0, 1691706546002.0469], [0, 1691706546002.049], [0, 1691706546002.056], [0, 1691706547000.701], [0, 1691706547001.174], [0, 1691706547001.174], [0, 1691706547001.19], [0, 1691706548001.3271], [0, 1691706548001.332], [0, 1691706548001.335], [0, 1691706548001.857], [0, 1691706549001.4841], [0, 1691706549001.4841], [0, 1691706549001.485], [0, 1691706549001.7852], [0, 1691706550001.63], [0, 1691706550001.635], [0, 1691706550001.642], [0, 1691706550001.642], [0, 1691706551001.7559], [0, 1691706551001.7568], [0, 1691706551001.76], [0, 1691706551001.7632], [0, 1691706552001.895], [0, 1691706552001.898], [0, 1691706552001.902], [0, 1691706552001.903], [0, 1691706553002.057], [0, 1691706553002.059], [0, 1691706553002.061], [0, 1691706553002.062], [0, 1691706554001.177], [0, 1691706554001.1829], [0, 1691706554001.187], [0, 1691706554001.19], [0, 1691706555001.3088], [0, 1691706555001.31], [0, 1691706555001.312], [0, 1691706555001.312], [0, 1691706556001.427], [0, 1691706556001.428], [0, 1691706556001.429], [0, 1691706556001.4312], [0, 1691706557001.561], [0, 1691706557001.564], [0, 1691706557001.564], [0, 1691706557001.567], [0, 1691706558001.3088], [0, 1691706558001.6902], [0, 1691706558001.7212], [0, 1691706558001.7212], [0, 1691706559001.458], [0, 1691706559001.4631], [0, 1691706559001.8618], [0, 1691706559001.864], [0, 1691706560001.596], [0, 1691706560001.6], [0, 1691706560002.002], [0, 1691706560002.007], [0, 1691706561001.142], [0, 1691706561001.149], [0, 1691706561001.7349], [0, 1691706561001.74], [0, 1691706562001.291], [0, 1691706562001.293], [0, 1691706562001.8792], [0, 1691706562001.881], [0, 1691706563001.391], [0, 1691706563001.4038], [0, 1691706563001.582], [0, 1691706563001.666], [0, 1691706564001.547], [0, 1691706564001.559], [0, 1691706564001.568], [0, 1691706564001.569], [0, 1691706565001.2212], [0, 1691706565001.682], [0, 1691706565001.6892], [0, 1691706565001.6938], [0, 1691706566001.373], [0, 1691706566001.373], [0, 1691706566001.8281], [0, 1691706566001.829], [0, 1691706567001.519], [0, 1691706567001.521], [0, 1691706567001.966], [0, 1691706567001.97], [0, 1691706568001.665], [0, 1691706568001.666], [0, 1691706568002.105], [0, 1691706568002.105], [0, 1691706569001.26], [0, 1691706569001.261], [0, 1691706569001.8188], [0, 1691706569001.821], [0, 1691706570001.117], [0, 1691706570001.398], [0, 1691706570001.943], [0, 1691706570001.949], [0, 1691706571001.272], [0, 1691706571001.2732], [0, 1691706571001.545], [0, 1691706571002.069], [0, 1691706572001.202], [0, 1691706572001.203], [0, 1691706572001.396], [0, 1691706572001.675], [0, 1691706573001.324], [0, 1691706573001.3408], [0, 1691706573001.342], [0, 1691706573001.7979], [0, 1691706574001.482], [0, 1691706574001.485], [0, 1691706574001.486], [0, 1691706574001.948], [0, 1691706575000.747], [0, 1691706575001.2979], [0, 1691706575001.6], [0, 1691706575001.613], [0, 1691706576001.434], [0, 1691706576001.7239], [0, 1691706576001.73], [0, 1691706576001.734], [0, 1691706577001.59], [0, 1691706577001.594], [0, 1691706577001.85], [0, 1691706577001.8718], [0, 1691706578001.7139], [0, 1691706578001.716], [0, 1691706578001.7239], [0, 1691706578002.004], [0, 1691706579001.158], [0, 1691706579001.8728], [0, 1691706579001.874], [0, 1691706579001.877], [0, 1691706580001.2942], [0, 1691706580002.011], [0, 1691706580002.011], [0, 1691706580002.013], [0, 1691706581001.15], [0, 1691706581001.151], [0, 1691706581001.152], [0, 1691706581001.434], [0, 1691706582001.271], [0, 1691706582001.275], [0, 1691706582001.2769], [0, 1691706582001.568], [0, 1691706583001.396], [0, 1691706583001.399], [0, 1691706583001.407], [0, 1691706583001.698], [0, 1691706584001.557], [0, 1691706584001.558], [0, 1691706584001.56], [0, 1691706584001.847], [0, 1691706585001.6829], [0, 1691706585001.692], [0, 1691706585001.6938], [0, 1691706585001.983], [0, 1691706586001.8098], [0, 1691706586001.8162], [0, 1691706586001.818], [0, 1691706586002.113], [0, 1691706587001.244], [0, 1691706587001.934], [0, 1691706587001.935], [0, 1691706587001.937], [0, 1691706588001.3691], [0, 1691706588002.0532], [0, 1691706588002.0532], [0, 1691706588002.054], [0, 1691706589001.215], [0, 1691706589001.216], [0, 1691706589001.2202], [0, 1691706589001.5308], [0, 1691706590001.354], [0, 1691706590001.354], [0, 1691706590001.3591], [0, 1691706590001.6719], [0, 1691706591001.4731], [0, 1691706591001.48], [0, 1691706591001.482], [0, 1691706591001.812], [0, 1691706592001.622], [0, 1691706592001.623], [0, 1691706592001.628], [0, 1691706592001.9531], [0, 1691706593001.77], [0, 1691706593001.771], [0, 1691706593001.7751], [0, 1691706593002.094], [0, 1691706594001.8828], [0, 1691706594001.961], [0, 1691706594002.03], [0, 1691706594002.697], [0, 1691706595001.192], [0, 1691706595001.1992], [0, 1691706595001.203], [0, 1691706595001.844], [0, 1691706596001.3198], [0, 1691706596001.325], [0, 1691706596001.3289], [0, 1691706596001.976], [0, 1691706597001.466], [0, 1691706597001.468], [0, 1691706597001.469], [0, 1691706597002.1172], [0, 1691706598001.259], [0, 1691706598001.608], [0, 1691706598001.6099], [0, 1691706598001.611], [0, 1691706599001.406], [0, 1691706599001.7642], [0, 1691706599001.7659], [0, 1691706599001.7678], [0, 1691706600001.565], [0, 1691706600001.962], [0, 1691706600001.968], [0, 1691706600001.968]]}]'
comp_ids:{1, 2, 3, 4}
2023-08-10T17:30:08-05:00 INFO: query check RC: 0
faed353e55987d90877c2717fe9a26d684bb88aa76c52f7a42ac59663fba8cf5
2023-08-10T17:30:40-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2587    798 --:--:-- --:--:-- --:--:--  3397
{"datasource":{"id":1,"uid":"MMut1-e4z","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-08-10T17:30:41-05:00 INFO: Checking grafana data
2023-08-10T17:30:41-05:00 INFO: Grafana data check, rc: 0
2023-08-10T17:30:41-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-08-10T17:30:46-05:00 INFO: DONE
2023-08-10 17:30:56 INFO: ----------------------------------------------
2023-08-10 17:30:56 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;32mPASSED[0m
papi_store_test: [01;32mPASSED[0m
updtr_status_test: [01;32mPASSED[0m
ldms_stream_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;32mPASSED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;32mPASSED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;31mFAILED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;32mPASSED[0m
agg_slurm_test: [01;31mFAILED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;32mPASSED[0m
ldmsd_auth_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;32mPASSED[0m
cont-test-ldms: [01;32mPASSED[0m
run_inside_cont_test.py: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;32mPASSED[0m
ldmsd_long_config_test: [01;32mPASSED[0m
maestro_raft_test: [01;32mPASSED[0m
ldmsd_stream_test2: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
libovis_log_test: [01;32mPASSED[0m
cont-test-maestro: [01;32mPASSED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_stream_status_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
ldms_rail_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;32mPASSED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 50/52
------------------------------------------
