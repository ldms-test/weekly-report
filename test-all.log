2023-03-07 21:07:40 INFO: WORK_DIR: /mnt/300G/data/2023-03-07-210739
2023-03-07 21:07:40 INFO: LOG: /mnt/300G/data/2023-03-07-210739/cygnus-weekly.log
~/cron/ldms-test ~/cron/ldms-test
/mnt/300G/data/2023-03-07-210739 ~/cron/ldms-test ~/cron/ldms-test
2023-03-07 21:07:41 INFO: Skip building on host because GIT SHA has not changed: 661e35a010a7de2ebce0e7918406804bd1fbd726
661e35a010a7de2ebce0e7918406804bd1fbd726
OVIS_LDMS_OVIS_GIT_LONG "661e35a010a7de2ebce0e7918406804bd1fbd726"
2023-03-07 21:07:41 INFO: Skip building containerized binary because GIT SHA has not changed: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:07:41 INFO: -- Installation process succeeded --
2023-03-07 21:07:41 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-03-07-210739
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-03-07-210739
HEAD is now at b94e0a4 2023-03-07-143022
[master 677c09b] 2023-03-07-210739
 2 files changed, 15 insertions(+), 2073 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   b94e0a4..677c09b  master -> master
~/cron/ldms-test /mnt/300G/data/2023-03-07-210739
2023-03-07 21:07:43 INFO: ==== OVIS+SOS Installation Completed ====
2023-03-07 21:07:43 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-03-07-210739 ~/cron/ldms-test ~/cron/ldms-test
2023-03-07 21:07:43 INFO: ======== direct_ldms_ls_conn_test ========
2023-03-07 21:07:43 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/direct_ldms_ls_conn_test
2023-03-07 21:07:44,521 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-03-07 21:07:44,522 TADA INFO   test-id: 68479353043b9b8f310b0d816949acd653e327ec80cc2ad9b7859fa54e1763f0
2023-03-07 21:07:44,522 TADA INFO   test-suite: LDMSD
2023-03-07 21:07:44,522 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-03-07 21:07:44,522 TADA INFO   test-user: narate
2023-03-07 21:07:44,522 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:07:44,758 __main__ INFO starting munged on cygnus-01-iw
2023-03-07 21:07:45,144 __main__ INFO starting munged on localhost
2023-03-07 21:07:45,377 __main__ INFO starting ldmsd on cygnus-01-iw
2023-03-07 21:07:45,678 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-03-07 21:07:50,873 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-03-07 21:07:50,873 __main__ INFO Stopping sampler daemon ...
2023-03-07 21:07:56,298 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-03-07 21:07:56,335 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-03-07 21:07:56,373 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-03-07 21:07:56,374 TADA INFO test direct_ldms_ls_conn_test ended
2023-03-07 21:07:56,583 __main__ INFO stopping munged on cygnus-01-iw
2023-03-07 21:07:56,997 __main__ INFO stopping munged on localhost
2023-03-07 21:07:57 INFO: ----------------------------------------------
2023-03-07 21:07:57 INFO: ======== direct_prdcr_subscribe_test ========
2023-03-07 21:07:57 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/direct_prdcr_subscribe_test
2023-03-07 21:07:57,828 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-03-07 21:07:57,828 TADA INFO   test-id: ba06240628744f6be3bfe15722f9bc230d577abf9461a3839faf9a911a33cdcc
2023-03-07 21:07:57,829 TADA INFO   test-suite: LDMSD
2023-03-07 21:07:57,829 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-03-07 21:07:57,829 TADA INFO   test-user: narate
2023-03-07 21:07:57,829 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:07:59,773 __main__ INFO starting munged on cygnus-01-iw
2023-03-07 21:08:00,550 __main__ INFO starting munged on cygnus-02-iw
2023-03-07 21:08:01,294 __main__ INFO starting munged on cygnus-03-iw
2023-03-07 21:08:02,054 __main__ INFO starting munged on cygnus-04-iw
2023-03-07 21:08:02,361 __main__ INFO starting munged on localhost
2023-03-07 21:08:02,597 __main__ INFO starting ldmsd on cygnus-01-iw
2023-03-07 21:08:03,085 __main__ INFO starting ldmsd on cygnus-02-iw
2023-03-07 21:08:03,573 __main__ INFO starting ldmsd on cygnus-03-iw
2023-03-07 21:08:04,113 __main__ INFO starting ldmsd on cygnus-04-iw
Traceback (most recent call last):
  File "direct_prdcr_subscribe_test", line 501, in <module>
    publish(d, _type="json", data=text_data)
  File "direct_prdcr_subscribe_test", line 339, in publish
    raise RuntimeError("publish failed ({}): {}".format(rc, out))
RuntimeError: publish failed (110): Timeout connecting to remote peer
Error 110 publishing file.

2023-03-07 21:09:11,119 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: skipped
2023-03-07 21:09:11,120 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: skipped
2023-03-07 21:09:11,120 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: skipped
2023-03-07 21:09:11,121 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: skipped
2023-03-07 21:09:11,121 TADA INFO assertion 4, ldmsd_stream data check on agg-2: skipped
2023-03-07 21:09:11,121 TADA INFO assertion 5, Stopping the producers succeeds: skipped
2023-03-07 21:09:11,122 TADA INFO assertion 6, Restarting the producers succeeds: skipped
2023-03-07 21:09:11,122 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: skipped
2023-03-07 21:09:11,122 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: skipped
2023-03-07 21:09:11,122 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: skipped
2023-03-07 21:09:11,123 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: skipped
2023-03-07 21:09:11,123 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: skipped
2023-03-07 21:09:11,123 TADA INFO assertion 12, stream-sampler-1 is not running: skipped
2023-03-07 21:09:11,124 TADA INFO assertion 13, stream-sampler-1 has restarted: skipped
2023-03-07 21:09:11,124 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 21:09:11,124 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 21:09:11,124 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: skipped
2023-03-07 21:09:11,125 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: skipped
2023-03-07 21:09:11,125 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: skipped
2023-03-07 21:09:11,125 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: skipped
2023-03-07 21:09:11,125 TADA INFO test direct_prdcr_subscribe_test ended
2023-03-07 21:09:11,329 __main__ INFO stopping munged on cygnus-01-iw
2023-03-07 21:09:11,946 __main__ INFO stopping munged on cygnus-02-iw
2023-03-07 21:09:12,612 __main__ INFO stopping munged on cygnus-03-iw
2023-03-07 21:09:13,031 __main__ INFO stopping ldmsd on cygnus-03-iw
2023-03-07 21:09:13,457 __main__ INFO stopping munged on cygnus-04-iw
2023-03-07 21:09:13,882 __main__ INFO stopping munged on localhost
2023-03-07 21:09:14 INFO: ----------------------------------------------
2023-03-07 21:09:14 INFO: ======== agg_slurm_test ========
2023-03-07 21:09:14 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/agg_slurm_test
2023-03-07 21:09:14,769 TADA INFO starting test `agg_slurm_test`
2023-03-07 21:09:14,769 TADA INFO   test-id: fba0d9a5712b4f6213f5db8fcbfa1809f2b7710594268a610d9b8dd2df517872
2023-03-07 21:09:14,769 TADA INFO   test-suite: LDMSD
2023-03-07 21:09:14,770 TADA INFO   test-name: agg_slurm_test
2023-03-07 21:09:14,770 TADA INFO   test-user: narate
2023-03-07 21:09:14,770 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:09:14,771 __main__ INFO -- Get or create the cluster --
2023-03-07 21:09:28,753 __main__ INFO -- Preparing syspapi JSON file --
2023-03-07 21:09:28,857 __main__ INFO -- Preparing jobpapi JSON file --
2023-03-07 21:09:28,967 __main__ INFO -- Preparing job script & programs --
2023-03-07 21:09:30,324 __main__ INFO -- Start daemons --
2023-03-07 21:09:42,627 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:09:47,630 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 21:09:47,767 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-03-07 21:09:47,907 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-03-07 21:09:52,910 __main__ INFO -- Submitting jobs --
2023-03-07 21:09:53,041 __main__ INFO job_one: 1
2023-03-07 21:09:53,152 __main__ INFO job_two: 2
2023-03-07 21:10:03,163 __main__ INFO -- Cancelling jobs --
2023-03-07 21:10:03,163 __main__ INFO job_one: 1
2023-03-07 21:10:03,348 __main__ INFO job_two: 2
2023-03-07 21:11:15,403 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-03-07 21:11:15,404 TADA INFO assertion 3, meminfo data verification: No data missing, passed
2023-03-07 21:11:15,405 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: No data missing, passed
2023-03-07 21:11:15,405 TADA INFO test agg_slurm_test ended
2023-03-07 21:11:29 INFO: ----------------------------------------------
2023-03-07 21:11:30 INFO: ======== papi_sampler_test ========
2023-03-07 21:11:30 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/papi_sampler_test
2023-03-07 21:11:31,187 TADA INFO starting test `papi_sampler_test`
2023-03-07 21:11:31,187 TADA INFO   test-id: 20f76a71020cbd1e124d68dd4e2347db81dba687879865fba8daa302d471bd33
2023-03-07 21:11:31,187 TADA INFO   test-suite: LDMSD
2023-03-07 21:11:31,187 TADA INFO   test-name: papi_sampler_test
2023-03-07 21:11:31,187 TADA INFO   test-user: narate
2023-03-07 21:11:31,187 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:11:31,188 __main__ INFO -- Get or create the cluster --
2023-03-07 21:11:36,582 __main__ INFO -- Start daemons --
2023-03-07 21:11:46,576 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-03-07 21:11:46,830 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-03-07 21:11:51,953 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-03-07 21:11:52,135 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-03-07 21:11:52,135 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-03-07 21:12:05,995 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-03-07 21:12:05,995 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-03-07 21:12:05,995 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-03-07 21:12:05,996 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-03-07 21:12:06,192 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-03-07 21:12:12,039 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-03-07 21:12:12,040 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-03-07 21:12:12,040 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-03-07 21:12:12,040 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-03-07 21:12:12,263 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-03-07 21:12:12,264 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/meminfo', 'node-1/papi1/3.0', 'node-1/papi0/2.0'}), passed
2023-03-07 21:12:22,859 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-03-07 21:13:03,196 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-03-07 21:13:05,568 TADA INFO assertion 8, Missing config file attribute is logged: : papi_sampler[519]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-03-07 21:13:11,088 TADA INFO assertion 9, Bad config file is logged: : papi_sampler: configuration file syntax error., passed
2023-03-07 21:13:11,088 __main__ INFO -- Finishing Test --
2023-03-07 21:13:11,088 TADA INFO test papi_sampler_test ended
2023-03-07 21:13:11,088 __main__ INFO -- Cleaning up files --
2023-03-07 21:13:11,089 __main__ INFO -- Removing the virtual cluster --
2023-03-07 21:13:22 INFO: ----------------------------------------------
2023-03-07 21:13:23 INFO: ======== papi_store_test ========
2023-03-07 21:13:23 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/papi_store_test
2023-03-07 21:13:24,159 TADA INFO starting test `papi_store_test`
2023-03-07 21:13:24,160 TADA INFO   test-id: 2c5bff679ba4d80bb6e0002562bd899d3da0c20c160d53f6ae621faec68da658
2023-03-07 21:13:24,160 TADA INFO   test-suite: LDMSD
2023-03-07 21:13:24,160 TADA INFO   test-name: papi_store_test
2023-03-07 21:13:24,160 TADA INFO   test-user: narate
2023-03-07 21:13:24,161 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:13:24,161 __main__ INFO -- Get or create the cluster --
2023-03-07 21:13:31,822 __main__ INFO -- Start daemons --
2023-03-07 21:14:04,777 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-03-07 21:14:04,777 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-03-07 21:14:04,777 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-03-07 21:14:04,777 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-03-07 21:14:04,778 TADA INFO test papi_store_test ended
2023-03-07 21:14:17 INFO: ----------------------------------------------
2023-03-07 21:14:17 INFO: ======== store_app_test ========
2023-03-07 21:14:17 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/store_app_test
2023-03-07 21:14:18,719 TADA INFO starting test `store_app_test`
2023-03-07 21:14:18,720 TADA INFO   test-id: d0d22d146542b33737ff492f89ee47e4750079c37aff6d1206126a1114509352
2023-03-07 21:14:18,720 TADA INFO   test-suite: LDMSD
2023-03-07 21:14:18,720 TADA INFO   test-name: store_app_test
2023-03-07 21:14:18,720 TADA INFO   test-user: narate
2023-03-07 21:14:18,720 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:14:18,721 __main__ INFO -- Get or create the cluster --
2023-03-07 21:14:33,385 __main__ INFO -- Preparing job script & programs --
2023-03-07 21:14:33,767 __main__ INFO -- Start daemons --
2023-03-07 21:14:46,059 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:14:51,066 __main__ INFO -- Submitting jobs --
2023-03-07 21:14:51,295 __main__ INFO job_one: 1
2023-03-07 21:14:56,469 __main__ INFO job_two: 2
2023-03-07 21:15:05,738 __main__ INFO Verifying data ...
2023-03-07 21:17:12,068 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-03-07 21:17:12,069 TADA INFO test store_app_test ended
2023-03-07 21:17:26 INFO: ----------------------------------------------
2023-03-07 21:17:26 INFO: ======== syspapi_test ========
2023-03-07 21:17:26 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/syspapi_test
2023-03-07 21:17:27,695 TADA INFO starting test `syspapi_test`
2023-03-07 21:17:27,696 TADA INFO   test-id: 8afb047f2d1cf88ceda1cf8e58f1c3d2f2d0a37484fd5d46d3f08e9bb32d898c
2023-03-07 21:17:27,696 TADA INFO   test-suite: LDMSD
2023-03-07 21:17:27,696 TADA INFO   test-name: syspapi_test
2023-03-07 21:17:27,696 TADA INFO   test-user: narate
2023-03-07 21:17:27,696 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:17:27,697 __main__ INFO -- Get or create the cluster --
2023-03-07 21:17:39,115 __main__ INFO -- Write syspapi JSON config files --
2023-03-07 21:17:39,115 __main__ INFO    - db/syspapi-1.json
2023-03-07 21:17:39,115 __main__ INFO    - db/syspapi-bad.json
2023-03-07 21:17:39,116 __main__ INFO -- Start daemons --
2023-03-07 21:17:47,581 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:17:52,582 __main__ INFO -- Verifying --
2023-03-07 21:17:52,715 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-03-07 21:17:52,715 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-03-07 21:17:52,856 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-03-07 21:17:54,993 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-03-07 21:17:55,092 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-03-07 21:17:55,207 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-03-07 21:18:17,196 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-03-07 21:18:17,196 __main__ INFO  events succeeded: 77
2023-03-07 21:18:17,196 __main__ INFO  events failed: 114
2023-03-07 21:18:17,196 TADA INFO test syspapi_test ended
2023-03-07 21:18:30 INFO: ----------------------------------------------
2023-03-07 21:18:31 INFO: ======== agg_test ========
2023-03-07 21:18:31 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/agg_test
2023-03-07 21:18:32,197 TADA INFO starting test `agg_test`
2023-03-07 21:18:32,197 TADA INFO   test-id: 137a2c97209bcfc40ecb15d4335bd8ad5b50efae7d93af413df03479b2b7dce0
2023-03-07 21:18:32,198 TADA INFO   test-suite: LDMSD
2023-03-07 21:18:32,198 TADA INFO   test-name: agg_test
2023-03-07 21:18:32,198 TADA INFO   test-user: narate
2023-03-07 21:18:32,198 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:18:32,198 __main__ INFO -- Get or create the cluster --
2023-03-07 21:18:50,253 __main__ INFO -- Start daemons --
2023-03-07 21:18:59,595 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:19:04,600 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 21:19:04,728 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-03-07 21:19:05,520 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-03-07 21:19:05,521 __main__ INFO -- Terminating ldmsd on node-1 --
2023-03-07 21:19:07,892 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 21:19:08,132 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-2/meminfo'}), passed
2023-03-07 21:19:08,133 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-03-07 21:19:13,832 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-3/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:19:13,962 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-03-07 21:19:13,962 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-03-07 21:19:16,327 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-4/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-03-07 21:19:16,446 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-03-07 21:19:16,546 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 21:19:16,546 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-03-07 21:19:22,261 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-03-07 21:19:22,262 TADA INFO test agg_test ended
2023-03-07 21:19:38 INFO: ----------------------------------------------
2023-03-07 21:19:39 INFO: ======== failover_test ========
2023-03-07 21:19:39 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/failover_test
2023-03-07 21:19:39,825 TADA INFO starting test `failover_test`
2023-03-07 21:19:39,825 TADA INFO   test-id: 8fcb88fd4ce506b00a36a41fd1401b96efae13964488f1399133c957e795ba1f
2023-03-07 21:19:39,825 TADA INFO   test-suite: LDMSD
2023-03-07 21:19:39,825 TADA INFO   test-name: failover_test
2023-03-07 21:19:39,825 TADA INFO   test-user: narate
2023-03-07 21:19:39,826 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:19:39,826 __main__ INFO -- Get or create the cluster --
2023-03-07 21:19:57,518 __main__ INFO -- Start daemons --
2023-03-07 21:20:06,785 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:20:21,801 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 21:20:21,918 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-03-07 21:20:22,719 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-03-07 21:20:22,719 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-03-07 21:20:28,087 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:20:28,196 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:20:28,326 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-03-07 21:20:28,453 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-03-07 21:20:28,454 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-03-07 21:20:49,176 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-03-07 21:20:49,301 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:20:49,301 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-03-07 21:20:54,656 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:20:54,763 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:20:54,884 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-03-07 21:20:55,008 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-03-07 21:20:55,008 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-03-07 21:21:15,700 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:21:15,814 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-3/meminfo', 'node-4/meminfo', 'node-2/meminfo', 'node-1/meminfo'}), passed
2023-03-07 21:21:15,814 TADA INFO test failover_test ended
2023-03-07 21:21:31 INFO: ----------------------------------------------
2023-03-07 21:21:32 INFO: ======== ldmsd_auth_ovis_test ========
2023-03-07 21:21:32 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_auth_ovis_test
2023-03-07 21:21:32,728 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-03-07 21:21:32,728 TADA INFO   test-id: 861498d9faf6dad1e1c58507e46c62359b256d314175ef3375ed3a894341c90e
2023-03-07 21:21:32,728 TADA INFO   test-suite: LDMSD
2023-03-07 21:21:32,728 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-03-07 21:21:32,728 TADA INFO   test-user: narate
2023-03-07 21:21:32,728 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:21:32,729 __main__ INFO -- Get or create the cluster --
2023-03-07 21:21:37,959 __main__ INFO -- Start daemons --
2023-03-07 21:21:39,982 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:21:45,100 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-03-07 21:21:45,227 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-03-07 21:21:45,357 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-03-07 21:21:45,652 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-03-07 21:21:45,653 TADA INFO test ldmsd_auth_ovis_test ended
2023-03-07 21:21:57 INFO: ----------------------------------------------
2023-03-07 21:21:57 INFO: ======== ldmsd_auth_test ========
2023-03-07 21:21:57 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_auth_test
2023-03-07 21:21:58,694 TADA INFO starting test `ldmsd_auth_test`
2023-03-07 21:21:58,694 TADA INFO   test-id: e99a7dc188870d14222f35eb2bb76de87c0163e2f79e3a281dcd3a7351b59d01
2023-03-07 21:21:58,694 TADA INFO   test-suite: LDMSD
2023-03-07 21:21:58,694 TADA INFO   test-name: ldmsd_auth_test
2023-03-07 21:21:58,695 TADA INFO   test-user: narate
2023-03-07 21:21:58,695 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:21:58,695 __main__ INFO -- Get or create the cluster --
2023-03-07 21:22:16,617 __main__ INFO -- Start daemons --
2023-03-07 21:22:35,625 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:22:40,760 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-03-07 21:22:40,878 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-03-07 21:22:41,005 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-03-07 21:22:41,138 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-03-07 21:22:41,264 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-03-07 21:22:41,264 TADA INFO test ldmsd_auth_test ended
2023-03-07 21:22:56 INFO: ----------------------------------------------
2023-03-07 21:22:57 INFO: ======== ldmsd_ctrl_test ========
2023-03-07 21:22:57 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_ctrl_test
2023-03-07 21:22:58,467 TADA INFO starting test `ldmsd_ctrl_test`
2023-03-07 21:22:58,467 TADA INFO   test-id: 0cec2a6e2c4890cc5e91ae49935add6213dab13479cfe63d4780bff18d3d946c
2023-03-07 21:22:58,467 TADA INFO   test-suite: LDMSD
2023-03-07 21:22:58,467 TADA INFO   test-name: ldmsd_ctrl_test
2023-03-07 21:22:58,467 TADA INFO   test-user: narate
2023-03-07 21:22:58,468 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:22:58,468 __main__ INFO -- Get or create the cluster --
2023-03-07 21:23:08,008 __main__ INFO -- Start daemons --
2023-03-07 21:23:12,412 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:23:18,535 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-03-07 21:23:19,652 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-03-07 21:23:20,253 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-03-07 21:23:20,855 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-03-07 21:23:21,456 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-03-07 21:23:22,058 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-03-07 21:23:22,659 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-03-07 21:23:23,261 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-03-07 21:23:40,464 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-03-07 21:23:57,659 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-03-07 21:23:57,660 TADA INFO test ldmsd_ctrl_test ended
2023-03-07 21:24:10 INFO: ----------------------------------------------
2023-03-07 21:24:11 INFO: ======== ldmsd_stream_test ========
2023-03-07 21:24:11 INFO: CMD: python3 ldmsd_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_stream_test
2023-03-07 21:24:11,961 TADA INFO starting test `ldmsd_stream_test`
2023-03-07 21:24:11,961 TADA INFO   test-id: 33d8b4d7a2c19959ac788c5f9ffc5dd1a2ba5392b4e92c88ba224e08cec36bea
2023-03-07 21:24:11,961 TADA INFO   test-suite: LDMSD
2023-03-07 21:24:11,961 TADA INFO   test-name: ldmsd_stream_test
2023-03-07 21:24:11,961 TADA INFO   test-user: narate
2023-03-07 21:24:11,961 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:24:23,130 __main__ INFO waiting for libraries to be available across all containers...
2023-03-07 21:24:24,030 __main__ INFO _lib_avail: True
2023-03-07 21:25:31,301 __main__ INFO test ldmsd_stream_subscribe with large json streams
2023-03-07 21:25:37,426 __main__ INFO --- Sending stream to ldmsd_stream_subscriber
2023-03-07 21:25:50,652 __main__ INFO --- Verifying the received streams
2023-03-07 21:25:52,268 TADA INFO assertion 1, ldmsd_stream_subscribe receives large json streams: Verify all streams were received correctly, passed
2023-03-07 21:25:52,493 __main__ INFO test LDMSD with large json streams
2023-03-07 21:25:58,575 __main__ INFO --- Sending stream to samplerd
2023-03-07 21:26:17,283 __main__ INFO --- Verifying the streams received by samplerd
2023-03-07 21:26:17,471 TADA INFO assertion 2, samplerd receives large json streams: Traceback (most recent call last):
  File "/tada-src/python/stream_check.py", line 46, in <module>
    fo = open(fout)
FileNotFoundError: [Errno 2] No such file or directory: '/data/samplerd-large-json.out'
, failed
Traceback (most recent call last):
  File "ldmsd_stream_test", line 433, in ldmsd_case
    result = test_stream_sampler_check(samplerd_cont, cont_fin, cont_samplerd_fout, SAMPLE_SIZES[data_sz])
  File "ldmsd_stream_test", line 273, in test_stream_sampler_check
    raise LDMSDStreamTestFail(out)
__main__.LDMSDStreamTestFail: Traceback (most recent call last):
  File "/tada-src/python/stream_check.py", line 46, in <module>
    fo = open(fout)
FileNotFoundError: [Errno 2] No such file or directory: '/data/samplerd-large-json.out'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "ldmsd_stream_test", line 482, in <module>
    ldmsd_case(pub_cont, samplerd_cont, agg_cont, t, sz)
  File "ldmsd_stream_test", line 435, in ldmsd_case
    test.assert_test(ASSERTIONS[_assert_name], False, "{}".format(e))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: ldmsd_stream ..., Traceback (most recent call last):
  File "/tada-src/python/stream_check.py", line 46, in <module>
    fo = open(fout)
FileNotFoundError: [Errno 2] No such file or directory: '/data/samplerd-large-json.out'
: FAILED
2023-03-07 21:26:17,473 TADA INFO assertion 3, agg receives large json streams: skipped
2023-03-07 21:26:17,473 TADA INFO assertion 4, ldmsd_stream_subscribe receives small json streams: skipped
2023-03-07 21:26:17,473 TADA INFO assertion 5, samplerd receives small json streams: skipped
2023-03-07 21:26:17,473 TADA INFO assertion 6, agg receives small json streams: skipped
2023-03-07 21:26:17,473 TADA INFO assertion 7, ldmsd_stream_subscribe receives large string streams: skipped
2023-03-07 21:26:17,473 TADA INFO assertion 8, samplerd receives large string streams: skipped
2023-03-07 21:26:17,474 TADA INFO assertion 9, agg receives large string streams: skipped
2023-03-07 21:26:17,474 TADA INFO assertion 10, ldmsd_stream_subscribe receives small string streams: skipped
2023-03-07 21:26:17,474 TADA INFO assertion 11, samplerd receives small string streams: skipped
2023-03-07 21:26:17,474 TADA INFO assertion 12, agg receives small string streams: skipped
2023-03-07 21:26:17,474 TADA INFO test ldmsd_stream_test ended
2023-03-07 21:26:30 INFO: ----------------------------------------------
2023-03-07 21:26:31 INFO: ======== maestro_cfg_test ========
2023-03-07 21:26:31 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/maestro_cfg_test
2023-03-07 21:26:31,909 TADA INFO starting test `maestro_cfg_test`
2023-03-07 21:26:31,910 TADA INFO   test-id: 85cf244ac3f04e345c118042c81d371fb9cc6f0a1af8b84ab96a4b797138acdf
2023-03-07 21:26:31,910 TADA INFO   test-suite: LDMSD
2023-03-07 21:26:31,910 TADA INFO   test-name: maestro_cfg_test
2023-03-07 21:26:31,910 TADA INFO   test-user: narate
2023-03-07 21:26:31,910 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:26:41,917 __main__ INFO -- Get or create cluster --
2023-03-07 21:27:08,798 __main__ INFO -- Start daemons --
2023-03-07 21:27:23,738 __main__ INFO ... make sure ldmsd's are up
2023-03-07 21:27:31,480 TADA INFO assertion 1, load maestro etcd cluster: etcd cluster loaded successfully, passed
2023-03-07 21:28:11,529 TADA INFO assertion 2, config ldmsd cluster with maestro: Maestro ldmsd configuration successful, passed
2023-03-07 21:28:13,144 TADA INFO assertion 3, verify sampler daemons: OK, passed
2023-03-07 21:28:13,731 TADA INFO assertion 4, verify L1 aggregator daemons: OK, passed
2023-03-07 21:28:13,988 TADA INFO assertion 5, verify L2 aggregator daemon: OK, passed
2023-03-07 21:28:14,244 TADA INFO assertion 6, verify data storage: OK, passed
---Wait for config to write to file---
2023-03-07 21:28:14,244 TADA INFO test maestro_cfg_test ended
2023-03-07 21:28:32 INFO: ----------------------------------------------
2023-03-07 21:28:33 INFO: ======== mt-slurm-test ========
2023-03-07 21:28:33 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1678246153.914059', '1,1678246153.914059', '2,1678246153.914059', '3,1678246153.914059', '4,1678246153.914059', '5,1678246153.914059', '6,1678246154.985207', '7,1678246154.985207', '8,1678246154.985207', '9,1678246155.973572', '10,1678246155.973572', '11,1678246155.973572', '12,1678246155.973572', '13,1678246155.973572', '14,1678246156.983641', '15,1678246156.983641', '16,1678246156.983641', '17,1678246156.983641', '18,1678246157.967826', '19,1678246157.967826', '20,1678246157.967826', '21,1678246158.987114', '22,1678246158.987114', '23,1678246158.987114', '24,1678246158.987114', '25,1678246158.987114', '26,1678246158.987114', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-03-07 21:29:53 INFO: ----------------------------------------------
2023-03-07 21:29:53 INFO: ======== ovis_ev_test ========
2023-03-07 21:29:53 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ovis_ev_test
2023-03-07 21:29:54,700 __main__ INFO -- Create the cluster -- 
2023-03-07 21:30:04,118 TADA INFO starting test `ovis_ev_test`
2023-03-07 21:30:04,118 TADA INFO   test-id: 7a63f74a001b67a2890cd08f3a662b57a54194f9ca3dbebf9bad8ffd972a0a60
2023-03-07 21:30:04,118 TADA INFO   test-suite: test_ovis_ev
2023-03-07 21:30:04,118 TADA INFO   test-name: ovis_ev_test
2023-03-07 21:30:04,118 TADA INFO   test-user: narate
2023-03-07 21:30:04,118 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:30:04,119 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-03-07 21:30:04,119 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-03-07 21:30:04,119 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-03-07 21:30:04,119 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-03-07 21:30:04,120 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-03-07 21:30:04,120 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-03-07 21:30:04,120 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-03-07 21:30:04,120 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-03-07 21:30:04,120 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-03-07 21:30:04,120 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-03-07 21:30:04,121 TADA INFO test ovis_ev_test ended
2023-03-07 21:30:14 INFO: ----------------------------------------------
2023-03-07 21:30:15 INFO: ======== prdcr_subscribe_test ========
2023-03-07 21:30:15 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/prdcr_subscribe_test
2023-03-07 21:30:16,451 TADA INFO starting test `prdcr_subscribe_test`
2023-03-07 21:30:16,452 TADA INFO   test-id: 633985d804e849603a5bc8d5e604e50a1965454c1974cc73c1776a63f5c25498
2023-03-07 21:30:16,452 TADA INFO   test-suite: LDMSD
2023-03-07 21:30:16,452 TADA INFO   test-name: prdcr_subscribe_test
2023-03-07 21:30:16,452 TADA INFO   test-user: narate
2023-03-07 21:30:16,452 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
Traceback (most recent call last):
  File "prdcr_subscribe_test", line 377, in <module>
    start_publisher(cont, host, cont.ldmsd_spec["listen_port"], "json", data_file)
  File "prdcr_subscribe_test", line 264, in start_publisher
    raise RuntimeError("Failed to start ldmsd_stream_publish. Error {}".format(rc))
RuntimeError: Failed to start ldmsd_stream_publish. Error 111
2023-03-07 21:30:59,553 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: skipped
2023-03-07 21:30:59,553 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: skipped
2023-03-07 21:30:59,553 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 4, ldmsd_stream data check on agg-2: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 5, Stopping the producers succeeds: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 6, Restarting the producers succeeds: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: skipped
2023-03-07 21:30:59,554 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 12, stream-sampler-1 is not running: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 13, stream-sampler-1 has restarted: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: skipped
2023-03-07 21:30:59,555 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: skipped
2023-03-07 21:30:59,556 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: skipped
2023-03-07 21:30:59,556 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: skipped
2023-03-07 21:30:59,556 TADA INFO assertion 19, stream-sampler-2 removes agg-1 stream client after disconnected: skipped
2023-03-07 21:30:59,556 TADA INFO test prdcr_subscribe_test ended
2023-03-07 21:31:12 INFO: ----------------------------------------------
2023-03-07 21:31:13 INFO: ======== set_array_test ========
2023-03-07 21:31:13 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/set_array_test
2023-03-07 21:31:13,810 TADA INFO starting test `set_array_test`
2023-03-07 21:31:13,811 TADA INFO   test-id: 51aea1c0b40f653c77c545b6da93da948ddc228dfd39fff91520950e2afd023d
2023-03-07 21:31:13,811 TADA INFO   test-suite: LDMSD
2023-03-07 21:31:13,811 TADA INFO   test-name: set_array_test
2023-03-07 21:31:13,811 TADA INFO   test-user: narate
2023-03-07 21:31:13,811 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:31:13,811 __main__ INFO -- Get or create the cluster --
2023-03-07 21:31:18,853 __main__ INFO -- Start daemons --
2023-03-07 21:31:20,801 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:31:50,534 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 1 snapshots, passed
2023-03-07 21:31:50,534 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-03-07 21:31:50,535 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-03-07 21:31:50,535 TADA INFO test set_array_test ended
2023-03-07 21:32:01 INFO: ----------------------------------------------
2023-03-07 21:32:02 INFO: ======== setgroup_test ========
2023-03-07 21:32:02 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/setgroup_test
2023-03-07 21:32:03,522 TADA INFO starting test `setgroup_test`
2023-03-07 21:32:03,523 TADA INFO   test-id: 6d4e2025b237eae370ff60f869ad40943d21f9ac16bdda612e6469becef4fc14
2023-03-07 21:32:03,523 TADA INFO   test-suite: LDMSD
2023-03-07 21:32:03,523 TADA INFO   test-name: setgroup_test
2023-03-07 21:32:03,523 TADA INFO   test-user: narate
2023-03-07 21:32:03,523 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:32:03,524 __main__ INFO -- Get or create the cluster --
2023-03-07 21:32:12,887 __main__ INFO -- Start daemons --
2023-03-07 21:32:17,303 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:32:22,308 __main__ INFO -- ldms_ls to agg-2 --
2023-03-07 21:32:22,435 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-03-07 21:32:24,690 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-03-07 21:32:24,690 __main__ INFO -- Removing test_2 from grp --
2023-03-07 21:32:25,155 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp', 'node-1/test_2'}, failed
Traceback (most recent call last):
  File "setgroup_test", line 357, in <module>
    test.assert_test(3, False, "expect {}, got {}".format(expect, sets))
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: LDMSD setgroup 2-level aggregation test, expect {'node-1/test_1', 'node-1/grp'}, got {'node-1/test_1', 'node-1/grp', 'node-1/test_2'}: FAILED
2023-03-07 21:32:25,156 TADA INFO assertion 4, test_2 is removed from grp on agg-1: skipped
2023-03-07 21:32:25,156 TADA INFO assertion 5, test_2 is removed from grp on agg-2: skipped
2023-03-07 21:32:25,156 TADA INFO assertion 6, test_2 is added back to grp on sampler: skipped
2023-03-07 21:32:25,156 TADA INFO assertion 7, test_2 is added back to grp on agg-1: skipped
2023-03-07 21:32:25,156 TADA INFO assertion 8, test_2 is added back to grp on agg-2: skipped
2023-03-07 21:32:25,156 TADA INFO test setgroup_test ended
2023-03-07 21:32:37 INFO: ----------------------------------------------
2023-03-07 21:32:38 INFO: ======== slurm_stream_test ========
2023-03-07 21:32:38 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/slurm_stream_test
2023-03-07 21:32:39,499 TADA INFO starting test `slurm_stream_test`
2023-03-07 21:32:39,500 TADA INFO   test-id: b53b667d71b226ca89a5dfbb5bb289a5a6fb1f78408a11e0e76c1a8035526776
2023-03-07 21:32:39,500 TADA INFO   test-suite: LDMSD
2023-03-07 21:32:39,500 TADA INFO   test-name: slurm_stream_test
2023-03-07 21:32:39,500 TADA INFO   test-user: narate
2023-03-07 21:32:39,500 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:32:39,501 __main__ INFO -- Get or create the cluster --
2023-03-07 21:32:46,771 __main__ INFO -- Start daemons --
2023-03-07 21:32:49,475 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:33:19,345 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,345 __main__ INFO 12345
2023-03-07 21:33:19,345 __main__ INFO 12345
2023-03-07 21:33:19,346 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,346 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-03-07 21:33:19,346 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,346 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,346 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,346 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,466 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,469 __main__ INFO 12345
2023-03-07 21:33:19,469 __main__ INFO 12345
2023-03-07 21:33:19,470 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,470 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-03-07 21:33:19,470 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,470 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,471 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,471 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-03-07 21:33:19,598 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,598 __main__ INFO 12346
2023-03-07 21:33:19,598 __main__ INFO 12346
2023-03-07 21:33:19,598 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,599 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-03-07 21:33:19,599 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,599 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,599 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,599 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,716 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,716 __main__ INFO 12346
2023-03-07 21:33:19,716 __main__ INFO 12346
2023-03-07 21:33:19,716 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,716 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-03-07 21:33:19,717 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,717 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,717 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,717 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-03-07 21:33:19,835 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,836 __main__ INFO 12347
2023-03-07 21:33:19,836 __main__ INFO 12347
2023-03-07 21:33:19,836 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,836 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-03-07 21:33:19,836 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,836 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,836 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,837 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,951 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:19,951 __main__ INFO 12347
2023-03-07 21:33:19,952 __main__ INFO 12347
2023-03-07 21:33:19,952 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,952 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-03-07 21:33:19,952 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,952 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,952 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:19,952 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-03-07 21:33:20,062 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,062 __main__ INFO 12348
2023-03-07 21:33:20,062 __main__ INFO 12348
2023-03-07 21:33:20,062 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,062 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-03-07 21:33:20,062 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,062 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,063 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,063 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,168 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,168 __main__ INFO 12348
2023-03-07 21:33:20,168 __main__ INFO 12348
2023-03-07 21:33:20,169 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,169 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-03-07 21:33:20,169 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,169 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,169 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,170 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-03-07 21:33:20,276 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,276 __main__ INFO 12355
2023-03-07 21:33:20,276 __main__ INFO 12355
2023-03-07 21:33:20,276 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,277 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,278 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,278 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,278 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,395 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,396 __main__ INFO 12355
2023-03-07 21:33:20,396 __main__ INFO 12355
2023-03-07 21:33:20,396 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,396 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-03-07 21:33:20,396 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,396 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,396 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,397 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,397 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,397 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,397 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,397 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-03-07 21:33:20,507 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,507 __main__ INFO 12356
2023-03-07 21:33:20,507 __main__ INFO 12356
2023-03-07 21:33:20,507 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,508 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,509 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,509 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,509 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,617 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,617 __main__ INFO 12356
2023-03-07 21:33:20,617 __main__ INFO 12356
2023-03-07 21:33:20,617 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,617 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-03-07 21:33:20,617 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,618 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-03-07 21:33:20,733 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,733 __main__ INFO 12357
2023-03-07 21:33:20,733 __main__ INFO 12357
2023-03-07 21:33:20,734 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,734 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,735 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,735 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,735 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,845 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,845 __main__ INFO 12357
2023-03-07 21:33:20,845 __main__ INFO 12357
2023-03-07 21:33:20,845 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,845 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-03-07 21:33:20,845 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,846 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,846 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,846 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,846 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,846 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,847 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,847 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-03-07 21:33:20,950 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:20,951 __main__ INFO 12358
2023-03-07 21:33:20,951 __main__ INFO 12358
2023-03-07 21:33:20,951 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,951 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-03-07 21:33:20,951 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,951 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:20,952 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,064 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-03-07 21:33:21,064 __main__ INFO 12358
2023-03-07 21:33:21,064 __main__ INFO 12358
2023-03-07 21:33:21,064 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,065 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-03-07 21:33:21,065 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,065 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,065 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,065 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,066 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,066 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,066 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:21,066 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-03-07 21:33:23,197 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-03-07 21:33:23,198 __main__ INFO 12353
2023-03-07 21:33:23,198 __main__ INFO 12353
2023-03-07 21:33:23,198 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,198 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-03-07 21:33:23,198 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,198 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,198 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-03-07 21:33:23,199 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-03-07 21:33:23,200 TADA INFO test slurm_stream_test ended
2023-03-07 21:33:34 INFO: ----------------------------------------------
2023-03-07 21:33:35 INFO: ======== spank_notifier_test ========
2023-03-07 21:33:35 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/spank_notifier_test
2023-03-07 21:33:36,069 TADA INFO starting test `spank_notifier_test`
2023-03-07 21:33:36,069 TADA INFO   test-id: 591d5170484c40638a7e61f1391a5969aef0f01ea8f2fec73c124f26a6133991
2023-03-07 21:33:36,069 TADA INFO   test-suite: Slurm_Plugins
2023-03-07 21:33:36,069 TADA INFO   test-name: spank_notifier_test
2023-03-07 21:33:36,070 TADA INFO   test-user: narate
2023-03-07 21:33:36,070 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:33:36,070 __main__ INFO -- Create the cluster --
2023-03-07 21:34:02,212 __main__ INFO -- Cleanup output --
2023-03-07 21:34:02,543 __main__ INFO -- Test bad plugstack config --
2023-03-07 21:34:02,544 __main__ INFO Starting slurm ...
2023-03-07 21:34:17,091 __main__ INFO Starting slurm ... OK
2023-03-07 21:34:37,567 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 21:34:37,679 __main__ INFO   jobid = 1
2023-03-07 21:34:37,890 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 21:34:38,008 __main__ INFO   jobid = 2
2023-03-07 21:34:38,211 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 21:34:38,340 __main__ INFO   jobid = 3
2023-03-07 21:34:38,551 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 21:34:38,670 __main__ INFO   jobid = 4
2023-03-07 21:34:48,336 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-03-07 21:34:48,336 __main__ INFO Killin slurm ...
2023-03-07 21:34:51,296 __main__ INFO Killin slurm ... OK
2023-03-07 21:35:11,316 __main__ INFO -- Start daemons --
2023-03-07 21:35:22,320 __main__ INFO Starting slurm ... OK
2023-03-07 21:35:42,576 __main__ INFO -- Submitting job with no stream listener --
2023-03-07 21:35:42,799 __main__ INFO -- Submitting job with num_tasks 8 --
2023-03-07 21:35:42,919 __main__ INFO   jobid = 5
2023-03-07 21:35:58,935 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-03-07 21:35:58,935 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-03-07 21:36:04,833 __main__ INFO -- Submitting job with listener --
2023-03-07 21:36:05,055 __main__ INFO -- Submitting job with num_tasks 1 --
2023-03-07 21:36:05,172 __main__ INFO   jobid = 6
2023-03-07 21:36:05,368 __main__ INFO -- Submitting job with num_tasks 2 --
2023-03-07 21:36:05,491 __main__ INFO   jobid = 7
2023-03-07 21:36:05,707 __main__ INFO -- Submitting job with num_tasks 4 --
2023-03-07 21:36:05,832 __main__ INFO   jobid = 8
2023-03-07 21:36:06,024 __main__ INFO -- Submitting job with num_tasks 8 --
2023-03-07 21:36:06,138 __main__ INFO   jobid = 9
2023-03-07 21:36:06,341 __main__ INFO -- Submitting job with num_tasks 27 --
2023-03-07 21:36:06,450 __main__ INFO   jobid = 10
2023-03-07 21:36:28,241 __main__ INFO -- Verifying Events --
2023-03-07 21:36:28,241 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-03-07 21:36:28,241 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 21:36:28,242 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 21:36:28,242 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 21:36:28,242 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 21:36:28,242 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-03-07 21:36:28,242 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 21:36:28,243 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 21:36:28,244 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 21:36:28,245 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 21:36:28,245 TADA INFO assertion 22, 27-task job: first event is 'init': `init` verified, passed
2023-03-07 21:36:28,245 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-03-07 21:36:28,245 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-03-07 21:36:28,246 TADA INFO assertion 25, 27-task job: third event is 'task_exit': `task_exit` verified, passed
2023-03-07 21:36:28,246 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` verified, passed
2023-03-07 21:36:28,246 __main__ INFO job 7 multi-tenant with dict_keys([6])
2023-03-07 21:36:28,246 __main__ INFO job 10 multi-tenant with dict_keys([6, 7])
2023-03-07 21:36:28,246 __main__ INFO job 10 multi-tenant with dict_keys([8])
2023-03-07 21:36:28,246 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-03-07 21:36:28,246 __main__ INFO job 10 multi-tenant with dict_keys([9])
2023-03-07 21:36:28,246 TADA INFO assertion 50, Multi-tenant verification: Multi-tenant jobs found, passed
2023-03-07 21:36:28,459 __main__ INFO -- Submitting job that crashes listener --
2023-03-07 21:36:28,576 __main__ INFO   jobid = 11
2023-03-07 21:36:38,813 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: job output file created, passed
2023-03-07 21:36:38,924 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: job output file created, passed
2023-03-07 21:36:38,925 TADA INFO test spank_notifier_test ended
2023-03-07 21:36:55 INFO: ----------------------------------------------
2023-03-07 21:36:56 INFO: ======== ldms_list_test ========
2023-03-07 21:36:56 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldms_list_test
2023-03-07 21:36:56,949 TADA INFO starting test `ldms_list_test`
2023-03-07 21:36:56,950 TADA INFO   test-id: af5449c248c4aba3877506f8cd854b716cbc453fed9fa52f2cbb2f1f2d275102
2023-03-07 21:36:56,950 TADA INFO   test-suite: LDMSD
2023-03-07 21:36:56,950 TADA INFO   test-name: ldms_list_test
2023-03-07 21:36:56,950 TADA INFO   test-user: narate
2023-03-07 21:36:56,950 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:36:56,951 __main__ INFO -- Get or create the cluster --
2023-03-07 21:37:00,061 __main__ INFO -- Start daemons --
2023-03-07 21:37:06,512 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:37:08,515 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-03-07 21:37:14,551 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-03-07 21:37:14,551 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-03-07 21:37:14,552 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-03-07 21:37:14,552 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-03-07 21:37:14,552 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-03-07 21:37:14,552 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-03-07 21:37:14,553 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-03-07 21:37:14,553 __main__ INFO 2nd sampling on the sampler...
2023-03-07 21:37:21,762 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-03-07 21:37:21,763 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-03-07 21:37:21,763 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-03-07 21:37:21,763 __main__ INFO 2nd update on the aggregator...
2023-03-07 21:37:28,972 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-03-07 21:37:28,972 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-03-07 21:37:28,973 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-03-07 21:37:28,973 __main__ INFO 3rd sampling on the sampler...
2023-03-07 21:37:36,182 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-03-07 21:37:36,183 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-03-07 21:37:36,183 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-03-07 21:37:36,183 __main__ INFO 3rd update on the aggregator...
2023-03-07 21:37:43,393 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-03-07 21:37:43,393 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-03-07 21:37:43,393 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-03-07 21:37:43,393 __main__ INFO 4th sampling on the sampler...
2023-03-07 21:37:50,608 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-03-07 21:37:50,608 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-03-07 21:37:50,608 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-03-07 21:37:50,609 __main__ INFO 4th update on the aggregator...
2023-03-07 21:37:57,818 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-03-07 21:37:57,818 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-03-07 21:37:57,819 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-03-07 21:37:57,819 __main__ INFO 5th sampling on the sampler...
2023-03-07 21:38:05,028 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-03-07 21:38:05,029 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-03-07 21:38:05,029 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-03-07 21:38:05,029 __main__ INFO 5th update on the aggregator...
2023-03-07 21:38:12,239 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-03-07 21:38:12,239 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-03-07 21:38:12,239 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-03-07 21:38:12,239 __main__ INFO 6th sampling on the sampler...
2023-03-07 21:38:19,449 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-03-07 21:38:19,449 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-03-07 21:38:19,450 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-03-07 21:38:19,450 __main__ INFO 6th update on the updator...
2023-03-07 21:38:26,659 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-03-07 21:38:26,660 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-03-07 21:38:26,660 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-03-07 21:38:26,660 TADA INFO test ldms_list_test ended
2023-03-07 21:38:37 INFO: ----------------------------------------------
2023-03-07 21:38:38 INFO: ======== quick_set_add_rm_test ========
2023-03-07 21:38:38 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/quick_set_add_rm_test
2023-03-07 21:38:38,895 TADA INFO starting test `quick_set_add_rm_test`
2023-03-07 21:38:38,895 TADA INFO   test-id: d8a409cf2196a7d68061759c37bbf12724baf51a32e7640015bc2ccbc60dbd68
2023-03-07 21:38:38,895 TADA INFO   test-suite: LDMSD
2023-03-07 21:38:38,895 TADA INFO   test-name: quick_set_add_rm_test
2023-03-07 21:38:38,895 TADA INFO   test-user: narate
2023-03-07 21:38:38,895 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:38:38,896 __main__ INFO -- Get or create the cluster --
2023-03-07 21:38:46,257 __main__ INFO -- Start samp.py --
2023-03-07 21:38:51,373 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-03-07 21:38:51,374 __main__ INFO -- Start daemons --
2023-03-07 21:38:59,115 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:39:04,711 TADA INFO assertion 2, verify data: verified, passed
2023-03-07 21:39:09,321 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-03-07 21:39:13,934 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-03-07 21:39:18,562 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-03-07 21:39:23,688 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-03-07 21:39:23,688 TADA INFO test quick_set_add_rm_test ended
2023-03-07 21:39:35 INFO: ----------------------------------------------
2023-03-07 21:39:36 INFO: ======== set_array_hang_test ========
2023-03-07 21:39:36 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/set_array_hang_test
2023-03-07 21:39:37,341 TADA INFO starting test `set_array_hang_test`
2023-03-07 21:39:37,341 TADA INFO   test-id: d6f3bb6663ba22faa6cd1c2312821e285d4573627b81b2c3bca6188ce9b6693d
2023-03-07 21:39:37,341 TADA INFO   test-suite: LDMSD
2023-03-07 21:39:37,341 TADA INFO   test-name: set_array_hang_test
2023-03-07 21:39:37,342 TADA INFO   test-user: narate
2023-03-07 21:39:37,342 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:39:37,342 __main__ INFO -- Get or create the cluster --
2023-03-07 21:39:40,456 __main__ INFO -- Start processes --
2023-03-07 21:39:40,457 __main__ INFO starting interactive set_array_samp.py
2023-03-07 21:39:43,471 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-03-07 21:39:46,488 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-03-07 21:39:53,697 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-03-07 21:40:00,907 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-03-07 21:40:04,512 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-03-07 21:40:11,721 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-03-07 21:40:11,722 TADA INFO test set_array_hang_test ended
2023-03-07 21:40:22 INFO: ----------------------------------------------
2023-03-07 21:40:23 INFO: ======== ldmsd_autointerval_test ========
2023-03-07 21:40:23 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_autointerval_test
2023-03-07 21:40:24,026 TADA INFO starting test `ldmsd_autointerval_test`
2023-03-07 21:40:24,026 TADA INFO   test-id: 89a05f1819bf4cbd5962cdccc616e920f7a624aa7119728cad07439403a40c7e
2023-03-07 21:40:24,026 TADA INFO   test-suite: LDMSD
2023-03-07 21:40:24,026 TADA INFO   test-name: ldmsd_autointerval_test
2023-03-07 21:40:24,026 TADA INFO   test-user: narate
2023-03-07 21:40:24,026 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:40:24,027 __main__ INFO -- Get or create the cluster --
2023-03-07 21:40:31,472 __main__ INFO -- Start daemons --
2023-03-07 21:40:35,317 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:40:41,840 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-03-07 21:40:44,084 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-03-07 21:40:44,084 __main__ INFO Let them run for a while to collect data ...
2023-03-07 21:40:54,095 __main__ INFO Setting sample interval to 1000000 ...
2023-03-07 21:41:02,335 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-03-07 21:41:02,335 __main__ INFO Let them run for a while to collect data ...
2023-03-07 21:41:12,337 __main__ INFO Setting sample interval to 2000000 ...
2023-03-07 21:41:20,559 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-03-07 21:41:20,559 __main__ INFO Let them run for a while to collect data ...
2023-03-07 21:41:30,796 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-03-07 21:41:30,916 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-03-07 21:41:30,916 TADA INFO test ldmsd_autointerval_test ended
2023-03-07 21:41:43 INFO: ----------------------------------------------
2023-03-07 21:41:43 INFO: ======== ldms_record_test ========
2023-03-07 21:41:43 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldms_record_test
2023-03-07 21:41:44,605 TADA INFO starting test `ldms_record_test`
2023-03-07 21:41:44,606 TADA INFO   test-id: 025133dc6c6e74fa81b500bb035b9dda2f1a30e60c07c8b0c710ac33e807db28
2023-03-07 21:41:44,606 TADA INFO   test-suite: LDMSD
2023-03-07 21:41:44,606 TADA INFO   test-name: ldms_record_test
2023-03-07 21:41:44,606 TADA INFO   test-user: narate
2023-03-07 21:41:44,606 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:41:44,607 __main__ INFO -- Get or create the cluster --
2023-03-07 21:41:47,745 __main__ INFO -- Start daemons --
2023-03-07 21:41:54,122 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:41:56,125 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-03-07 21:42:02,159 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-03-07 21:42:02,159 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-03-07 21:42:02,160 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-03-07 21:42:02,160 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-03-07 21:42:02,160 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-03-07 21:42:02,160 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-03-07 21:42:02,161 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-03-07 21:42:02,161 __main__ INFO 2nd sampling on the sampler...
2023-03-07 21:42:09,370 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-03-07 21:42:09,371 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-03-07 21:42:09,371 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-03-07 21:42:09,371 __main__ INFO 2nd update on the aggregator...
2023-03-07 21:42:16,580 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-03-07 21:42:16,580 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-03-07 21:42:16,580 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-03-07 21:42:16,581 __main__ INFO 3rd sampling on the sampler...
2023-03-07 21:42:23,790 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-03-07 21:42:23,790 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-03-07 21:42:23,790 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-03-07 21:42:23,791 __main__ INFO 3rd update on the aggregator...
2023-03-07 21:42:31,000 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-03-07 21:42:31,000 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-03-07 21:42:31,000 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-03-07 21:42:31,000 __main__ INFO 4th sampling on the sampler...
2023-03-07 21:42:38,215 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-03-07 21:42:38,215 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-03-07 21:42:38,215 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-03-07 21:42:38,215 __main__ INFO 4th update on the aggregator...
2023-03-07 21:42:45,425 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-03-07 21:42:45,425 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-03-07 21:42:45,425 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-03-07 21:42:45,426 __main__ INFO 5th sampling on the sampler...
2023-03-07 21:42:52,634 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-03-07 21:42:52,635 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-03-07 21:42:52,635 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-03-07 21:42:52,635 __main__ INFO 5th update on the aggregator...
2023-03-07 21:42:59,844 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-03-07 21:42:59,844 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-03-07 21:42:59,845 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-03-07 21:42:59,845 __main__ INFO 6th sampling on the sampler...
2023-03-07 21:43:07,054 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-03-07 21:43:07,055 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-03-07 21:43:07,055 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-03-07 21:43:07,055 __main__ INFO 6th update on the updator...
2023-03-07 21:43:14,264 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-03-07 21:43:14,265 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-03-07 21:43:14,265 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-03-07 21:43:14,266 TADA INFO test ldms_record_test ended
2023-03-07 21:43:25 INFO: ----------------------------------------------
2023-03-07 21:43:25 INFO: ======== ldms_schema_digest_test ========
2023-03-07 21:43:25 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldms_schema_digest_test
2023-03-07 21:43:26,575 TADA INFO starting test `ldms_schema_digest_test`
2023-03-07 21:43:26,575 TADA INFO   test-id: 5ec42623eb0cf9a395fe21b1caf0577a09c8ea7f38c42fb7ac6143c4dc0f46b1
2023-03-07 21:43:26,575 TADA INFO   test-suite: LDMSD
2023-03-07 21:43:26,575 TADA INFO   test-name: ldms_schema_digest_test
2023-03-07 21:43:26,575 TADA INFO   test-user: narate
2023-03-07 21:43:26,575 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:43:26,576 __main__ INFO -- Get or create the cluster --
2023-03-07 21:43:34,084 __main__ INFO -- Start daemons --
2023-03-07 21:43:37,297 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:43:42,441 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-03-07 21:43:42,547 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-03-07 21:43:42,676 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-03-07 21:43:42,868 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-03-07 21:43:42,868 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-03-07 21:43:42,869 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-03-07 21:43:45,260 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-03-07 21:43:45,260 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-03-07 21:43:45,260 TADA INFO test ldms_schema_digest_test ended
2023-03-07 21:43:57 INFO: ----------------------------------------------
2023-03-07 21:43:58 INFO: ======== ldmsd_decomp_test ========
2023-03-07 21:43:58 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_decomp_test
2023-03-07 21:43:58,968 TADA INFO starting test `ldmsd_decomp_test`
2023-03-07 21:43:58,968 TADA INFO   test-id: 4482ca72649b6f7c9e1047cfc5059ccc6cf02a04d31d87e3882aa2a7b24e9e99
2023-03-07 21:43:58,969 TADA INFO   test-suite: LDMSD
2023-03-07 21:43:58,969 TADA INFO   test-name: ldmsd_decomp_test
2023-03-07 21:43:58,969 TADA INFO   test-user: narate
2023-03-07 21:43:58,969 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:43:58,970 __main__ INFO -- Get or create the cluster --
2023-03-07 21:44:14,815 __main__ INFO -- Start daemons --
2023-03-07 21:44:25,237 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:45:19,942 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-03-07 21:45:19,942 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-03-07 21:45:19,942 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-03-07 21:45:19,942 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-03-07 21:45:19,942 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-03-07 21:45:19,943 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-03-07 21:45:19,944 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-03-07 21:45:19,946 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-03-07 21:45:19,947 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-03-07 21:45:20,022 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-03-07 21:45:20,026 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-03-07 21:45:20,029 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-03-07 21:45:20,038 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-03-07 21:45:20,040 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-03-07 21:45:20,042 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-03-07 21:45:20,115 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-03-07 21:45:20,118 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-03-07 21:45:20,121 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-03-07 21:45:20,130 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-03-07 21:45:20,131 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-03-07 21:45:20,132 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-03-07 21:45:20,161 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-03-07 21:45:20,163 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-03-07 21:45:20,165 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-03-07 21:45:20,169 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-03-07 21:45:20,170 TADA INFO test ldmsd_decomp_test ended
2023-03-07 21:45:20,170 TADA INFO test ldmsd_decomp_test ended
2023-03-07 21:45:35 INFO: ----------------------------------------------
2023-03-07 21:45:36 INFO: ======== ldmsd_stream_status_test ========
2023-03-07 21:45:36 INFO: CMD: python3 ldmsd_stream_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_stream_status_test
2023-03-07 21:45:36,966 __main__ INFO -- Get or create the cluster --
2023-03-07 21:45:36,966 TADA INFO starting test `ldmsd_stream_status`
2023-03-07 21:45:36,966 TADA INFO   test-id: 8ffce795d22a298af52e6fba62802be9a45985b7a491a663627c2e0108af22af
2023-03-07 21:45:36,967 TADA INFO   test-suite: LDMSD
2023-03-07 21:45:36,967 TADA INFO   test-name: ldmsd_stream_status
2023-03-07 21:45:36,967 TADA INFO   test-user: narate
2023-03-07 21:45:36,967 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:45:45,439 __main__ INFO -- Start daemons --
2023-03-07 21:45:49,232 __main__ INFO waiting ... for all LDMSDs to start
2023-03-07 21:45:49,560 __main__ INFO All LDMSDs are up.
2023-03-07 21:45:50,782 TADA INFO assertion 1, No Stream data: {} == {}, passed
2023-03-07 21:45:52,103 TADA INFO assertion 2, stream_status -- one stream message: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247150, 'last_ts': 1678247150, 'count': 1, 'total_bytes': 6}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678247150, 'last_ts': 1678247150, 'count': 1, 'total_bytes': 6}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 1, 'total_bytes': 6, 'first_ts': 1678247150, 'last_ts': 1678247150}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 1, 'total_bytes': 6, 'first_ts': 1678247150, 'last_ts': 1678247150}, 'pub': {}}}, passed
2023-03-07 21:45:54,565 TADA INFO assertion 3, stream_status --  multiple stream messages: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247150, 'last_ts': 1678247153, 'count': 3, 'total_bytes': 18, 'msg/sec': 1.0, 'bytes/sec': 6.0}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678247150, 'last_ts': 1678247153, 'count': 3, 'total_bytes': 18, 'msg/sec': 1.0, 'bytes/sec': 6.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, passed
2023-03-07 21:45:55,794 TADA INFO assertion 4, prdcr_stream_status to agg -- one producer: {'_OVERALL_': {'samplerd-1': {'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678247153, 'first_ts': 1678247150, 'bytes/sec': 6.0, 'count': 3}}}, 'foo': {'samplerd-1': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678247153, 'first_ts': 1678247150, 'bytes/sec': 6.0, 'count': 3}}}} == {'foo': {'samplerd-1': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, '_OVERALL_': {'samplerd-1': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}}, passed
2023-03-07 21:45:59,615 TADA INFO assertion 5, stream_status -- mulitple streams: {'bar': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247157, 'last_ts': 1678247158, 'count': 3, 'total_bytes': 48, 'msg/sec': 3.0, 'bytes/sec': 48.0}, 'publishers': {}}, 'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247155, 'last_ts': 1678247157, 'count': 2, 'total_bytes': 12, 'msg/sec': 1.0, 'bytes/sec': 6.0}, 'publishers': {}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678247155, 'last_ts': 1678247158, 'count': 5, 'total_bytes': 60, 'msg/sec': 1.666667, 'bytes/sec': 20.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247155, 'last_ts': 1678247157, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}, 'publishers': {}}, 'bar': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 48, 'first_ts': 1678247157, 'last_ts': 1678247158, 'bytes/sec': 48.0, 'msg/sec': 3.0}, 'pub': {}, 'publishers': {}}, '_OVERALL_': {'recv': {'count': 5, 'total_bytes': 60, 'first_ts': 1678247155, 'last_ts': 1678247158, 'bytes/sec': 20.0, 'msg/sec': 1.666667}, 'pub': {}}}, passed
2023-03-07 21:46:00,847 TADA INFO assertion 6, prdcr_stream_status to agg -- two producers: {'_OVERALL_': {'samplerd-2': {'pub': {}, 'recv': {'msg/sec': 1.666667, 'total_bytes': 60, 'last_ts': 1678247158, 'first_ts': 1678247155, 'bytes/sec': 20.0, 'count': 5}}, 'samplerd-1': {'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678247153, 'first_ts': 1678247150, 'bytes/sec': 6.0, 'count': 3}}}, 'foo': {'samplerd-2': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 12, 'last_ts': 1678247157, 'first_ts': 1678247155, 'bytes/sec': 6.0, 'count': 2}}, 'samplerd-1': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 1.0, 'total_bytes': 18, 'last_ts': 1678247153, 'first_ts': 1678247150, 'bytes/sec': 6.0, 'count': 3}}}, 'bar': {'samplerd-2': {'mode': 'not subscribed', 'pub': {}, 'recv': {'msg/sec': 3.0, 'total_bytes': 48, 'last_ts': 1678247158, 'first_ts': 1678247157, 'bytes/sec': 48.0, 'count': 3}}}} == {'foo': {'samplerd-1': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}, 'samplerd-2': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247155, 'last_ts': 1678247157, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}}, 'bar': {'samplerd-2': {'mode': 'not subscribed', 'recv': {'count': 3, 'total_bytes': 48, 'first_ts': 1678247157, 'last_ts': 1678247158, 'bytes/sec': 48.0, 'msg/sec': 3.0}, 'pub': {}}}, '_OVERALL_': {'samplerd-1': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247150, 'last_ts': 1678247153, 'bytes/sec': 6.0, 'msg/sec': 1.0}, 'pub': {}}, 'samplerd-2': {'recv': {'count': 5, 'total_bytes': 60, 'first_ts': 1678247155, 'last_ts': 1678247158, 'bytes/sec': 20.0, 'msg/sec': 1.666667}, 'pub': {}}}}, passed
2023-03-07 21:46:04,532 TADA INFO assertion 7, stream_status to agg after one producer republished stream: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247162, 'last_ts': 1678247163, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}, 'publishers': {'samplerd-1': {'recv': {'first_ts': 1678247162, 'last_ts': 1678247163, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678247162, 'last_ts': 1678247163, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247162, 'last_ts': 1678247163, 'bytes/sec': 12.0, 'msg/sec': 2.0}, 'pub': {}, 'publishers': {'samplerd-1': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247162, 'last_ts': 1678247163, 'bytes/sec': 12.0, 'msg/sec': 2.0}}}}, '_OVERALL_': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247162, 'last_ts': 1678247163, 'bytes/sec': 12.0, 'msg/sec': 2.0}, 'pub': {}}}, passed
2023-03-07 21:46:06,151 TADA INFO assertion 8, stream_status to agg after two producers republished stream: {'foo': {'mode': 'not subscribed', 'pub': {}, 'recv': {'first_ts': 1678247162, 'last_ts': 1678247164, 'count': 5, 'total_bytes': 30, 'msg/sec': 2.5, 'bytes/sec': 15.0}, 'publishers': {'samplerd-1': {'recv': {'first_ts': 1678247162, 'last_ts': 1678247163, 'count': 2, 'total_bytes': 12, 'msg/sec': 2.0, 'bytes/sec': 12.0}}, 'samplerd-2': {'recv': {'first_ts': 1678247164, 'last_ts': 1678247164, 'count': 3, 'total_bytes': 18}}}}, '_OVERALL_': {'pub': {}, 'recv': {'first_ts': 1678247162, 'last_ts': 1678247164, 'count': 5, 'total_bytes': 30, 'msg/sec': 2.5, 'bytes/sec': 15.0}}} == {'foo': {'mode': 'not subscribed', 'recv': {'count': 5, 'total_bytes': 30, 'first_ts': 1678247162, 'last_ts': 1678247164, 'bytes/sec': 15.0, 'msg/sec': 2.5}, 'pub': {}, 'publishers': {'samplerd-1': {'recv': {'count': 2, 'total_bytes': 12, 'first_ts': 1678247162, 'last_ts': 1678247163, 'bytes/sec': 12.0, 'msg/sec': 2.0}}, 'samplerd-2': {'recv': {'count': 3, 'total_bytes': 18, 'first_ts': 1678247164, 'last_ts': 1678247164}}}}, '_OVERALL_': {'recv': {'count': 5, 'total_bytes': 30, 'first_ts': 1678247162, 'last_ts': 1678247164, 'bytes/sec': 15.0, 'msg/sec': 2.5}, 'pub': {}}}, passed
2023-03-07 21:46:06,151 TADA INFO test ldmsd_stream_status ended
2023-03-07 21:46:18 INFO: ----------------------------------------------
2023-03-07 21:46:19 INFO: ======== store_list_record_test ========
2023-03-07 21:46:19 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/store_list_record_test
2023-03-07 21:46:19,904 __main__ INFO -- Get or create the cluster --
2023-03-07 21:46:19,904 TADA INFO starting test `store_sos_lists_test`
2023-03-07 21:46:19,904 TADA INFO   test-id: a25f95c25b3fcd42edffa08968a7e9b792fcdb5c16aa8112410d1e821d4a4171
2023-03-07 21:46:19,904 TADA INFO   test-suite: LDMSD
2023-03-07 21:46:19,904 TADA INFO   test-name: store_sos_lists_test
2023-03-07 21:46:19,904 TADA INFO   test-user: narate
2023-03-07 21:46:19,904 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:46:27,267 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:46:31,069 __main__ INFO All sampler daemons are up.
2023-03-07 21:46:31,203 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-03-07 21:46:31,303 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-03-07 21:46:42,450 TADA INFO assertion 3, store_sos is storing data.: agg.file_exists/store/sos/record_record_record, failed
Traceback (most recent call last):
  File "store_list_record_test", line 427, in <module>
    test.assert_test(SOS_DATABASE_CREATED, result, reason)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test store_sos storing lists, agg.file_exists/store/sos/record_record_record: FAILED
2023-03-07 21:46:42,452 TADA INFO assertion 4, store_sos stores data correctly.: skipped
2023-03-07 21:46:42,452 TADA INFO assertion 5, store_sos stores data after restarted correctly.: skipped
2023-03-07 21:46:42,453 TADA INFO assertion 6, store_csv is storing data.: skipped
2023-03-07 21:46:42,453 TADA INFO assertion 7, store_csv stores data correctly.: skipped
2023-03-07 21:46:42,453 TADA INFO assertion 8, store_csv stores data after restarted correctly.: skipped
2023-03-07 21:46:42,454 TADA INFO test store_sos_lists_test ended
2023-03-07 21:46:54 INFO: ----------------------------------------------
2023-03-07 21:46:55 INFO: ======== maestro_raft_test ========
2023-03-07 21:46:55 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/maestro_raft_test
2023-03-07 21:46:56,382 TADA INFO starting test `maestro_raft_test`
2023-03-07 21:46:56,382 TADA INFO   test-id: d42f811d1b6d3eccab02488422346c6b3376d3b33e876552c834663a3e9e1e10
2023-03-07 21:46:56,382 TADA INFO   test-suite: LDMSD
2023-03-07 21:46:56,383 TADA INFO   test-name: maestro_raft_test
2023-03-07 21:46:56,383 TADA INFO   test-user: narate
2023-03-07 21:46:56,383 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:47:06,394 __main__ INFO -- Get or create cluster --
2023-03-07 21:47:41,250 __main__ INFO -- Start daemons --
2023-03-07 21:48:52,446 __main__ INFO -- making known hosts (ssh) --
2023-03-07 21:48:59,477 __main__ INFO ... make sure ldmsd's are up
2023-03-07 21:49:15,170 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: [('FOLLOWER', 2), ('LEADER', 1)], passed
2023-03-07 21:49:27,624 TADA INFO assertion 2, All ldmsds are up and configured: sets verified, passed
2023-03-07 21:49:27,896 TADA INFO assertion 3, Data are being stored: data check, passed
2023-03-07 21:49:32,794 TADA INFO assertion 4, New leader elected: checked, passed
2023-03-07 21:49:44,676 TADA INFO assertion 5, Restarted ldmsd is configured: sets verified, passed
2023-03-07 21:49:44,988 TADA INFO assertion 6, New data are presented in the store: data check, passed
2023-03-07 21:49:55,981 TADA INFO assertion 7, The restarted maestro becomes a follower: checked, passed
---Wait for config to write to file---
2023-03-07 21:49:55,981 TADA INFO test maestro_raft_test ended
2023-03-07 21:50:17 INFO: ----------------------------------------------
2023-03-07 21:50:18 INFO: ======== ovis_json_test ========
2023-03-07 21:50:18 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ovis_json_test
2023-03-07 21:50:18,722 __main__ INFO -- Create the cluster -- 
2023-03-07 21:50:24,043 TADA INFO starting test `ovis_json_test`
2023-03-07 21:50:24,043 TADA INFO   test-id: 74d2e823241df5f558c30823106d027cdb4047cc758317aa64f6160acfb2c8b9
2023-03-07 21:50:24,043 TADA INFO   test-suite: OVIS-LIB
2023-03-07 21:50:24,043 TADA INFO   test-name: ovis_json_test
2023-03-07 21:50:24,043 TADA INFO   test-user: narate
2023-03-07 21:50:24,044 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:50:24,044 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-03-07 21:50:24,044 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-03-07 21:50:24,045 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-03-07 21:50:24,045 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-03-07 21:50:24,045 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-03-07 21:50:24,045 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-03-07 21:50:24,045 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-03-07 21:50:24,045 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-03-07 21:50:24,046 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,046 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,046 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,046 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,046 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,046 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,047 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,047 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-03-07 21:50:24,047 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-03-07 21:50:24,047 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-03-07 21:50:24,047 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-03-07 21:50:24,047 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-03-07 21:50:24,047 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-03-07 21:50:24,048 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-03-07 21:50:24,048 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-03-07 21:50:24,048 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-03-07 21:50:24,048 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-03-07 21:50:24,048 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-03-07 21:50:24,048 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-03-07 21:50:24,048 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,049 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,050 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-03-07 21:50:24,050 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-03-07 21:50:24,050 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-03-07 21:50:24,050 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-03-07 21:50:24,050 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-03-07 21:50:24,050 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-03-07 21:50:24,051 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-03-07 21:50:24,051 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-03-07 21:50:24,051 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-03-07 21:50:24,051 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-03-07 21:50:24,051 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-03-07 21:50:24,051 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-03-07 21:50:24,051 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-03-07 21:50:24,052 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-03-07 21:50:24,052 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-03-07 21:50:24,052 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-03-07 21:50:24,052 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-03-07 21:50:24,052 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-03-07 21:50:24,052 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-03-07 21:50:24,052 TADA INFO test ovis_json_test ended
2023-03-07 21:50:34 INFO: ----------------------------------------------
2023-03-07 21:50:35 INFO: ======== updtr_add_test ========
2023-03-07 21:50:35 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_add_test
2023-03-07 21:50:36,377 __main__ INFO -- Get or create the cluster --
2023-03-07 21:50:36,378 TADA INFO starting test `updtr_add test`
2023-03-07 21:50:36,378 TADA INFO   test-id: c708a19d9752897732c2ba14af4123eaaffb21e5225b8f31ad2262fe8e2151a2
2023-03-07 21:50:36,378 TADA INFO   test-suite: LDMSD
2023-03-07 21:50:36,378 TADA INFO   test-name: updtr_add test
2023-03-07 21:50:36,378 TADA INFO   test-user: narate
2023-03-07 21:50:36,378 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:50:44,516 __main__ INFO -- Start daemons --
2023-03-07 21:50:48,238 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:50:48,559 __main__ INFO All LDMSDs are up.
2023-03-07 21:50:49,762 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:50:50,977 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:50:52,201 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:50:53,433 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:50:54,635 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:50:57,077 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:50:59,528 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:51:00,746 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-03-07 21:51:00,746 __main__ INFO --- done ---
2023-03-07 21:51:00,747 TADA INFO test updtr_add test ended
2023-03-07 21:51:12 INFO: ----------------------------------------------
2023-03-07 21:51:13 INFO: ======== updtr_del_test ========
2023-03-07 21:51:13 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_del_test
2023-03-07 21:51:14,436 __main__ INFO -- Get or create the cluster --
2023-03-07 21:51:14,436 TADA INFO starting test `updtr_add test`
2023-03-07 21:51:14,436 TADA INFO   test-id: f1503b6c38f231762c8d99c14c95cc527867217d6544986993bc250135069c66
2023-03-07 21:51:14,437 TADA INFO   test-suite: LDMSD
2023-03-07 21:51:14,437 TADA INFO   test-name: updtr_add test
2023-03-07 21:51:14,437 TADA INFO   test-user: narate
2023-03-07 21:51:14,437 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:51:22,302 __main__ INFO -- Start daemons --
2023-03-07 21:51:25,947 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:51:26,256 __main__ INFO All LDMSDs are up.
2023-03-07 21:51:27,481 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:51:28,704 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 21:51:29,918 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:51:31,139 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:51:31,139 __main__ INFO --- done ---
2023-03-07 21:51:31,139 TADA INFO test updtr_add test ended
2023-03-07 21:51:43 INFO: ----------------------------------------------
2023-03-07 21:51:43 INFO: ======== updtr_match_add_test ========
2023-03-07 21:51:44 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_match_add_test
2023-03-07 21:51:44,707 __main__ INFO -- Get or create the cluster --
2023-03-07 21:51:44,707 TADA INFO starting test `updtr_add test`
2023-03-07 21:51:44,707 TADA INFO   test-id: 71d52b67a885f0961368ffbbf999902c77c99192df22e648c5b8f1890d25a27f
2023-03-07 21:51:44,707 TADA INFO   test-suite: LDMSD
2023-03-07 21:51:44,707 TADA INFO   test-name: updtr_add test
2023-03-07 21:51:44,707 TADA INFO   test-user: narate
2023-03-07 21:51:44,707 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:51:52,518 __main__ INFO -- Start daemons --
2023-03-07 21:51:56,164 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:51:56,484 __main__ INFO All LDMSDs are up.
2023-03-07 21:51:57,701 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:51:58,921 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:52:00,143 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:52:01,351 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:52:02,572 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 21:52:02,572 __main__ INFO --- done ---
2023-03-07 21:52:02,573 TADA INFO test updtr_add test ended
2023-03-07 21:52:14 INFO: ----------------------------------------------
2023-03-07 21:52:15 INFO: ======== updtr_match_del_test ========
2023-03-07 21:52:15 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_match_del_test
2023-03-07 21:52:16,182 __main__ INFO -- Get or create the cluster --
2023-03-07 21:52:16,182 TADA INFO starting test `updtr_add test`
2023-03-07 21:52:16,183 TADA INFO   test-id: a4231e05eaeeedcf1dfa6ee12262f74e3c879fc3b3aa54593c63c050c1bb22f1
2023-03-07 21:52:16,183 TADA INFO   test-suite: LDMSD
2023-03-07 21:52:16,183 TADA INFO   test-name: updtr_add test
2023-03-07 21:52:16,183 TADA INFO   test-user: narate
2023-03-07 21:52:16,183 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:52:23,972 __main__ INFO -- Start daemons --
2023-03-07 21:52:27,739 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:52:28,063 __main__ INFO All LDMSDs are up.
2023-03-07 21:52:29,273 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-03-07 21:52:30,503 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:52:31,715 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:52:32,928 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:52:34,155 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:52:35,382 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:52:36,610 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:52:36,610 __main__ INFO --- done ---
2023-03-07 21:52:36,611 TADA INFO test updtr_add test ended
2023-03-07 21:52:48 INFO: ----------------------------------------------
2023-03-07 21:52:49 INFO: ======== updtr_prdcr_add_test ========
2023-03-07 21:52:49 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_prdcr_add_test
2023-03-07 21:52:50,288 __main__ INFO -- Get or create the cluster --
2023-03-07 21:52:50,288 TADA INFO starting test `updtr_add test`
2023-03-07 21:52:50,288 TADA INFO   test-id: d68ebea0e7305b0ffdac0eb3fa44641f046a392022f9eeb3e192ecb2d221521c
2023-03-07 21:52:50,288 TADA INFO   test-suite: LDMSD
2023-03-07 21:52:50,288 TADA INFO   test-name: updtr_add test
2023-03-07 21:52:50,288 TADA INFO   test-user: narate
2023-03-07 21:52:50,288 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:52:58,066 __main__ INFO -- Start daemons --
2023-03-07 21:53:01,774 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:53:02,079 __main__ INFO All LDMSDs are up.
2023-03-07 21:53:03,295 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:53:05,741 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:53:08,183 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:53:09,402 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-03-07 21:53:10,641 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:53:10,641 __main__ INFO --- done ---
2023-03-07 21:53:10,641 TADA INFO test updtr_add test ended
2023-03-07 21:53:22 INFO: ----------------------------------------------
2023-03-07 21:53:23 INFO: ======== updtr_prdcr_del_test ========
2023-03-07 21:53:23 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_prdcr_del_test
2023-03-07 21:53:24,220 __main__ INFO -- Get or create the cluster --
2023-03-07 21:53:24,220 TADA INFO starting test `updtr_add test`
2023-03-07 21:53:24,220 TADA INFO   test-id: ab7ef8559ea31e1a3f3df779ad634e960d71ccc185812f754ed3933d62c55c17
2023-03-07 21:53:24,221 TADA INFO   test-suite: LDMSD
2023-03-07 21:53:24,221 TADA INFO   test-name: updtr_add test
2023-03-07 21:53:24,221 TADA INFO   test-user: narate
2023-03-07 21:53:24,221 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:53:32,019 __main__ INFO -- Start daemons --
2023-03-07 21:53:35,675 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:53:35,988 __main__ INFO All LDMSDs are up.
2023-03-07 21:53:37,206 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:53:38,436 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 21:53:39,672 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:53:42,123 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 21:53:42,123 __main__ INFO --- done ---
2023-03-07 21:53:42,123 TADA INFO test updtr_add test ended
2023-03-07 21:53:54 INFO: ----------------------------------------------
2023-03-07 21:53:55 INFO: ======== updtr_start_test ========
2023-03-07 21:53:55 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_start_test
2023-03-07 21:53:55,997 __main__ INFO -- Get or create the cluster --
2023-03-07 21:53:55,997 TADA INFO starting test `updtr_add test`
2023-03-07 21:53:55,997 TADA INFO   test-id: 6944aee7e8c65414e776f5da492b2465ff0ce0147d88a91018c753fe3f04a327
2023-03-07 21:53:55,997 TADA INFO   test-suite: LDMSD
2023-03-07 21:53:55,997 TADA INFO   test-name: updtr_add test
2023-03-07 21:53:55,997 TADA INFO   test-user: narate
2023-03-07 21:53:55,997 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:54:03,880 __main__ INFO -- Start daemons --
2023-03-07 21:54:07,640 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:54:07,952 __main__ INFO All LDMSDs are up.
2023-03-07 21:54:09,166 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:54:10,402 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:54:11,614 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-03-07 21:54:12,834 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:54:14,057 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-03-07 21:54:16,507 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 21:54:17,723 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-03-07 21:54:20,163 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 21:54:22,619 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 21:54:25,063 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-03-07 21:54:26,293 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-03-07 21:54:26,293 __main__ INFO --- done ---
2023-03-07 21:54:26,293 TADA INFO test updtr_add test ended
2023-03-07 21:54:38 INFO: ----------------------------------------------
2023-03-07 21:54:39 INFO: ======== updtr_status_test ========
2023-03-07 21:54:39 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/updtr_status_test
2023-03-07 21:54:40,042 __main__ INFO -- Get or create the cluster --
2023-03-07 21:54:40,043 TADA INFO starting test `updtr_status test`
2023-03-07 21:54:40,043 TADA INFO   test-id: 6e32b5bca11c3ad3eb255811c17fc285dc673006866de22b4742788e68121f44
2023-03-07 21:54:40,043 TADA INFO   test-suite: LDMSD
2023-03-07 21:54:40,043 TADA INFO   test-name: updtr_status test
2023-03-07 21:54:40,043 TADA INFO   test-user: narate
2023-03-07 21:54:40,043 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:54:50,511 __main__ INFO -- Start daemons --
2023-03-07 21:54:55,428 __main__ INFO Waiting ... for all LDMSDs to start
2023-03-07 21:54:55,872 __main__ INFO All LDMSDs are up.
2023-03-07 21:54:57,082 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-03-07 21:54:58,307 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-03-07 21:54:59,518 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:55:00,736 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:55:01,951 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-03-07 21:55:01,951 __main__ INFO --- done ---
2023-03-07 21:55:01,952 TADA INFO test updtr_status test ended
2023-03-07 21:55:14 INFO: ----------------------------------------------
2023-03-07 21:55:15 INFO: ======== ldmsd_flex_decomp_test ========
2023-03-07 21:55:15 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldmsd_flex_decomp_test
2023-03-07 21:55:16,405 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-03-07 21:55:16,405 TADA INFO   test-id: 9ff3ff7d845269f2fe7819bfcd5b34862abbf51c15e6da5f8eb64af9ebe9fdcd
2023-03-07 21:55:16,405 TADA INFO   test-suite: LDMSD
2023-03-07 21:55:16,405 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-03-07 21:55:16,405 TADA INFO   test-user: narate
2023-03-07 21:55:16,405 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:55:16,406 __main__ INFO -- Get or create the cluster --
2023-03-07 21:55:32,236 __main__ INFO -- Start daemons --
2023-03-07 21:55:42,719 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-03-07 21:56:31,838 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-03-07 21:56:31,838 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-03-07 21:56:31,838 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-03-07 21:56:31,838 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-03-07 21:56:31,838 TADA INFO assertion 5, record sos schema check: OK, passed
2023-03-07 21:56:31,838 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-03-07 21:56:31,839 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-03-07 21:56:31,839 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-03-07 21:56:31,839 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-03-07 21:56:31,839 TADA INFO assertion 10, record csv schema check: OK, passed
2023-03-07 21:56:31,839 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-03-07 21:56:31,840 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-03-07 21:56:31,840 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-03-07 21:56:31,840 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-03-07 21:56:31,840 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-03-07 21:56:31,842 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-03-07 21:56:31,907 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-03-07 21:56:31,910 TADA INFO assertion 18, fill sos data check: OK, passed
2023-03-07 21:56:31,912 TADA INFO assertion 19, filter sos data check: OK, passed
2023-03-07 21:56:31,919 TADA INFO assertion 20, record sos data check: OK, passed
2023-03-07 21:56:31,921 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-03-07 21:56:31,983 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-03-07 21:56:31,986 TADA INFO assertion 23, fill csv data check: OK, passed
2023-03-07 21:56:31,987 TADA INFO assertion 24, filter csv data check: OK, passed
2023-03-07 21:56:31,995 TADA INFO assertion 25, record csv data check: OK, passed
2023-03-07 21:56:31,996 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-03-07 21:56:32,019 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-03-07 21:56:32,021 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-03-07 21:56:32,022 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-03-07 21:56:32,025 TADA INFO assertion 30, record kafka data check: OK, passed
2023-03-07 21:56:32,026 TADA INFO test ldmsd_flex_decomp_test ended
2023-03-07 21:56:32,026 TADA INFO test ldmsd_flex_decomp_test ended
2023-03-07 21:56:47 INFO: ----------------------------------------------
2023-03-07 21:56:48 INFO: ======== ldms_set_info_test ========
2023-03-07 21:56:48 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/ldms_set_info_test
2023-03-07 21:56:58,471 TADA INFO starting test `ldms_set_info_test`
2023-03-07 21:56:58,472 TADA INFO   test-id: 16ee52a3d2d4abec8c15731d1ff094298596f11bfeef997073e7b0322dbe8c39
2023-03-07 21:56:58,472 TADA INFO   test-suite: LDMSD
2023-03-07 21:56:58,472 TADA INFO   test-name: ldms_set_info_test
2023-03-07 21:56:58,472 TADA INFO   test-user: narate
2023-03-07 21:56:58,472 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:56:58,473 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 3, Get a value : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 4, Unset a pair : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-03-07 21:56:58,473 TADA INFO assertion 7, Server resetting a key : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 8, Server unset a key : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 9, Server add a key : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 10, Adding a key : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-03-07 21:56:58,474 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-03-07 21:56:58,475 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-03-07 21:56:58,475 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-03-07 21:56:58,475 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-03-07 21:56:58,475 TADA INFO test ldms_set_info_test ended
2023-03-07 21:57:09 INFO: ----------------------------------------------
2023-03-07 21:57:10 INFO: ======== slurm_sampler2_test ========
2023-03-07 21:57:10 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/slurm_sampler2_test
2023-03-07 21:57:10,733 TADA INFO starting test `slurm_sampler2_test`
2023-03-07 21:57:10,733 TADA INFO   test-id: 03368e16a3b7e01dba5425e91d18961243923fb80bf32953463051b8bc021afb
2023-03-07 21:57:10,733 TADA INFO   test-suite: LDMSD
2023-03-07 21:57:10,733 TADA INFO   test-name: slurm_sampler2_test
2023-03-07 21:57:10,733 TADA INFO   test-user: narate
2023-03-07 21:57:10,734 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:57:10,734 __main__ INFO -- Get or create the cluster --
2023-03-07 21:57:24,651 __main__ INFO -- Add users --
2023-03-07 21:57:29,716 __main__ INFO -- Preparing job script & programs --
2023-03-07 21:57:30,465 __main__ INFO -- Start daemons --
2023-03-07 21:57:52,487 TADA INFO assertion 1, Processing the stream data from slurm_notifier: The metric values are as expected on all nodes., passed
2023-03-07 21:57:56,168 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: The metric values are as expected on all nodes., passed
2023-03-07 21:57:57,845 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: The metric values are as expected on all nodes., passed
2023-03-07 21:57:59,508 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: The metric values are as expected on all nodes., passed
2023-03-07 21:58:01,195 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: [node-1]: The job_list is not as expected. {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1} != {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 3, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1}, failed
Traceback (most recent call last):
  File "slurm_sampler2_test", line 880, in <module>
    test_all_step(job_1, node_jobs, DELETE_COMPLETE_JOBS)
  File "slurm_sampler2_test", line 691, in test_all_step
    "The metric values are as expected on all nodes." if passed else reason)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test the slurm_sampler2 plugin, [node-1]: The job_list is not as expected. {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1} != {'job_id': 1001, 'app_id': 0, 'user': '', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 3, 'job_size': 4, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 1}: FAILED
2023-03-07 21:58:01,196 TADA INFO assertion 3.1, Expanding the set heap -- job_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 4.1, Multi-tenant -- job_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 3.2, Expanding the set heap -- step_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 4.2, Multi-tenant -- step_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 3.3, Expanding the set heap -- task_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 4.3, Multi-tenant -- task_init: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: skipped
2023-03-07 21:58:01,197 TADA INFO assertion 4.4, Multi-tenant -- task_exit: skipped
2023-03-07 21:58:01,198 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: skipped
2023-03-07 21:58:01,198 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: skipped
2023-03-07 21:58:01,198 TADA INFO assertion 4.5, Multi-tenant -- job_exit: skipped
2023-03-07 21:58:01,198 TADA INFO test slurm_sampler2_test ended
2023-03-07 21:58:15 INFO: ----------------------------------------------
2023-03-07 21:58:16 INFO: ======== run_inside_cont_test.py ========
2023-03-07 21:58:16 INFO: CMD: python3 run_inside_cont_test.py --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-03-07-210739/data/run_inside_cont_test.py
2023-03-07 21:58:17,248 inside_cont_test INFO ===========================================================
2023-03-07 21:58:17,249 inside_cont_test INFO plugin_config_cmd: Start testing plugin_config_cmd
2023-03-07 21:58:17,250 TADA INFO starting test `plugin_config_cmd`
2023-03-07 21:58:17,250 TADA INFO   test-id: 404409890ff383d54b8c52ea14c0d779c5b5b54d31e8b096c4a6841f7ae7c5ab
2023-03-07 21:58:17,251 TADA INFO   test-suite: LDMSD
2023-03-07 21:58:17,251 TADA INFO   test-name: plugin_config_cmd
2023-03-07 21:58:17,251 TADA INFO   test-user: narate
2023-03-07 21:58:17,251 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 21:58:17,251 inside_cont_test INFO plugin_config_cmd: Preparing the containers
ready ...2023-03-07 22:55:05,868 inside_cont_test INFO plugin_config_cmd: Running the test script
2023-03-07 22:55:09,334 TADA INFO assertion status-1, Get the plugin statuses: status is as expected, passed
2023-03-07 22:55:09,334 TADA INFO assertion load-1, Load a non-existing plugin: resp['errcode'] (4294967295) != 0, passed
2023-03-07 22:55:09,334 TADA INFO assertion load-2, load a plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,335 TADA INFO assertion load-3, load a loadded plugin: resp['errcode'] (17) == 17, passed
2023-03-07 22:55:09,335 TADA INFO assertion config-1, Configure a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-03-07 22:55:09,335 TADA INFO assertion config-2, Misconfigure a loadded plugin: resp['errcode'] (22) != 0, passed
2023-03-07 22:55:09,335 TADA INFO assertion config-3, Correctly configure a loaded plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,335 TADA INFO assertion start-1, Start a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-03-07 22:55:09,335 TADA INFO assertion start-2, Start a store plugin: resp['errcode'] (22) == 22, passed
2023-03-07 22:55:09,335 TADA INFO assertion start-4, Start a sampler plugin using a negative interval: resp['errcode'] (22) == 22, passed
2023-03-07 22:55:09,336 TADA INFO assertion start-5, Start a sampler plugin without an offset: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,336 TADA INFO assertion start-3, Start a running sampler plugin: resp['errcode'] (16) == 16, passed
2023-03-07 22:55:09,336 TADA INFO assertion start-6, Start a sampler plugin with an offset larger than half of interval: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,336 TADA INFO assertion start-7, Start a sampler plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,336 TADA INFO assertion start-8, Check the status of the plugins: status is as expected, passed
2023-03-07 22:55:09,336 TADA INFO assertion stop-1, Stop a p lugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-03-07 22:55:09,337 TADA INFO assertion stop-2, Stop a sampler plugin that is not running: resp['errcode'] (22) != 0, passed
2023-03-07 22:55:09,337 TADA INFO assertion stop-3, Stop a running sampler plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,337 TADA INFO assertion stop-4, Check the status of the plugins: status is as expected, passed
2023-03-07 22:55:09,337 TADA INFO assertion term-1, Terminate a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-03-07 22:55:09,337 TADA INFO assertion term-2, Terminate a running sampler plugin: resp['errcode'] (22) == 22, passed
2023-03-07 22:55:09,337 TADA INFO assertion term-3, Terminate an in-used store plugin: resp['errcode'] (22) == 22, passed
2023-03-07 22:55:09,338 TADA INFO assertion term-4, Terminate a sampler plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,338 TADA INFO assertion term-5, Terminate a store plugin: resp['errcode'] (0) == 0, passed
2023-03-07 22:55:09,338 TADA INFO assertion term-6, Check the status of the plugins: status is as expected, passed
2023-03-07 22:55:09,338 TADA INFO test plugin_config_cmd ended
Remove containers? ...2023-03-07 22:55:14,393 inside_cont_test INFO plugin_config_cmd: done
2023-03-07 22:55:15,221 inside_cont_test INFO ===========================================================
2023-03-07 22:55:15,221 inside_cont_test INFO prdcr_config_cmd: Start testing prdcr_config_cmd
2023-03-07 22:55:15,223 TADA INFO starting test `prdcr_config_cmd`
2023-03-07 22:55:15,223 TADA INFO   test-id: b23d922fb977f9af0b718b5fc7c03855a66f6f0e567d236f34ea8fe81a37ee9c
2023-03-07 22:55:15,223 TADA INFO   test-suite: LDMSD
2023-03-07 22:55:15,223 TADA INFO   test-name: prdcr_config_cmd
2023-03-07 22:55:15,223 TADA INFO   test-user: narate
2023-03-07 22:55:15,223 TADA INFO   commit-id: 661e35a010a7de2ebce0e7918406804bd1fbd726
2023-03-07 22:55:15,223 inside_cont_test INFO prdcr_config_cmd: Preparing the containers
ready ...