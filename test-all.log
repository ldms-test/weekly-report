2023-07-29 10:48:31 INFO: WORK_DIR: /mnt/300G/data/2023-07-29-104830
2023-07-29 10:48:31 INFO: LOG: /mnt/300G/data/2023-07-29-104830/cygnus-weekly.log
2023-07-29 10:48:31 INFO: OVIS_NEW_GIT_SHA: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:48:31 INFO: OVIS_OLD_GIT_SHA: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:48:31 INFO: CONT_GIT_SHA: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:48:31 INFO: -----------------------------------------------
2023-07-29 10:48:31 INFO: LDMS_TEST_REPO: https://github.com/ovis-hpc/ldms-test
2023-07-29 10:48:31 INFO: LDMS_TEST_BRANCH: master
2023-07-29 10:48:31 INFO: LDMS_TEST_NEW_GIT_SHA: 06f8c14962a37e7ac0f80f6dda14766a5b246c8f
2023-07-29 10:48:31 INFO: LDMS_TEST_OLD_GIT_SHA: fb5e44882f90ae264f47566353d7fc4b1897caac
~/cron/ldms-test ~/cron/ldms-test
/mnt/300G/data/2023-07-29-104830 ~/cron/ldms-test ~/cron/ldms-test
2023-07-29 10:48:32 INFO: Skip building on host because GIT SHA has not changed: 
925affbefabc13830ec3385b8ea358a0296d2a42
OVIS_LDMS_OVIS_GIT_LONG "925affbefabc13830ec3385b8ea358a0296d2a42"
2023-07-29 10:48:32 INFO: Skip building containerized binary because GIT SHA has not changed: 
2023-07-29 10:48:32 INFO: -- Installation process succeeded --
2023-07-29 10:48:32 INFO: ---------------------------------------------------------------
~/cron/ldms-test /mnt/300G/data/2023-07-29-104830
~/cron/ldms-test/weekly-report ~/cron/ldms-test /mnt/300G/data/2023-07-29-104830
HEAD is now at 0cce202 2023-07-29-084228
[master 7261a19] 2023-07-29-104830
 2 files changed, 23 insertions(+), 15822 deletions(-)
 rewrite test-all.log (99%)
To github.com:ldms-test/weekly-report
   0cce202..7261a19  master -> master
~/cron/ldms-test /mnt/300G/data/2023-07-29-104830
2023-07-29 10:48:33 INFO: ==== OVIS+SOS Installation Completed ====
2023-07-29 10:48:33 INFO: ==== Start batch testing ====
~/cron/ldms-test /mnt/300G/data/2023-07-29-104830 ~/cron/ldms-test ~/cron/ldms-test
2023-07-29 10:48:33 INFO: ======== direct_ldms_ls_conn_test ========
2023-07-29 10:48:33 INFO: CMD: python3 direct_ldms_ls_conn_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/direct_ldms_ls_conn_test
2023-07-29 10:48:34,444 TADA INFO starting test `direct_ldms_ls_conn_test`
2023-07-29 10:48:34,444 TADA INFO   test-id: 82cc7f817de32df44bbc9824b35cbb77c3b4bace66f0f263609cb868625629da
2023-07-29 10:48:34,444 TADA INFO   test-suite: LDMSD
2023-07-29 10:48:34,444 TADA INFO   test-name: direct_ldms_ls_conn_test
2023-07-29 10:48:34,444 TADA INFO   test-user: narate
2023-07-29 10:48:34,444 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:48:34,891 __main__ INFO starting munged on cygnus-01-iw
2023-07-29 10:48:35,206 __main__ INFO starting munged on localhost
2023-07-29 10:48:35,435 __main__ INFO starting ldmsd on cygnus-01-iw
2023-07-29 10:48:35,729 TADA INFO assertion 0, Start ldmsd sampler and munged: OK, passed
2023-07-29 10:48:40,923 TADA INFO assertion 1, ldms_ls to the sampler: OK, passed
2023-07-29 10:48:40,923 __main__ INFO Stopping sampler daemon ...
2023-07-29 10:48:46,336 TADA INFO assertion 2, Kill the sampler: OK, passed
2023-07-29 10:48:46,382 TADA INFO assertion 3, ldms_ls to the dead sampler: got expected output, passed
2023-07-29 10:48:46,424 TADA INFO assertion 4, ldms_ls to a dead host: got expected output, passed
2023-07-29 10:48:46,425 TADA INFO test direct_ldms_ls_conn_test ended
2023-07-29 10:48:46,629 __main__ INFO stopping munged on cygnus-01-iw
2023-07-29 10:48:47,044 __main__ INFO stopping munged on localhost
2023-07-29 10:48:47 INFO: ----------------------------------------------
2023-07-29 10:48:47 INFO: ======== direct_prdcr_subscribe_test ========
2023-07-29 10:48:47 INFO: CMD: python3 direct_prdcr_subscribe_test --prefix /opt/ovis --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/direct_prdcr_subscribe_test
2023-07-29 10:48:47,885 TADA INFO starting test `direct_prdcr_subscribe_test`
2023-07-29 10:48:47,885 TADA INFO   test-id: 64280c74c2bffa76297874bcab33d0473756cea336133daca1bea49a55346d80
2023-07-29 10:48:47,885 TADA INFO   test-suite: LDMSD
2023-07-29 10:48:47,885 TADA INFO   test-name: direct_prdcr_subscribe_test
2023-07-29 10:48:47,885 TADA INFO   test-user: narate
2023-07-29 10:48:47,886 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:48:49,792 __main__ INFO starting munged on cygnus-01-iw
2023-07-29 10:48:50,589 __main__ INFO starting munged on cygnus-05-iw
2023-07-29 10:48:51,361 __main__ INFO starting munged on cygnus-03-iw
2023-07-29 10:48:52,168 __main__ INFO starting munged on cygnus-04-iw
2023-07-29 10:48:52,484 __main__ INFO starting munged on localhost
2023-07-29 10:48:52,723 __main__ INFO starting ldmsd on cygnus-01-iw
2023-07-29 10:48:53,253 __main__ INFO starting ldmsd on cygnus-05-iw
2023-07-29 10:48:53,793 __main__ INFO starting ldmsd on cygnus-03-iw
2023-07-29 10:48:54,287 __main__ INFO starting ldmsd on cygnus-04-iw
2023-07-29 10:49:01,259 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-07-29 10:49:01,260 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-07-29 10:49:01,261 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-07-29 10:49:01,261 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-07-29 10:49:01,262 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verified, passed
2023-07-29 10:49:01,311 TADA INFO assertion 5, Stopping the producers succeeds: agg-1 producers stopped, passed
2023-07-29 10:49:02,313 TADA INFO assertion 6, Restarting the producers succeeds: agg-1 producers started, passed
2023-07-29 10:49:08,999 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-07-29 10:49:09,000 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-07-29 10:49:09,001 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-07-29 10:49:09,001 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-07-29 10:49:09,002 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verified, passed
2023-07-29 10:49:09,003 __main__ INFO stopping sampler-1
2023-07-29 10:49:10,463 TADA INFO assertion 12, stream-sampler-1 is not running: sampler-1 stopped, passed
2023-07-29 10:49:10,463 __main__ INFO starting sampler-1
2023-07-29 10:49:11,718 TADA INFO assertion 13, stream-sampler-1 has restarted: sampler-1 running, passed
2023-07-29 10:49:11,718 __main__ INFO allow some time for prdcr to reconnect ...
2023-07-29 10:49:17,646 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-07-29 10:49:17,647 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-07-29 10:49:17,648 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verified, passed
2023-07-29 10:49:17,649 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: unsubscribed, passed
2023-07-29 10:49:19,982 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-07-29 10:49:19,983 TADA INFO test direct_prdcr_subscribe_test ended
2023-07-29 10:49:20,197 __main__ INFO stopping munged on cygnus-01-iw
2023-07-29 10:49:20,612 __main__ INFO stopping ldmsd on cygnus-01-iw
2023-07-29 10:49:21,062 __main__ INFO stopping munged on cygnus-05-iw
2023-07-29 10:49:21,542 __main__ INFO stopping ldmsd on cygnus-05-iw
2023-07-29 10:49:21,998 __main__ INFO stopping munged on cygnus-03-iw
2023-07-29 10:49:22,429 __main__ INFO stopping ldmsd on cygnus-03-iw
2023-07-29 10:49:22,857 __main__ INFO stopping munged on cygnus-04-iw
2023-07-29 10:49:23,281 __main__ INFO stopping ldmsd on cygnus-04-iw
2023-07-29 10:49:23,497 __main__ INFO stopping munged on localhost
2023-07-29 10:49:23 INFO: ----------------------------------------------
2023-07-29 10:49:23 INFO: ======== agg_slurm_test ========
2023-07-29 10:49:23 INFO: CMD: python3 agg_slurm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/agg_slurm_test
2023-07-29 10:49:24,399 TADA INFO starting test `agg_slurm_test`
2023-07-29 10:49:24,400 TADA INFO   test-id: 31d7270936193f6a19b0e4fe4e223bfaf1d3e11bfc89770d38b91294cc8cf277
2023-07-29 10:49:24,400 TADA INFO   test-suite: LDMSD
2023-07-29 10:49:24,400 TADA INFO   test-name: agg_slurm_test
2023-07-29 10:49:24,400 TADA INFO   test-user: narate
2023-07-29 10:49:24,400 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:49:24,401 __main__ INFO -- Get or create the cluster --
2023-07-29 10:49:37,943 __main__ INFO -- Preparing syspapi JSON file --
2023-07-29 10:49:38,051 __main__ INFO -- Preparing jobpapi JSON file --
2023-07-29 10:49:38,153 __main__ INFO -- Preparing job script & programs --
2023-07-29 10:49:39,501 __main__ INFO -- Start daemons --
2023-07-29 10:50:11,129 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 10:50:16,132 __main__ INFO -- ldms_ls to agg-2 --
2023-07-29 10:50:16,247 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-07-29 10:50:16,358 __main__ INFO -- Give syspapi some time to work before submitting job --
2023-07-29 10:50:21,364 __main__ INFO -- Submitting jobs --
2023-07-29 10:50:21,497 __main__ INFO job_one: 1
2023-07-29 10:50:21,609 __main__ INFO job_two: 2
2023-07-29 10:50:31,619 __main__ INFO -- Cancelling jobs --
2023-07-29 10:50:31,619 __main__ INFO job_one: 1
2023-07-29 10:50:31,735 __main__ INFO job_two: 2
2023-07-29 10:51:43,662 TADA INFO assertion 2, slurm data verification: get expected data from store, passed
2023-07-29 10:51:43,663 TADA INFO assertion 3, meminfo data verification: No data missing, passed
2023-07-29 10:51:43,663 TADA INFO assertion 4, (SYS/JOB) PAPI data verification: No data missing, passed
2023-07-29 10:51:43,664 TADA INFO test agg_slurm_test ended
2023-07-29 10:51:57 INFO: ----------------------------------------------
2023-07-29 10:51:58 INFO: ======== papi_sampler_test ========
2023-07-29 10:51:58 INFO: CMD: python3 papi_sampler_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/papi_sampler_test
2023-07-29 10:51:59,373 TADA INFO starting test `papi_sampler_test`
2023-07-29 10:51:59,373 TADA INFO   test-id: 6d07c1c234e3c336ce4734279e8b0e842e426a4615f411bb61084368840a71df
2023-07-29 10:51:59,373 TADA INFO   test-suite: LDMSD
2023-07-29 10:51:59,373 TADA INFO   test-name: papi_sampler_test
2023-07-29 10:51:59,373 TADA INFO   test-user: narate
2023-07-29 10:51:59,373 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:51:59,374 __main__ INFO -- Get or create the cluster --
2023-07-29 10:52:04,367 __main__ INFO -- Start daemons --
2023-07-29 10:52:18,070 TADA INFO assertion 0, ldmsd has started: verified, passed
2023-07-29 10:52:18,281 TADA INFO assertion 1.1, Non-papi job is submitted: jobid(1) > 0, passed
2023-07-29 10:52:23,400 TADA INFO assertion 1.2, Non-papi job is running before ldms_ls: STATE = RUNNING, passed
2023-07-29 10:52:23,578 TADA INFO assertion 1.3, Non-papi job is running after ldms_ls: STATE = RUNNING, passed
2023-07-29 10:52:23,578 TADA INFO assertion 1, Non-papi job does not create set: verified, passed
2023-07-29 10:52:37,389 TADA INFO assertion 2, papi job creates set: PAPI set created, passed
2023-07-29 10:52:37,390 TADA INFO assertion 2.2, Schema name is set accordingly: schema name == papi0, passed
2023-07-29 10:52:37,390 TADA INFO assertion 2.1, Events in papi job set created according to config file: {'PAPI_TOT_INS'} == {'PAPI_TOT_INS'}, passed
2023-07-29 10:52:37,390 TADA INFO assertion 2.3, PAPI set has correct job_id: 2 == 2, passed
2023-07-29 10:52:37,618 TADA INFO assertion 2.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-07-29 10:52:43,458 TADA INFO assertion 3, papi job creates set: PAPI set created, passed
2023-07-29 10:52:43,458 TADA INFO assertion 3.2, Schema name is set accordingly: schema name == papi1, passed
2023-07-29 10:52:43,458 TADA INFO assertion 3.1, Events in papi job set created according to config file: {'PAPI_TOT_INS', 'PAPI_BR_MSP'} == {'PAPI_TOT_INS', 'PAPI_BR_MSP'}, passed
2023-07-29 10:52:43,458 TADA INFO assertion 3.3, PAPI set has correct job_id: 3 == 3, passed
2023-07-29 10:52:43,656 TADA INFO assertion 3.4, PAPI set has correct task_pids: jobid/ranks/pids verified, passed
2023-07-29 10:52:43,656 TADA INFO assertion 4, Multiple, concurrent jobs results in concurrent, multiple sets: LDMS sets ({'node-1/papi1/3.0', 'node-1/meminfo', 'node-1/papi0/2.0'}), passed
2023-07-29 10:52:54,142 TADA INFO assertion 6, PAPI set persists within `job_expiry` after job exited: verified, passed
2023-07-29 10:53:34,453 TADA INFO assertion 7, PAPI set is deleted after `2.2 x job_expiry` since job exited: node-1/meminfo deleted, passed
2023-07-29 10:53:36,792 TADA INFO assertion 8, Missing config file attribute is logged: : sampler.papi_sampler: papi_sampler[515]: papi_config object must contain either the 'file' or 'config' attribute., passed
2023-07-29 10:53:42,215 TADA INFO assertion 9, Bad config file is logged: : sampler.papi_sampler: configuration file syntax error., passed
2023-07-29 10:53:42,215 __main__ INFO -- Finishing Test --
2023-07-29 10:53:42,215 TADA INFO test papi_sampler_test ended
2023-07-29 10:53:42,215 __main__ INFO -- Cleaning up files --
2023-07-29 10:53:42,216 __main__ INFO -- Removing the virtual cluster --
2023-07-29 10:53:53 INFO: ----------------------------------------------
2023-07-29 10:53:54 INFO: ======== papi_store_test ========
2023-07-29 10:53:54 INFO: CMD: python3 papi_store_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/papi_store_test
2023-07-29 10:53:55,188 TADA INFO starting test `papi_store_test`
2023-07-29 10:53:55,189 TADA INFO   test-id: c194e84cbbf5183bca7f0d12fe94c4b387a82eb6054d82625f792a07dc581da2
2023-07-29 10:53:55,189 TADA INFO   test-suite: LDMSD
2023-07-29 10:53:55,189 TADA INFO   test-name: papi_store_test
2023-07-29 10:53:55,189 TADA INFO   test-user: narate
2023-07-29 10:53:55,189 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:53:55,190 __main__ INFO -- Get or create the cluster --
2023-07-29 10:54:02,617 __main__ INFO -- Start daemons --
2023-07-29 10:54:45,623 TADA INFO assertion 1, Every job in the input data is represented in the output: {1, 2, 3, 4} = {1, 2, 3, 4}, passed
2023-07-29 10:54:45,623 TADA INFO assertion 2, Every event in every job results in a separate row in the output: verified, passed
2023-07-29 10:54:45,623 TADA INFO assertion 3, The schema name in the output matches the event name: verified, passed
2023-07-29 10:54:45,624 TADA INFO assertion 4, Each rank in the job results in a row per event in the output: verified, passed
2023-07-29 10:54:45,624 TADA INFO test papi_store_test ended
2023-07-29 10:54:57 INFO: ----------------------------------------------
2023-07-29 10:54:58 INFO: ======== store_app_test ========
2023-07-29 10:54:58 INFO: CMD: python3 store_app_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/store_app_test
2023-07-29 10:54:59,508 TADA INFO starting test `store_app_test`
2023-07-29 10:54:59,509 TADA INFO   test-id: d65095e95251e4753bb97a3827ae254684c4cf1b1295c9ccb6e20ecd37070d6c
2023-07-29 10:54:59,509 TADA INFO   test-suite: LDMSD
2023-07-29 10:54:59,509 TADA INFO   test-name: store_app_test
2023-07-29 10:54:59,509 TADA INFO   test-user: narate
2023-07-29 10:54:59,510 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:54:59,511 __main__ INFO -- Get or create the cluster --
2023-07-29 10:55:13,413 __main__ INFO -- Preparing job script & programs --
2023-07-29 10:55:13,819 __main__ INFO -- Start daemons --
2023-07-29 10:55:46,808 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 10:55:51,815 __main__ INFO -- Submitting jobs --
2023-07-29 10:55:52,050 __main__ INFO job_one: 1
2023-07-29 10:55:57,304 __main__ INFO job_two: 2
2023-07-29 10:56:06,624 __main__ INFO Verifying data ...
2023-07-29 10:58:10,008 TADA INFO assertion 1, Verify data: sos data is not empty and sos data < ldms_ls data, passed
2023-07-29 10:58:10,009 TADA INFO test store_app_test ended
2023-07-29 10:58:23 INFO: ----------------------------------------------
2023-07-29 10:58:24 INFO: ======== syspapi_test ========
2023-07-29 10:58:24 INFO: CMD: python3 syspapi_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/syspapi_test
2023-07-29 10:58:25,285 TADA INFO starting test `syspapi_test`
2023-07-29 10:58:25,285 TADA INFO   test-id: a5e34304bcf7d4a138b7930ebea98419430303e5a6c4e7e493009443f3504af3
2023-07-29 10:58:25,285 TADA INFO   test-suite: LDMSD
2023-07-29 10:58:25,285 TADA INFO   test-name: syspapi_test
2023-07-29 10:58:25,285 TADA INFO   test-user: narate
2023-07-29 10:58:25,286 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:58:25,286 __main__ INFO -- Get or create the cluster --
2023-07-29 10:58:36,584 __main__ INFO -- Write syspapi JSON config files --
2023-07-29 10:58:36,584 __main__ INFO    - db/syspapi-1.json
2023-07-29 10:58:36,585 __main__ INFO    - db/syspapi-bad.json
2023-07-29 10:58:36,587 __main__ INFO -- Start daemons --
2023-07-29 10:58:56,758 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 10:59:01,764 __main__ INFO -- Verifying --
2023-07-29 10:59:01,887 TADA INFO assertion 1, verify set creation by cfg_file: set existed (with correct instance name), passed
2023-07-29 10:59:01,887 TADA INFO assertion 2, verify schema name by cfg_file: verify schema name, passed
2023-07-29 10:59:01,996 TADA INFO assertion 3, verify metrics (events) by cfg_file: verify events (metrics), passed
2023-07-29 10:59:04,108 TADA INFO assertion 4, verify increment counters: verify increment of supported counters, passed
2023-07-29 10:59:04,229 TADA INFO assertion 5, verify cfg_file syntax error report: verify JSON parse error, passed
2023-07-29 10:59:04,331 TADA INFO assertion 6, verify cfg_file unsupported events report: verify unsupported event report, passed
2023-07-29 10:59:25,542 TADA INFO assertion 7, verify cfg_file for many events: each event has either 'sucees' or 'failed' report, passed
2023-07-29 10:59:25,542 __main__ INFO  events succeeded: 77
2023-07-29 10:59:25,542 __main__ INFO  events failed: 114
2023-07-29 10:59:25,542 TADA INFO test syspapi_test ended
2023-07-29 10:59:38 INFO: ----------------------------------------------
2023-07-29 10:59:39 INFO: ======== agg_test ========
2023-07-29 10:59:39 INFO: CMD: python3 agg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/agg_test
2023-07-29 10:59:40,472 TADA INFO starting test `agg_test`
2023-07-29 10:59:40,472 TADA INFO   test-id: 33cddfa6486d93da67b7c3d9dafee55741a019d92b59741ec45d004dfa113628
2023-07-29 10:59:40,472 TADA INFO   test-suite: LDMSD
2023-07-29 10:59:40,472 TADA INFO   test-name: agg_test
2023-07-29 10:59:40,472 TADA INFO   test-user: narate
2023-07-29 10:59:40,472 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 10:59:40,473 __main__ INFO -- Get or create the cluster --
2023-07-29 10:59:57,741 __main__ INFO -- Start daemons --
2023-07-29 11:00:34,285 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:00:39,290 __main__ INFO -- ldms_ls to agg-2 --
2023-07-29 11:00:39,415 TADA INFO assertion 1, ldms_ls agg-2: dir result verified, passed
2023-07-29 11:00:40,172 TADA INFO assertion 2, meminfo data verification: data verified, passed
2023-07-29 11:00:40,173 __main__ INFO -- Terminating ldmsd on node-1 --
2023-07-29 11:00:42,521 TADA INFO assertion 3, node-1 ldmsd terminated, sets removed from agg-11: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-29 11:00:42,742 TADA INFO assertion 4, node-1 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-07-29 11:00:42,742 __main__ INFO -- Resurrecting ldmsd on node-1 --
2023-07-29 11:00:52,301 TADA INFO assertion 5, node-1 ldmsd revived, sets added to agg-11: list({'node-1/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-3/meminfo'}), passed
2023-07-29 11:00:52,419 TADA INFO assertion 6, node-1 ldmsd revived, sets added to agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-07-29 11:00:52,419 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-07-29 11:00:54,758 TADA INFO assertion 7, agg-11 ldmsd terminated, sets removed from agg-2: list({'node-2/meminfo', 'node-4/meminfo'}) == expect({'node-2/meminfo', 'node-4/meminfo'}), passed
2023-07-29 11:00:54,872 TADA INFO assertion 8, agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-07-29 11:00:54,983 TADA INFO assertion 9, agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-29 11:00:54,983 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-07-29 11:01:04,565 TADA INFO assertion 10, agg-11 ldmsd revived, sets added to agg-2: list({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}) == expect({'node-1/meminfo', 'node-2/meminfo', 'node-4/meminfo', 'node-3/meminfo'}), passed
2023-07-29 11:01:04,565 TADA INFO test agg_test ended
2023-07-29 11:01:20 INFO: ----------------------------------------------
2023-07-29 11:01:20 INFO: ======== failover_test ========
2023-07-29 11:01:20 INFO: CMD: python3 failover_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/failover_test
2023-07-29 11:01:21,579 TADA INFO starting test `failover_test`
2023-07-29 11:01:21,579 TADA INFO   test-id: 2f5db80a71b2ec07071a47609ee07f4980bac9a519849e618907bbcedc47deed
2023-07-29 11:01:21,579 TADA INFO   test-suite: LDMSD
2023-07-29 11:01:21,579 TADA INFO   test-name: failover_test
2023-07-29 11:01:21,579 TADA INFO   test-user: narate
2023-07-29 11:01:21,579 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:01:21,580 __main__ INFO -- Get or create the cluster --
2023-07-29 11:01:39,333 __main__ INFO -- Start daemons --
2023-07-29 11:02:15,948 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:02:30,963 __main__ INFO -- ldms_ls to agg-2 --
2023-07-29 11:02:31,072 TADA INFO assertion 1, 
ldms_ls agg-2: dir result verified, passed
2023-07-29 11:02:31,863 TADA INFO assertion 2, 
meminfo data verification: data verified, passed
2023-07-29 11:02:31,864 __main__ INFO -- Terminating ldmsd on agg-11 --
2023-07-29 11:02:37,213 TADA INFO assertion 3, 
agg-11 ldmsd terminated, sets added to agg-12: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:02:37,325 TADA INFO assertion 4, 
agg-11 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:02:37,432 TADA INFO assertion 5, 
agg-11 ldmsd terminated, node-1 ldmsd is still running: list({'node-1/meminfo'}) == expect({'node-1/meminfo'}), passed
2023-07-29 11:02:37,558 TADA INFO assertion 6, 
agg-11 ldmsd terminated, node-3 ldmsd is still running: list({'node-3/meminfo'}) == expect({'node-3/meminfo'}), passed
2023-07-29 11:02:37,558 __main__ INFO -- Resurrecting ldmsd on agg-11 --
2023-07-29 11:03:02,138 TADA INFO assertion 7, 
agg-11 ldmsd revived, sets removed from agg-12: list({'node-4/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:03:02,260 TADA INFO assertion 8, 
agg-11 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:03:02,260 __main__ INFO -- Terminating ldmsd on agg-12 --
2023-07-29 11:03:07,598 TADA INFO assertion 9, 
agg-12 ldmsd terminated, sets added to agg-11: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:03:07,708 TADA INFO assertion 10, 
agg-12 ldmsd terminated, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:03:07,821 TADA INFO assertion 11, 
agg-12 ldmsd terminated, node-2 ldmsd is still running: list({'node-2/meminfo'}) == expect({'node-2/meminfo'}), passed
2023-07-29 11:03:07,931 TADA INFO assertion 12, 
agg-12 ldmsd terminated, node-4 ldmsd is still running: list({'node-4/meminfo'}) == expect({'node-4/meminfo'}), passed
2023-07-29 11:03:07,931 __main__ INFO -- Resurrecting ldmsd on agg-12 --
2023-07-29 11:03:32,496 TADA INFO assertion 13, 
agg-12 ldmsd revived, sets removed from agg-11: list({'node-3/meminfo', 'node-1/meminfo'}) == expect({'node-3/meminfo', 'node-1/meminfo'}), passed
2023-07-29 11:03:32,614 TADA INFO assertion 14, 
agg-12 ldmsd revived, all sets running on agg-2: list({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}) == expect({'node-4/meminfo', 'node-3/meminfo', 'node-1/meminfo', 'node-2/meminfo'}), passed
2023-07-29 11:03:32,614 TADA INFO test failover_test ended
2023-07-29 11:03:48 INFO: ----------------------------------------------
2023-07-29 11:03:49 INFO: ======== ldmsd_auth_ovis_test ========
2023-07-29 11:03:49 INFO: CMD: python3 ldmsd_auth_ovis_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_auth_ovis_test
2023-07-29 11:03:50,021 TADA INFO starting test `ldmsd_auth_ovis_test`
2023-07-29 11:03:50,021 TADA INFO   test-id: efa476e1ac2d96667eae8ba62dc073a32e0369f6372c63abb37df8017f3696da
2023-07-29 11:03:50,021 TADA INFO   test-suite: LDMSD
2023-07-29 11:03:50,021 TADA INFO   test-name: ldmsd_auth_ovis_test
2023-07-29 11:03:50,021 TADA INFO   test-user: narate
2023-07-29 11:03:50,021 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:03:50,022 __main__ INFO -- Get or create the cluster --
2023-07-29 11:03:55,054 __main__ INFO -- Start daemons --
2023-07-29 11:04:00,920 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:04:06,045 TADA INFO assertion 1, ldms_ls with auth none: verified, passed
2023-07-29 11:04:06,160 TADA INFO assertion 2, ldms_ls with wrong secret: verified, passed
2023-07-29 11:04:06,274 TADA INFO assertion 3, ldms_ls 'dir' with right secret: verified, passed
2023-07-29 11:04:06,560 TADA INFO assertion 4, ldms_ls 'read' with right secret: verified, passed
2023-07-29 11:04:06,560 TADA INFO test ldmsd_auth_ovis_test ended
2023-07-29 11:04:18 INFO: ----------------------------------------------
2023-07-29 11:04:19 INFO: ======== ldmsd_auth_test ========
2023-07-29 11:04:19 INFO: CMD: python3 ldmsd_auth_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_auth_test
2023-07-29 11:04:19,929 TADA INFO starting test `ldmsd_auth_test`
2023-07-29 11:04:19,930 TADA INFO   test-id: 0f7248fe558a29d8edec348c020d41471c2e7c6ae44e59d78b3bfe76c6bdd86c
2023-07-29 11:04:19,930 TADA INFO   test-suite: LDMSD
2023-07-29 11:04:19,930 TADA INFO   test-name: ldmsd_auth_test
2023-07-29 11:04:19,930 TADA INFO   test-user: narate
2023-07-29 11:04:19,930 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:04:19,931 __main__ INFO -- Get or create the cluster --
2023-07-29 11:04:37,473 __main__ INFO -- Start daemons --
2023-07-29 11:05:23,561 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:05:28,707 TADA INFO assertion 1, root@agg-2(dom3) ldms_ls to agg-2:10000: see all sets, passed
2023-07-29 11:05:28,814 TADA INFO assertion 2, user@agg-2(dom3) ldms_ls to agg-2:10000: see only meminfo, passed
2023-07-29 11:05:28,928 TADA INFO assertion 3, root@headnode(dom4) ldms_ls to agg-2:10001: see all sets, passed
2023-07-29 11:05:29,035 TADA INFO assertion 4, user@headnode(dom4) ldms_ls to agg-2:10001: see only meminfo, passed
2023-07-29 11:05:29,142 TADA INFO assertion 5, root@headnode(dom4) ldms_ls to agg-11:10000: connection rejected, passed
2023-07-29 11:05:29,142 TADA INFO test ldmsd_auth_test ended
2023-07-29 11:05:44 INFO: ----------------------------------------------
2023-07-29 11:05:45 INFO: ======== ldmsd_ctrl_test ========
2023-07-29 11:05:45 INFO: CMD: python3 ldmsd_ctrl_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_ctrl_test
2023-07-29 11:05:46,329 TADA INFO starting test `ldmsd_ctrl_test`
2023-07-29 11:05:46,329 TADA INFO   test-id: 6cf69b1afab4abad3217edee5c0cd8f925f72c187c6eb0326a275a1302273f16
2023-07-29 11:05:46,330 TADA INFO   test-suite: LDMSD
2023-07-29 11:05:46,330 TADA INFO   test-name: ldmsd_ctrl_test
2023-07-29 11:05:46,330 TADA INFO   test-user: narate
2023-07-29 11:05:46,330 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:05:46,330 __main__ INFO -- Get or create the cluster --
2023-07-29 11:05:55,461 __main__ INFO -- Start daemons --
2023-07-29 11:06:11,556 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:06:17,678 TADA INFO assertion 1, ldmsd_controller interactive session: connected, passed
2023-07-29 11:06:18,793 TADA INFO assertion 2, ldmsctl interactive session: connected, passed
2023-07-29 11:06:19,395 TADA INFO assertion 3, ldmsd_controller start bogus producer: expected output verified, passed
2023-07-29 11:06:19,996 TADA INFO assertion 4, ldmsctl start bogus producer: expected output verified, passed
2023-07-29 11:06:20,598 TADA INFO assertion 5, ldmsd_controller bogus command: expected output verified, passed
2023-07-29 11:06:21,199 TADA INFO assertion 6, ldmsctl bogus command: expected output verified, passed
2023-07-29 11:06:21,801 TADA INFO assertion 7, ldmsd_controller load bogus plugin: expected output verified, passed
2023-07-29 11:06:22,402 TADA INFO assertion 8, ldmsctl load bogus plugin: expected output verified, passed
2023-07-29 11:06:39,603 TADA INFO assertion 9, ldmsd_controller prdcr/updtr: verified, passed
2023-07-29 11:06:56,799 TADA INFO assertion 10, ldmsctl prdcr/updtr: verified, passed
2023-07-29 11:06:56,799 TADA INFO test ldmsd_ctrl_test ended
2023-07-29 11:07:09 INFO: ----------------------------------------------
2023-07-29 11:07:10 INFO: ======== ldmsd_stream_test2 ========
2023-07-29 11:07:10 INFO: CMD: python3 ldmsd_stream_test2 --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_stream_test2
2023-07-29 11:07:11,073 TADA INFO starting test `ldmsd_stream_test`
2023-07-29 11:07:11,073 TADA INFO   test-id: cb6413762791802faf58db551edf2c81d6f888972694ecc19766722a5fefc991
2023-07-29 11:07:11,073 TADA INFO   test-suite: LDMSD
2023-07-29 11:07:11,073 TADA INFO   test-name: ldmsd_stream_test
2023-07-29 11:07:11,074 TADA INFO   test-user: narate
2023-07-29 11:07:11,074 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:07:11,074 __main__ INFO -- Get or create the cluster --
2023-07-29 11:07:20,324 __main__ INFO -- Start daemons --
2023-07-29 11:07:37,670 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:07:39,673 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-925affb-new 
2023-07-29 11:07:42,691 root INFO starting /tada-src/python/pypubsub.py on narate-ldmsd_stream_test2-925affb-agg-2 
2023-07-29 11:07:53,087 TADA INFO assertion 1, Check data from old ldmsd_stream at agg-1: , passed
2023-07-29 11:07:53,087 TADA INFO assertion 2, Check data from old ldmsd_stream at agg-2: , passed
2023-07-29 11:07:53,088 TADA INFO assertion 3, Check data from old ldmsd_stream at the last subscriber: , passed
2023-07-29 11:07:53,088 TADA INFO assertion 4, Check data from the matching new ldms stream at agg-1: , passed
2023-07-29 11:07:53,088 TADA INFO assertion 5, Check data from the matching new ldms stream at agg-2: , passed
2023-07-29 11:07:53,089 TADA INFO assertion 6, Check data from the matching new ldms stream at the last subscriber: , passed
2023-07-29 11:07:53,089 TADA INFO assertion 7, Check data from the non-matching new ldms stream at agg-1: , passed
2023-07-29 11:07:53,089 TADA INFO assertion 8, Check data from the non-matching new ldms stream at agg-2: , passed
2023-07-29 11:07:53,090 TADA INFO assertion 9, Check data from the non-matching new ldms stream at last subscriber: , passed
2023-07-29 11:07:53,629 TADA INFO assertion 10, Check stream_stats before stream data transfer: , passed
2023-07-29 11:07:53,629 TADA INFO assertion 11, Check stream_client_stats before stream data transfer: , passed
2023-07-29 11:07:53,629 TADA INFO assertion 12, Check stream_stats after stream data transfer: , passed
2023-07-29 11:07:53,629 TADA INFO assertion 13, Check stream_client_stats after stream data transfer: , passed
2023-07-29 11:07:53,630 TADA INFO test ldmsd_stream_test ended
2023-07-29 11:08:06 INFO: ----------------------------------------------
2023-07-29 11:08:07 INFO: ======== maestro_cfg_test ========
2023-07-29 11:08:07 INFO: CMD: python3 maestro_cfg_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/maestro_cfg_test
2023-07-29 11:08:07,939 TADA INFO starting test `maestro_cfg_test`
2023-07-29 11:08:07,940 TADA INFO   test-id: c64a426181617a0e335eeed1a4767b3921a0d082eb26ba0ff6e185752c3bad44
2023-07-29 11:08:07,940 TADA INFO   test-suite: LDMSD
2023-07-29 11:08:07,940 TADA INFO   test-name: maestro_cfg_test
2023-07-29 11:08:07,940 TADA INFO   test-user: narate
2023-07-29 11:08:07,940 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:08:17,952 __main__ INFO -- Get or create cluster --
2023-07-29 11:08:43,379 __main__ INFO -- Start daemons --
2023-07-29 11:09:45,116 __main__ INFO ... make sure ldmsd's are up
2023-07-29 11:09:52,626 TADA INFO assertion 1, load maestro etcd cluster: etcd cluster loaded successfully, passed
2023-07-29 11:10:32,672 TADA INFO assertion 2, config ldmsd cluster with maestro: Maestro ldmsd configuration successful, passed
2023-07-29 11:10:34,301 TADA INFO assertion 3, verify sampler daemons: OK, passed
2023-07-29 11:10:34,907 TADA INFO assertion 4, verify L1 aggregator daemons: OK, passed
2023-07-29 11:10:35,153 TADA INFO assertion 5, verify L2 aggregator daemon: OK, passed
2023-07-29 11:10:35,460 TADA INFO assertion 6, verify data storage: OK, passed
---Wait for config to write to file---
2023-07-29 11:10:35,461 TADA INFO test maestro_cfg_test ended
2023-07-29 11:10:53 INFO: ----------------------------------------------
2023-07-29 11:10:54 INFO: ======== mt-slurm-test ========
2023-07-29 11:10:54 INFO: CMD: python3 mt-slurm-test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/mt-slurm-test
-- Get or create the cluster --
-- Start daemons --
... wait a bit to make sure ldmsd's are up
Every job in input data represented in output: : Passed
['# task_rank,timestamp', '0,1690647111.990375', '1,1690647111.990375', '2,1690647111.990375', '3,1690647111.990375', '4,1690647111.990375', '5,1690647111.990375', '6,1690647111.990375', '7,1690647111.990375', '8,1690647112.910133', '9,1690647113.924524', '10,1690647113.924524', '11,1690647113.924524', '12,1690647113.924524', '13,1690647113.924524', '14,1690647114.944830', '15,1690647114.944830', '16,1690647114.944830', '17,1690647114.944830', '18,1690647115.927397', '19,1690647115.927397', '20,1690647115.927397', '21,1690647116.917364', '22,1690647116.917364', '23,1690647116.917364', '24,1690647116.917364', '25,1690647116.917364', '26,1690647116.917364', '# Records 27/27.', '']
Job 10000 has 27 rank: : Passed
Job 10100 has 64 rank: : Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_size in metric set matches database: job_size input match 27: Passed
27
27
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10000 job_id in metric set matches database: job_id match: Passed
For Job 10000 task_rank in metric set matches database: task_rank match: Passed
For Job 10000 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_size in metric set matches database: job_size input match 64: Passed
64
64
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
For Job 10100 job_id in metric set matches database: job_id match: Passed
For Job 10100 task_rank in metric set matches database: task_rank match: Passed
For Job 10100 job_pid in metric set matches database: task_pid match: Passed
Job 10000 has 3 nodes: node count 3 correct: Passed
Job 10100 has 4 nodes: node count 4 correct: Passed
2023-07-29 11:12:31 INFO: ----------------------------------------------
2023-07-29 11:12:32 INFO: ======== ovis_ev_test ========
2023-07-29 11:12:32 INFO: CMD: python3 ovis_ev_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ovis_ev_test
2023-07-29 11:12:32,722 __main__ INFO -- Create the cluster -- 
2023-07-29 11:12:42,003 TADA INFO starting test `ovis_ev_test`
2023-07-29 11:12:42,003 TADA INFO   test-id: fabd9736cb9ac0a25aa199ae09bddd29d852a9939233b751d4525d1bef525c38
2023-07-29 11:12:42,003 TADA INFO   test-suite: test_ovis_ev
2023-07-29 11:12:42,003 TADA INFO   test-name: ovis_ev_test
2023-07-29 11:12:42,003 TADA INFO   test-user: narate
2023-07-29 11:12:42,003 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:12:42,004 TADA INFO assertion 1, Test posting an event without timeout: ovis_ev delivered the expected event., passed
2023-07-29 11:12:42,004 TADA INFO assertion 2, Test posting an event with a current timeout: ovis_ev delivered the expected event., passed
2023-07-29 11:12:42,004 TADA INFO assertion 3, Test posting an event with a future timeout: ovis_ev delivered the expected event., passed
2023-07-29 11:12:42,005 TADA INFO assertion 4, Test reposting a posted event: ev_post returned EBUSY when posted an already posted event, passed
2023-07-29 11:12:42,005 TADA INFO assertion 5, Test canceling a posted event: ovis_ev delivered the expected event., passed
2023-07-29 11:12:42,005 TADA INFO assertion 6, Test rescheduling a posted event: ovis_ev delivered the expected event., passed
2023-07-29 11:12:42,005 TADA INFO assertion 7, Test event deliver order: The event delivery order was correct., passed
2023-07-29 11:12:42,005 TADA INFO assertion 8, Test flushing events: Expected status (1) == delivered status (1), passed
2023-07-29 11:12:42,005 TADA INFO assertion 9, Test posting event on a flushed worker: Expected status (0) == delivered status (0), passed
2023-07-29 11:12:42,005 TADA INFO assertion 10, Test the case that multiple threads post the same event: ev_post returned the expected return code., passed
2023-07-29 11:12:42,006 TADA INFO test ovis_ev_test ended
2023-07-29 11:12:52 INFO: ----------------------------------------------
2023-07-29 11:12:53 INFO: ======== prdcr_subscribe_test ========
2023-07-29 11:12:53 INFO: CMD: python3 prdcr_subscribe_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/prdcr_subscribe_test
2023-07-29 11:12:54,344 TADA INFO starting test `prdcr_subscribe_test`
2023-07-29 11:12:54,345 TADA INFO   test-id: d7cacf7a3d6bf560a812257b41819f02a39969751b282d8d984c2d3e5ab2178c
2023-07-29 11:12:54,345 TADA INFO   test-suite: LDMSD
2023-07-29 11:12:54,345 TADA INFO   test-name: prdcr_subscribe_test
2023-07-29 11:12:54,345 TADA INFO   test-user: narate
2023-07-29 11:12:54,345 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:13:41,391 TADA INFO assertion 0, ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds: verify JSON data, passed
2023-07-29 11:13:41,392 TADA INFO assertion 1, ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds: verify STRING data, passed
2023-07-29 11:13:41,392 TADA INFO assertion 2, ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds: verify JSON data, passed
2023-07-29 11:13:41,392 TADA INFO assertion 3, ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds: verify STRING data, passed
2023-07-29 11:13:41,393 TADA INFO assertion 4, ldmsd_stream data check on agg-2: agg2 stream data verification, passed
2023-07-29 11:13:41,762 TADA INFO assertion 5, Stopping the producers succeeds: , passed
2023-07-29 11:13:42,118 TADA INFO assertion 6, Restarting the producers succeeds: , passed
2023-07-29 11:13:50,131 TADA INFO assertion 7, JSON stream data resumes after producer restart on stream-sampler-1: verify JSON data, passed
2023-07-29 11:13:50,131 TADA INFO assertion 8, STRING stream data resumes after producer rerestart on stream-sampler-1: verify STRING data, passed
2023-07-29 11:13:50,131 TADA INFO assertion 9, JSON stream data resumes after producer restart on stream-sampler-2: verify JSON data, passed
2023-07-29 11:13:50,131 TADA INFO assertion 10, STRING stream data resumes after producer rerestart on stream-sampler-2: verify STRING data, passed
2023-07-29 11:13:50,132 TADA INFO assertion 11, ldmsd_stream data resume check on agg-2: agg2 stream data verification, passed
2023-07-29 11:13:51,346 TADA INFO assertion 12, stream-sampler-1 is not running: (running == False), passed
2023-07-29 11:13:56,775 TADA INFO assertion 13, stream-sampler-1 has restarted: (running == True), passed
2023-07-29 11:14:04,343 TADA INFO assertion 14, JSON stream data resumes after stream-sampler-1 restart: verify JSON data, passed
2023-07-29 11:14:04,343 TADA INFO assertion 15, STRING stream data resumes after stream-sampler-1 restart: verify STRING data, passed
2023-07-29 11:14:04,344 TADA INFO assertion 16, ldmsd_stream data check on agg-2 after stream-sampler-1 restart: agg2 stream data verification, passed
2023-07-29 11:14:04,715 TADA INFO assertion 17, agg-1 unsubscribes stream-sampler-1: , passed
2023-07-29 11:14:07,996 TADA INFO assertion 18, agg-1 receives data only from stream-sampler-2: data verified, passed
2023-07-29 11:14:07,996 TADA INFO test prdcr_subscribe_test ended
2023-07-29 11:14:20 INFO: ----------------------------------------------
2023-07-29 11:14:21 INFO: ======== set_array_test ========
2023-07-29 11:14:21 INFO: CMD: python3 set_array_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/set_array_test
2023-07-29 11:14:22,364 TADA INFO starting test `set_array_test`
2023-07-29 11:14:22,364 TADA INFO   test-id: bb73182dfea4fe46ed74ca7e3bc72ddd544414da583275c1ecccfb78d00b1544
2023-07-29 11:14:22,364 TADA INFO   test-suite: LDMSD
2023-07-29 11:14:22,365 TADA INFO   test-name: set_array_test
2023-07-29 11:14:22,365 TADA INFO   test-user: narate
2023-07-29 11:14:22,365 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:14:22,365 __main__ INFO -- Get or create the cluster --
2023-07-29 11:14:27,595 __main__ INFO -- Start daemons --
2023-07-29 11:14:33,480 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:15:00,536 TADA INFO assertion 1, 1st update got some callbacks: verified hunk of 2 snapshots, passed
2023-07-29 11:15:00,537 TADA INFO assertion 2, 2nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-07-29 11:15:00,537 TADA INFO assertion 3, 3nd update got N callbacks: verified hunk of 5 snapshots, passed
2023-07-29 11:15:00,537 TADA INFO test set_array_test ended
2023-07-29 11:15:11 INFO: ----------------------------------------------
2023-07-29 11:15:12 INFO: ======== setgroup_test ========
2023-07-29 11:15:12 INFO: CMD: python3 setgroup_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/setgroup_test
2023-07-29 11:15:13,548 TADA INFO starting test `setgroup_test`
2023-07-29 11:15:13,549 TADA INFO   test-id: 8cbc3a0309668e6d02aa3702fa85de23f5ddc511401e8162afba079e09e5f536
2023-07-29 11:15:13,549 TADA INFO   test-suite: LDMSD
2023-07-29 11:15:13,549 TADA INFO   test-name: setgroup_test
2023-07-29 11:15:13,549 TADA INFO   test-user: narate
2023-07-29 11:15:13,550 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:15:13,550 __main__ INFO -- Get or create the cluster --
2023-07-29 11:15:23,160 __main__ INFO -- Start daemons --
2023-07-29 11:15:39,385 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:15:44,390 __main__ INFO -- ldms_ls to agg-2 --
2023-07-29 11:15:44,506 TADA INFO assertion 1, ldms_ls grp on agg-2: dir result verified, passed
2023-07-29 11:15:46,741 TADA INFO assertion 2, members on agg-2 are being updated: data verified, passed
2023-07-29 11:15:46,741 __main__ INFO -- Removing test_2 from grp --
2023-07-29 11:15:47,215 TADA INFO assertion 3, test_2 is removed fom grp on sampler: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:15:51,340 TADA INFO assertion 4, test_2 is removed from grp on agg-1: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:15:55,461 TADA INFO assertion 5, test_2 is removed from grp on agg-2: expect {'node-1/grp', 'node-1/test_1'}, got {'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:15:59,466 __main__ INFO -- Adding test_2 back into grp --
2023-07-29 11:15:59,927 TADA INFO assertion 6, test_2 is added back to grp on sampler: expect {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, got {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:16:04,050 TADA INFO assertion 7, test_2 is added back to grp on agg-1: expect {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, got {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:16:06,178 TADA INFO assertion 8, test_2 is added back to grp on agg-2: expect {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, got {'node-1/test_2', 'node-1/grp', 'node-1/test_1'}, passed
2023-07-29 11:16:08,181 TADA INFO test setgroup_test ended
2023-07-29 11:16:21 INFO: ----------------------------------------------
2023-07-29 11:16:21 INFO: ======== slurm_stream_test ========
2023-07-29 11:16:21 INFO: CMD: python3 slurm_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/slurm_stream_test
2023-07-29 11:16:22,639 TADA INFO starting test `slurm_stream_test`
2023-07-29 11:16:22,639 TADA INFO   test-id: 2aa0b1dd917bd101ef495f4f3195d643cdf889f15e14deaec2cd1866e56133d7
2023-07-29 11:16:22,639 TADA INFO   test-suite: LDMSD
2023-07-29 11:16:22,639 TADA INFO   test-name: slurm_stream_test
2023-07-29 11:16:22,639 TADA INFO   test-user: narate
2023-07-29 11:16:22,639 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:16:22,640 __main__ INFO -- Get or create the cluster --
2023-07-29 11:16:29,798 __main__ INFO -- Start daemons --
2023-07-29 11:16:40,312 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:17:10,336 TADA INFO assertion 1, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,336 __main__ INFO 12345
2023-07-29 11:17:10,337 __main__ INFO 12345
2023-07-29 11:17:10,337 TADA INFO assertion 2, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,337 TADA INFO assertion 3, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-07-29 11:17:10,337 TADA INFO assertion 4, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,337 TADA INFO assertion 5, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,337 TADA INFO assertion 6, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,338 TADA INFO assertion 7, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,449 TADA INFO assertion 8, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,450 __main__ INFO 12345
2023-07-29 11:17:10,450 __main__ INFO 12345
2023-07-29 11:17:10,450 TADA INFO assertion 9, job_start correctly represented in metric set: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,450 TADA INFO assertion 10, job_end correctly represented in metric set: with mutl jobs running, for Job 12345, passed
2023-07-29 11:17:10,450 TADA INFO assertion 11, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,450 TADA INFO assertion 12, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,451 TADA INFO assertion 13, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,451 TADA INFO assertion 14, task_pid correctly represented: with mult jobs running for Job 12345, passed
2023-07-29 11:17:10,558 TADA INFO assertion 15, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,558 __main__ INFO 12346
2023-07-29 11:17:10,558 __main__ INFO 12346
2023-07-29 11:17:10,558 TADA INFO assertion 16, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,558 TADA INFO assertion 17, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-07-29 11:17:10,559 TADA INFO assertion 18, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,559 TADA INFO assertion 19, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,559 TADA INFO assertion 20, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,559 TADA INFO assertion 21, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,664 TADA INFO assertion 22, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,664 __main__ INFO 12346
2023-07-29 11:17:10,664 __main__ INFO 12346
2023-07-29 11:17:10,664 TADA INFO assertion 23, job_start correctly represented in metric set: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,665 TADA INFO assertion 24, job_end correctly represented in metric set: with mutl jobs running, for Job 12346, passed
2023-07-29 11:17:10,665 TADA INFO assertion 25, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,665 TADA INFO assertion 26, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,665 TADA INFO assertion 27, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,665 TADA INFO assertion 28, task_pid correctly represented: with mult jobs running for Job 12346, passed
2023-07-29 11:17:10,782 TADA INFO assertion 29, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,782 __main__ INFO 12347
2023-07-29 11:17:10,782 __main__ INFO 12347
2023-07-29 11:17:10,782 TADA INFO assertion 30, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,782 TADA INFO assertion 31, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-07-29 11:17:10,783 TADA INFO assertion 32, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,783 TADA INFO assertion 33, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,783 TADA INFO assertion 34, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,783 TADA INFO assertion 35, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,896 TADA INFO assertion 36, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:10,896 __main__ INFO 12347
2023-07-29 11:17:10,896 __main__ INFO 12347
2023-07-29 11:17:10,897 TADA INFO assertion 37, job_start correctly represented in metric set: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,897 TADA INFO assertion 38, job_end correctly represented in metric set: with mutl jobs running, for Job 12347, passed
2023-07-29 11:17:10,897 TADA INFO assertion 39, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,897 TADA INFO assertion 40, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,898 TADA INFO assertion 41, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:10,898 TADA INFO assertion 42, task_pid correctly represented: with mult jobs running for Job 12347, passed
2023-07-29 11:17:11,019 TADA INFO assertion 43, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,020 __main__ INFO 12348
2023-07-29 11:17:11,020 __main__ INFO 12348
2023-07-29 11:17:11,020 TADA INFO assertion 44, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,020 TADA INFO assertion 45, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-07-29 11:17:11,020 TADA INFO assertion 46, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,020 TADA INFO assertion 47, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,021 TADA INFO assertion 48, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,021 TADA INFO assertion 49, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,143 TADA INFO assertion 50, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,143 __main__ INFO 12348
2023-07-29 11:17:11,143 __main__ INFO 12348
2023-07-29 11:17:11,143 TADA INFO assertion 51, job_start correctly represented in metric set: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,143 TADA INFO assertion 52, job_end correctly represented in metric set: with mutl jobs running, for Job 12348, passed
2023-07-29 11:17:11,143 TADA INFO assertion 53, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,144 TADA INFO assertion 54, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,144 TADA INFO assertion 55, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,144 TADA INFO assertion 56, task_pid correctly represented: with mult jobs running for Job 12348, passed
2023-07-29 11:17:11,254 TADA INFO assertion 57, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,254 __main__ INFO 12355
2023-07-29 11:17:11,255 __main__ INFO 12355
2023-07-29 11:17:11,255 TADA INFO assertion 58, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,255 TADA INFO assertion 59, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-07-29 11:17:11,255 TADA INFO assertion 60, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,255 TADA INFO assertion 61, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,255 TADA INFO assertion 62, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,255 TADA INFO assertion 63, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,256 TADA INFO assertion 64, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,256 TADA INFO assertion 65, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,256 TADA INFO assertion 66, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,256 TADA INFO assertion 67, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,361 TADA INFO assertion 68, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,362 __main__ INFO 12355
2023-07-29 11:17:11,362 __main__ INFO 12355
2023-07-29 11:17:11,362 TADA INFO assertion 69, job_start correctly represented in metric set: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,362 TADA INFO assertion 70, job_end correctly represented in metric set: with mutl jobs running, for Job 12355, passed
2023-07-29 11:17:11,362 TADA INFO assertion 71, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,362 TADA INFO assertion 72, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,362 TADA INFO assertion 73, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,363 TADA INFO assertion 74, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,363 TADA INFO assertion 75, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,363 TADA INFO assertion 76, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,363 TADA INFO assertion 77, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,363 TADA INFO assertion 78, task_pid correctly represented: with mult jobs running for Job 12355, passed
2023-07-29 11:17:11,474 TADA INFO assertion 79, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,475 __main__ INFO 12356
2023-07-29 11:17:11,475 __main__ INFO 12356
2023-07-29 11:17:11,475 TADA INFO assertion 80, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,475 TADA INFO assertion 81, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-07-29 11:17:11,475 TADA INFO assertion 82, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,475 TADA INFO assertion 83, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,475 TADA INFO assertion 84, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,476 TADA INFO assertion 85, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,476 TADA INFO assertion 86, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,476 TADA INFO assertion 87, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,476 TADA INFO assertion 88, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,476 TADA INFO assertion 89, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,581 TADA INFO assertion 90, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,581 __main__ INFO 12356
2023-07-29 11:17:11,581 __main__ INFO 12356
2023-07-29 11:17:11,582 TADA INFO assertion 91, job_start correctly represented in metric set: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 92, job_end correctly represented in metric set: with mutl jobs running, for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 93, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 94, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 95, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 96, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,582 TADA INFO assertion 97, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,583 TADA INFO assertion 98, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,583 TADA INFO assertion 99, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,583 TADA INFO assertion 100, task_pid correctly represented: with mult jobs running for Job 12356, passed
2023-07-29 11:17:11,690 TADA INFO assertion 101, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,691 __main__ INFO 12357
2023-07-29 11:17:11,691 __main__ INFO 12357
2023-07-29 11:17:11,691 TADA INFO assertion 102, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,691 TADA INFO assertion 103, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-07-29 11:17:11,691 TADA INFO assertion 104, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,691 TADA INFO assertion 105, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 106, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 107, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 108, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 109, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 110, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,692 TADA INFO assertion 111, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,808 TADA INFO assertion 112, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,808 __main__ INFO 12357
2023-07-29 11:17:11,808 __main__ INFO 12357
2023-07-29 11:17:11,808 TADA INFO assertion 113, job_start correctly represented in metric set: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 114, job_end correctly represented in metric set: with mutl jobs running, for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 115, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 116, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 117, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 118, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,809 TADA INFO assertion 119, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,810 TADA INFO assertion 120, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,810 TADA INFO assertion 121, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,810 TADA INFO assertion 122, task_pid correctly represented: with mult jobs running for Job 12357, passed
2023-07-29 11:17:11,928 TADA INFO assertion 123, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:11,929 __main__ INFO 12358
2023-07-29 11:17:11,929 __main__ INFO 12358
2023-07-29 11:17:11,929 TADA INFO assertion 124, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,929 TADA INFO assertion 125, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-07-29 11:17:11,929 TADA INFO assertion 126, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,929 TADA INFO assertion 127, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,929 TADA INFO assertion 128, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,930 TADA INFO assertion 129, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,930 TADA INFO assertion 130, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,930 TADA INFO assertion 131, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,930 TADA INFO assertion 132, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:11,930 TADA INFO assertion 133, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,042 TADA INFO assertion 134, Job properly assigned to correct slot: correct job_id fills next slot, passed
2023-07-29 11:17:12,042 __main__ INFO 12358
2023-07-29 11:17:12,042 __main__ INFO 12358
2023-07-29 11:17:12,042 TADA INFO assertion 135, job_start correctly represented in metric set: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 136, job_end correctly represented in metric set: with mutl jobs running, for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 137, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 138, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 139, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 140, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,043 TADA INFO assertion 141, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,044 TADA INFO assertion 142, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,044 TADA INFO assertion 143, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:12,044 TADA INFO assertion 144, task_pid correctly represented: with mult jobs running for Job 12358, passed
2023-07-29 11:17:14,156 TADA INFO assertion 145, new job correctly replaces oldest slot: correct job_id fills next slot, passed
2023-07-29 11:17:14,156 __main__ INFO 12353
2023-07-29 11:17:14,156 __main__ INFO 12353
2023-07-29 11:17:14,156 TADA INFO assertion 146, new job_start correctly represented in metric set: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,156 TADA INFO assertion 147, new job_end correctly represented in metric set: with mutl jobs running, for Job 12353, passed
2023-07-29 11:17:14,156 TADA INFO assertion 148, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 149, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 150, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 151, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 152, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 153, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 154, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,157 TADA INFO assertion 155, new job's task replaces oldest slot: with mult jobs running for Job 12353, passed
2023-07-29 11:17:14,158 __main__ INFO -- Test Finished --
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
Delivering events...
2023-07-29 11:17:14,158 TADA INFO test slurm_stream_test ended
2023-07-29 11:17:25 INFO: ----------------------------------------------
2023-07-29 11:17:26 INFO: ======== spank_notifier_test ========
2023-07-29 11:17:26 INFO: CMD: python3 spank_notifier_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/spank_notifier_test
2023-07-29 11:17:27,132 TADA INFO starting test `spank_notifier_test`
2023-07-29 11:17:27,132 TADA INFO   test-id: cce9b62a5d316ae13b4b632b5a1c316335a896fdd9b27cfb82ec07a57df8d10c
2023-07-29 11:17:27,132 TADA INFO   test-suite: Slurm_Plugins
2023-07-29 11:17:27,132 TADA INFO   test-name: spank_notifier_test
2023-07-29 11:17:27,132 TADA INFO   test-user: narate
2023-07-29 11:17:27,132 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:17:27,133 __main__ INFO -- Create the cluster --
2023-07-29 11:17:52,468 __main__ INFO -- Cleanup output --
2023-07-29 11:17:52,786 __main__ INFO -- Test bad plugstack config --
2023-07-29 11:17:52,786 __main__ INFO Starting slurm ...
2023-07-29 11:18:06,614 __main__ INFO Starting slurm ... OK
2023-07-29 11:18:27,099 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-29 11:18:27,218 __main__ INFO   jobid = 1
2023-07-29 11:18:27,443 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-29 11:18:27,558 __main__ INFO   jobid = 2
2023-07-29 11:18:27,763 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-29 11:18:27,883 __main__ INFO   jobid = 3
2023-07-29 11:18:28,100 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-29 11:18:28,210 __main__ INFO   jobid = 4
2023-07-29 11:18:37,731 TADA INFO assertion 60, Bad config does not affect jobs: jobs verified, passed
2023-07-29 11:18:37,731 __main__ INFO Killin slurm ...
2023-07-29 11:18:40,626 __main__ INFO Killin slurm ... OK
2023-07-29 11:19:00,637 __main__ INFO -- Start daemons --
2023-07-29 11:19:10,998 __main__ INFO Starting slurm ... OK
2023-07-29 11:19:31,248 __main__ INFO -- Submitting job with no stream listener --
2023-07-29 11:19:31,459 __main__ INFO -- Submitting job with num_tasks 8 --
2023-07-29 11:19:31,575 __main__ INFO   jobid = 5
2023-07-29 11:19:47,559 TADA INFO assertion 0, Missing stream listener on node-1 does not affect job execution: job output file created, passed
2023-07-29 11:19:47,560 TADA INFO assertion 1, Missing stream listener on node-2 does not affect job execution: job output file created, passed
2023-07-29 11:19:53,412 __main__ INFO -- Submitting job with listener --
2023-07-29 11:19:53,616 __main__ INFO -- Submitting job with num_tasks 1 --
2023-07-29 11:19:53,736 __main__ INFO   jobid = 6
2023-07-29 11:19:53,956 __main__ INFO -- Submitting job with num_tasks 2 --
2023-07-29 11:19:54,078 __main__ INFO   jobid = 7
2023-07-29 11:19:54,295 __main__ INFO -- Submitting job with num_tasks 4 --
2023-07-29 11:19:54,409 __main__ INFO   jobid = 8
2023-07-29 11:19:54,597 __main__ INFO -- Submitting job with num_tasks 8 --
2023-07-29 11:19:54,714 __main__ INFO   jobid = 9
2023-07-29 11:19:54,928 __main__ INFO -- Submitting job with num_tasks 27 --
2023-07-29 11:19:55,056 __main__ INFO   jobid = 10
2023-07-29 11:20:16,794 __main__ INFO -- Verifying Events --
2023-07-29 11:20:16,794 TADA INFO assertion 2, 1-task job: first event is 'init': `init` verified, passed
2023-07-29 11:20:16,794 TADA INFO assertion 3, 1-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-29 11:20:16,794 TADA INFO assertion 4, 1-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-29 11:20:16,795 TADA INFO assertion 5, 1-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-29 11:20:16,795 TADA INFO assertion 6, 1-task job: fourth event is 'exit': `exit` verified, passed
2023-07-29 11:20:16,795 TADA INFO assertion 7, 2-task job: first event is 'init': `init` verified, passed
2023-07-29 11:20:16,795 TADA INFO assertion 8, 2-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-29 11:20:16,795 TADA INFO assertion 9, 2-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 10, 2-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 11, 2-task job: fourth event is 'exit': `exit` verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 12, 4-task job: first event is 'init': `init` verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 13, 4-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 14, 4-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-29 11:20:16,796 TADA INFO assertion 15, 4-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-29 11:20:16,797 TADA INFO assertion 16, 4-task job: fourth event is 'exit': `exit` verified, passed
2023-07-29 11:20:16,797 TADA INFO assertion 17, 8-task job: first event is 'init': `init` verified, passed
2023-07-29 11:20:16,797 TADA INFO assertion 18, 8-task job: 'step_init' event contains subscriber data: `init` subscriber_data verified, passed
2023-07-29 11:20:16,797 TADA INFO assertion 19, 8-task job: second event is 'task_init_priv': `task_init_priv` verified, passed
2023-07-29 11:20:16,797 TADA INFO assertion 20, 8-task job: third event is 'task_exit': `task_exit` verified, passed
2023-07-29 11:20:16,798 TADA INFO assertion 21, 8-task job: fourth event is 'exit': `exit` verified, passed
2023-07-29 11:20:16,798 TADA INFO assertion 26, 27-task job: fourth event is 'exit': `exit` event missing, failed
Traceback (most recent call last):
  File "spank_notifier_test", line 492, in <module>
    verify_jobinfo(cluster, test, node_events, jobinfo)
  File "spank_notifier_test", line 191, in verify_jobinfo
    test.assert_test(assert_no + 4, False, "`exit` event missing")
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: The test to verify ldmsd-stream SPANK notifier., `exit` event missing: FAILED
2023-07-29 11:20:16,799 TADA INFO assertion 22, 27-task job: first event is 'init': skipped
2023-07-29 11:20:16,799 TADA INFO assertion 23, 27-task job: 'step_init' event contains subscriber data: skipped
2023-07-29 11:20:16,800 TADA INFO assertion 24, 27-task job: second event is 'task_init_priv': skipped
2023-07-29 11:20:16,800 TADA INFO assertion 25, 27-task job: third event is 'task_exit': skipped
2023-07-29 11:20:16,800 TADA INFO assertion 50, Multi-tenant verification: skipped
2023-07-29 11:20:16,800 TADA INFO assertion 51, Killing stream listener does not affect job execution on node-1: skipped
2023-07-29 11:20:16,800 TADA INFO assertion 52, Killing stream listener does not affect job execution on node-2: skipped
2023-07-29 11:20:16,800 TADA INFO test spank_notifier_test ended
2023-07-29 11:20:33 INFO: ----------------------------------------------
2023-07-29 11:20:34 INFO: ======== ldms_list_test ========
2023-07-29 11:20:34 INFO: CMD: python3 ldms_list_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_list_test
2023-07-29 11:20:34,991 TADA INFO starting test `ldms_list_test`
2023-07-29 11:20:34,991 TADA INFO   test-id: 8ab3412403467250310b8063afb63a743c14ab7b965b20f47e4ba67d132d7961
2023-07-29 11:20:34,991 TADA INFO   test-suite: LDMSD
2023-07-29 11:20:34,991 TADA INFO   test-name: ldms_list_test
2023-07-29 11:20:34,991 TADA INFO   test-user: narate
2023-07-29 11:20:34,991 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:20:34,992 __main__ INFO -- Get or create the cluster --
2023-07-29 11:20:37,996 __main__ INFO -- Start daemons --
2023-07-29 11:20:46,308 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:20:48,311 __main__ INFO start list_samp.py and list_agg.py interactive sessions
2023-07-29 11:20:54,346 TADA INFO assertion 1, check list_sampler on list_agg.py: OK, passed
2023-07-29 11:20:54,347 TADA INFO assertion 2, (1st update) check set1 on list_samp.py: OK, passed
2023-07-29 11:20:54,347 TADA INFO assertion 3, (1st update) check set3_p on list_samp.py: OK, passed
2023-07-29 11:20:54,348 TADA INFO assertion 4, (1st update)check set3_c on list_samp.py: OK, passed
2023-07-29 11:20:54,348 TADA INFO assertion 5, (1st update)check set1 on list_agg.py: OK, passed
2023-07-29 11:20:54,348 TADA INFO assertion 6, (1st update)check set3_p on list_agg.py: OK, passed
2023-07-29 11:20:54,348 TADA INFO assertion 7, (1st update)check set3_c on list_agg.py: OK, passed
2023-07-29 11:20:54,349 __main__ INFO 2nd sampling on the sampler...
2023-07-29 11:21:01,558 TADA INFO assertion 8, (2nd update) check set1 on list_samp.py: OK, passed
2023-07-29 11:21:01,558 TADA INFO assertion 9, (2nd update) check set3_p on list_samp.py: OK, passed
2023-07-29 11:21:01,559 TADA INFO assertion 10, (2nd update) check set3_c on list_samp.py: OK, passed
2023-07-29 11:21:01,559 __main__ INFO 2nd update on the aggregator...
2023-07-29 11:21:08,768 TADA INFO assertion 11, (2nd update) check set1 on list_agg.py: OK, passed
2023-07-29 11:21:08,769 TADA INFO assertion 12, (2nd update) check set3_p on list_agg.py: OK, passed
2023-07-29 11:21:08,769 TADA INFO assertion 13, (2nd update) check set3_c on list_agg.py: OK, passed
2023-07-29 11:21:08,769 __main__ INFO 3rd sampling on the sampler...
2023-07-29 11:21:15,978 TADA INFO assertion 14, (3rd update) check set1 on list_samp.py: OK, passed
2023-07-29 11:21:15,978 TADA INFO assertion 15, (3rd update) check set3_p on list_samp.py: OK, passed
2023-07-29 11:21:15,978 TADA INFO assertion 16, (3rd update) check set3_c on list_samp.py: OK, passed
2023-07-29 11:21:15,978 __main__ INFO 3rd update on the aggregator...
2023-07-29 11:21:23,188 TADA INFO assertion 17, (3rd update) check set1 on list_agg.py: OK, passed
2023-07-29 11:21:23,188 TADA INFO assertion 18, (3rd update) check set3_p on list_agg.py: OK, passed
2023-07-29 11:21:23,188 TADA INFO assertion 19, (3rd update) check set3_c on list_agg.py: OK, passed
2023-07-29 11:21:23,189 __main__ INFO 4th sampling on the sampler...
2023-07-29 11:21:30,398 TADA INFO assertion 20, (4th update; list uncahnged) check set1 on list_samp.py: OK, passed
2023-07-29 11:21:30,398 TADA INFO assertion 21, (4th update; list uncahnged) check set3_p on list_samp.py: OK, passed
2023-07-29 11:21:30,398 TADA INFO assertion 22, (4th update; list uncahnged) check set3_c on list_samp.py: OK, passed
2023-07-29 11:21:30,399 __main__ INFO 4th update on the aggregator...
2023-07-29 11:21:37,607 TADA INFO assertion 23, (4th update; list uncahnged) check set1 on list_agg.py: OK, passed
2023-07-29 11:21:37,608 TADA INFO assertion 24, (4th update; list uncahnged) check set3_p on list_agg.py: OK, passed
2023-07-29 11:21:37,608 TADA INFO assertion 25, (4th update; list uncahnged) check set3_c on list_agg.py: OK, passed
2023-07-29 11:21:37,608 __main__ INFO 5th sampling on the sampler...
2023-07-29 11:21:44,817 TADA INFO assertion 26, (5th update; list del) check set1 on list_samp.py: OK, passed
2023-07-29 11:21:44,817 TADA INFO assertion 27, (5th update; list del) check set3_p on list_samp.py: OK, passed
2023-07-29 11:21:44,818 TADA INFO assertion 28, (5th update; list del) check set3_c on list_samp.py: OK, passed
2023-07-29 11:21:44,818 __main__ INFO 5th update on the aggregator...
2023-07-29 11:21:52,027 TADA INFO assertion 29, (5th update; list del) check set1 on list_agg.py: OK, passed
2023-07-29 11:21:52,027 TADA INFO assertion 30, (5th update; list del) check set3_p on list_agg.py: OK, passed
2023-07-29 11:21:52,028 TADA INFO assertion 31, (5th update; list del) check set3_c on list_agg.py: OK, passed
2023-07-29 11:21:52,028 __main__ INFO 6th sampling on the sampler...
2023-07-29 11:21:59,237 TADA INFO assertion 32, (6th update; list unchanged) check set1 on list_samp.py: OK, passed
2023-07-29 11:21:59,237 TADA INFO assertion 33, (6th update; list unchanged) check set3_p on list_samp.py: OK, passed
2023-07-29 11:21:59,238 TADA INFO assertion 34, (6th update; list unchanged) check set3_c on list_samp.py: OK, passed
2023-07-29 11:21:59,238 __main__ INFO 6th update on the updator...
2023-07-29 11:22:06,446 TADA INFO assertion 35, (6th update; list unchanged) check set1 on list_agg.py: OK, passed
2023-07-29 11:22:06,447 TADA INFO assertion 36, (6th update; list unchanged) check set3_p on list_agg.py: OK, passed
2023-07-29 11:22:06,447 TADA INFO assertion 37, (6th update; list unchanged) check set3_c on list_agg.py: OK, passed
2023-07-29 11:22:06,448 TADA INFO test ldms_list_test ended
2023-07-29 11:22:17 INFO: ----------------------------------------------
2023-07-29 11:22:18 INFO: ======== quick_set_add_rm_test ========
2023-07-29 11:22:18 INFO: CMD: python3 quick_set_add_rm_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/quick_set_add_rm_test
2023-07-29 11:22:18,709 TADA INFO starting test `quick_set_add_rm_test`
2023-07-29 11:22:18,709 TADA INFO   test-id: 1a52435a3798903f85df457458f5b56edaca62db8f32449979b91446efd8eee8
2023-07-29 11:22:18,709 TADA INFO   test-suite: LDMSD
2023-07-29 11:22:18,709 TADA INFO   test-name: quick_set_add_rm_test
2023-07-29 11:22:18,709 TADA INFO   test-user: narate
2023-07-29 11:22:18,709 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:22:18,710 __main__ INFO -- Get or create the cluster --
2023-07-29 11:22:25,935 __main__ INFO -- Start samp.py --
2023-07-29 11:22:31,051 TADA INFO assertion 1, start samp.py: prompt checked, passed
2023-07-29 11:22:31,051 __main__ INFO -- Start daemons --
2023-07-29 11:22:40,753 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:22:46,358 TADA INFO assertion 2, verify data: verified, passed
2023-07-29 11:22:50,958 TADA INFO assertion 3, samp.py adds set1 / verify data: verified, passed
2023-07-29 11:22:55,559 TADA INFO assertion 4, samp.py removes set1 / verify data: verified, passed
2023-07-29 11:23:00,155 TADA INFO assertion 5, samp.py quickly adds and removes set2 / verify data: verified, passed
2023-07-29 11:23:05,285 TADA INFO assertion 6, agg-1 log stays empty: verified, passed
2023-07-29 11:23:05,286 TADA INFO test quick_set_add_rm_test ended
2023-07-29 11:23:17 INFO: ----------------------------------------------
2023-07-29 11:23:18 INFO: ======== set_array_hang_test ========
2023-07-29 11:23:18 INFO: CMD: python3 set_array_hang_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/set_array_hang_test
2023-07-29 11:23:18,811 TADA INFO starting test `set_array_hang_test`
2023-07-29 11:23:18,811 TADA INFO   test-id: 018b4297ffd2df31ccd8b2905385c036645e8618a360abd03f404a2272ff902d
2023-07-29 11:23:18,812 TADA INFO   test-suite: LDMSD
2023-07-29 11:23:18,812 TADA INFO   test-name: set_array_hang_test
2023-07-29 11:23:18,812 TADA INFO   test-user: narate
2023-07-29 11:23:18,812 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:23:18,812 __main__ INFO -- Get or create the cluster --
2023-07-29 11:23:21,772 __main__ INFO -- Start processes --
2023-07-29 11:23:21,772 __main__ INFO starting interactive set_array_samp.py
2023-07-29 11:23:24,784 TADA INFO assertion 1, start set_array_samp.py: data verified, passed
2023-07-29 11:23:27,803 TADA INFO assertion 2, start set_array_agg.py: data verified, passed
2023-07-29 11:23:35,012 TADA INFO assertion 3, agg update before the 1st sample: data verified, passed
2023-07-29 11:23:42,222 TADA INFO assertion 4, sampling 2 times then agg update: data verified, passed
2023-07-29 11:23:45,827 TADA INFO assertion 5, agg update w/o new sampling: data verified, passed
2023-07-29 11:23:53,037 TADA INFO assertion 6, sampling 5 times then agg update: data verified, passed
2023-07-29 11:23:53,037 TADA INFO test set_array_hang_test ended
2023-07-29 11:24:03 INFO: ----------------------------------------------
2023-07-29 11:24:04 INFO: ======== ldmsd_autointerval_test ========
2023-07-29 11:24:04 INFO: CMD: python3 ldmsd_autointerval_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_autointerval_test
2023-07-29 11:24:05,345 TADA INFO starting test `ldmsd_autointerval_test`
2023-07-29 11:24:05,345 TADA INFO   test-id: afb2318e332062a48b8832df75145d895136c7dc705164cb2a73d6c6bccde580
2023-07-29 11:24:05,345 TADA INFO   test-suite: LDMSD
2023-07-29 11:24:05,345 TADA INFO   test-name: ldmsd_autointerval_test
2023-07-29 11:24:05,345 TADA INFO   test-user: narate
2023-07-29 11:24:05,345 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:24:05,346 __main__ INFO -- Get or create the cluster --
2023-07-29 11:24:12,466 __main__ INFO -- Start daemons --
2023-07-29 11:24:27,955 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:24:34,478 TADA INFO assertion 1, start all daemons and interactive controller: OK, passed
2023-07-29 11:24:36,702 TADA INFO assertion 2, verify sampling interval and update hints: verified, passed
2023-07-29 11:24:36,702 __main__ INFO Let them run for a while to collect data ...
2023-07-29 11:24:46,712 __main__ INFO Setting sample interval to 1000000 ...
2023-07-29 11:24:54,945 TADA INFO assertion 3, set and verify 2nd sampling interval / update hints: verified, passed
2023-07-29 11:24:54,945 __main__ INFO Let them run for a while to collect data ...
2023-07-29 11:25:04,949 __main__ INFO Setting sample interval to 2000000 ...
2023-07-29 11:25:13,195 TADA INFO assertion 4, set and verify 3rd sampling interval / update hints: verified, passed
2023-07-29 11:25:13,195 __main__ INFO Let them run for a while to collect data ...
2023-07-29 11:25:23,402 TADA INFO assertion 5, verify SOS data: timestamp differences in SOS show all 3 intervals, passed
2023-07-29 11:25:23,510 TADA INFO assertion 6, verify 'oversampled' in the agg2 log: OK, passed
2023-07-29 11:25:23,510 TADA INFO test ldmsd_autointerval_test ended
2023-07-29 11:25:35 INFO: ----------------------------------------------
2023-07-29 11:25:36 INFO: ======== ldms_record_test ========
2023-07-29 11:25:36 INFO: CMD: python3 ldms_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_record_test
2023-07-29 11:25:37,178 TADA INFO starting test `ldms_record_test`
2023-07-29 11:25:37,178 TADA INFO   test-id: 9a3a348d4e41224beb18fdaf10b4a65990f556ae712d7b0158f6e9f466872859
2023-07-29 11:25:37,178 TADA INFO   test-suite: LDMSD
2023-07-29 11:25:37,178 TADA INFO   test-name: ldms_record_test
2023-07-29 11:25:37,178 TADA INFO   test-user: narate
2023-07-29 11:25:37,178 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:25:37,179 __main__ INFO -- Get or create the cluster --
2023-07-29 11:25:40,301 __main__ INFO -- Start daemons --
2023-07-29 11:25:48,636 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:25:50,639 __main__ INFO start record_samp.py and record_agg.py interactive sessions
2023-07-29 11:25:56,674 TADA INFO assertion 1, check record_sampler on record_agg.py: OK, passed
2023-07-29 11:25:56,674 TADA INFO assertion 2, (1st update) check set1 on record_samp.py: OK, passed
2023-07-29 11:25:56,675 TADA INFO assertion 3, (1st update) check set3_p on record_samp.py: OK, passed
2023-07-29 11:25:56,675 TADA INFO assertion 4, (1st update) check set3_c on record_samp.py: OK, passed
2023-07-29 11:25:56,675 TADA INFO assertion 5, (1st update) check set1 on record_agg.py: OK, passed
2023-07-29 11:25:56,676 TADA INFO assertion 6, (1st update) check set3_p on record_agg.py: OK, passed
2023-07-29 11:25:56,676 TADA INFO assertion 7, (1st update) check set3_c on record_agg.py: OK, passed
2023-07-29 11:25:56,676 __main__ INFO 2nd sampling on the sampler...
2023-07-29 11:26:03,885 TADA INFO assertion 8, (2nd update) check set1 on record_samp.py: OK, passed
2023-07-29 11:26:03,885 TADA INFO assertion 9, (2nd update) check set3_p on record_samp.py: OK, passed
2023-07-29 11:26:03,886 TADA INFO assertion 10, (2nd update) check set3_c on record_samp.py: OK, passed
2023-07-29 11:26:03,886 __main__ INFO 2nd update on the aggregator...
2023-07-29 11:26:11,095 TADA INFO assertion 11, (2nd update) check set1 on record_agg.py: OK, passed
2023-07-29 11:26:11,095 TADA INFO assertion 12, (2nd update) check set3_p on record_agg.py: OK, passed
2023-07-29 11:26:11,096 TADA INFO assertion 13, (2nd update) check set3_c on record_agg.py: OK, passed
2023-07-29 11:26:11,096 __main__ INFO 3rd sampling on the sampler...
2023-07-29 11:26:18,305 TADA INFO assertion 14, (3rd update) check set1 on record_samp.py: OK, passed
2023-07-29 11:26:18,305 TADA INFO assertion 15, (3rd update) check set3_p on record_samp.py: OK, passed
2023-07-29 11:26:18,305 TADA INFO assertion 16, (3rd update) check set3_c on record_samp.py: OK, passed
2023-07-29 11:26:18,306 __main__ INFO 3rd update on the aggregator...
2023-07-29 11:26:25,515 TADA INFO assertion 17, (3rd update) check set1 on record_agg.py: OK, passed
2023-07-29 11:26:25,515 TADA INFO assertion 18, (3rd update) check set3_p on record_agg.py: OK, passed
2023-07-29 11:26:25,515 TADA INFO assertion 19, (3rd update) check set3_c on record_agg.py: OK, passed
2023-07-29 11:26:25,516 __main__ INFO 4th sampling on the sampler...
2023-07-29 11:26:32,725 TADA INFO assertion 20, (4th update; record uncahnged) check set1 on record_samp.py: OK, passed
2023-07-29 11:26:32,725 TADA INFO assertion 21, (4th update; record uncahnged) check set3_p on record_samp.py: OK, passed
2023-07-29 11:26:32,726 TADA INFO assertion 22, (4th update; record uncahnged) check set3_c on record_samp.py: OK, passed
2023-07-29 11:26:32,726 __main__ INFO 4th update on the aggregator...
2023-07-29 11:26:39,934 TADA INFO assertion 23, (4th update; record uncahnged) check set1 on record_agg.py: OK, passed
2023-07-29 11:26:39,934 TADA INFO assertion 24, (4th update; record uncahnged) check set3_p on record_agg.py: OK, passed
2023-07-29 11:26:39,935 TADA INFO assertion 25, (4th update; record uncahnged) check set3_c on record_agg.py: OK, passed
2023-07-29 11:26:39,935 __main__ INFO 5th sampling on the sampler...
2023-07-29 11:26:47,144 TADA INFO assertion 26, (5th update; record del) check set1 on record_samp.py: OK, passed
2023-07-29 11:26:47,145 TADA INFO assertion 27, (5th update; record del) check set3_p on record_samp.py: OK, passed
2023-07-29 11:26:47,145 TADA INFO assertion 28, (5th update; record del) check set3_c on record_samp.py: OK, passed
2023-07-29 11:26:47,145 __main__ INFO 5th update on the aggregator...
2023-07-29 11:26:54,354 TADA INFO assertion 29, (5th update; record del) check set1 on record_agg.py: OK, passed
2023-07-29 11:26:54,354 TADA INFO assertion 30, (5th update; record del) check set3_p on record_agg.py: OK, passed
2023-07-29 11:26:54,355 TADA INFO assertion 31, (5th update; record del) check set3_c on record_agg.py: OK, passed
2023-07-29 11:26:54,355 __main__ INFO 6th sampling on the sampler...
2023-07-29 11:27:01,564 TADA INFO assertion 32, (6th update; record unchanged) check set1 on record_samp.py: OK, passed
2023-07-29 11:27:01,564 TADA INFO assertion 33, (6th update; record unchanged) check set3_p on record_samp.py: OK, passed
2023-07-29 11:27:01,565 TADA INFO assertion 34, (6th update; record unchanged) check set3_c on record_samp.py: OK, passed
2023-07-29 11:27:01,565 __main__ INFO 6th update on the updator...
2023-07-29 11:27:08,774 TADA INFO assertion 35, (6th update; record unchanged) check set1 on record_agg.py: OK, passed
2023-07-29 11:27:08,774 TADA INFO assertion 36, (6th update; record unchanged) check set3_p on record_agg.py: OK, passed
2023-07-29 11:27:08,775 TADA INFO assertion 37, (6th update; record unchanged) check set3_c on record_agg.py: OK, passed
2023-07-29 11:27:08,775 TADA INFO test ldms_record_test ended
2023-07-29 11:27:19 INFO: ----------------------------------------------
2023-07-29 11:27:20 INFO: ======== ldms_schema_digest_test ========
2023-07-29 11:27:20 INFO: CMD: python3 ldms_schema_digest_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_schema_digest_test
2023-07-29 11:27:21,289 TADA INFO starting test `ldms_schema_digest_test`
2023-07-29 11:27:21,289 TADA INFO   test-id: d6f256799efee89ed2bc758925d63d6e2a0678cd2780f32b80a0a6142e94ff04
2023-07-29 11:27:21,289 TADA INFO   test-suite: LDMSD
2023-07-29 11:27:21,289 TADA INFO   test-name: ldms_schema_digest_test
2023-07-29 11:27:21,289 TADA INFO   test-user: narate
2023-07-29 11:27:21,289 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:27:21,290 __main__ INFO -- Get or create the cluster --
2023-07-29 11:27:28,329 __main__ INFO -- Start daemons --
2023-07-29 11:27:39,312 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:27:44,440 TADA INFO assertion 1, No schema digest from ldms_ls -v sampler: verified, passed
2023-07-29 11:27:44,562 TADA INFO assertion 2, Schema digest from ldms_ls -vv sampler is not empty: verified, passed
2023-07-29 11:27:44,678 TADA INFO assertion 3, Schema digest from ldms_ls -vv agg-1 is not empty: verified, passed
2023-07-29 11:27:44,861 TADA INFO assertion 4, Schema digest from Python ldms dir agg-1 is not empty: verified, passed
2023-07-29 11:27:44,862 TADA INFO assertion 5, Schema digest from Python ldms lokoup agg-1 is not empty: verified, passed
2023-07-29 11:27:44,862 TADA INFO assertion 6, All digests of the same set are the same: , passed
2023-07-29 11:27:47,274 TADA INFO assertion 7, Sets of same schema yield the same digest: check, passed
2023-07-29 11:27:47,274 TADA INFO assertion 8, Different schema (1-off metric) yield different digest: check, passed
2023-07-29 11:27:47,275 TADA INFO test ldms_schema_digest_test ended
2023-07-29 11:27:59 INFO: ----------------------------------------------
2023-07-29 11:28:00 INFO: ======== ldmsd_decomp_test ========
2023-07-29 11:28:00 INFO: CMD: python3 ldmsd_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_decomp_test
2023-07-29 11:28:00,844 TADA INFO starting test `ldmsd_decomp_test`
2023-07-29 11:28:00,845 TADA INFO   test-id: 34859828b4fa36cc13aa99896db039916d9a1acb055fc1a810732b588c610487
2023-07-29 11:28:00,845 TADA INFO   test-suite: LDMSD
2023-07-29 11:28:00,845 TADA INFO   test-name: ldmsd_decomp_test
2023-07-29 11:28:00,845 TADA INFO   test-user: narate
2023-07-29 11:28:00,845 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:28:00,846 __main__ INFO -- Get or create the cluster --
2023-07-29 11:28:16,483 __main__ INFO -- Start daemons --
2023-07-29 11:28:46,037 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:29:40,693 TADA INFO assertion 1, `as_is` decomposition, test_sampler_8d2b8bd sos schema check: OK, passed
2023-07-29 11:29:40,693 TADA INFO assertion 2, `as_is` decomposition, test_sampler_95772b6 sos schema check: OK, passed
2023-07-29 11:29:40,693 TADA INFO assertion 3, `as_is` decomposition, record_sampler_e1f021f sos schema check: OK, passed
2023-07-29 11:29:40,693 TADA INFO assertion 4, `static` decomposition, fill sos schema check: OK, passed
2023-07-29 11:29:40,693 TADA INFO assertion 5, `static` decomposition, filter sos schema check: OK, passed
2023-07-29 11:29:40,693 TADA INFO assertion 6, `static` decomposition, record sos schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 7, `as_is` decomposition, test_sampler_8d2b8bd csv schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 8, `as_is` decomposition, test_sampler_95772b6 csv schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 9, `as_is` decomposition, record_sampler_e1f021f csv schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 10, `static` decomposition, fill csv schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 11, `static` decomposition, filter csv schema check: OK, passed
2023-07-29 11:29:40,694 TADA INFO assertion 12, `static` decomposition, record csv schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 13, `as_is` decomposition, test_sampler_8d2b8bd kafka schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 14, `as_is` decomposition, test_sampler_95772b6 kafka schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 15, `as_is` decomposition, record_sampler_e1f021f kafka schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 16, `static` decomposition, fill kafka schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 17, `static` decomposition, filter kafka schema check: OK, passed
2023-07-29 11:29:40,695 TADA INFO assertion 18, `static` decomposition, record kafka schema check: OK, passed
2023-07-29 11:29:40,697 TADA INFO assertion 19, `as_is` decomposition, test_sampler_8d2b8bd sos data check: OK, passed
2023-07-29 11:29:40,699 TADA INFO assertion 20, `as_is` decomposition, test_sampler_95772b6 sos data check: OK, passed
2023-07-29 11:29:40,779 TADA INFO assertion 21, `as_is` decomposition, record_sampler_e1f021f sos data check: OK, passed
2023-07-29 11:29:40,783 TADA INFO assertion 22, `static` decomposition, fill sos data check: OK, passed
2023-07-29 11:29:40,787 TADA INFO assertion 23, `static` decomposition, filter sos data check: OK, passed
2023-07-29 11:29:40,797 TADA INFO assertion 24, `static` decomposition, record sos data check: OK, passed
2023-07-29 11:29:40,799 TADA INFO assertion 25, `as_is` decomposition, test_sampler_8d2b8bd csv data check: OK, passed
2023-07-29 11:29:40,800 TADA INFO assertion 26, `as_is` decomposition, test_sampler_95772b6 csv data check: OK, passed
2023-07-29 11:29:40,877 TADA INFO assertion 27, `as_is` decomposition, record_sampler_e1f021f csv data check: OK, passed
2023-07-29 11:29:40,881 TADA INFO assertion 28, `static` decomposition, fill csv data check: OK, passed
2023-07-29 11:29:40,885 TADA INFO assertion 29, `static` decomposition, filter csv data check: OK, passed
2023-07-29 11:29:40,895 TADA INFO assertion 30, `static` decomposition, record csv data check: OK, passed
2023-07-29 11:29:40,896 TADA INFO assertion 31, `as_is` decomposition, test_sampler_8d2b8bd kafka data check: OK, passed
2023-07-29 11:29:40,896 TADA INFO assertion 32, `as_is` decomposition, test_sampler_95772b6 kafka data check: OK, passed
2023-07-29 11:29:40,925 TADA INFO assertion 33, `as_is` decomposition, record_sampler_e1f021f kafka data check: OK, passed
2023-07-29 11:29:40,927 TADA INFO assertion 34, `static` decomposition, fill kafka data check: OK, passed
2023-07-29 11:29:40,929 TADA INFO assertion 35, `static` decomposition, filter kafka data check: OK, passed
2023-07-29 11:29:40,934 TADA INFO assertion 36, `static` decomposition, record kafka data check: OK, passed
2023-07-29 11:29:40,934 TADA INFO test ldmsd_decomp_test ended
2023-07-29 11:29:40,934 TADA INFO test ldmsd_decomp_test ended
2023-07-29 11:29:56 INFO: ----------------------------------------------
2023-07-29 11:29:57 INFO: ======== store_list_record_test ========
2023-07-29 11:29:57 INFO: CMD: python3 store_list_record_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/store_list_record_test
2023-07-29 11:29:57,840 __main__ INFO -- Get or create the cluster --
2023-07-29 11:29:57,840 TADA INFO starting test `store_sos_lists_test`
2023-07-29 11:29:57,840 TADA INFO   test-id: ea9008236173cec6e6d23f8eebb457812c0dfc451e0f786cef6a3bbc89686324
2023-07-29 11:29:57,841 TADA INFO   test-suite: LDMSD
2023-07-29 11:29:57,841 TADA INFO   test-name: store_sos_lists_test
2023-07-29 11:29:57,841 TADA INFO   test-user: narate
2023-07-29 11:29:57,841 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:30:05,182 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:30:20,730 __main__ INFO All sampler daemons are up.
2023-07-29 11:30:20,841 TADA INFO assertion 1, aggregator with store_sos has started properly.: agg_sos.check_ldmsd(), passed
2023-07-29 11:30:20,959 TADA INFO assertion 2, aggregator with store_csv has started properly.: agg_csv.check_ldmsd(), passed
2023-07-29 11:30:22,262 TADA INFO assertion 3, store_sos is storing data.: file_exists(a) for a in supported_schema, passed
2023-07-29 11:30:22,984 TADA INFO assertion 4, store_sos stores data correctly.: verify_data(db) for db in all_db, passed
2023-07-29 11:30:31,854 TADA INFO assertion 5, store_sos stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-07-29 11:30:32,498 TADA INFO assertion 6, store_sos reports multiple list errror messages resulted by the config file.: store_sos reported the multiple list error messages., passed
2023-07-29 11:30:37,273 TADA INFO assertion 7, store_sos reports multiple list errror messages resulted by ldmsd_controller.: store_sos reported the multiple list error messages., passed
2023-07-29 11:30:37,583 TADA INFO assertion 8, store_csv is storing data.: file_exists(a) for a in supported_schema, passed
2023-07-29 11:30:44,002 TADA INFO assertion 9, store_csv stores data correctly.: verify_data(db) for db in all_db, passed
2023-07-29 11:30:53,636 TADA INFO assertion 10, store_csv stores data after restarted correctly.: verify_data(db) for db in all_db, passed
2023-07-29 11:30:54,278 TADA INFO assertion 11, store_csv reports multiple list errror messages resulted by the config file.: store_csv reported the multiple list error messages., passed
2023-07-29 11:30:59,092 TADA INFO assertion 12, store_csv reports multiple list errror messages resulted by ldmsd_controller.: store_csv reported the multiple list error messages., passed
2023-07-29 11:30:59,092 TADA INFO test store_sos_lists_test ended
2023-07-29 11:31:11 INFO: ----------------------------------------------
2023-07-29 11:31:12 INFO: ======== maestro_raft_test ========
2023-07-29 11:31:12 INFO: CMD: python3 maestro_raft_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/maestro_raft_test
2023-07-29 11:31:12,973 TADA INFO starting test `maestro_raft_test`
2023-07-29 11:31:12,973 TADA INFO   test-id: 1164afad0fd3aa1b4dbf081930d6f1bd792aac5c68f5987b30b2fe154d9824ec
2023-07-29 11:31:12,973 TADA INFO   test-suite: LDMSD
2023-07-29 11:31:12,973 TADA INFO   test-name: maestro_raft_test
2023-07-29 11:31:12,974 TADA INFO   test-user: narate
2023-07-29 11:31:12,974 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:31:22,985 __main__ INFO -- Get or create cluster --
2023-07-29 11:31:56,625 __main__ INFO -- Start daemons --
2023-07-29 11:33:31,490 __main__ INFO -- making known hosts (ssh) --
2023-07-29 11:33:38,469 __main__ INFO ... make sure ldmsd's are up
2023-07-29 11:33:56,750 TADA INFO assertion 1, Statuses of maestros, 1 leader + 2 followers: [('FOLLOWER', 2), ('LEADER', 1)], passed
2023-07-29 11:34:09,171 TADA INFO assertion 2, All ldmsds are up and configured: sets verified, passed
2023-07-29 11:34:09,437 TADA INFO assertion 3, Data are being stored: data check, passed
2023-07-29 11:34:14,299 TADA INFO assertion 4, New leader elected: checked, passed
2023-07-29 11:34:49,180 TADA INFO assertion 5, Restarted ldmsd is configured: sets verified, passed
2023-07-29 11:34:49,493 TADA INFO assertion 6, New data are presented in the store: data check, passed
2023-07-29 11:35:00,481 TADA INFO assertion 7, The restarted maestro becomes a follower: checked, passed
---Wait for config to write to file---
2023-07-29 11:35:00,482 TADA INFO test maestro_raft_test ended
2023-07-29 11:35:21 INFO: ----------------------------------------------
2023-07-29 11:35:22 INFO: ======== ovis_json_test ========
2023-07-29 11:35:22 INFO: CMD: python3 ovis_json_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ovis_json_test
2023-07-29 11:35:23,117 __main__ INFO -- Create the cluster -- 
2023-07-29 11:35:28,433 TADA INFO starting test `ovis_json_test`
2023-07-29 11:35:28,433 TADA INFO   test-id: 9d742d6758d94d181dfc5490c5c4bdeb00b87ec71884ddb3b128a3bed21b058a
2023-07-29 11:35:28,433 TADA INFO   test-suite: OVIS-LIB
2023-07-29 11:35:28,433 TADA INFO   test-name: ovis_json_test
2023-07-29 11:35:28,433 TADA INFO   test-user: narate
2023-07-29 11:35:28,433 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:35:28,434 TADA INFO assertion 1, Test creating a JSON integer entity: (type is JSON_INT_VALUE) && (1 == e->value.int_), passed
2023-07-29 11:35:28,434 TADA INFO assertion 2, Test creating a JSON boolean entity: (type is JSON_BOOL_VALUE) && (1 == e->value.bool_), passed
2023-07-29 11:35:28,434 TADA INFO assertion 3, Test creating a JSON float entity: (type is JSON_FLOAT_VALUE) && (1.1 == e->value.double_), passed
2023-07-29 11:35:28,434 TADA INFO assertion 4, Test creating a JSON string entity: (type is JSON_STRING_VALUE) && (foo == e->value.str_->str), passed
2023-07-29 11:35:28,435 TADA INFO assertion 5, Test creating a JSON attribute entity: (type is JSON_ATTR_VALUE) && (name == <attr name>) && (value == <attr value>), passed
2023-07-29 11:35:28,435 TADA INFO assertion 6, Test creating a JSON list entity: (type is JSON_LIST_VALUE) && (0 == Number of elements) && (list is empty), passed
2023-07-29 11:35:28,435 TADA INFO assertion 7, Test creating a JSON dictionary entity: (type is JSON_DICT_VALUE) && (dict table is empty), passed
2023-07-29 11:35:28,435 TADA INFO assertion 8, Test creating a JSON null entity: (type is JSON_NULL_VALUE) && (0 == e->value.int_), passed
2023-07-29 11:35:28,435 TADA INFO assertion 9, Test parsing a JSON integer string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,435 TADA INFO assertion 10, Test parsing a JSON false boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,435 TADA INFO assertion 11, Test parsing a JSON true boolean string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 12, Test parsing a JSON float string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 13, Test parsing a JSON string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 15, Test parsing a JSON dict string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 16, Test parsing a JSON null string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 17, Test parsing an invalid string: (0 == json_parse_buffer()) && is_same_entity(expected, o), passed
2023-07-29 11:35:28,436 TADA INFO assertion 17, Test parsing an invalid string: 0 != json_parse_buffer(), passed
2023-07-29 11:35:28,437 TADA INFO assertion 18, Test dumping a JSON integer entity: 1 == 1, passed
2023-07-29 11:35:28,437 TADA INFO assertion 19, Test dumping a JSON false boolean entity: false == false, passed
2023-07-29 11:35:28,437 TADA INFO assertion 20, Test dumping a JSON true boolean entity: true == true, passed
2023-07-29 11:35:28,437 TADA INFO assertion 21, Test dumping a JSON float entity: 1.100000 == 1.100000, passed
2023-07-29 11:35:28,437 TADA INFO assertion 22, Test dumping a JSON string entity: "foo" == "foo", passed
2023-07-29 11:35:28,437 TADA INFO assertion 23, Test dumping a JSON attr entity: "name":"foo" == jb->buf, passed
2023-07-29 11:35:28,437 TADA INFO assertion 24, Test dumping a JSON list entity: [1,false,1.100000,"foo",[],{},null] == [1,false,1.100000,"foo",[],{},null], passed
2023-07-29 11:35:28,438 TADA INFO assertion 25, Test dumping a JSON dict entity: {"int":1,"bool":true,"float":1.100000,"string":"foo","list":[1,false,1.100000,"foo",[],{},null],"dict":{"attr_1":"value_1"},"null":null} == {"null":null,"list":[1,false,1.100000,"foo",[],{},null],"string":"foo","float":1.100000,"bool":true,"dict":{"attr_1":"value_1"},"int":1}, passed
2023-07-29 11:35:28,438 TADA INFO assertion 26, Test dumping a JSON null entity: null == null, passed
2023-07-29 11:35:28,438 TADA INFO assertion 27, Test dumping a JSON entity to a non-empty jbuf: This is a book."FOO" == This is a book."FOO", passed
2023-07-29 11:35:28,438 TADA INFO assertion 28, Test copying a JSON integer entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,438 TADA INFO assertion 29, Test copying a JSON false boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,438 TADA INFO assertion 30, Test copying a JSON true boolean entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,438 TADA INFO assertion 31, Test copying a JSON float entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 32, Test copying a JSON string entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 33, Test copying a JSON attribute entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 34, Test copying a JSON list entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 35, Test copying a JSON dict entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 36, Test copying a JSON null entity: is_same_entity(expected, json_entity_copy(expected), passed
2023-07-29 11:35:28,439 TADA INFO assertion 37, Test obtaining the number of attributes: 7 == json_attr_count(dict), passed
2023-07-29 11:35:28,439 TADA INFO assertion 38, Test finding an existing attribute: 0 != json_attr_find(), passed
2023-07-29 11:35:28,440 TADA INFO assertion 39, Test finding a non-existng attribute: 0 == json_attr_find(), passed
2023-07-29 11:35:28,440 TADA INFO assertion 40, Test finding the value of an existing attribute: 0 != json_value_find(), passed
2023-07-29 11:35:28,440 TADA INFO assertion 41, Test finding the value of a non-existing attribute: 0 == json_value_find(), passed
2023-07-29 11:35:28,440 TADA INFO assertion 42, Test adding a new attribute to a dictionary: (0 == json_attr_add() && (0 != json_attr_find()), passed
2023-07-29 11:35:28,440 TADA INFO assertion 43, Test replacing the value of an existing attribute: (0 == json_attr_add()) && (0 != json_value_find()) && (is_same_entity(old_v, new_v)), passed
2023-07-29 11:35:28,440 TADA INFO assertion 44, Test removing an existing attribute: (0 = json_attr_rem()) && (0 == json_attr_find()), passed
2023-07-29 11:35:28,441 TADA INFO assertion 45, Test removing a non-existing attribute: (ENOENT == json_attr_rem()), passed
2023-07-29 11:35:28,441 TADA INFO assertion 46, Test creating a dictionary by json_dict_build: expected == json_dict_build(...), passed
2023-07-29 11:35:28,441 TADA INFO assertion 47, Test adding attributes and replacing attribute values by json_dict_build: expected == json_dict_build(d, ...), passed
2023-07-29 11:35:28,441 TADA INFO assertion 48, Test json_dict_merge(): The merged dictionary is correct., passed
2023-07-29 11:35:28,441 TADA INFO assertion 49, Test json_list_len(): 7 == json_list_len(), passed
2023-07-29 11:35:28,441 TADA INFO assertion 50, Test adding items to a list: 0 == strcmp(exp_str, json_entity_dump(l)->buf, passed
2023-07-29 11:35:28,441 TADA INFO assertion 51, Test removing an existing item by json_item_rem(): 0 == json_item_rem(), passed
2023-07-29 11:35:28,442 TADA INFO assertion 52, Test removing a non-existing item by json_item_rem(): ENOENT == json_item_rem(), passed
2023-07-29 11:35:28,442 TADA INFO assertion 53, Test popping an existing item from a list by json_item_pop(): NULL == json_item_pop(len + 3), passed
2023-07-29 11:35:28,442 TADA INFO assertion 54, Test popping a non-existing item from a list by json_item_pop(): NULL != json_item_pop(len - 1), passed
2023-07-29 11:35:28,442 TADA INFO test ovis_json_test ended
2023-07-29 11:35:39 INFO: ----------------------------------------------
2023-07-29 11:35:40 INFO: ======== updtr_add_test ========
2023-07-29 11:35:40 INFO: CMD: python3 updtr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_add_test
2023-07-29 11:35:40,776 __main__ INFO -- Get or create the cluster --
2023-07-29 11:35:40,777 TADA INFO starting test `updtr_add test`
2023-07-29 11:35:40,777 TADA INFO   test-id: 6089005d47ccf20e97128d49c28e6c39d18aceaddeb2448de14b618b7d58643a
2023-07-29 11:35:40,777 TADA INFO   test-suite: LDMSD
2023-07-29 11:35:40,777 TADA INFO   test-name: updtr_add test
2023-07-29 11:35:40,777 TADA INFO   test-user: narate
2023-07-29 11:35:40,777 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:35:48,352 __main__ INFO -- Start daemons --
2023-07-29 11:36:03,786 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:36:04,109 __main__ INFO All LDMSDs are up.
2023-07-29 11:36:05,313 TADA INFO assertion 1, Add an updater with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:36:06,525 TADA INFO assertion 2, Add an updater with a zero interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:36:07,735 TADA INFO assertion 3, Add an updater with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:36:08,953 TADA INFO assertion 4, Add an updater with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:36:10,157 TADA INFO assertion 5, Add an updater with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:36:12,609 TADA INFO assertion 6, Add an updater without an offset: report(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'without_offset', 'interval': '1000000', 'offset': '0', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:36:15,050 TADA INFO assertion 7, Add an updater with a valid offset: report(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'with_offset', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:36:16,268 TADA INFO assertion 8, Add an updater with an existing name: report(rc = 17) == expect(rc = 17), passed
2023-07-29 11:36:16,268 __main__ INFO --- done ---
2023-07-29 11:36:16,269 TADA INFO test updtr_add test ended
2023-07-29 11:36:28 INFO: ----------------------------------------------
2023-07-29 11:36:29 INFO: ======== updtr_del_test ========
2023-07-29 11:36:29 INFO: CMD: python3 updtr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_del_test
2023-07-29 11:36:29,908 __main__ INFO -- Get or create the cluster --
2023-07-29 11:36:29,908 TADA INFO starting test `updtr_add test`
2023-07-29 11:36:29,908 TADA INFO   test-id: 328dd50d6d2c6d113cb89c22b943cafa7207620a14e93d63051f39f920b5a400
2023-07-29 11:36:29,908 TADA INFO   test-suite: LDMSD
2023-07-29 11:36:29,908 TADA INFO   test-name: updtr_add test
2023-07-29 11:36:29,908 TADA INFO   test-user: narate
2023-07-29 11:36:29,908 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:36:37,401 __main__ INFO -- Start daemons --
2023-07-29 11:36:52,861 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:36:53,159 __main__ INFO All LDMSDs are up.
2023-07-29 11:36:54,369 TADA INFO assertion 1, updtr_del a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:36:55,577 TADA INFO assertion 2, updtr_del a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-29 11:36:56,794 TADA INFO assertion 3, updtr_del a stopped updater: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:36:58,004 TADA INFO assertion 4, updtr_del a just-added updater: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:36:58,004 __main__ INFO --- done ---
2023-07-29 11:36:58,004 TADA INFO test updtr_add test ended
2023-07-29 11:37:09 INFO: ----------------------------------------------
2023-07-29 11:37:10 INFO: ======== updtr_match_add_test ========
2023-07-29 11:37:10 INFO: CMD: python3 updtr_match_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_match_add_test
2023-07-29 11:37:11,546 __main__ INFO -- Get or create the cluster --
2023-07-29 11:37:11,546 TADA INFO starting test `updtr_add test`
2023-07-29 11:37:11,546 TADA INFO   test-id: 971cccb69fd5487f5be3343a231f6aae41cffd093e522584d5784e4611c7f5a9
2023-07-29 11:37:11,546 TADA INFO   test-suite: LDMSD
2023-07-29 11:37:11,546 TADA INFO   test-name: updtr_add test
2023-07-29 11:37:11,546 TADA INFO   test-user: narate
2023-07-29 11:37:11,546 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:37:18,945 __main__ INFO -- Start daemons --
2023-07-29 11:37:34,417 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:37:34,734 __main__ INFO All LDMSDs are up.
2023-07-29 11:37:35,945 TADA INFO assertion 1, updtr_match_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:37:37,160 TADA INFO assertion 2, updtr_match_add with an invalid match: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:37:38,374 TADA INFO assertion 3, updtr_match_add of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:37:39,585 TADA INFO assertion 4, A success updtr_match_add: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:37:40,793 TADA INFO assertion 5, updtr_match_add of a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-29 11:37:40,793 __main__ INFO --- done ---
2023-07-29 11:37:40,793 TADA INFO test updtr_add test ended
2023-07-29 11:37:52 INFO: ----------------------------------------------
2023-07-29 11:37:53 INFO: ======== updtr_match_del_test ========
2023-07-29 11:37:53 INFO: CMD: python3 updtr_match_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_match_del_test
2023-07-29 11:37:54,361 __main__ INFO -- Get or create the cluster --
2023-07-29 11:37:54,361 TADA INFO starting test `updtr_add test`
2023-07-29 11:37:54,361 TADA INFO   test-id: 3c89e735ec0fc26d626ac42fb128dd0180532fb47e90155abdef235c2437717c
2023-07-29 11:37:54,362 TADA INFO   test-suite: LDMSD
2023-07-29 11:37:54,362 TADA INFO   test-name: updtr_add test
2023-07-29 11:37:54,362 TADA INFO   test-user: narate
2023-07-29 11:37:54,362 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:38:01,787 __main__ INFO -- Start daemons --
2023-07-29 11:38:17,207 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:38:17,534 __main__ INFO All LDMSDs are up.
2023-07-29 11:38:18,754 TADA INFO assertion 1, Send updtr_match_del with an invalid regex: report(rc = 2) == expect(rc = 22), passed
2023-07-29 11:38:19,981 TADA INFO assertion 2, Send updtr_match_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:38:21,196 TADA INFO assertion 3, Send updtr_match_del with a non-existing inst match: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:38:22,416 TADA INFO assertion 4, Send updtr_match_del with a non-existing schema match: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:38:23,636 TADA INFO assertion 5, Send updater_match_del with an invalid match type: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:38:24,853 TADA INFO assertion 6, Send updater_match_del with a valid regex of the inst type: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:38:26,077 TADA INFO assertion 7, Send updater_match_del with a valid regex of the schema type: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:38:26,077 __main__ INFO --- done ---
2023-07-29 11:38:26,077 TADA INFO test updtr_add test ended
2023-07-29 11:38:38 INFO: ----------------------------------------------
2023-07-29 11:38:39 INFO: ======== updtr_prdcr_add_test ========
2023-07-29 11:38:39 INFO: CMD: python3 updtr_prdcr_add_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_prdcr_add_test
2023-07-29 11:38:39,785 __main__ INFO -- Get or create the cluster --
2023-07-29 11:38:39,786 TADA INFO starting test `updtr_add test`
2023-07-29 11:38:39,786 TADA INFO   test-id: e31b0f973680bfd23322c449d680ff6746097c5b4bd2c2dd13e9421a33c27115
2023-07-29 11:38:39,786 TADA INFO   test-suite: LDMSD
2023-07-29 11:38:39,786 TADA INFO   test-name: updtr_add test
2023-07-29 11:38:39,786 TADA INFO   test-user: narate
2023-07-29 11:38:39,786 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:38:47,142 __main__ INFO -- Start daemons --
2023-07-29 11:39:02,541 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:39:02,868 __main__ INFO All LDMSDs are up.
2023-07-29 11:39:04,084 TADA INFO assertion 1, Send updtr_prdcr_add with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:39:06,497 TADA INFO assertion 2, Send updtr_prdcr_add with a regex matching no prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:39:08,925 TADA INFO assertion 3, Send updtr_prdcdr_add with a regex matching some prdcrs: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:39:10,145 TADA INFO assertion 4, Send updtr_prdcdr_add to a running updtr: report(rc = 16) == expect(rc = 16), passed
2023-07-29 11:39:11,359 TADA INFO assertion 5, Send updtr_prdcr_add to a not-existing updtr: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:39:11,359 __main__ INFO --- done ---
2023-07-29 11:39:11,360 TADA INFO test updtr_add test ended
2023-07-29 11:39:23 INFO: ----------------------------------------------
2023-07-29 11:39:24 INFO: ======== updtr_prdcr_del_test ========
2023-07-29 11:39:24 INFO: CMD: python3 updtr_prdcr_del_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_prdcr_del_test
2023-07-29 11:39:25,020 __main__ INFO -- Get or create the cluster --
2023-07-29 11:39:25,020 TADA INFO starting test `updtr_add test`
2023-07-29 11:39:25,020 TADA INFO   test-id: 649f596e3270e52f56c4442ffc25bd01cfe66566a0a3831da376c8f77c0fb19f
2023-07-29 11:39:25,020 TADA INFO   test-suite: LDMSD
2023-07-29 11:39:25,020 TADA INFO   test-name: updtr_add test
2023-07-29 11:39:25,020 TADA INFO   test-user: narate
2023-07-29 11:39:25,020 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:39:32,827 __main__ INFO -- Start daemons --
2023-07-29 11:39:48,174 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:39:48,480 __main__ INFO All LDMSDs are up.
2023-07-29 11:39:49,695 TADA INFO assertion 1, Send updtr_prdcr_del with an invalid regex: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:39:50,905 TADA INFO assertion 2, Send updtr_prdcr_del to a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-29 11:39:52,109 TADA INFO assertion 3, Send updtr_prdcr_del to a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:39:54,545 TADA INFO assertion 4, Send updtr_prdcr_del successfully: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-29 11:39:54,545 __main__ INFO --- done ---
2023-07-29 11:39:54,545 TADA INFO test updtr_add test ended
2023-07-29 11:40:06 INFO: ----------------------------------------------
2023-07-29 11:40:07 INFO: ======== updtr_start_test ========
2023-07-29 11:40:07 INFO: CMD: python3 updtr_start_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_start_test
2023-07-29 11:40:08,127 __main__ INFO -- Get or create the cluster --
2023-07-29 11:40:08,127 TADA INFO starting test `updtr_add test`
2023-07-29 11:40:08,128 TADA INFO   test-id: ff71cc0954eb632a0f6665f0b1c572b451ef64c11d9667b67fa61dfc06af7284
2023-07-29 11:40:08,128 TADA INFO   test-suite: LDMSD
2023-07-29 11:40:08,128 TADA INFO   test-name: updtr_add test
2023-07-29 11:40:08,128 TADA INFO   test-user: narate
2023-07-29 11:40:08,128 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:40:15,675 __main__ INFO -- Start daemons --
2023-07-29 11:40:31,061 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:40:31,385 __main__ INFO All LDMSDs are up.
2023-07-29 11:40:32,601 TADA INFO assertion 1, updtr_start with a negative interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:40:33,817 TADA INFO assertion 2, updtr_start with an alphabet interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:40:35,036 TADA INFO assertion 3, updtr_start with a negative offset: report(rc = 0) == expect(rc = 0), passed
2023-07-29 11:40:36,249 TADA INFO assertion 4, updtr_start with an alphabet offset: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:40:37,464 TADA INFO assertion 5, updtr_start without an offset larger than interval: report(rc = 22) == expect(rc = 22), passed
2023-07-29 11:40:39,881 TADA INFO assertion 6, updtr_start that changes offset to no offset: report(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'offset2none', 'interval': '1000000', 'offset': '0', 'sync': 'false', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-29 11:40:41,091 TADA INFO assertion 7, updtr_start of a non-existing updater: report(rc = 2) == expect(rc = 2), passed
2023-07-29 11:40:43,516 TADA INFO assertion 8, updtr_start with a valid interval: report(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_int', 'interval': '2000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-29 11:40:45,928 TADA INFO assertion 9, updtr_start with a valid offset: report(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'valid_offset', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-29 11:40:48,347 TADA INFO assertion 10, updtr_start without giving interval and offset: report(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}]) == expect(rc = 0, status = [{'name': 'all', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [], 'outstanding count': 0, 'oversampled count': 0}], passed
2023-07-29 11:40:49,567 TADA INFO assertion 11, updtr_start a running updater: report(rc = 16) == expect(rc = 16), passed
2023-07-29 11:40:49,567 __main__ INFO --- done ---
2023-07-29 11:40:49,567 TADA INFO test updtr_add test ended
2023-07-29 11:41:01 INFO: ----------------------------------------------
2023-07-29 11:41:02 INFO: ======== updtr_status_test ========
2023-07-29 11:41:02 INFO: CMD: python3 updtr_status_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/updtr_status_test
2023-07-29 11:41:03,150 __main__ INFO -- Get or create the cluster --
2023-07-29 11:41:03,150 TADA INFO starting test `updtr_status test`
2023-07-29 11:41:03,150 TADA INFO   test-id: dfe4092ee46498877eea9a720542f1f17c4a833796d4f78e74bdce9049c9bc14
2023-07-29 11:41:03,150 TADA INFO   test-suite: LDMSD
2023-07-29 11:41:03,150 TADA INFO   test-name: updtr_status test
2023-07-29 11:41:03,151 TADA INFO   test-user: narate
2023-07-29 11:41:03,151 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:41:12,909 __main__ INFO -- Start daemons --
2023-07-29 11:41:33,438 __main__ INFO Waiting ... for all LDMSDs to start
2023-07-29 11:41:33,839 __main__ INFO All LDMSDs are up.
2023-07-29 11:41:35,043 TADA INFO assertion 1, Send 'updtr_status' to an LDMSD without any Updaters: [], passed
2023-07-29 11:41:36,304 TADA INFO assertion 2, Send 'updtr_status name=foo', where updtr 'foo' doesn't exist.: report(updtr 'foo' doesn't exist.) == expect(updtr 'foo' doesn't exist.), passed
2023-07-29 11:41:37,519 TADA INFO assertion 3, Send 'updtr_status name=all', where 'all' exists.: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:41:38,737 TADA INFO assertion 4, Send 'updtr_status' to an LDMSD with a single Updater: report([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'agg11', 'interval': '1000000', 'offset': '200000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'agg11', 'host': 'L1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:41:39,949 TADA INFO assertion 5, Send 'updtr_status' to an LDMSD with 2 updaters: report([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]) == expect([{'name': 'meminfo', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'RUNNING', 'producers': [{'name': 'sampler-1', 'host': 'sampler-1', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}, {'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}, {'name': 'sampler-2', 'interval': '1000000', 'offset': '100000', 'sync': 'true', 'mode': 'Pull', 'auto': 'false', 'state': 'STOPPED', 'producers': [{'name': 'sampler-2', 'host': 'sampler-2', 'port': 10000, 'transport': 'sock', 'state': 'CONNECTED'}], 'outstanding count': 0, 'oversampled count': 0}]), passed
2023-07-29 11:41:39,949 __main__ INFO --- done ---
2023-07-29 11:41:39,949 TADA INFO test updtr_status test ended
2023-07-29 11:41:52 INFO: ----------------------------------------------
2023-07-29 11:41:53 INFO: ======== ldmsd_flex_decomp_test ========
2023-07-29 11:41:53 INFO: CMD: python3 ldmsd_flex_decomp_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_flex_decomp_test
2023-07-29 11:41:54,265 TADA INFO starting test `ldmsd_flex_decomp_test`
2023-07-29 11:41:54,265 TADA INFO   test-id: ef65938644ce8ca01a25880e7937dfc83b7f25abcf73d0d9f62832ac9647fad3
2023-07-29 11:41:54,265 TADA INFO   test-suite: LDMSD
2023-07-29 11:41:54,265 TADA INFO   test-name: ldmsd_flex_decomp_test
2023-07-29 11:41:54,266 TADA INFO   test-user: narate
2023-07-29 11:41:54,266 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:41:54,266 __main__ INFO -- Get or create the cluster --
2023-07-29 11:42:09,730 __main__ INFO -- Start daemons --
2023-07-29 11:42:39,145 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:43:28,272 TADA INFO assertion 1, test_sampler_95772b6 sos schema check: OK, passed
2023-07-29 11:43:28,272 TADA INFO assertion 2, record_sampler_e1f021f sos schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 3, fill sos schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 4, filter sos schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 5, record sos schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 6, test_sampler_95772b6 csv schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 7, record_sampler_e1f021f csv schema check: OK, passed
2023-07-29 11:43:28,273 TADA INFO assertion 8, fill csv schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 9, filter csv schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 10, record csv schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 11, test_sampler_95772b6 kafka schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 12, record_sampler_e1f021f kafka schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 13, fill kafka schema check: OK, passed
2023-07-29 11:43:28,274 TADA INFO assertion 14, filter kafka schema check: OK, passed
2023-07-29 11:43:28,275 TADA INFO assertion 15, record kafka schema check: OK, passed
2023-07-29 11:43:28,276 TADA INFO assertion 16, test_sampler_95772b6 sos data check: OK, passed
2023-07-29 11:43:28,350 TADA INFO assertion 17, record_sampler_e1f021f sos data check: OK, passed
2023-07-29 11:43:28,353 TADA INFO assertion 18, fill sos data check: OK, passed
2023-07-29 11:43:28,355 TADA INFO assertion 19, filter sos data check: OK, passed
2023-07-29 11:43:28,364 TADA INFO assertion 20, record sos data check: OK, passed
2023-07-29 11:43:28,365 TADA INFO assertion 21, test_sampler_95772b6 csv data check: OK, passed
2023-07-29 11:43:28,440 TADA INFO assertion 22, record_sampler_e1f021f csv data check: OK, passed
2023-07-29 11:43:28,443 TADA INFO assertion 23, fill csv data check: OK, passed
2023-07-29 11:43:28,445 TADA INFO assertion 24, filter csv data check: OK, passed
2023-07-29 11:43:28,454 TADA INFO assertion 25, record csv data check: OK, passed
2023-07-29 11:43:28,455 TADA INFO assertion 26, test_sampler_95772b6 kafka data check: OK, passed
2023-07-29 11:43:28,478 TADA INFO assertion 27, record_sampler_e1f021f kafka data check: OK, passed
2023-07-29 11:43:28,479 TADA INFO assertion 28, fill kafka data check: OK, passed
2023-07-29 11:43:28,480 TADA INFO assertion 29, filter kafka data check: OK, passed
2023-07-29 11:43:28,484 TADA INFO assertion 30, record kafka data check: OK, passed
2023-07-29 11:43:28,484 TADA INFO test ldmsd_flex_decomp_test ended
2023-07-29 11:43:28,484 TADA INFO test ldmsd_flex_decomp_test ended
2023-07-29 11:43:43 INFO: ----------------------------------------------
2023-07-29 11:43:44 INFO: ======== ldms_set_info_test ========
2023-07-29 11:43:44 INFO: CMD: python3 ldms_set_info_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_set_info_test
2023-07-29 11:43:55,184 TADA INFO starting test `ldms_set_info_test`
2023-07-29 11:43:55,184 TADA INFO   test-id: a4fbfc35eb3362a02ebdb3dce6470e3af20a4b0cd43286c53e26a35ad363710a
2023-07-29 11:43:55,184 TADA INFO   test-suite: LDMSD
2023-07-29 11:43:55,184 TADA INFO   test-name: ldms_set_info_test
2023-07-29 11:43:55,184 TADA INFO   test-user: narate
2023-07-29 11:43:55,184 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:43:55,185 TADA INFO assertion 1, Adding set info key value pairs : -, passed
2023-07-29 11:43:55,185 TADA INFO assertion 2, Reset value of an existing pair : -, passed
2023-07-29 11:43:55,185 TADA INFO assertion 3, Get a value : -, passed
2023-07-29 11:43:55,185 TADA INFO assertion 4, Unset a pair : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 5, Traverse the local set info : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 6, Verifying the set info at the 1st level : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 7, Server resetting a key : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 8, Server unset a key : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 9, Server add a key : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 10, Adding a key : -, passed
2023-07-29 11:43:55,186 TADA INFO assertion 11, Add a key that is already in the remote list : -, passed
2023-07-29 11:43:55,187 TADA INFO assertion 12, Unset a key that appears in both local and remote list : -, passed
2023-07-29 11:43:55,187 TADA INFO assertion 13, Verifying the set_info at the 2nd level : -, passed
2023-07-29 11:43:55,187 TADA INFO assertion 14, Test set info propagation: resetting a key on the set origin : -, passed
2023-07-29 11:43:55,187 TADA INFO assertion 15, Test set info propagation: unsetting a key on the set origin : -, passed
2023-07-29 11:43:55,187 TADA INFO assertion 16, Test set info propagation: adding a key on the set origin : -, passed
2023-07-29 11:43:55,188 TADA INFO test ldms_set_info_test ended
2023-07-29 11:44:05 INFO: ----------------------------------------------
2023-07-29 11:44:06 INFO: ======== slurm_sampler2_test ========
2023-07-29 11:44:06 INFO: CMD: python3 slurm_sampler2_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/slurm_sampler2_test
2023-07-29 11:44:07,506 TADA INFO starting test `slurm_sampler2_test`
2023-07-29 11:44:07,506 TADA INFO   test-id: e9fbed7f32ed2226b6d3b0ce23604fadd6bfa7f9435607f8264fe9721fde62ba
2023-07-29 11:44:07,506 TADA INFO   test-suite: LDMSD
2023-07-29 11:44:07,506 TADA INFO   test-name: slurm_sampler2_test
2023-07-29 11:44:07,507 TADA INFO   test-user: narate
2023-07-29 11:44:07,507 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:44:07,507 __main__ INFO -- Get or create the cluster --
2023-07-29 11:44:20,748 __main__ INFO -- Add users --
2023-07-29 11:44:25,838 __main__ INFO -- Preparing job script & programs --
2023-07-29 11:44:26,511 __main__ INFO -- Start daemons --
2023-07-29 11:45:07,630 TADA INFO assertion 1, Processing the stream data from slurm_notifier: [node-1]: The job_list is not as expected. {'job_id': 1, 'app_id': 0, 'user': 'root', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 1, 'job_size': 8, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 2} != {'job_id': 1, 'app_id': 0, 'user': 'root', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 8, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 2}, failed
Traceback (most recent call last):
  File "slurm_sampler2_test", line 855, in <module>
    "The metric values are as expected on all nodes." if passed else reason)
  File "/home/narate/cron/ldms-test/TADA.py", line 157, in assert_test
    raise AssertionException(self.test_desc + ", " + cond_str + ": FAILED")
TADA.AssertionException: Test the slurm_sampler2 plugin, [node-1]: The job_list is not as expected. {'job_id': 1, 'app_id': 0, 'user': 'root', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 1, 'job_size': 8, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 2} != {'job_id': 1, 'app_id': 0, 'user': 'root', 'job_name': 'job.sh', 'job_tag': '', 'job_state': 4, 'job_size': 8, 'job_uid': 0, 'job_gid': 0, 'job_start': None, 'job_end': None, 'node_count': 4, 'task_count': 2}: FAILED
2023-07-29 11:45:07,632 TADA INFO assertion 2.1, Deleting completed jobs -- job_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 3.1, Expanding the set heap -- job_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 4.1, Multi-tenant -- job_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 2.2, Deleting completed jobs -- step_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 3.2, Expanding the set heap -- step_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 4.2, Multi-tenant -- step_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 2.3, Deleting completed jobs -- task_init: skipped
2023-07-29 11:45:07,633 TADA INFO assertion 3.3, Expanding the set heap -- task_init: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 4.3, Multi-tenant -- task_init: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 2.4, Deleting completed jobs -- task_exit: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 3.4, Expanding the set heap -- task_exit: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 4.4, Multi-tenant -- task_exit: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 2.5, Deleting completed jobs -- job_exit: skipped
2023-07-29 11:45:07,634 TADA INFO assertion 3.5, Expanding the set heap -- job_exit: skipped
2023-07-29 11:45:07,635 TADA INFO assertion 4.5, Multi-tenant -- job_exit: skipped
2023-07-29 11:45:07,635 TADA INFO test slurm_sampler2_test ended
2023-07-29 11:45:21 INFO: ----------------------------------------------
2023-07-29 11:45:22 INFO: ======== run_inside_cont_test.py ========
2023-07-29 11:45:22 INFO: CMD: python3 run_inside_cont_test.py --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/run_inside_cont_test.py
2023-07-29 11:45:23,530 inside_cont_test INFO ===========================================================
2023-07-29 11:45:23,530 inside_cont_test INFO plugin_config_cmd: Start testing plugin_config_cmd
2023-07-29 11:45:23,532 TADA INFO starting test `plugin_config_cmd`
2023-07-29 11:45:23,532 TADA INFO   test-id: ff863645a608625aeb2e7993e297104e5cf81a429330709a6e9708bf18aa528d
2023-07-29 11:45:23,532 TADA INFO   test-suite: LDMSD
2023-07-29 11:45:23,532 TADA INFO   test-name: plugin_config_cmd
2023-07-29 11:45:23,532 TADA INFO   test-user: narate
2023-07-29 11:45:23,532 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:45:23,533 inside_cont_test INFO plugin_config_cmd: Preparing the containers
2023-07-29 11:45:32,913 inside_cont_test INFO plugin_config_cmd: Running the test script
2023-07-29 11:45:36,383 TADA INFO assertion status-1, Get the plugin statuses: status is as expected, passed
2023-07-29 11:45:36,384 TADA INFO assertion load-1, Load a non-existing plugin: resp['errcode'] (4294967295) != 0, passed
2023-07-29 11:45:36,384 TADA INFO assertion load-2, load a plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,384 TADA INFO assertion load-3, load a loadded plugin: resp['errcode'] (17) == 17, passed
2023-07-29 11:45:36,384 TADA INFO assertion config-1, Configure a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-07-29 11:45:36,384 TADA INFO assertion config-2, Misconfigure a loadded plugin: resp['errcode'] (22) != 0, passed
2023-07-29 11:45:36,385 TADA INFO assertion config-3, Correctly configure a loaded plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,385 TADA INFO assertion start-1, Start a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-07-29 11:45:36,385 TADA INFO assertion start-2, Start a store plugin: resp['errcode'] (22) == 22, passed
2023-07-29 11:45:36,385 TADA INFO assertion start-4, Start a sampler plugin using a negative interval: resp['errcode'] (22) == 22, passed
2023-07-29 11:45:36,385 TADA INFO assertion start-5, Start a sampler plugin without an offset: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,385 TADA INFO assertion start-3, Start a running sampler plugin: resp['errcode'] (16) == 16, passed
2023-07-29 11:45:36,386 TADA INFO assertion start-6, Start a sampler plugin with an offset larger than half of interval: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,386 TADA INFO assertion start-7, Start a sampler plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,386 TADA INFO assertion start-8, Check the status of the plugins: status is as expected, passed
2023-07-29 11:45:36,386 TADA INFO assertion stop-1, Stop a p lugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-07-29 11:45:36,386 TADA INFO assertion stop-2, Stop a sampler plugin that is not running: resp['errcode'] (22) != 0, passed
2023-07-29 11:45:36,386 TADA INFO assertion stop-3, Stop a running sampler plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,387 TADA INFO assertion stop-4, Check the status of the plugins: status is as expected, passed
2023-07-29 11:45:36,387 TADA INFO assertion term-1, Terminate a plugin that hasn't been loaded: resp['errcode'] (2) == 2, passed
2023-07-29 11:45:36,387 TADA INFO assertion term-2, Terminate a running sampler plugin: resp['errcode'] (22) == 22, passed
2023-07-29 11:45:36,387 TADA INFO assertion term-3, Terminate an in-used store plugin: resp['errcode'] (22) == 22, passed
2023-07-29 11:45:36,387 TADA INFO assertion term-4, Terminate a sampler plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,387 TADA INFO assertion term-5, Terminate a store plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:45:36,388 TADA INFO assertion term-6, Check the status of the plugins: status is as expected, passed
2023-07-29 11:45:36,388 TADA INFO test plugin_config_cmd ended
2023-07-29 11:45:36,388 inside_cont_test INFO plugin_config_cmd: done
2023-07-29 11:45:37,245 inside_cont_test INFO ===========================================================
2023-07-29 11:45:37,245 inside_cont_test INFO prdcr_config_cmd: Start testing prdcr_config_cmd
2023-07-29 11:45:37,247 TADA INFO starting test `prdcr_config_cmd`
2023-07-29 11:45:37,247 TADA INFO   test-id: 7de1a62e861149663a172d3985818cd5126b84fe7c6590062580df0ef5a33fff
2023-07-29 11:45:37,247 TADA INFO   test-suite: LDMSD
2023-07-29 11:45:37,247 TADA INFO   test-name: prdcr_config_cmd
2023-07-29 11:45:37,247 TADA INFO   test-user: narate
2023-07-29 11:45:37,247 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:45:37,247 inside_cont_test INFO prdcr_config_cmd: Preparing the containers
2023-07-29 11:46:43,721 inside_cont_test INFO prdcr_config_cmd: Running the test script
2023-07-29 11:46:52,249 TADA INFO assertion status-1, LDMSD has no producers.: len(status) == 0, passed
2023-07-29 11:46:52,250 TADA INFO assertion status-2, Get prdcr_status of a non-existing producer.: resp['errcode'] (2) == 2, passed
2023-07-29 11:46:52,250 TADA INFO assertion status-3, Get the result of a single producer: status is as expected, passed
2023-07-29 11:46:52,250 TADA INFO assertion status-4, Get the result of a single producer with sets: status is as expected, passed
2023-07-29 11:46:52,250 TADA INFO assertion status-5, Get the result of a passive producer: status is as expected, passed
2023-07-29 11:46:52,250 TADA INFO assertion status-6, Get the results of two producers: status is as expected, passed
2023-07-29 11:46:52,250 TADA INFO assertion add-1, prdcr_add an active producer: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-2, prdcr_add a passive producer: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-3, prdcr_add with a string interval: resp['errcode'] (22) == 22, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-4, prdcr_add with a negative reconnect: resp['errcode'] (22) == 22, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-5, prdcr_add with zero reconnect: resp['errcode'] (22) == 22, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-6, prdcr_add with an invalid type: resp['errcode'] (22) == 22, passed
2023-07-29 11:46:52,251 TADA INFO assertion add-7, prdcr_add with a negative port: resp['errcode'] (22) == 22, passed
2023-07-29 11:46:52,252 TADA INFO assertion add-8, prdcr_add with a non-existing host: resp['errcode'] (97) == 97, passed
2023-07-29 11:46:52,252 TADA INFO assertion add-9, prdcr_add an existing producer: resp['errcode'] (17) == 17, passed
2023-07-29 11:46:52,252 TADA INFO assertion add-10, prdcr_add using the interval attribute: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,252 TADA INFO assertion start-1, prdcr_start a non-existing producer: resp['errcode'] (2) == 2, passed
2023-07-29 11:46:52,252 TADA INFO assertion start-2.1, prdcr_start a stopped producer -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,252 TADA INFO assertion start-2.2, prdcdr_start a stopped producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,253 TADA INFO assertion start-3.1, prdcr_start a running producer -- checking the errcode: resp['errcode'] (16) == 16, passed
2023-07-29 11:46:52,253 TADA INFO assertion start-3.2, prdcr_start a running producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,253 TADA INFO assertion start_regex-1, prdcr_start_regex using an invalid regex: resp['errcode'] (2) != 0, passed
2023-07-29 11:46:52,253 TADA INFO assertion start_regex-2.1, prdcr_start_regex matching no producers -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,253 TADA INFO assertion start_regex-2.2, prdcr_start_regex matching no producers -- checking the statuses: status is as expected, passed
2023-07-29 11:46:52,253 TADA INFO assertion start_regex-3.1, prdcr_start_regex matching running producers -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,254 TADA INFO assertion start_regex-3.2, prdcr_start_regex matching running producers -- checking the statuses: status is as expected, passed
2023-07-29 11:46:52,254 TADA INFO assertion stop-1, prdcr_stop a non-existing producer: resp['errcode'] (2) == 2, passed
2023-07-29 11:46:52,254 TADA INFO assertion stop-2, prdcr_stop a never-started producer: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,254 TADA INFO assertion stop-3.1, prdcr_stop a connected producer -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,254 TADA INFO assertion stop-3.2, prdcr_stop a connected producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,254 TADA INFO assertion stop-4, prdcr_stop a stopped producer: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,255 TADA INFO assertion stop_regex-1, prdcr_stop_regex using an invalid regex: resp['errcode'] (2) != 0, passed
2023-07-29 11:46:52,255 TADA INFO assertion stop_regex-2.1, prdcr_stop_regex matching no producers -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,255 TADA INFO assertion stop_regex-2.2, prdcr_stop_regex matching no producers -- checking the status: status is as expected, passed
2023-07-29 11:46:52,255 TADA INFO assertion stop_regex-3.1, prdcr_stop_regex matching a running producer -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,255 TADA INFO assertion stop_regex-3.2, prdcr_stop_regex matching a running producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,255 TADA INFO assertion del-1, prdcr_del a non-existing producer: resp['errcode'] (2) == 2, passed
2023-07-29 11:46:52,256 TADA INFO assertion del-2.1, prdcr_del a stopped producer -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:46:52,256 TADA INFO assertion del-2.2, prdcr_del a stopped producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,256 TADA INFO assertion del-3.1, prdcr_del a running producer -- checking the errcode: resp['errcode'] (16) == 16, passed
2023-07-29 11:46:52,256 TADA INFO assertion del-3.2, prdcr_del a running producer -- checking the status: status is as expected, passed
2023-07-29 11:46:52,256 TADA INFO assertion start-4.1, prdcr_start a passive producer -- checking the errcode: skipped
2023-07-29 11:46:52,257 TADA INFO assertion start-4.2, prdcr_start a passive producer -- checking the status: skipped
2023-07-29 11:46:52,257 TADA INFO test prdcr_config_cmd ended
2023-07-29 11:46:52,257 inside_cont_test INFO prdcr_config_cmd: done
2023-07-29 11:46:58,393 inside_cont_test INFO ===========================================================
2023-07-29 11:46:58,394 inside_cont_test INFO strgp_config_cmd: Start testing strgp_config_cmd
2023-07-29 11:46:58,395 TADA INFO starting test `strgp_config_cmd`
2023-07-29 11:46:58,395 TADA INFO   test-id: bd78c3cc31990c3afa2d04ef3bb9d66359eb3250989ca53ba315bb429f7ed4cb
2023-07-29 11:46:58,396 TADA INFO   test-suite: LDMSD
2023-07-29 11:46:58,396 TADA INFO   test-name: strgp_config_cmd
2023-07-29 11:46:58,396 TADA INFO   test-user: narate
2023-07-29 11:46:58,396 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:46:58,396 inside_cont_test INFO strgp_config_cmd: Preparing the containers
2023-07-29 11:48:20,124 inside_cont_test INFO strgp_config_cmd: Running the test script
2023-07-29 11:48:23,622 TADA INFO assertion status-1, LDMSD has no storage policies: len(status) == 0, passed
2023-07-29 11:48:23,622 TADA INFO assertion status-2, Get the status of a non-existing storage policy: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,623 TADA INFO assertion status-3, Get the status of a storage policy with a single producer: status is as expected, passed
2023-07-29 11:48:23,623 TADA INFO assertion status-4, Get the status of a storage policy with a single metric: status is as expected, passed
2023-07-29 11:48:23,623 TADA INFO assertion status-5, Sending strgp_status with no attributes: status is as expected, passed
2023-07-29 11:48:23,623 TADA INFO assertion status-6, Get the status of a stopped storage policy: status is as expected, passed
2023-07-29 11:48:23,623 TADA INFO assertion add-1.1, Add a new strgp -- checking the error code: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,623 TADA INFO assertion add-1.2, Add a new strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,624 TADA INFO assertion add-2, Add an existing strgp: resp['errcode'] (17) == 17, passed
2023-07-29 11:48:23,624 TADA INFO assertion prdcr_add-1, strgp_prdcr_add with an invalid regex: resp['errcode'] (2) != 0, passed
2023-07-29 11:48:23,624 TADA INFO assertion prdcr_add-2, strgp_prdcr_add to a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,624 TADA INFO assertion prdcr_add-3, strgp_prdcr_add to a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,624 TADA INFO assertion prdcr_add-4.1, strgp_prdcr_add to a strgp -- checking the error code: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,624 TADA INFO assertion prdcr_add-4.2, strgp_prdcr_add to a strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,625 TADA INFO assertion metric_add-1, strgp_metric_add to a non existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,625 TADA INFO assertion metric_add-2, strgp_metric_add to a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,625 TADA INFO assertion metric_add-3.1, strgp_metric_add to a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,625 TADA INFO assertion metric_add-3.2, strgp_metric_add to a stopped strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,625 TADA INFO assertion start-1, strgp_start a non existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,625 TADA INFO assertion start-2, strgp_start a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-3, strgp_start a strgp with a non-configured plugin: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-4.1, strgp_start a strgp with a producer filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-4.2, strgp_start a strgp with a producer filter -- checking the status: status is as expected, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-5.1, strgp_start a strgp with a metric filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-5.2, strgp_start a strgp with a metric filter -- checking the status: status is as expected, passed
2023-07-29 11:48:23,626 TADA INFO assertion start-6.1, strgp_start a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,627 TADA INFO assertion start-6.2, strgp_start a stopped strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,627 TADA INFO assertion start-6.3, strgp_start a stopped strgp -- checking the database: Database is not empty., passed
2023-07-29 11:48:23,627 TADA INFO assertion prdcr_del-1, strgp_prdcr_del a non existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,627 TADA INFO assertion prdcr_del-2, strgp_prdcr_del a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,627 TADA INFO assertion prdcr_del-3, strgp_prdcr_del a strgp that doesn't have the prdcr regex: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,627 TADA INFO assertion prdcr_del-4.1, strgp_prdcr_del a strgp with a producer filter -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,628 TADA INFO assertion prdcr_del-4.2, strgp_prdcr_del a strgp with a producer filter -- checking the status: status is as expected, passed
2023-07-29 11:48:23,628 TADA INFO assertion metric_del-1, strgp_metric_del a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,628 TADA INFO assertion metric_del-2, strgp_metric_del a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,628 TADA INFO assertion metric_del-3, strgp_metric_del a strgp that doesn't contain the metric name: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,628 TADA INFO assertion metric_del-4.1, strgp_metric_del from a strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,628 TADA INFO assertion metric_del-4.2, strgp_metric_del from a strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,629 TADA INFO assertion stop-1, strgp_stop a non existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,629 TADA INFO assertion stop-2, strgp_stop a stopped strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,629 TADA INFO assertion stop-3.1, strgp_stop a running strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,629 TADA INFO assertion stop-3.2, strgp_stop a running strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,629 TADA INFO assertion del-1, Delete a non-existing strgp: resp['errcode'] (2) == 2, passed
2023-07-29 11:48:23,629 TADA INFO assertion del-2, Delete a running strgp: resp['errcode'] (16) == 16, passed
2023-07-29 11:48:23,630 TADA INFO assertion del-3.1, Delete a stopped strgp -- checking the errcode: resp['errcode'] (0) == 0, passed
2023-07-29 11:48:23,630 TADA INFO assertion del-3.2, Delete a stopped strgp -- checking the status: status is as expected, passed
2023-07-29 11:48:23,630 TADA INFO test strgp_config_cmd ended
2023-07-29 11:48:23,630 inside_cont_test INFO strgp_config_cmd: done
2023-07-29 11:48:41 INFO: ----------------------------------------------
2023-07-29 11:48:42 INFO: ======== libovis_log_test ========
2023-07-29 11:48:42 INFO: CMD: python3 libovis_log_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/libovis_log_test
2023-07-29 11:48:43,103 TADA INFO starting test `libovis_log_test`
2023-07-29 11:48:43,103 TADA INFO   test-id: f7a0a218f49aa23c562620719dc4c638d24d29ec427947a07021a62ab0973deb
2023-07-29 11:48:43,103 TADA INFO   test-suite: LDMSD
2023-07-29 11:48:43,104 TADA INFO   test-name: libovis_log_test
2023-07-29 11:48:43,104 TADA INFO   test-user: narate
2023-07-29 11:48:43,104 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:48:43,104 __main__ INFO -- Create the cluster -- 
2023-07-29 11:48:47,885 __main__ INFO -- Start daemons --
2023-07-29 11:48:50,065 TADA INFO assertion 1, Call ovis_log_init() with valid arguments: 'return_code=0' and 'liovis_log_test' in 'Sat Jul 29 11:48:49 2023:         : libovis_log_test: return_code=0
', passed
2023-07-29 11:48:51,176 TADA INFO assertion 2, Call ovis_log_init() with name = NULL: ('return_code=0' and ': :') in 'Sat Jul 29 11:48:50 2023:         : : return_code=0
', passed
2023-07-29 11:48:52,304 TADA INFO assertion 3, Call ovis_log_init() with an invalid level: 'return_code=22' in 'Sat Jul 29 11:48:51 2023:         : : return_code=22
', passed
2023-07-29 11:48:53,413 TADA INFO assertion 4, Call ovis_log_init() with an invalid mode: 'return_code=22' in 'Sat Jul 29 11:48:52 2023:         : : return_code=22
', passed
2023-07-29 11:48:54,044 TADA INFO assertion 6, Log messages to a file: 0 == ovis_log_open(/var/log/6.log) # (0), passed
2023-07-29 11:48:55,155 TADA INFO assertion 5, Log messages to stdout: 'return_code=0' in 'Sat Jul 29 11:48:54 2023:         : : return_code=0
', passed
2023-07-29 11:48:55,263 TADA INFO assertion 7, Open the log file at a non-existing path: 'Could not open the log file' in 'Sat Jul 29 11:48:55 2023:         : test: result=0
Sat Jul 29 11:48:55 2023:    ERROR: test: Could not open the log file named '/data/log/foo/7.log'
Sat Jul 29 11:48:55 2023:    ERROR: test: Failed to open the log file at /data/log/foo/7.log. Error 22
', passed
2023-07-29 11:48:55,779 TADA INFO assertion 8, Reopen the log file at another path: ovis_log_open() closes and opens the second path successfully, passed
2023-07-29 11:48:56,229 TADA INFO assertion 9, Convert 'DEBUG,INFO' integer to a string: DEBUG,INFO == DEBUG,INFO (expected), passed
2023-07-29 11:48:56,331 TADA INFO assertion 10, Convert 'DEBUG,WARNING' integer to a string: DEBUG,WARNING == DEBUG,WARNING (expected), passed
2023-07-29 11:48:56,449 TADA INFO assertion 11, Convert 'DEBUG,ERROR' integer to a string: DEBUG,ERROR == DEBUG,ERROR (expected), passed
2023-07-29 11:48:56,570 TADA INFO assertion 12, Convert 'DEBUG,CRITICAL' integer to a string: DEBUG,CRITICAL == DEBUG,CRITICAL (expected), passed
2023-07-29 11:48:56,681 TADA INFO assertion 13, Convert 'INFO,WARNING' integer to a string: INFO,WARNING == INFO,WARNING (expected), passed
2023-07-29 11:48:56,791 TADA INFO assertion 14, Convert 'INFO,ERROR' integer to a string: INFO,ERROR == INFO,ERROR (expected), passed
2023-07-29 11:48:56,904 TADA INFO assertion 15, Convert 'INFO,CRITICAL' integer to a string: INFO,CRITICAL == INFO,CRITICAL (expected), passed
2023-07-29 11:48:57,015 TADA INFO assertion 16, Convert 'WARNING,ERROR' integer to a string: WARNING,ERROR == WARNING,ERROR (expected), passed
2023-07-29 11:48:57,126 TADA INFO assertion 17, Convert 'WARNING,CRITICAL' integer to a string: WARNING,CRITICAL == WARNING,CRITICAL (expected), passed
2023-07-29 11:48:57,227 TADA INFO assertion 18, Convert 'ERROR,CRITICAL' integer to a string: ERROR,CRITICAL == ERROR,CRITICAL (expected), passed
2023-07-29 11:48:57,342 TADA INFO assertion 19, Convert 'DEBUG,INFO,WARNING' integer to a string: DEBUG,INFO,WARNING == DEBUG,INFO,WARNING (expected), passed
2023-07-29 11:48:57,447 TADA INFO assertion 20, Convert 'DEBUG,INFO,ERROR' integer to a string: DEBUG,INFO,ERROR == DEBUG,INFO,ERROR (expected), passed
2023-07-29 11:48:57,567 TADA INFO assertion 21, Convert 'DEBUG,INFO,CRITICAL' integer to a string: DEBUG,INFO,CRITICAL == DEBUG,INFO,CRITICAL (expected), passed
2023-07-29 11:48:57,683 TADA INFO assertion 22, Convert 'DEBUG,WARNING,ERROR' integer to a string: DEBUG,WARNING,ERROR == DEBUG,WARNING,ERROR (expected), passed
2023-07-29 11:48:57,793 TADA INFO assertion 23, Convert 'DEBUG,WARNING,CRITICAL' integer to a string: DEBUG,WARNING,CRITICAL == DEBUG,WARNING,CRITICAL (expected), passed
2023-07-29 11:48:57,895 TADA INFO assertion 24, Convert 'DEBUG,ERROR,CRITICAL' integer to a string: DEBUG,ERROR,CRITICAL == DEBUG,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:58,014 TADA INFO assertion 25, Convert 'INFO,WARNING,ERROR' integer to a string: INFO,WARNING,ERROR == INFO,WARNING,ERROR (expected), passed
2023-07-29 11:48:58,133 TADA INFO assertion 26, Convert 'INFO,WARNING,CRITICAL' integer to a string: INFO,WARNING,CRITICAL == INFO,WARNING,CRITICAL (expected), passed
2023-07-29 11:48:58,246 TADA INFO assertion 27, Convert 'INFO,ERROR,CRITICAL' integer to a string: INFO,ERROR,CRITICAL == INFO,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:58,350 TADA INFO assertion 28, Convert 'WARNING,ERROR,CRITICAL' integer to a string: WARNING,ERROR,CRITICAL == WARNING,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:58,465 TADA INFO assertion 29, Convert 'DEBUG,INFO,WARNING,ERROR' integer to a string: DEBUG,INFO,WARNING,ERROR == DEBUG,INFO,WARNING,ERROR (expected), passed
2023-07-29 11:48:58,563 TADA INFO assertion 30, Convert 'DEBUG,INFO,WARNING,CRITICAL' integer to a string: DEBUG,INFO,WARNING,CRITICAL == DEBUG,INFO,WARNING,CRITICAL (expected), passed
2023-07-29 11:48:58,694 TADA INFO assertion 31, Convert 'DEBUG,INFO,ERROR,CRITICAL' integer to a string: DEBUG,INFO,ERROR,CRITICAL == DEBUG,INFO,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:58,803 TADA INFO assertion 32, Convert 'DEBUG,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,WARNING,ERROR,CRITICAL == DEBUG,WARNING,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:58,915 TADA INFO assertion 33, Convert 'INFO,WARNING,ERROR,CRITICAL' integer to a string: INFO,WARNING,ERROR,CRITICAL == INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:59,026 TADA INFO assertion 34, Convert 'DEBUG,INFO,WARNING,ERROR,CRITICAL' integer to a string: DEBUG,INFO,WARNING,ERROR,CRITICAL == DEBUG,INFO,WARNING,ERROR,CRITICAL (expected), passed
2023-07-29 11:48:59,138 TADA INFO assertion 35, Convert 'DEBUG,' integer to a string: DEBUG, == DEBUG, (expected), passed
2023-07-29 11:48:59,250 TADA INFO assertion 36, Convert 'INFO,' integer to a string: INFO, == INFO, (expected), passed
2023-07-29 11:48:59,348 TADA INFO assertion 37, Convert 'WARNING,' integer to a string: WARNING, == WARNING, (expected), passed
2023-07-29 11:48:59,456 TADA INFO assertion 38, Convert 'ERROR,' integer to a string: ERROR, == ERROR, (expected), passed
2023-07-29 11:48:59,581 TADA INFO assertion 39, Convert 'CRITICAL,' integer to a string: CRITICAL, == CRITICAL, (expected), passed
2023-07-29 11:48:59,696 TADA INFO assertion 40, Convert an invalid integer to a level string: (null) == (null) (expected), passed
2023-07-29 11:48:59,802 TADA INFO assertion 41, Convert the 'DEBUG,INFO' to an integer: 3 == 3 (expected), passed
2023-07-29 11:48:59,917 TADA INFO assertion 42, Convert the 'DEBUG,WARNING' to an integer: 5 == 5 (expected), passed
2023-07-29 11:49:00,025 TADA INFO assertion 43, Convert the 'DEBUG,ERROR' to an integer: 9 == 9 (expected), passed
2023-07-29 11:49:00,125 TADA INFO assertion 44, Convert the 'DEBUG,CRITICAL' to an integer: 17 == 17 (expected), passed
2023-07-29 11:49:00,219 TADA INFO assertion 45, Convert the 'INFO,WARNING' to an integer: 6 == 6 (expected), passed
2023-07-29 11:49:00,331 TADA INFO assertion 46, Convert the 'INFO,ERROR' to an integer: 10 == 10 (expected), passed
2023-07-29 11:49:00,451 TADA INFO assertion 47, Convert the 'INFO,CRITICAL' to an integer: 18 == 18 (expected), passed
2023-07-29 11:49:00,572 TADA INFO assertion 48, Convert the 'WARNING,ERROR' to an integer: 12 == 12 (expected), passed
2023-07-29 11:49:00,680 TADA INFO assertion 49, Convert the 'WARNING,CRITICAL' to an integer: 20 == 20 (expected), passed
2023-07-29 11:49:00,783 TADA INFO assertion 50, Convert the 'ERROR,CRITICAL' to an integer: 24 == 24 (expected), passed
2023-07-29 11:49:00,895 TADA INFO assertion 51, Convert the 'DEBUG,INFO,WARNING' to an integer: 7 == 7 (expected), passed
2023-07-29 11:49:00,998 TADA INFO assertion 52, Convert the 'DEBUG,INFO,ERROR' to an integer: 11 == 11 (expected), passed
2023-07-29 11:49:01,104 TADA INFO assertion 53, Convert the 'DEBUG,INFO,CRITICAL' to an integer: 19 == 19 (expected), passed
2023-07-29 11:49:01,207 TADA INFO assertion 54, Convert the 'DEBUG,WARNING,ERROR' to an integer: 13 == 13 (expected), passed
2023-07-29 11:49:01,318 TADA INFO assertion 55, Convert the 'DEBUG,WARNING,CRITICAL' to an integer: 21 == 21 (expected), passed
2023-07-29 11:49:01,415 TADA INFO assertion 56, Convert the 'DEBUG,ERROR,CRITICAL' to an integer: 25 == 25 (expected), passed
2023-07-29 11:49:01,529 TADA INFO assertion 57, Convert the 'INFO,WARNING,ERROR' to an integer: 14 == 14 (expected), passed
2023-07-29 11:49:01,639 TADA INFO assertion 58, Convert the 'INFO,WARNING,CRITICAL' to an integer: 22 == 22 (expected), passed
2023-07-29 11:49:01,749 TADA INFO assertion 59, Convert the 'INFO,ERROR,CRITICAL' to an integer: 26 == 26 (expected), passed
2023-07-29 11:49:01,871 TADA INFO assertion 60, Convert the 'WARNING,ERROR,CRITICAL' to an integer: 28 == 28 (expected), passed
2023-07-29 11:49:01,983 TADA INFO assertion 61, Convert the 'DEBUG,INFO,WARNING,ERROR' to an integer: 15 == 15 (expected), passed
2023-07-29 11:49:02,099 TADA INFO assertion 62, Convert the 'DEBUG,INFO,WARNING,CRITICAL' to an integer: 23 == 23 (expected), passed
2023-07-29 11:49:02,217 TADA INFO assertion 63, Convert the 'DEBUG,INFO,ERROR,CRITICAL' to an integer: 27 == 27 (expected), passed
2023-07-29 11:49:02,338 TADA INFO assertion 64, Convert the 'DEBUG,WARNING,ERROR,CRITICAL' to an integer: 29 == 29 (expected), passed
2023-07-29 11:49:02,438 TADA INFO assertion 65, Convert the 'INFO,WARNING,ERROR,CRITICAL' to an integer: 30 == 30 (expected), passed
2023-07-29 11:49:02,545 TADA INFO assertion 66, Convert the 'DEBUG,INFO,WARNING,ERROR,CRITICAL' to an integer: 31 == 31 (expected), passed
2023-07-29 11:49:02,672 TADA INFO assertion 67, Convert the 'DEBUG,' to an integer: 1 == 1 (expected), passed
2023-07-29 11:49:02,774 TADA INFO assertion 68, Convert the 'INFO,' to an integer: 2 == 2 (expected), passed
2023-07-29 11:49:02,882 TADA INFO assertion 69, Convert the 'WARNING,' to an integer: 4 == 4 (expected), passed
2023-07-29 11:49:02,981 TADA INFO assertion 70, Convert the 'ERROR,' to an integer: 8 == 8 (expected), passed
2023-07-29 11:49:03,082 TADA INFO assertion 71, Convert the 'CRITICAL,' to an integer: 16 == 16 (expected), passed
2023-07-29 11:49:03,195 TADA INFO assertion 72, Convert the 'DEBUG' to an integer: 31 == 31 (expected), passed
2023-07-29 11:49:03,295 TADA INFO assertion 73, Convert the 'INFO' to an integer: 30 == 30 (expected), passed
2023-07-29 11:49:03,405 TADA INFO assertion 74, Convert the 'WARNING' to an integer: 28 == 28 (expected), passed
2023-07-29 11:49:03,503 TADA INFO assertion 75, Convert the 'ERROR' to an integer: 24 == 24 (expected), passed
2023-07-29 11:49:03,604 TADA INFO assertion 76, Convert the 'CRITICAL' to an integer: 16 == 16 (expected), passed
2023-07-29 11:49:03,715 TADA INFO assertion 77, Convert an invalid level string to an integer: -22 == -22 (expected), passed
2023-07-29 11:49:04,344 TADA INFO assertion 78, Verify that no messages were printed when the level is QUIET.: No messages were printed., passed
2023-07-29 11:49:04,645 TADA INFO assertion 79, Verify that messages of DEBUG,INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:04,931 TADA INFO assertion 80, Verify that messages of DEBUG,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:05,240 TADA INFO assertion 81, Verify that messages of DEBUG,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:05,566 TADA INFO assertion 82, Verify that messages of DEBUG,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:05,886 TADA INFO assertion 83, Verify that messages of INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:06,210 TADA INFO assertion 84, Verify that messages of INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:06,524 TADA INFO assertion 85, Verify that messages of INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:06,841 TADA INFO assertion 86, Verify that messages of WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:07,160 TADA INFO assertion 87, Verify that messages of WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:07,492 TADA INFO assertion 88, Verify that messages of ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:07,838 TADA INFO assertion 89, Verify that messages of DEBUG,INFO,WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:08,133 TADA INFO assertion 90, Verify that messages of DEBUG,INFO,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:08,449 TADA INFO assertion 91, Verify that messages of DEBUG,INFO,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:08,763 TADA INFO assertion 92, Verify that messages of DEBUG,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:09,098 TADA INFO assertion 93, Verify that messages of DEBUG,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:09,427 TADA INFO assertion 94, Verify that messages of DEBUG,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:09,751 TADA INFO assertion 95, Verify that messages of INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:10,068 TADA INFO assertion 96, Verify that messages of INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:10,382 TADA INFO assertion 97, Verify that messages of INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:10,702 TADA INFO assertion 98, Verify that messages of WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:11,055 TADA INFO assertion 99, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:11,392 TADA INFO assertion 100, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:11,706 TADA INFO assertion 101, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:12,027 TADA INFO assertion 102, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:12,342 TADA INFO assertion 103, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:12,662 TADA INFO assertion 104, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:12,959 TADA INFO assertion 105, Verify that messages of DEBUG, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:13,258 TADA INFO assertion 106, Verify that messages of INFO, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:13,566 TADA INFO assertion 107, Verify that messages of WARNING, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:13,912 TADA INFO assertion 108, Verify that messages of ERROR, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:14,225 TADA INFO assertion 109, Verify that messages of CRITICAL, were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:14,572 TADA INFO assertion 110, Verify that messages of DEBUG were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:14,903 TADA INFO assertion 111, Verify that messages of INFO were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:15,208 TADA INFO assertion 112, Verify that messages of WARNING were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:15,531 TADA INFO assertion 113, Verify that messages of ERROR were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:15,863 TADA INFO assertion 114, Verify that messages of CRITICAL were reported.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:16,503 TADA INFO assertion 116, Verify that ovis_log_close() works properly: ovis_log_close() works properly., passed
2023-07-29 11:49:18,346 TADA INFO assertion 115, Verify that applications can open, rename, and reopen log files to perform log rotation.: ovis_log supports open, rename (external), and reopen., passed
2023-07-29 11:49:18,773 TADA INFO assertion 117, Test a ovis_log_register() call with valid arguments: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}], passed
2023-07-29 11:49:18,874 TADA INFO assertion 118, Test a ovis_log_register() call with NULL name: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-07-29 11:49:18,985 TADA INFO assertion 119, Test a ovis_log_register() call with NULL desc: [{'idx': 0, 'return_code': 22}] == [{'idx': 0, 'return_code': 22}], passed
2023-07-29 11:49:19,106 TADA INFO assertion 120, Test a ovis_log_register() call with an existing subsystem: [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}] == [{'idx': 0, 'return_code': 0, 'name': 'my_subsys', 'desc': 'my_subsys_desc', 'level': -1}, {'idx': 1, 'return_code': 17}], passed
2023-07-29 11:49:19,783 TADA INFO assertion 122, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:20,123 TADA INFO assertion 123, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:20,437 TADA INFO assertion 124, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:20,770 TADA INFO assertion 125, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:21,088 TADA INFO assertion 126, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:21,411 TADA INFO assertion 127, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:21,732 TADA INFO assertion 128, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:22,042 TADA INFO assertion 129, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:22,363 TADA INFO assertion 130, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:22,687 TADA INFO assertion 131, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:23,024 TADA INFO assertion 132, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:23,359 TADA INFO assertion 133, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:23,666 TADA INFO assertion 134, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:23,994 TADA INFO assertion 135, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:24,327 TADA INFO assertion 136, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:24,678 TADA INFO assertion 137, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:25,011 TADA INFO assertion 138, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:25,344 TADA INFO assertion 139, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:25,663 TADA INFO assertion 140, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:25,977 TADA INFO assertion 141, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:26,296 TADA INFO assertion 142, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:26,592 TADA INFO assertion 143, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:26,918 TADA INFO assertion 144, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:27,248 TADA INFO assertion 145, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:27,586 TADA INFO assertion 146, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:27,902 TADA INFO assertion 147, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:28,206 TADA INFO assertion 148, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:28,527 TADA INFO assertion 149, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:28,831 TADA INFO assertion 150, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:29,164 TADA INFO assertion 151, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:29,486 TADA INFO assertion 152, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:29,812 TADA INFO assertion 153, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:30,119 TADA INFO assertion 154, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:30,435 TADA INFO assertion 155, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:30,761 TADA INFO assertion 156, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:31,065 TADA INFO assertion 157, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:31,391 TADA INFO assertion 158, Verify that messages of DEBUG,INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:31,735 TADA INFO assertion 159, Verify that messages of DEBUG,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:32,057 TADA INFO assertion 160, Verify that messages of DEBUG,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:32,373 TADA INFO assertion 161, Verify that messages of DEBUG,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:32,693 TADA INFO assertion 162, Verify that messages of INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:33,026 TADA INFO assertion 163, Verify that messages of INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:33,333 TADA INFO assertion 164, Verify that messages of INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:33,656 TADA INFO assertion 165, Verify that messages of WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:33,994 TADA INFO assertion 166, Verify that messages of WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:34,308 TADA INFO assertion 167, Verify that messages of ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:34,634 TADA INFO assertion 168, Verify that messages of DEBUG,INFO,WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:34,963 TADA INFO assertion 169, Verify that messages of DEBUG,INFO,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:35,295 TADA INFO assertion 170, Verify that messages of DEBUG,INFO,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:35,611 TADA INFO assertion 171, Verify that messages of DEBUG,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:35,928 TADA INFO assertion 172, Verify that messages of DEBUG,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:36,243 TADA INFO assertion 173, Verify that messages of DEBUG,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:36,558 TADA INFO assertion 174, Verify that messages of INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:36,870 TADA INFO assertion 175, Verify that messages of INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:37,186 TADA INFO assertion 176, Verify that messages of INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:37,506 TADA INFO assertion 177, Verify that messages of WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:37,831 TADA INFO assertion 178, Verify that messages of DEBUG,INFO,WARNING,ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:38,148 TADA INFO assertion 179, Verify that messages of DEBUG,INFO,WARNING,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:38,470 TADA INFO assertion 180, Verify that messages of DEBUG,INFO,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:38,805 TADA INFO assertion 181, Verify that messages of DEBUG,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:39,122 TADA INFO assertion 182, Verify that messages of INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:39,470 TADA INFO assertion 183, Verify that messages of DEBUG,INFO,WARNING,ERROR,CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:39,800 TADA INFO assertion 184, Verify that messages of DEBUG, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:40,116 TADA INFO assertion 185, Verify that messages of INFO, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:40,449 TADA INFO assertion 186, Verify that messages of WARNING, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:40,761 TADA INFO assertion 187, Verify that messages of ERROR, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:41,063 TADA INFO assertion 188, Verify that messages of CRITICAL, were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:41,391 TADA INFO assertion 189, Verify that messages of DEBUG were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:41,723 TADA INFO assertion 190, Verify that messages of INFO were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:42,066 TADA INFO assertion 191, Verify that messages of WARNING were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:42,398 TADA INFO assertion 192, Verify that messages of ERROR were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:42,732 TADA INFO assertion 193, Verify that messages of CRITICAL were reported from a subsystem.: Only the messages with the level not less than 'ERROR' were logged., passed
2023-07-29 11:49:43,379 TADA INFO assertion 195, Verify that ovis_log_set_level_by_regex() returns an error when the given regular expression string is invalid.: 'result=22' in 'Sat Jul 29 11:49:43 2023:         : test: result=22
Sat Jul 29 11:49:43 2023:         : test: result=22
', passed
2023-07-29 11:49:43,689 TADA INFO assertion 194, Verify that ovis_log_set_level_by_regex() returns ENOENT when the given regular expression string doesn't match any logs.: 'result=2' in 'Sat Jul 29 11:49:43 2023:         : test: result=2
', passed
2023-07-29 11:49:44,004 TADA INFO assertion 196, Verify that ovis_log_set_level_by_regex() sets the level of the matched log subsystems to the given value.: ('config:' in 'Sat Jul 29 11:49:43 2023:         : config: ALWAYS' and (('CRITICAL' in 'Sat Jul 29 11:49:43 2023:         : config: ALWAYS') or ('ALWAYS' in Sat Jul 29 11:49:43 2023:         : config: ALWAYS)), passed
2023-07-29 11:49:44,650 TADA INFO assertion 197, Verify that ovis_log_list() works correctly.: '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]' == '[{'name': 'test (default)', 'desc': 'The default log subsystem', 'level': 'CRITICAL,'}, {'name': 'config', 'desc': 'config', 'level': 'default'}, {'name': 'xprt', 'desc': 'xprt', 'level': 'ERROR,CRITICAL'}, {'name': 'xprt.ldms', 'desc': 'xprt.ldms', 'level': 'INFO,CRITICAL'}, {'name': 'xprt.zap', 'desc': 'xprt.zap', 'level': 'WARNING,'}]', passed
2023-07-29 11:49:44,651 TADA INFO test libovis_log_test ended
2023-07-29 11:49:55 INFO: ----------------------------------------------
2023-07-29 11:49:56 INFO: ======== ldmsd_long_config_test ========
2023-07-29 11:49:56 INFO: CMD: python3 ldmsd_long_config_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldmsd_long_config_test
2023-07-29 11:49:57,091 TADA INFO starting test `ldmsd_long_config_line`
2023-07-29 11:49:57,091 TADA INFO   test-id: 121ab172d8a6f43e88aa127473227d0ff0f65acc9af580d132ad0201ac74f558
2023-07-29 11:49:57,091 TADA INFO   test-suite: LDMSD
2023-07-29 11:49:57,091 TADA INFO   test-name: ldmsd_long_config_line
2023-07-29 11:49:57,091 TADA INFO   test-user: narate
2023-07-29 11:49:57,091 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:49:57,092 __main__ INFO ---Get or create the cluster --
2023-07-29 11:50:04,401 __main__ INFO --- Start daemons ---
2023-07-29 11:50:20,920 TADA INFO assertion 1, LDMSD correctly processes a config line in a config file: LDMSD processed the long config line in the config file correctly., passed
2023-07-29 11:50:21,449 TADA INFO assertion 2, LDMSD correctly handle a config line from ldmsd_controller: LDMSD receives the correct message from ldmsd_controller., passed
2023-07-29 11:50:22,070 TADA INFO assertion 3, LDMSD correctly handle a config line from ldmsctl: LDMSD receives the correct message from ldmsctl., passed
2023-07-29 11:50:22,071 TADA INFO test ldmsd_long_config_line ended
2023-07-29 11:50:34 INFO: ----------------------------------------------
2023-07-29 11:50:35 INFO: ======== ldms_rail_test ========
2023-07-29 11:50:35 INFO: CMD: python3 ldms_rail_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_rail_test
2023-07-29 11:50:35,823 TADA INFO starting test `ldms_rail_test`
2023-07-29 11:50:35,823 TADA INFO   test-id: 742b162bf2a32e7e353ca54dc4b3cac42c4679a74a93af18620cf8a1d2611bc2
2023-07-29 11:50:35,823 TADA INFO   test-suite: LDMSD
2023-07-29 11:50:35,823 TADA INFO   test-name: ldms_rail_test
2023-07-29 11:50:35,823 TADA INFO   test-user: narate
2023-07-29 11:50:35,823 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:50:35,824 __main__ INFO -- Get or create the cluster --
2023-07-29 11:50:42,864 __main__ INFO -- Start daemons --
2023-07-29 11:50:47,585 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:50:49,587 __main__ INFO start ldms_rail_server.py and ldms_rail_client.py interactive sessions
2023-07-29 11:50:52,605 TADA INFO assertion 1, Start interactive LDMS server: OK, passed
2023-07-29 11:50:55,622 TADA INFO assertion 2, Start interactive LDMS client: OK, passed
2023-07-29 11:50:59,227 TADA INFO assertion 3, Client rail has 8 endpoints on 8 thread pools: OK, passed
2023-07-29 11:51:02,832 TADA INFO assertion 4, Server rail has 8 endpoints on 8 thread pools: OK, passed
2023-07-29 11:51:06,437 TADA INFO assertion 5, Sets on client are processed by different threads: OK, passed
2023-07-29 11:51:10,042 TADA INFO assertion 6, Verify sets on the client: OK, passed
2023-07-29 11:51:13,059 TADA INFO assertion 7, Start interactive LDMS client2: OK, passed
2023-07-29 11:51:16,664 TADA INFO assertion 8, Client2 rail has 8 endpoints on 4 thread pools: OK, passed
2023-07-29 11:51:19,680 TADA INFO assertion 9, Client3 (wrong auth) cannot connect: OK, passed
2023-07-29 11:51:22,701 TADA INFO assertion 10, Start interactive client4 (for push mode): OK, passed
2023-07-29 11:51:22,701 __main__ INFO waiting push ...
2023-07-29 11:51:24,704 __main__ INFO server: sampling new data (2)
2023-07-29 11:51:29,310 __main__ INFO client4: set pushes received
2023-07-29 11:51:29,310 __main__ INFO client4: verifying data in sets
2023-07-29 11:51:32,915 __main__ INFO client4: verifying threads-sets-endpoints spread
2023-07-29 11:51:43,728 TADA INFO assertion 11, Client4 got push callback from the corresponding thread: OK, passed
2023-07-29 11:51:46,745 TADA INFO assertion 12, Client5 started (for clean-up path test): OK, passed
2023-07-29 11:51:46,745 __main__ INFO xprt close by client1
2023-07-29 11:51:59,561 TADA INFO assertion 13, Active-side close: client1 clean up: OK, passed
2023-07-29 11:52:03,166 TADA INFO assertion 14, Active-side close: server-side clean up: OK, passed
2023-07-29 11:52:19,586 TADA INFO assertion 15, Passive-side close: client2 clean up: OK, passed
2023-07-29 11:52:19,587 TADA INFO assertion 16, Passive-side close: server-side clean up: OK, passed
2023-07-29 11:52:25,193 TADA INFO assertion 17, Active-side term: server-side clean up: OK, passed
2023-07-29 11:52:32,402 TADA INFO assertion 18, Passive-side term: client5 clean up: OK, passed
2023-07-29 11:52:52,854 TADA INFO assertion 19, server -> client overspending send: error message verified, passed
2023-07-29 11:53:03,668 TADA INFO assertion 20, client -> server overspending send: error message verified, passed
2023-07-29 11:53:07,273 TADA INFO assertion 21, verify send credits on the server: OK, passed
2023-07-29 11:53:10,878 TADA INFO assertion 22, verify send credits on the client: OK, passed
2023-07-29 11:53:18,987 TADA INFO assertion 23, server unblock, verify recv data: recv data verified, passed
2023-07-29 11:53:27,098 TADA INFO assertion 24, client unblock, verify recv data: recv data verified, passed
2023-07-29 11:53:30,703 TADA INFO assertion 25, verify send credits on the server: OK, passed
2023-07-29 11:53:34,308 TADA INFO assertion 26, verify send credits on the client: OK, passed
2023-07-29 11:53:37,913 TADA INFO assertion 27, server -> client send after credited back: OK, passed
2023-07-29 11:53:41,518 TADA INFO assertion 28, client -> server send after credited back: OK, passed
2023-07-29 11:53:45,123 TADA INFO assertion 29, verify send credits on the server: OK, passed
2023-07-29 11:53:48,728 TADA INFO assertion 30, verify send credits on the client: OK, passed
2023-07-29 11:53:52,333 TADA INFO assertion 31, server unblock, verify recv data: OK, passed
2023-07-29 11:53:55,938 TADA INFO assertion 32, client unblock, verify recv data: OK, passed
2023-07-29 11:53:59,543 TADA INFO assertion 33, verify send credits on the server: OK, passed
2023-07-29 11:54:03,147 TADA INFO assertion 34, verify send credits on the client: OK, passed
2023-07-29 11:54:06,752 TADA INFO assertion 35, verify send-credit deposits on the server: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-07-29 11:54:10,357 TADA INFO assertion 36, verify send-credit deposits on the client: expected [(17, 0), (32, 0), (32, 0)], got [(17, 0), (32, 0), (32, 0)], passed
2023-07-29 11:54:10,358 TADA INFO test ldms_rail_test ended
2023-07-29 11:54:22 INFO: ----------------------------------------------
2023-07-29 11:54:23 INFO: ======== ldms_stream_test ========
2023-07-29 11:54:23 INFO: CMD: python3 ldms_stream_test --prefix /home/narate/cron/ldms-test/ldms-containers/ovis --runtime docker --image ovishpc/ldms-dev --src /mnt/300G/data --data_root /mnt/300G/data/2023-07-29-104830/data/ldms_stream_test
2023-07-29 11:54:24,013 TADA INFO starting test `ldms_stream_test`
2023-07-29 11:54:24,013 TADA INFO   test-id: 484a98bc45c0e390974d2dac5464f6ba625c6f2a3ab44dc119b4e81b235bf3bd
2023-07-29 11:54:24,013 TADA INFO   test-suite: LDMSD
2023-07-29 11:54:24,013 TADA INFO   test-name: ldms_stream_test
2023-07-29 11:54:24,013 TADA INFO   test-user: narate
2023-07-29 11:54:24,013 TADA INFO   commit-id: 925affbefabc13830ec3385b8ea358a0296d2a42
2023-07-29 11:54:24,014 __main__ INFO -- Get or create the cluster --
2023-07-29 11:54:41,377 __main__ INFO -- Adding 'foo' and 'bar' users --
2023-07-29 11:54:51,812 __main__ INFO -- Start daemons --
2023-07-29 11:55:04,302 __main__ INFO ... wait a bit to make sure ldmsd's are up
2023-07-29 11:55:06,304 __main__ INFO start interactive stream servers
2023-07-29 11:55:06,304 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-7 
2023-07-29 11:55:09,323 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-6 
2023-07-29 11:55:12,341 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-5 
2023-07-29 11:55:15,360 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-4 
2023-07-29 11:55:18,379 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-3 
2023-07-29 11:55:21,402 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-2 
2023-07-29 11:55:24,420 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-1 
2023-07-29 11:55:27,439 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-4 
2023-07-29 11:55:30,957 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-5 
2023-07-29 11:55:34,476 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-6 
2023-07-29 11:55:37,996 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-7 
2023-07-29 11:55:41,517 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-4 as foo
2023-07-29 11:55:45,033 __main__ INFO starting /tada-src/python/ldms_stream_publish.py on narate-ldms_stream_test-925affb-node-4 as bar
2023-07-29 11:55:48,551 __main__ INFO starting /tada-src/python/ldms_stream_client.py on narate-ldms_stream_test-925affb-node-8 as foo
2023-07-29 11:55:52,071 TADA INFO assertion 1, Publishing oversize data results in an error: checking..., passed
2023-07-29 11:55:52,583 __main__ INFO getting data from srv1
2023-07-29 11:55:55,090 __main__ INFO getting data from srv2
2023-07-29 11:55:57,597 __main__ INFO getting data from srv3
2023-07-29 11:56:00,103 __main__ INFO getting data from srv4
2023-07-29 11:56:02,609 __main__ INFO getting data from srv5
2023-07-29 11:56:05,115 __main__ INFO getting data from srv6
2023-07-29 11:56:07,620 __main__ INFO getting data from srv7
2023-07-29 11:56:10,126 __main__ INFO getting data from cli8foo
2023-07-29 11:56:12,633 TADA INFO assertion 2, JSON support (l3-stream): client data verified, passed
2023-07-29 11:56:12,633 __main__ INFO publishing 'four' on l3-stream by pub4
2023-07-29 11:56:13,136 __main__ INFO publishing 'five' on l3-stream by pub5
2023-07-29 11:56:13,638 __main__ INFO publishing 'six' on l3-stream by pub6
2023-07-29 11:56:14,141 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-07-29 11:56:14,644 TADA INFO assertion 301, send-credit taken: credits: [114, 128, 128, 128], passed
2023-07-29 11:56:14,644 __main__ INFO obtaining all client data (0)
2023-07-29 11:56:14,644 __main__ INFO getting data from srv1
2023-07-29 11:56:17,150 __main__ INFO getting data from srv2
2023-07-29 11:56:19,657 __main__ INFO getting data from srv3
2023-07-29 11:56:22,163 __main__ INFO getting data from srv4
2023-07-29 11:56:24,670 __main__ INFO getting data from srv5
2023-07-29 11:56:27,176 __main__ INFO getting data from srv6
2023-07-29 11:56:29,683 __main__ INFO getting data from srv7
2023-07-29 11:56:32,189 __main__ INFO getting data from cli8foo
2023-07-29 11:56:34,695 __main__ INFO obtaining all client data (1)
2023-07-29 11:56:34,695 __main__ INFO getting data from srv1
2023-07-29 11:56:37,202 __main__ INFO getting data from srv2
2023-07-29 11:56:39,708 __main__ INFO getting data from srv3
2023-07-29 11:56:42,214 __main__ INFO getting data from srv4
2023-07-29 11:56:44,719 __main__ INFO getting data from srv5
2023-07-29 11:56:47,225 __main__ INFO getting data from srv6
2023-07-29 11:56:49,731 __main__ INFO getting data from srv7
2023-07-29 11:56:52,236 __main__ INFO getting data from cli8foo
2023-07-29 11:56:54,743 __main__ INFO obtaining all client data (2)
2023-07-29 11:56:54,743 __main__ INFO getting data from srv1
2023-07-29 11:56:57,249 __main__ INFO getting data from srv2
2023-07-29 11:56:59,755 __main__ INFO getting data from srv3
2023-07-29 11:57:02,261 __main__ INFO getting data from srv4
2023-07-29 11:57:04,767 __main__ INFO getting data from srv5
2023-07-29 11:57:07,272 __main__ INFO getting data from srv6
2023-07-29 11:57:09,778 __main__ INFO getting data from srv7
2023-07-29 11:57:12,284 __main__ INFO getting data from cli8foo
2023-07-29 11:57:14,790 __main__ INFO obtaining all client data (3)
2023-07-29 11:57:14,790 __main__ INFO getting data from srv1
2023-07-29 11:57:17,296 __main__ INFO getting data from srv2
2023-07-29 11:57:19,802 __main__ INFO getting data from srv3
2023-07-29 11:57:22,307 __main__ INFO getting data from srv4
2023-07-29 11:57:24,813 __main__ INFO getting data from srv5
2023-07-29 11:57:27,319 __main__ INFO getting data from srv6
2023-07-29 11:57:29,824 __main__ INFO getting data from srv7
2023-07-29 11:57:32,330 __main__ INFO getting data from cli8foo
2023-07-29 11:57:35,338 TADA INFO assertion 302, send-credit returned: credits: [128, 128, 128, 128], passed
2023-07-29 11:57:35,339 TADA INFO assertion 303, stream delivery spread among rails: tids: {202, 203, 204, 205}, passed
2023-07-29 11:57:35,340 TADA INFO assertion 3, l3-stream delivery: client data verified, passed
2023-07-29 11:57:35,340 __main__ INFO publishing 'four' on l2-stream by pub4
2023-07-29 11:57:35,842 __main__ INFO publishing 'five' on l2-stream by pub5
2023-07-29 11:57:36,344 __main__ INFO publishing 'six' on l2-stream by pub6
2023-07-29 11:57:36,846 __main__ INFO publishing 'seven' on l2-stream by pub7
2023-07-29 11:57:37,349 __main__ INFO obtaining all client data (0)
2023-07-29 11:57:37,349 __main__ INFO getting data from srv1
2023-07-29 11:57:39,856 __main__ INFO getting data from srv2
2023-07-29 11:57:42,362 __main__ INFO getting data from srv3
2023-07-29 11:57:44,868 __main__ INFO getting data from srv4
2023-07-29 11:57:47,374 __main__ INFO getting data from srv5
2023-07-29 11:57:49,881 __main__ INFO getting data from srv6
2023-07-29 11:57:52,387 __main__ INFO getting data from srv7
2023-07-29 11:57:54,893 __main__ INFO getting data from cli8foo
2023-07-29 11:57:57,400 __main__ INFO obtaining all client data (1)
2023-07-29 11:57:57,400 __main__ INFO getting data from srv1
2023-07-29 11:57:59,907 __main__ INFO getting data from srv2
2023-07-29 11:58:02,413 __main__ INFO getting data from srv3
2023-07-29 11:58:04,920 __main__ INFO getting data from srv4
2023-07-29 11:58:07,425 __main__ INFO getting data from srv5
2023-07-29 11:58:09,931 __main__ INFO getting data from srv6
2023-07-29 11:58:12,437 __main__ INFO getting data from srv7
2023-07-29 11:58:14,942 __main__ INFO getting data from cli8foo
2023-07-29 11:58:17,450 TADA INFO assertion 4, l2-stream delivery: client data verified, passed
2023-07-29 11:58:17,450 __main__ INFO publishing 'four' on l1-stream by pub4
2023-07-29 11:58:17,951 __main__ INFO publishing 'five' on l1-stream by pub5
2023-07-29 11:58:18,453 __main__ INFO publishing 'six' on l1-stream by pub6
2023-07-29 11:58:18,956 __main__ INFO publishing 'seven' on l1-stream by pub7
2023-07-29 11:58:19,458 __main__ INFO obtaining all client data (0)
2023-07-29 11:58:19,458 __main__ INFO getting data from srv1
2023-07-29 11:58:21,965 __main__ INFO getting data from srv2
2023-07-29 11:58:24,470 __main__ INFO getting data from srv3
2023-07-29 11:58:26,977 __main__ INFO getting data from srv4
2023-07-29 11:58:29,483 __main__ INFO getting data from srv5
2023-07-29 11:58:31,990 __main__ INFO getting data from srv6
2023-07-29 11:58:34,496 __main__ INFO getting data from srv7
2023-07-29 11:58:37,002 __main__ INFO getting data from cli8foo
2023-07-29 11:58:39,509 __main__ INFO obtaining all client data (1)
2023-07-29 11:58:39,509 __main__ INFO getting data from srv1
2023-07-29 11:58:42,015 __main__ INFO getting data from srv2
2023-07-29 11:58:44,521 __main__ INFO getting data from srv3
2023-07-29 11:58:47,027 __main__ INFO getting data from srv4
2023-07-29 11:58:49,533 __main__ INFO getting data from srv5
2023-07-29 11:58:52,038 __main__ INFO getting data from srv6
2023-07-29 11:58:54,544 __main__ INFO getting data from srv7
2023-07-29 11:58:57,050 __main__ INFO getting data from cli8foo
2023-07-29 11:58:59,557 TADA INFO assertion 5, l1-stream delivery: client data verified, passed
2023-07-29 11:58:59,557 __main__ INFO publishing 'four' on x-stream by pub4
2023-07-29 11:59:00,059 __main__ INFO publishing 'five' on x-stream by pub5
2023-07-29 11:59:00,561 __main__ INFO publishing 'six' on x-stream by pub6
2023-07-29 11:59:01,063 __main__ INFO publishing 'seven' on x-stream by pub7
2023-07-29 11:59:01,565 __main__ INFO obtaining all client data (0)
2023-07-29 11:59:01,566 __main__ INFO getting data from srv1
2023-07-29 11:59:04,072 __main__ INFO getting data from srv2
2023-07-29 11:59:06,577 __main__ INFO getting data from srv3
2023-07-29 11:59:09,084 __main__ INFO getting data from srv4
2023-07-29 11:59:11,590 __main__ INFO getting data from srv5
2023-07-29 11:59:14,096 __main__ INFO getting data from srv6
2023-07-29 11:59:16,602 __main__ INFO getting data from srv7
2023-07-29 11:59:19,108 __main__ INFO getting data from cli8foo
2023-07-29 11:59:21,614 __main__ INFO obtaining all client data (1)
2023-07-29 11:59:21,614 __main__ INFO getting data from srv1
2023-07-29 11:59:24,120 __main__ INFO getting data from srv2
2023-07-29 11:59:26,626 __main__ INFO getting data from srv3
2023-07-29 11:59:29,132 __main__ INFO getting data from srv4
2023-07-29 11:59:31,638 __main__ INFO getting data from srv5
2023-07-29 11:59:34,143 __main__ INFO getting data from srv6
2023-07-29 11:59:36,649 __main__ INFO getting data from srv7
2023-07-29 11:59:39,155 __main__ INFO getting data from cli8foo
2023-07-29 11:59:41,662 TADA INFO assertion 6, x-stream delivery: client data verified, passed
2023-07-29 11:59:41,662 __main__ INFO publishing 'four' on nada by pub4
2023-07-29 11:59:42,163 __main__ INFO publishing 'five' on nada by pub5
2023-07-29 11:59:42,665 __main__ INFO publishing 'six' on nada by pub6
2023-07-29 11:59:43,167 __main__ INFO publishing 'seven' on nada by pub7
2023-07-29 11:59:43,668 __main__ INFO obtaining all client data (0)
2023-07-29 11:59:43,668 __main__ INFO getting data from srv1
2023-07-29 11:59:46,174 __main__ INFO getting data from srv2
2023-07-29 11:59:48,680 __main__ INFO getting data from srv3
2023-07-29 11:59:51,186 __main__ INFO getting data from srv4
2023-07-29 11:59:53,691 __main__ INFO getting data from srv5
2023-07-29 11:59:56,197 __main__ INFO getting data from srv6
2023-07-29 11:59:58,703 __main__ INFO getting data from srv7
2023-07-29 12:00:01,210 __main__ INFO getting data from cli8foo
2023-07-29 12:00:03,715 __main__ INFO obtaining all client data (1)
2023-07-29 12:00:03,716 __main__ INFO getting data from srv1
2023-07-29 12:00:06,221 __main__ INFO getting data from srv2
2023-07-29 12:00:08,727 __main__ INFO getting data from srv3
2023-07-29 12:00:11,232 __main__ INFO getting data from srv4
2023-07-29 12:00:13,738 __main__ INFO getting data from srv5
2023-07-29 12:00:16,243 __main__ INFO getting data from srv6
2023-07-29 12:00:18,749 __main__ INFO getting data from srv7
2023-07-29 12:00:21,255 __main__ INFO getting data from cli8foo
2023-07-29 12:00:23,761 TADA INFO assertion 7, nada delivery: client data verified, passed
2023-07-29 12:00:23,761 __main__ INFO publishing 'four' on l3-stream by pub4 (0400)
2023-07-29 12:00:24,263 __main__ INFO publishing 'five' on l3-stream by pub5 (0400)
2023-07-29 12:00:24,766 __main__ INFO publishing 'six' on l3-stream by pub6 (0400)
2023-07-29 12:00:25,268 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400)
2023-07-29 12:00:25,771 __main__ INFO obtaining all client data (0)
2023-07-29 12:00:25,771 __main__ INFO getting data from srv1
2023-07-29 12:00:28,278 __main__ INFO getting data from srv2
2023-07-29 12:00:30,784 __main__ INFO getting data from srv3
2023-07-29 12:00:33,290 __main__ INFO getting data from srv4
2023-07-29 12:00:35,797 __main__ INFO getting data from srv5
2023-07-29 12:00:38,303 __main__ INFO getting data from srv6
2023-07-29 12:00:40,809 __main__ INFO getting data from srv7
2023-07-29 12:00:43,315 __main__ INFO getting data from cli8foo
2023-07-29 12:00:45,821 __main__ INFO obtaining all client data (1)
2023-07-29 12:00:45,821 __main__ INFO getting data from srv1
2023-07-29 12:00:48,328 __main__ INFO getting data from srv2
2023-07-29 12:00:50,834 __main__ INFO getting data from srv3
2023-07-29 12:00:53,340 __main__ INFO getting data from srv4
2023-07-29 12:00:55,846 __main__ INFO getting data from srv5
2023-07-29 12:00:58,352 __main__ INFO getting data from srv6
2023-07-29 12:01:00,858 __main__ INFO getting data from srv7
2023-07-29 12:01:03,363 __main__ INFO getting data from cli8foo
2023-07-29 12:01:05,869 __main__ INFO obtaining all client data (2)
2023-07-29 12:01:05,869 __main__ INFO getting data from srv1
2023-07-29 12:01:08,376 __main__ INFO getting data from srv2
2023-07-29 12:01:10,881 __main__ INFO getting data from srv3
2023-07-29 12:01:13,387 __main__ INFO getting data from srv4
2023-07-29 12:01:15,893 __main__ INFO getting data from srv5
2023-07-29 12:01:18,398 __main__ INFO getting data from srv6
2023-07-29 12:01:20,904 __main__ INFO getting data from srv7
2023-07-29 12:01:23,409 __main__ INFO getting data from cli8foo
2023-07-29 12:01:25,915 __main__ INFO obtaining all client data (3)
2023-07-29 12:01:25,915 __main__ INFO getting data from srv1
2023-07-29 12:01:28,421 __main__ INFO getting data from srv2
2023-07-29 12:01:30,927 __main__ INFO getting data from srv3
2023-07-29 12:01:33,433 __main__ INFO getting data from srv4
2023-07-29 12:01:35,938 __main__ INFO getting data from srv5
2023-07-29 12:01:38,444 __main__ INFO getting data from srv6
2023-07-29 12:01:40,950 __main__ INFO getting data from srv7
2023-07-29 12:01:43,455 __main__ INFO getting data from cli8foo
2023-07-29 12:01:45,962 TADA INFO assertion 8, l3-stream by 'root' with 0400 permission: client data verified, passed
2023-07-29 12:01:45,962 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as foo
2023-07-29 12:01:46,465 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as foo
2023-07-29 12:01:46,967 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as foo
2023-07-29 12:01:47,470 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as foo
2023-07-29 12:01:47,973 __main__ INFO obtaining all client data (0)
2023-07-29 12:01:47,973 __main__ INFO getting data from srv1
2023-07-29 12:01:50,479 __main__ INFO getting data from srv2
2023-07-29 12:01:52,985 __main__ INFO getting data from srv3
2023-07-29 12:01:55,492 __main__ INFO getting data from srv4
2023-07-29 12:01:57,998 __main__ INFO getting data from srv5
2023-07-29 12:02:00,505 __main__ INFO getting data from srv6
2023-07-29 12:02:03,011 __main__ INFO getting data from srv7
2023-07-29 12:02:05,517 __main__ INFO getting data from cli8foo
2023-07-29 12:02:08,024 __main__ INFO obtaining all client data (1)
2023-07-29 12:02:08,024 __main__ INFO getting data from srv1
2023-07-29 12:02:10,531 __main__ INFO getting data from srv2
2023-07-29 12:02:13,037 __main__ INFO getting data from srv3
2023-07-29 12:02:15,543 __main__ INFO getting data from srv4
2023-07-29 12:02:18,049 __main__ INFO getting data from srv5
2023-07-29 12:02:20,555 __main__ INFO getting data from srv6
2023-07-29 12:02:23,061 __main__ INFO getting data from srv7
2023-07-29 12:02:25,566 __main__ INFO getting data from cli8foo
2023-07-29 12:02:28,072 __main__ INFO obtaining all client data (2)
2023-07-29 12:02:28,073 __main__ INFO getting data from srv1
2023-07-29 12:02:30,579 __main__ INFO getting data from srv2
2023-07-29 12:02:33,085 __main__ INFO getting data from srv3
2023-07-29 12:02:35,591 __main__ INFO getting data from srv4
2023-07-29 12:02:38,096 __main__ INFO getting data from srv5
2023-07-29 12:02:40,602 __main__ INFO getting data from srv6
2023-07-29 12:02:43,108 __main__ INFO getting data from srv7
2023-07-29 12:02:45,613 __main__ INFO getting data from cli8foo
2023-07-29 12:02:48,120 __main__ INFO obtaining all client data (3)
2023-07-29 12:02:48,120 __main__ INFO getting data from srv1
2023-07-29 12:02:50,627 __main__ INFO getting data from srv2
2023-07-29 12:02:53,132 __main__ INFO getting data from srv3
2023-07-29 12:02:55,638 __main__ INFO getting data from srv4
2023-07-29 12:02:58,143 __main__ INFO getting data from srv5
2023-07-29 12:03:00,649 __main__ INFO getting data from srv6
2023-07-29 12:03:03,155 __main__ INFO getting data from srv7
2023-07-29 12:03:05,660 __main__ INFO getting data from cli8foo
2023-07-29 12:03:08,168 TADA INFO assertion 9, l3-stream by 'root' as 'foo' with 0400 permission: client data verified, passed
2023-07-29 12:03:08,168 __main__ INFO publishing 'four' on l3-stream by pub4 (0400) root as bar
2023-07-29 12:03:08,671 __main__ INFO publishing 'five' on l3-stream by pub5 (0400) root as bar
2023-07-29 12:03:09,173 __main__ INFO publishing 'six' on l3-stream by pub6 (0400) root as bar
2023-07-29 12:03:09,676 __main__ INFO publishing 'seven' on l3-stream by pub7 (0400) root as bar
2023-07-29 12:03:10,178 __main__ INFO obtaining all client data (0)
2023-07-29 12:03:10,179 __main__ INFO getting data from srv1
2023-07-29 12:03:12,685 __main__ INFO getting data from srv2
2023-07-29 12:03:15,191 __main__ INFO getting data from srv3
2023-07-29 12:03:17,697 __main__ INFO getting data from srv4
2023-07-29 12:03:20,203 __main__ INFO getting data from srv5
2023-07-29 12:03:22,709 __main__ INFO getting data from srv6
2023-07-29 12:03:25,216 __main__ INFO getting data from srv7
2023-07-29 12:03:27,722 __main__ INFO getting data from cli8foo
2023-07-29 12:03:30,228 __main__ INFO obtaining all client data (1)
2023-07-29 12:03:30,228 __main__ INFO getting data from srv1
2023-07-29 12:03:32,735 __main__ INFO getting data from srv2
2023-07-29 12:03:35,241 __main__ INFO getting data from srv3
2023-07-29 12:03:37,747 __main__ INFO getting data from srv4
2023-07-29 12:03:40,253 __main__ INFO getting data from srv5
2023-07-29 12:03:42,759 __main__ INFO getting data from srv6
2023-07-29 12:03:45,264 __main__ INFO getting data from srv7
2023-07-29 12:03:47,770 __main__ INFO getting data from cli8foo
2023-07-29 12:03:50,276 __main__ INFO obtaining all client data (2)
2023-07-29 12:03:50,276 __main__ INFO getting data from srv1
2023-07-29 12:03:52,783 __main__ INFO getting data from srv2
2023-07-29 12:03:55,288 __main__ INFO getting data from srv3
2023-07-29 12:03:57,794 __main__ INFO getting data from srv4
2023-07-29 12:04:00,300 __main__ INFO getting data from srv5
2023-07-29 12:04:02,806 __main__ INFO getting data from srv6
2023-07-29 12:04:05,311 __main__ INFO getting data from srv7
2023-07-29 12:04:07,817 __main__ INFO getting data from cli8foo
2023-07-29 12:04:10,322 __main__ INFO obtaining all client data (3)
2023-07-29 12:04:10,323 __main__ INFO getting data from srv1
2023-07-29 12:04:12,829 __main__ INFO getting data from srv2
2023-07-29 12:04:15,335 __main__ INFO getting data from srv3
2023-07-29 12:04:17,840 __main__ INFO getting data from srv4
2023-07-29 12:04:20,346 __main__ INFO getting data from srv5
2023-07-29 12:04:22,851 __main__ INFO getting data from srv6
2023-07-29 12:04:25,357 __main__ INFO getting data from srv7
2023-07-29 12:04:27,863 __main__ INFO getting data from cli8foo
2023-07-29 12:04:30,369 TADA INFO assertion 10, l3-stream by 'root' as 'bar' with 0400 permission: client data verified, passed
2023-07-29 12:04:30,370 __main__ INFO publishing 'four' on l3-stream by pub4 (0440) root as bar
2023-07-29 12:04:30,872 __main__ INFO publishing 'five' on l3-stream by pub5 (0440) root as bar
2023-07-29 12:04:31,375 __main__ INFO publishing 'six' on l3-stream by pub6 (0440) root as bar
2023-07-29 12:04:31,877 __main__ INFO publishing 'seven' on l3-stream by pub7 (0440) root as bar
2023-07-29 12:04:32,380 __main__ INFO obtaining all client data (0)
2023-07-29 12:04:32,380 __main__ INFO getting data from srv1
2023-07-29 12:04:34,886 __main__ INFO getting data from srv2
2023-07-29 12:04:37,392 __main__ INFO getting data from srv3
2023-07-29 12:04:39,899 __main__ INFO getting data from srv4
2023-07-29 12:04:42,405 __main__ INFO getting data from srv5
2023-07-29 12:04:44,911 __main__ INFO getting data from srv6
2023-07-29 12:04:47,417 __main__ INFO getting data from srv7
2023-07-29 12:04:49,924 __main__ INFO getting data from cli8foo
2023-07-29 12:04:52,430 __main__ INFO obtaining all client data (1)
2023-07-29 12:04:52,430 __main__ INFO getting data from srv1
2023-07-29 12:04:54,936 __main__ INFO getting data from srv2
2023-07-29 12:04:57,443 __main__ INFO getting data from srv3
2023-07-29 12:04:59,949 __main__ INFO getting data from srv4
2023-07-29 12:05:02,455 __main__ INFO getting data from srv5
2023-07-29 12:05:04,960 __main__ INFO getting data from srv6
2023-07-29 12:05:07,466 __main__ INFO getting data from srv7
2023-07-29 12:05:09,972 __main__ INFO getting data from cli8foo
2023-07-29 12:05:12,477 __main__ INFO obtaining all client data (2)
2023-07-29 12:05:12,478 __main__ INFO getting data from srv1
2023-07-29 12:05:14,984 __main__ INFO getting data from srv2
2023-07-29 12:05:17,489 __main__ INFO getting data from srv3
2023-07-29 12:05:19,995 __main__ INFO getting data from srv4
2023-07-29 12:05:22,500 __main__ INFO getting data from srv5
2023-07-29 12:05:25,006 __main__ INFO getting data from srv6
2023-07-29 12:05:27,512 __main__ INFO getting data from srv7
2023-07-29 12:05:30,017 __main__ INFO getting data from cli8foo
2023-07-29 12:05:32,523 __main__ INFO obtaining all client data (3)
2023-07-29 12:05:32,523 __main__ INFO getting data from srv1
2023-07-29 12:05:35,030 __main__ INFO getting data from srv2
2023-07-29 12:05:37,536 __main__ INFO getting data from srv3
2023-07-29 12:05:40,042 __main__ INFO getting data from srv4
2023-07-29 12:05:42,547 __main__ INFO getting data from srv5
2023-07-29 12:05:45,053 __main__ INFO getting data from srv6
2023-07-29 12:05:47,558 __main__ INFO getting data from srv7
2023-07-29 12:05:50,064 __main__ INFO getting data from cli8foo
2023-07-29 12:05:52,571 TADA INFO assertion 11, l3-stream by 'root' as 'bar' with 0440 permission: client data verified, passed
2023-07-29 12:05:53,072 TADA INFO assertion 12, l3-stream by 'foo' as 'bar' results in an error: checking..., passed
2023-07-29 12:05:53,073 __main__ INFO publishing 'four' on l3-stream by pub4foo (0440)
2023-07-29 12:05:53,575 __main__ INFO obtaining all client data (0)
2023-07-29 12:05:53,575 __main__ INFO getting data from srv1
2023-07-29 12:05:56,082 __main__ INFO getting data from srv2
2023-07-29 12:05:58,588 __main__ INFO getting data from srv3
2023-07-29 12:06:01,094 __main__ INFO getting data from srv4
2023-07-29 12:06:03,600 __main__ INFO getting data from srv5
2023-07-29 12:06:06,106 __main__ INFO getting data from srv6
2023-07-29 12:06:08,612 __main__ INFO getting data from srv7
2023-07-29 12:06:11,117 __main__ INFO getting data from cli8foo
2023-07-29 12:06:13,624 TADA INFO assertion 13, l3-stream by 'foo' with 0440 permission: client data verified, passed
2023-07-29 12:06:13,624 __main__ INFO publishing 'four' on l3-stream by pub4bar (0440)
2023-07-29 12:06:14,127 __main__ INFO obtaining all client data (0)
2023-07-29 12:06:14,127 __main__ INFO getting data from srv1
2023-07-29 12:06:16,633 __main__ INFO getting data from srv2
2023-07-29 12:06:19,139 __main__ INFO getting data from srv3
2023-07-29 12:06:21,645 __main__ INFO getting data from srv4
2023-07-29 12:06:24,151 __main__ INFO getting data from srv5
2023-07-29 12:06:26,657 __main__ INFO getting data from srv6
2023-07-29 12:06:29,162 __main__ INFO getting data from srv7
2023-07-29 12:06:31,668 __main__ INFO getting data from cli8foo
2023-07-29 12:06:34,174 TADA INFO assertion 14, l3-stream by 'bar' with 0440 permission: client data verified, passed
2023-07-29 12:06:37,682 TADA INFO assertion 15, Blocking client and asynchronous client have the same data: verified, passed
2023-07-29 12:06:39,186 __main__ INFO publishing 'four' on l3-stream by srv4
2023-07-29 12:06:39,688 __main__ INFO obtaining all client data (0)
2023-07-29 12:06:39,689 __main__ INFO getting data from srv1
2023-07-29 12:06:42,195 __main__ INFO getting data from srv2
2023-07-29 12:06:44,701 __main__ INFO getting data from srv3
2023-07-29 12:06:47,207 __main__ INFO getting data from srv4
2023-07-29 12:06:49,713 __main__ INFO getting data from srv5
2023-07-29 12:06:52,219 __main__ INFO getting data from srv6
2023-07-29 12:06:54,724 __main__ INFO getting data from srv7
2023-07-29 12:06:57,230 __main__ INFO getting data from cli8foo
2023-07-29 12:06:59,737 TADA INFO assertion 20, l3-stream publish from L1 (srv4): client data verified, passed
2023-07-29 12:06:59,737 __main__ INFO publishing 'four' on nada by srv4
2023-07-29 12:07:00,239 __main__ INFO obtaining all client data (0)
2023-07-29 12:07:00,239 __main__ INFO getting data from srv1
2023-07-29 12:07:02,744 __main__ INFO getting data from srv2
2023-07-29 12:07:05,250 __main__ INFO getting data from srv3
2023-07-29 12:07:07,756 __main__ INFO getting data from srv4
2023-07-29 12:07:10,262 __main__ INFO getting data from srv5
2023-07-29 12:07:12,768 __main__ INFO getting data from srv6
2023-07-29 12:07:15,273 __main__ INFO getting data from srv7
2023-07-29 12:07:17,779 __main__ INFO getting data from cli8foo
2023-07-29 12:07:20,285 TADA INFO assertion 21, nada publish from L1 (srv4): client data verified, passed
2023-07-29 12:07:23,883 TADA INFO assertion 22, Check stream stats in each process: verified, passed
2023-07-29 12:07:27,485 TADA INFO assertion 23, Check stream client stats in each process: verified, passed
2023-07-29 12:07:29,990 TADA INFO assertion 16, srv-6 clean up properly after srv-3 exited: checking..., passed
2023-07-29 12:07:29,991 TADA INFO assertion 17, srv-7 clean up properly after srv-3 exited: checking..., passed
2023-07-29 12:07:29,991 TADA INFO assertion 18, srv-1 clean up properly after srv-3 exited: checking..., passed
2023-07-29 12:07:29,991 __main__ INFO starting /tada-src/python/ldms_stream_server.py on narate-ldms_stream_test-925affb-node-3 
2023-07-29 12:07:33,508 __main__ INFO publishing 'seven' on l3-stream by pub7
2023-07-29 12:07:34,010 __main__ INFO obtaining all client data (0)
2023-07-29 12:07:34,010 __main__ INFO getting data from srv1
2023-07-29 12:07:36,517 __main__ INFO getting data from srv2
2023-07-29 12:07:39,023 __main__ INFO getting data from srv3
2023-07-29 12:07:41,529 __main__ INFO getting data from srv4
2023-07-29 12:07:44,035 __main__ INFO getting data from srv5
2023-07-29 12:07:46,540 __main__ INFO getting data from srv6
2023-07-29 12:07:49,046 __main__ INFO getting data from srv7
2023-07-29 12:07:51,552 __main__ INFO getting data from cli8foo
2023-07-29 12:07:54,059 TADA INFO assertion 19, l3-stream successfully delivered after srv-3 restarted: client data verified, passed
2023-07-29 12:07:54,060 TADA INFO test ldms_stream_test ended
2023-07-29 12:08:09 INFO: ----------------------------------------------
2023-07-29 12:08:10 INFO: ======== test-ldms ========
2023-07-29 12:08:10 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-ldms/test.sh
2023-07-29T12:08:10-05:00 INFO: starting test-samp-1
79549d6c3fe4bbe654183817cbc0aa6a16b16104d09a5ce37896fb4bae746839
2023-07-29T12:08:12-05:00 INFO: starting test-samp-2
30d23b5ce1f10971151328990cb909c010b3760dccc9b8d287c2af560db4ed70
2023-07-29T12:08:14-05:00 INFO: starting test-samp-3
9f0f630d35b6fd6d7ee0fe52ebdfa30057c3a66e1c720860a25b341561053147
2023-07-29T12:08:16-05:00 INFO: starting test-samp-4
5d91823507520f26b93ff86fb225f766c6198b5eed9787ced02de757e9a3cd1b
2023-07-29T12:08:17-05:00 INFO: test-samp-1 is running
2023-07-29T12:08:18-05:00 INFO: test-samp-2 is running
2023-07-29T12:08:18-05:00 INFO: test-samp-3 is running
2023-07-29T12:08:18-05:00 INFO: test-samp-4 is running
2023-07-29T12:08:18-05:00 INFO: starting test-agg-11
0a733f668f07175c6adefede178be05cadc5e186495876dce9a85b6ea3bdc245
2023-07-29T12:08:19-05:00 INFO: starting test-agg-12
ac7fcd64e84d073ce17af9d033b8c41edeb28689864729c19a3a991f26231c3a
2023-07-29T12:08:21-05:00 INFO: test-agg-11 is running
2023-07-29T12:08:21-05:00 INFO: test-agg-12 is running
2023-07-29T12:08:21-05:00 INFO: starting test-agg-2
7a77045661f0b42e1f408f2c351ed55b111eff642ee4775cb4db21ff4575f9c5
2023-07-29T12:08:23-05:00 INFO: test-agg-2 is running
2023-07-29T12:08:23-05:00 INFO: Collecting data (into SOS)
2023-07-29T12:08:33-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-29T12:08:35-05:00 INFO: check rc: 0
2023-07-29T12:08:35-05:00 INFO: Cleaning up ...
test-samp-1
test-samp-2
test-samp-3
test-samp-4
test-agg-11
test-agg-12
test-agg-2
2023-07-29T12:08:40-05:00 INFO: DONE
2023-07-29 12:08:50 INFO: ----------------------------------------------
2023-07-29 12:08:50 INFO: ======== test-maestro ========
2023-07-29 12:08:50 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro/test.sh
2023-07-29T12:08:50-05:00 INFO: starting mtest-maestro
25a74f7deaadaa1f8085ce495d6642bc812c853bcc12400c458518ce5b551abe
2023-07-29T12:08:52-05:00 INFO: starting mtest-samp-1
0c93844046e15a476921ccec2c912dea7effd2b358cd9b272f8a08c19e2dfd36
2023-07-29T12:08:53-05:00 INFO: starting mtest-samp-2
cb0e4e0722ba26444b0199d05b6b267c195a4600af4c1f429d7516c52562effb
2023-07-29T12:08:55-05:00 INFO: starting mtest-samp-3
80ea007ea4c4261000623149351e88740d747e8eb66dd143de8875e60e991ed7
2023-07-29T12:08:56-05:00 INFO: starting mtest-samp-4
6abe4eacd4b150e138ca2b9a7f3c0da8e636265656be706d29944e246505682f
2023-07-29T12:08:58-05:00 INFO: mtest-samp-1 is running
2023-07-29T12:08:58-05:00 INFO: mtest-samp-2 is running
2023-07-29T12:08:58-05:00 INFO: mtest-samp-3 is running
2023-07-29T12:08:58-05:00 INFO: mtest-samp-4 is running
2023-07-29T12:08:58-05:00 INFO: starting mtest-agg-11
0e9fe92afeb8cd4d5945236825c6f846512346f56dcbb5fd6870977533e9f705
2023-07-29T12:08:59-05:00 INFO: starting mtest-agg-12
4cc74aa8da0560b24be1d02c00afefbad85da86cf69404eac8ad93fda159b4bc
2023-07-29T12:09:01-05:00 INFO: mtest-agg-11 is running
2023-07-29T12:09:01-05:00 INFO: mtest-agg-12 is running
2023-07-29T12:09:01-05:00 INFO: starting mtest-agg-2
ed8cafffeeabd87afa7b679ec40ec8da6855e69cb9ce0afc0365004c1db68e04
2023-07-29T12:09:02-05:00 INFO: mtest-agg-2 is running
2023-07-29T12:09:02-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-29T12:11:03-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-29T12:11:05-05:00 INFO: sos check rc: 0
2023-07-29T12:11:07-05:00 INFO: starting mtest-ui
9bf73bac16b4a2f910ebfdead49dcce3292743ad8d487b1dae92e5c9dea4e13b
2023-07-29T12:11:13-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2611136, 1690650546001.494], [2611136, 1690650546001.496], [2611136, 1690650546001.496], [2611136, 1690650546002.08], [2611516, 1690650547001.234], [2611516, 1690650547001.645], [2611516, 1690650547001.646], [2611516, 1690650547001.6482], [2611516, 1690650548001.391], [2611516, 1690650548001.8079], [2611516, 1690650548001.8079], [2611516, 1690650548001.811], [2611516, 1690650549001.535], [2611516, 1690650549001.956], [2611516, 1690650549001.961], [2611516, 1690650549001.964], [2611516, 1690650550001.6729], [2611516, 1690650550002.093], [2611516, 1690650550002.0999], [2611516, 1690650550002.101], [2611516, 1690650551001.2212], [2611516, 1690650551001.228], [2611516, 1690650551001.229], [2611516, 1690650551001.802], [2611516, 1690650552001.355], [2611516, 1690650552001.3608], [2611516, 1690650552001.363], [2611516, 1690650552001.938], [2611516, 1690650553001.509], [2611516, 1690650553001.516], [2611516, 1690650553001.521], [2611516, 1690650553002.0952], [2611516, 1690650554001.229], [2611516, 1690650554001.636], [2611516, 1690650554001.643], [2611516, 1690650554001.6472], [2611516, 1690650555001.365], [2611516, 1690650555001.7659], [2611516, 1690650555001.7678], [2611516, 1690650555001.7678], [2611516, 1690650556001.51], [2611516, 1690650556001.889], [2611516, 1690650556001.891], [2611516, 1690650556001.892], [2611516, 1690650557001.6519], [2611516, 1690650557002.027], [2611516, 1690650557002.028], [2611516, 1690650557002.0322], [2611516, 1690650558001.1829], [2611516, 1690650558001.185], [2611516, 1690650558001.188], [2611516, 1690650558001.803], [2611516, 1690650559001.314], [2611516, 1690650559001.324], [2611516, 1690650559001.325], [2611516, 1690650559001.9358], [2611516, 1690650560001.438], [2611516, 1690650560001.4421], [2611516, 1690650560001.443], [2611516, 1690650560002.073], [2611516, 1690650561001.217], [2611516, 1690650561001.575], [2611516, 1690650561001.581], [2611516, 1690650561001.582], [2611516, 1690650562001.355], [2611516, 1690650562001.7222], [2611516, 1690650562001.7222], [2611516, 1690650562001.7249], [2611516, 1690650563001.487], [2611516, 1690650563001.857], [2611516, 1690650563001.8591], [2611516, 1690650563001.861], [2611516, 1690650564001.64], [2611516, 1690650564002.015], [2611516, 1690650564002.017], [2611516, 1690650564002.018], [2611516, 1690650565001.16], [2611516, 1690650565001.17], [2611516, 1690650565001.173], [2611516, 1690650565001.792], [2611516, 1690650566001.2102], [2611516, 1690650566001.2852], [2611516, 1690650566001.296], [2611516, 1690650566001.933], [2611516, 1690650567001.3472], [2611516, 1690650567001.418], [2611516, 1690650567001.4211], [2611516, 1690650567002.077], [2611516, 1690650568001.222], [2611516, 1690650568001.491], [2611516, 1690650568001.549], [2611516, 1690650568001.554], [2611516, 1690650569001.367], [2611516, 1690650569001.646], [2611516, 1690650569001.6938], [2611516, 1690650569001.696], [2611516, 1690650570001.516], [2611516, 1690650570001.79], [2611516, 1690650570001.823], [2611516, 1690650570001.8298], [2611516, 1690650571001.655], [2611516, 1690650571001.922], [2611516, 1690650571001.9468], [2611516, 1690650571001.952], [2611516, 1690650572001.4988], [2611516, 1690650572001.791], [2611516, 1690650572002.063], [2611516, 1690650572002.081], [2611516, 1690650573000.286], [2611516, 1690650573001.195], [2611516, 1690650573001.219], [2611516, 1690650573001.939], [2611516, 1690650574001.366], [2611516, 1690650574001.367], [2611516, 1690650574001.3682], [2611516, 1690650574002.091], [2611516, 1690650575001.244], [2611516, 1690650575001.504], [2611516, 1690650575001.5051], [2611516, 1690650575001.506], [2611516, 1690650576001.391], [2611516, 1690650576001.622], [2611516, 1690650576001.625], [2611516, 1690650576001.6262], [2611516, 1690650577001.535], [2611516, 1690650577001.755], [2611516, 1690650577001.7568], [2611516, 1690650577001.762], [2611516, 1690650578001.684], [2611516, 1690650578001.904], [2611516, 1690650578001.906], [2611516, 1690650578001.9111], [2611516, 1690650579001.832], [2611516, 1690650579002.063], [2611516, 1690650579002.065], [2611516, 1690650579002.066], [2611516, 1690650580001.194], [2611516, 1690650580001.196], [2611516, 1690650580001.1992], [2611516, 1690650580001.974], [2611516, 1690650581001.314], [2611516, 1690650581001.3162], [2611516, 1690650581001.3188], [2611516, 1690650581002.115], [2611516, 1690650582001.268], [2611516, 1690650582001.451], [2611516, 1690650582001.455], [2611516, 1690650582001.4568], [2611516, 1690650583001.3928], [2611516, 1690650583001.573], [2611516, 1690650583001.574], [2611516, 1690650583001.58], [2611516, 1690650584001.555], [2611516, 1690650584001.717], [2611516, 1690650584001.7222], [2611516, 1690650584001.728], [2611516, 1690650585001.696], [2611516, 1690650585001.8298], [2611516, 1690650585001.8381], [2611516, 1690650585001.843], [2611516, 1690650586001.8381], [2611516, 1690650586001.962], [2611516, 1690650586001.971], [2611516, 1690650586001.977], [2611516, 1690650587001.975], [2611516, 1690650587002.086], [2611516, 1690650587002.092], [2611516, 1690650587002.0989], [2611516, 1690650588001.222], [2611516, 1690650588001.229], [2611516, 1690650588001.2349], [2611516, 1690650588001.239], [2611516, 1690650589001.3801], [2611516, 1690650589001.384], [2611516, 1690650589001.387], [2611516, 1690650589001.3892], [2611516, 1690650590001.512], [2611516, 1690650590001.518], [2611516, 1690650590001.523], [2611516, 1690650590001.5261], [2611516, 1690650591001.654], [2611516, 1690650591001.654], [2611516, 1690650591001.654], [2611516, 1690650591001.656], [2611516, 1690650592001.3591], [2611516, 1690650592001.7742], [2611516, 1690650592001.803], [2611516, 1690650592001.813], [2611516, 1690650593001.503], [2611516, 1690650593001.504], [2611516, 1690650593001.919], [2611516, 1690650593001.94], [2611516, 1690650594001.081], [2611516, 1690650594001.2039], [2611516, 1690650594001.28], [2611516, 1690650594001.4458], [2611516, 1690650595001.241], [2611516, 1690650595001.243], [2611516, 1690650595001.4028], [2611516, 1690650595001.601], [2611516, 1690650596001.364], [2611516, 1690650596001.3818], [2611516, 1690650596001.385], [2611516, 1690650596001.718], [2611516, 1690650597001.43], [2611516, 1690650597001.4321], [2611516, 1690650597001.504], [2611516, 1690650597001.524], [2611516, 1690650598001.559], [2611516, 1690650598001.567], [2611516, 1690650598001.6262], [2611516, 1690650598001.644], [2611516, 1690650599001.698], [2611516, 1690650599001.706], [2611516, 1690650599001.758], [2611516, 1690650599001.782], [2611516, 1690650600001.853], [2611516, 1690650600001.856], [2611516, 1690650600001.906], [2611516, 1690650600001.924], [2611516, 1690650601001.021], [2611516, 1690650601002.009], [2611516, 1690650601002.041], [2611516, 1690650601002.042], [2611516, 1690650602001.147], [2611516, 1690650602001.155], [2611516, 1690650602001.167], [2611516, 1690650602001.2632], [2611516, 1690650603001.29], [2611516, 1690650603001.291], [2611516, 1690650603001.2979], [2611516, 1690650603001.307], [2611516, 1690650604001.428], [2611516, 1690650604001.434], [2611516, 1690650604001.434], [2611516, 1690650604001.4348], [2611516, 1690650605001.585], [2611516, 1690650605001.59], [2611516, 1690650605001.592], [2611516, 1690650605001.594], [2611516, 1690650606001.71], [2611516, 1690650606001.7112], [2611516, 1690650606001.7148], [2611516, 1690650606001.719], [2611516, 1690650607001.842], [2611516, 1690650607001.849], [2611516, 1690650607001.85], [2611516, 1690650607001.8518], [2611516, 1690650608001.988], [2611516, 1690650608001.989], [2611516, 1690650608001.992], [2611516, 1690650608001.998], [2611516, 1690650609001.147], [2611516, 1690650609002.133], [2611516, 1690650609002.139], [2611516, 1690650609002.143], [2611320, 1690650610000.387], [2611320, 1690650610000.676], [2611320, 1690650610001.268], [2611320, 1690650610001.2732], [2611260, 1690650611001.4038], [2611260, 1690650611001.412], [2611260, 1690650611001.412], [2611260, 1690650611001.4138], [2611260, 1690650612001.544], [2611260, 1690650612001.544], [2611260, 1690650612001.546], [2611260, 1690650612001.547], [2611260, 1690650613000.715], [2611260, 1690650613001.16], [2611260, 1690650613001.622], [2611260, 1690650613001.661], [2611260, 1690650614001.314], [2611260, 1690650614001.3162], [2611260, 1690650614001.7468], [2611260, 1690650614001.752], [2611260, 1690650615001.481], [2611260, 1690650615001.485], [2611260, 1690650615001.485], [2611260, 1690650615001.601], [2611260, 1690650616001.615], [2611260, 1690650616001.6199], [2611260, 1690650616001.63], [2611260, 1690650616001.636], [2611260, 1690650617001.7559], [2611260, 1690650617001.758], [2611260, 1690650617001.773], [2611260, 1690650617001.773], [2611260, 1690650618001.8938], [2611260, 1690650618001.896], [2611260, 1690650618001.9001], [2611260, 1690650618001.906], [2611260, 1690650619002.046], [2611260, 1690650619002.046], [2611260, 1690650619002.049], [2611260, 1690650619002.05], [2611260, 1690650620001.205], [2611260, 1690650620001.209], [2611260, 1690650620001.211], [2611260, 1690650620001.2139], [2611260, 1690650621001.333], [2611260, 1690650621001.335], [2611260, 1690650621001.3389], [2611260, 1690650621001.3408], [2611260, 1690650622001.4631], [2611260, 1690650622001.482], [2611260, 1690650622001.4841], [2611260, 1690650622001.485], [2611260, 1690650623000.8052], [2611260, 1690650623000.8699], [2611260, 1690650623001.583], [2611260, 1690650623001.597], [2611260, 1690650624001.7249], [2611260, 1690650624001.7258], [2611260, 1690650624001.728], [2611260, 1690650624001.986], [2611260, 1690650625001.888], [2611260, 1690650625001.891], [2611260, 1690650625001.896], [2611260, 1690650625002.017], [2611260, 1690650626001.171], [2611260, 1690650626001.174], [2611260, 1690650626002.043], [2611260, 1690650626002.043], [2611260, 1690650627001.1929], [2611260, 1690650627001.195], [2611260, 1690650627001.311], [2611260, 1690650627001.3162], [2611260, 1690650628001.3398], [2611260, 1690650628001.342], [2611260, 1690650628001.4421], [2611260, 1690650628001.454], [2611260, 1690650629001.492], [2611260, 1690650629001.493], [2611260, 1690650629001.572], [2611260, 1690650629001.5842], [2611260, 1690650630001.618], [2611260, 1690650630001.621], [2611260, 1690650630001.685], [2611260, 1690650630001.7039], [2611260, 1690650631001.7742], [2611260, 1690650631001.7769], [2611260, 1690650631001.8052], [2611260, 1690650631001.847], [2611260, 1690650632001.918], [2611260, 1690650632001.928], [2611260, 1690650632001.937], [2611260, 1690650632001.967], [2611260, 1690650633002.061], [2611260, 1690650633002.065], [2611260, 1690650633002.069], [2611260, 1690650633002.102], [2611260, 1690650634001.185], [2611260, 1690650634001.188], [2611260, 1690650634001.194], [2611260, 1690650634001.218], [2611260, 1690650635001.313], [2611260, 1690650635001.3152], [2611260, 1690650635001.3198], [2611260, 1690650635001.334], [2611260, 1690650636001.472], [2611260, 1690650636001.474], [2611260, 1690650636001.487], [2611260, 1690650636001.517], [2611260, 1690650637001.624], [2611260, 1690650637001.625], [2611260, 1690650637001.628], [2611260, 1690650637001.6582], [2611260, 1690650638001.755], [2611260, 1690650638001.7568], [2611260, 1690650638001.759], [2611260, 1690650638001.7778], [2611260, 1690650639001.888], [2611260, 1690650639001.8901], [2611260, 1690650639001.895], [2611260, 1690650639001.903], [2611260, 1690650640002.044], [2611260, 1690650640002.05], [2611260, 1690650640002.051], [2611260, 1690650640002.0532], [2611516, 1690650641001.207], [2611516, 1690650641001.208], [2611516, 1690650641001.209], [2611516, 1690650641001.211], [2611516, 1690650642001.335], [2611516, 1690650642001.3389], [2611516, 1690650642001.3408], [2611516, 1690650642001.343], [2611516, 1690650643001.479], [2611516, 1690650643001.482], [2611516, 1690650643001.486], [2611516, 1690650643001.486], [2611516, 1690650644000.775], [2611516, 1690650644001.602], [2611516, 1690650644001.6052], [2611516, 1690650644001.608], [2611516, 1690650645001.7332], [2611516, 1690650645001.7349], [2611516, 1690650645001.74], [2611516, 1690650645001.7432], [2611516, 1690650646001.891], [2611516, 1690650646001.891], [2611516, 1690650646001.896], [2611516, 1690650646001.898], [2611516, 1690650647002.035], [2611516, 1690650647002.039], [2611516, 1690650647002.04], [2611516, 1690650647002.043], [2611516, 1690650648001.185], [2611516, 1690650648001.185], [2611516, 1690650648001.185], [2611516, 1690650648001.187], [2611516, 1690650649001.334], [2611516, 1690650649001.3381], [2611516, 1690650649001.3381], [2611516, 1690650649001.3408], [2611516, 1690650650001.4878], [2611516, 1690650650001.493], [2611516, 1690650650001.495], [2611516, 1690650650001.497], [2611516, 1690650651001.653], [2611516, 1690650651001.655], [2611516, 1690650651001.659], [2611516, 1690650651001.663], [2611516, 1690650652001.811], [2611516, 1690650652001.811], [2611516, 1690650652001.813], [2611516, 1690650652001.8162], [2611516, 1690650653001.958], [2611516, 1690650653001.96], [2611516, 1690650653001.961], [2611516, 1690650653001.963], [2611516, 1690650654002.0852], [2611516, 1690650654002.09], [2611516, 1690650654002.094], [2611516, 1690650654002.094], [2611516, 1690650655001.24], [2611516, 1690650655001.241], [2611516, 1690650655001.244], [2611516, 1690650655001.244], [2611516, 1690650656000.864], [2611516, 1690650656001.3608], [2611516, 1690650656001.374], [2611516, 1690650656001.376], [2611516, 1690650657001.517], [2611516, 1690650657001.518], [2611516, 1690650657001.5308], [2611516, 1690650657001.5308], [2611516, 1690650658001.572], [2611516, 1690650658001.6519], [2611516, 1690650658001.6582], [2611516, 1690650658001.663], [2611516, 1690650659001.707], [2611516, 1690650659001.769], [2611516, 1690650659001.771], [2611516, 1690650659001.782], [2611516, 1690650660001.759], [2611516, 1690650660001.836], [2611516, 1690650660001.9001], [2611516, 1690650660001.903], [2611516, 1690650661001.893], [2611516, 1690650661001.955], [2611516, 1690650661002.02], [2611516, 1690650661002.025], [2611516, 1690650662001.168], [2611516, 1690650662001.176], [2611516, 1690650662001.9998], [2611516, 1690650662002.034]]}, {"target": "component_id", "datapoints": [[3, 1690650546001.494], [1, 1690650546001.496], [4, 1690650546001.496], [2, 1690650546002.08], [2, 1690650547001.234], [1, 1690650547001.645], [3, 1690650547001.646], [4, 1690650547001.6482], [2, 1690650548001.391], [1, 1690650548001.8079], [4, 1690650548001.8079], [3, 1690650548001.811], [2, 1690650549001.535], [3, 1690650549001.956], [1, 1690650549001.961], [4, 1690650549001.964], [2, 1690650550001.6729], [4, 1690650550002.093], [3, 1690650550002.0999], [1, 1690650550002.101], [4, 1690650551001.2212], [3, 1690650551001.228], [1, 1690650551001.229], [2, 1690650551001.802], [4, 1690650552001.355], [3, 1690650552001.3608], [1, 1690650552001.363], [2, 1690650552001.938], [1, 1690650553001.509], [3, 1690650553001.516], [4, 1690650553001.521], [2, 1690650553002.0952], [2, 1690650554001.229], [1, 1690650554001.636], [4, 1690650554001.643], [3, 1690650554001.6472], [2, 1690650555001.365], [1, 1690650555001.7659], [3, 1690650555001.7678], [4, 1690650555001.7678], [2, 1690650556001.51], [1, 1690650556001.889], [4, 1690650556001.891], [3, 1690650556001.892], [2, 1690650557001.6519], [3, 1690650557002.027], [1, 1690650557002.028], [4, 1690650557002.0322], [1, 1690650558001.1829], [4, 1690650558001.185], [3, 1690650558001.188], [2, 1690650558001.803], [4, 1690650559001.314], [1, 1690650559001.324], [3, 1690650559001.325], [2, 1690650559001.9358], [4, 1690650560001.438], [3, 1690650560001.4421], [1, 1690650560001.443], [2, 1690650560002.073], [2, 1690650561001.217], [3, 1690650561001.575], [4, 1690650561001.581], [1, 1690650561001.582], [2, 1690650562001.355], [1, 1690650562001.7222], [3, 1690650562001.7222], [4, 1690650562001.7249], [2, 1690650563001.487], [3, 1690650563001.857], [1, 1690650563001.8591], [4, 1690650563001.861], [2, 1690650564001.64], [1, 1690650564002.015], [3, 1690650564002.017], [4, 1690650564002.018], [4, 1690650565001.16], [1, 1690650565001.17], [3, 1690650565001.173], [2, 1690650565001.792], [3, 1690650566001.2102], [4, 1690650566001.2852], [1, 1690650566001.296], [2, 1690650566001.933], [3, 1690650567001.3472], [4, 1690650567001.418], [1, 1690650567001.4211], [2, 1690650567002.077], [2, 1690650568001.222], [3, 1690650568001.491], [4, 1690650568001.549], [1, 1690650568001.554], [2, 1690650569001.367], [3, 1690650569001.646], [4, 1690650569001.6938], [1, 1690650569001.696], [2, 1690650570001.516], [3, 1690650570001.79], [4, 1690650570001.823], [1, 1690650570001.8298], [2, 1690650571001.655], [3, 1690650571001.922], [4, 1690650571001.9468], [1, 1690650571001.952], [3, 1690650572001.4988], [2, 1690650572001.791], [1, 1690650572002.063], [4, 1690650572002.081], [4, 1690650573000.286], [1, 1690650573001.195], [3, 1690650573001.219], [2, 1690650573001.939], [1, 1690650574001.366], [3, 1690650574001.367], [4, 1690650574001.3682], [2, 1690650574002.091], [2, 1690650575001.244], [3, 1690650575001.504], [1, 1690650575001.5051], [4, 1690650575001.506], [2, 1690650576001.391], [4, 1690650576001.622], [3, 1690650576001.625], [1, 1690650576001.6262], [2, 1690650577001.535], [1, 1690650577001.755], [3, 1690650577001.7568], [4, 1690650577001.762], [2, 1690650578001.684], [1, 1690650578001.904], [3, 1690650578001.906], [4, 1690650578001.9111], [2, 1690650579001.832], [3, 1690650579002.063], [1, 1690650579002.065], [4, 1690650579002.066], [3, 1690650580001.194], [1, 1690650580001.196], [4, 1690650580001.1992], [2, 1690650580001.974], [4, 1690650581001.314], [3, 1690650581001.3162], [1, 1690650581001.3188], [2, 1690650581002.115], [2, 1690650582001.268], [3, 1690650582001.451], [4, 1690650582001.455], [1, 1690650582001.4568], [2, 1690650583001.3928], [1, 1690650583001.573], [4, 1690650583001.574], [3, 1690650583001.58], [2, 1690650584001.555], [4, 1690650584001.717], [1, 1690650584001.7222], [3, 1690650584001.728], [2, 1690650585001.696], [4, 1690650585001.8298], [1, 1690650585001.8381], [3, 1690650585001.843], [2, 1690650586001.8381], [4, 1690650586001.962], [1, 1690650586001.971], [3, 1690650586001.977], [2, 1690650587001.975], [4, 1690650587002.086], [1, 1690650587002.092], [3, 1690650587002.0989], [1, 1690650588001.222], [4, 1690650588001.229], [2, 1690650588001.2349], [3, 1690650588001.239], [4, 1690650589001.3801], [1, 1690650589001.384], [3, 1690650589001.387], [2, 1690650589001.3892], [3, 1690650590001.512], [1, 1690650590001.518], [4, 1690650590001.523], [2, 1690650590001.5261], [1, 1690650591001.654], [2, 1690650591001.654], [3, 1690650591001.654], [4, 1690650591001.656], [1, 1690650592001.3591], [2, 1690650592001.7742], [3, 1690650592001.803], [4, 1690650592001.813], [2, 1690650593001.503], [1, 1690650593001.504], [3, 1690650593001.919], [4, 1690650593001.94], [1, 1690650594001.081], [2, 1690650594001.2039], [3, 1690650594001.28], [4, 1690650594001.4458], [1, 1690650595001.241], [2, 1690650595001.243], [3, 1690650595001.4028], [4, 1690650595001.601], [1, 1690650596001.364], [3, 1690650596001.3818], [2, 1690650596001.385], [4, 1690650596001.718], [3, 1690650597001.43], [4, 1690650597001.4321], [1, 1690650597001.504], [2, 1690650597001.524], [4, 1690650598001.559], [3, 1690650598001.567], [1, 1690650598001.6262], [2, 1690650598001.644], [4, 1690650599001.698], [3, 1690650599001.706], [1, 1690650599001.758], [2, 1690650599001.782], [4, 1690650600001.853], [3, 1690650600001.856], [1, 1690650600001.906], [2, 1690650600001.924], [4, 1690650601001.021], [3, 1690650601002.009], [2, 1690650601002.041], [1, 1690650601002.042], [4, 1690650602001.147], [3, 1690650602001.155], [1, 1690650602001.167], [2, 1690650602001.2632], [3, 1690650603001.29], [1, 1690650603001.291], [4, 1690650603001.2979], [2, 1690650603001.307], [1, 1690650604001.428], [3, 1690650604001.434], [4, 1690650604001.434], [2, 1690650604001.4348], [1, 1690650605001.585], [4, 1690650605001.59], [2, 1690650605001.592], [3, 1690650605001.594], [1, 1690650606001.71], [3, 1690650606001.7112], [2, 1690650606001.7148], [4, 1690650606001.719], [3, 1690650607001.842], [4, 1690650607001.849], [1, 1690650607001.85], [2, 1690650607001.8518], [1, 1690650608001.988], [2, 1690650608001.989], [3, 1690650608001.992], [4, 1690650608001.998], [4, 1690650609001.147], [1, 1690650609002.133], [3, 1690650609002.139], [2, 1690650609002.143], [1, 1690650610000.387], [3, 1690650610000.676], [2, 1690650610001.268], [4, 1690650610001.2732], [4, 1690650611001.4038], [1, 1690650611001.412], [3, 1690650611001.412], [2, 1690650611001.4138], [2, 1690650612001.544], [3, 1690650612001.544], [4, 1690650612001.546], [1, 1690650612001.547], [4, 1690650613000.715], [3, 1690650613001.16], [1, 1690650613001.622], [2, 1690650613001.661], [2, 1690650614001.314], [3, 1690650614001.3162], [1, 1690650614001.7468], [4, 1690650614001.752], [2, 1690650615001.481], [1, 1690650615001.485], [3, 1690650615001.485], [4, 1690650615001.601], [2, 1690650616001.615], [3, 1690650616001.6199], [1, 1690650616001.63], [4, 1690650616001.636], [3, 1690650617001.7559], [2, 1690650617001.758], [1, 1690650617001.773], [4, 1690650617001.773], [2, 1690650618001.8938], [4, 1690650618001.896], [3, 1690650618001.9001], [1, 1690650618001.906], [1, 1690650619002.046], [4, 1690650619002.046], [2, 1690650619002.049], [3, 1690650619002.05], [2, 1690650620001.205], [1, 1690650620001.209], [4, 1690650620001.211], [3, 1690650620001.2139], [1, 1690650621001.333], [4, 1690650621001.335], [3, 1690650621001.3389], [2, 1690650621001.3408], [4, 1690650622001.4631], [1, 1690650622001.482], [3, 1690650622001.4841], [2, 1690650622001.485], [3, 1690650623000.8052], [2, 1690650623000.8699], [1, 1690650623001.583], [4, 1690650623001.597], [4, 1690650624001.7249], [3, 1690650624001.7258], [1, 1690650624001.728], [2, 1690650624001.986], [3, 1690650625001.888], [2, 1690650625001.891], [4, 1690650625001.896], [1, 1690650625002.017], [1, 1690650626001.171], [4, 1690650626001.174], [2, 1690650626002.043], [3, 1690650626002.043], [3, 1690650627001.1929], [2, 1690650627001.195], [1, 1690650627001.311], [4, 1690650627001.3162], [2, 1690650628001.3398], [3, 1690650628001.342], [1, 1690650628001.4421], [4, 1690650628001.454], [2, 1690650629001.492], [3, 1690650629001.493], [1, 1690650629001.572], [4, 1690650629001.5842], [2, 1690650630001.618], [3, 1690650630001.621], [1, 1690650630001.685], [4, 1690650630001.7039], [3, 1690650631001.7742], [2, 1690650631001.7769], [1, 1690650631001.8052], [4, 1690650631001.847], [3, 1690650632001.918], [2, 1690650632001.928], [1, 1690650632001.937], [4, 1690650632001.967], [1, 1690650633002.061], [3, 1690650633002.065], [2, 1690650633002.069], [4, 1690650633002.102], [2, 1690650634001.185], [1, 1690650634001.188], [3, 1690650634001.194], [4, 1690650634001.218], [2, 1690650635001.313], [3, 1690650635001.3152], [1, 1690650635001.3198], [4, 1690650635001.334], [1, 1690650636001.472], [2, 1690650636001.474], [4, 1690650636001.487], [3, 1690650636001.517], [2, 1690650637001.624], [1, 1690650637001.625], [4, 1690650637001.628], [3, 1690650637001.6582], [1, 1690650638001.755], [4, 1690650638001.7568], [2, 1690650638001.759], [3, 1690650638001.7778], [1, 1690650639001.888], [4, 1690650639001.8901], [2, 1690650639001.895], [3, 1690650639001.903], [2, 1690650640002.044], [4, 1690650640002.05], [1, 1690650640002.051], [3, 1690650640002.0532], [3, 1690650641001.207], [2, 1690650641001.208], [4, 1690650641001.209], [1, 1690650641001.211], [2, 1690650642001.335], [1, 1690650642001.3389], [4, 1690650642001.3408], [3, 1690650642001.343], [4, 1690650643001.479], [2, 1690650643001.482], [1, 1690650643001.486], [3, 1690650643001.486], [4, 1690650644000.775], [1, 1690650644001.602], [3, 1690650644001.6052], [2, 1690650644001.608], [2, 1690650645001.7332], [3, 1690650645001.7349], [4, 1690650645001.74], [1, 1690650645001.7432], [3, 1690650646001.891], [4, 1690650646001.891], [1, 1690650646001.896], [2, 1690650646001.898], [4, 1690650647002.035], [1, 1690650647002.039], [3, 1690650647002.04], [2, 1690650647002.043], [1, 1690650648001.185], [2, 1690650648001.185], [3, 1690650648001.185], [4, 1690650648001.187], [4, 1690650649001.334], [1, 1690650649001.3381], [3, 1690650649001.3381], [2, 1690650649001.3408], [3, 1690650650001.4878], [1, 1690650650001.493], [4, 1690650650001.495], [2, 1690650650001.497], [3, 1690650651001.653], [4, 1690650651001.655], [1, 1690650651001.659], [2, 1690650651001.663], [1, 1690650652001.811], [4, 1690650652001.811], [3, 1690650652001.813], [2, 1690650652001.8162], [1, 1690650653001.958], [4, 1690650653001.96], [2, 1690650653001.961], [3, 1690650653001.963], [1, 1690650654002.0852], [4, 1690650654002.09], [2, 1690650654002.094], [3, 1690650654002.094], [4, 1690650655001.24], [1, 1690650655001.241], [2, 1690650655001.244], [3, 1690650655001.244], [1, 1690650656000.864], [2, 1690650656001.3608], [3, 1690650656001.374], [4, 1690650656001.376], [4, 1690650657001.517], [2, 1690650657001.518], [1, 1690650657001.5308], [3, 1690650657001.5308], [2, 1690650658001.572], [4, 1690650658001.6519], [1, 1690650658001.6582], [3, 1690650658001.663], [2, 1690650659001.707], [4, 1690650659001.769], [1, 1690650659001.771], [3, 1690650659001.782], [3, 1690650660001.759], [2, 1690650660001.836], [4, 1690650660001.9001], [1, 1690650660001.903], [3, 1690650661001.893], [2, 1690650661001.955], [4, 1690650661002.02], [1, 1690650661002.025], [1, 1690650662001.168], [4, 1690650662001.176], [2, 1690650662001.9998], [3, 1690650662002.034]]}, {"target": "job_id", "datapoints": [[0, 1690650546001.494], [0, 1690650546001.496], [0, 1690650546001.496], [0, 1690650546002.08], [0, 1690650547001.234], [0, 1690650547001.645], [0, 1690650547001.646], [0, 1690650547001.6482], [0, 1690650548001.391], [0, 1690650548001.8079], [0, 1690650548001.8079], [0, 1690650548001.811], [0, 1690650549001.535], [0, 1690650549001.956], [0, 1690650549001.961], [0, 1690650549001.964], [0, 1690650550001.6729], [0, 1690650550002.093], [0, 1690650550002.0999], [0, 1690650550002.101], [0, 1690650551001.2212], [0, 1690650551001.228], [0, 1690650551001.229], [0, 1690650551001.802], [0, 1690650552001.355], [0, 1690650552001.3608], [0, 1690650552001.363], [0, 1690650552001.938], [0, 1690650553001.509], [0, 1690650553001.516], [0, 1690650553001.521], [0, 1690650553002.0952], [0, 1690650554001.229], [0, 1690650554001.636], [0, 1690650554001.643], [0, 1690650554001.6472], [0, 1690650555001.365], [0, 1690650555001.7659], [0, 1690650555001.7678], [0, 1690650555001.7678], [0, 1690650556001.51], [0, 1690650556001.889], [0, 1690650556001.891], [0, 1690650556001.892], [0, 1690650557001.6519], [0, 1690650557002.027], [0, 1690650557002.028], [0, 1690650557002.0322], [0, 1690650558001.1829], [0, 1690650558001.185], [0, 1690650558001.188], [0, 1690650558001.803], [0, 1690650559001.314], [0, 1690650559001.324], [0, 1690650559001.325], [0, 1690650559001.9358], [0, 1690650560001.438], [0, 1690650560001.4421], [0, 1690650560001.443], [0, 1690650560002.073], [0, 1690650561001.217], [0, 1690650561001.575], [0, 1690650561001.581], [0, 1690650561001.582], [0, 1690650562001.355], [0, 1690650562001.7222], [0, 1690650562001.7222], [0, 1690650562001.7249], [0, 1690650563001.487], [0, 1690650563001.857], [0, 1690650563001.8591], [0, 1690650563001.861], [0, 1690650564001.64], [0, 1690650564002.015], [0, 1690650564002.017], [0, 1690650564002.018], [0, 1690650565001.16], [0, 1690650565001.17], [0, 1690650565001.173], [0, 1690650565001.792], [0, 1690650566001.2102], [0, 1690650566001.2852], [0, 1690650566001.296], [0, 1690650566001.933], [0, 1690650567001.3472], [0, 1690650567001.418], [0, 1690650567001.4211], [0, 1690650567002.077], [0, 1690650568001.222], [0, 1690650568001.491], [0, 1690650568001.549], [0, 1690650568001.554], [0, 1690650569001.367], [0, 1690650569001.646], [0, 1690650569001.6938], [0, 1690650569001.696], [0, 1690650570001.516], [0, 1690650570001.79], [0, 1690650570001.823], [0, 1690650570001.8298], [0, 1690650571001.655], [0, 1690650571001.922], [0, 1690650571001.9468], [0, 1690650571001.952], [0, 1690650572001.4988], [0, 1690650572001.791], [0, 1690650572002.063], [0, 1690650572002.081], [0, 1690650573000.286], [0, 1690650573001.195], [0, 1690650573001.219], [0, 1690650573001.939], [0, 1690650574001.366], [0, 1690650574001.367], [0, 1690650574001.3682], [0, 1690650574002.091], [0, 1690650575001.244], [0, 1690650575001.504], [0, 1690650575001.5051], [0, 1690650575001.506], [0, 1690650576001.391], [0, 1690650576001.622], [0, 1690650576001.625], [0, 1690650576001.6262], [0, 1690650577001.535], [0, 1690650577001.755], [0, 1690650577001.7568], [0, 1690650577001.762], [0, 1690650578001.684], [0, 1690650578001.904], [0, 1690650578001.906], [0, 1690650578001.9111], [0, 1690650579001.832], [0, 1690650579002.063], [0, 1690650579002.065], [0, 1690650579002.066], [0, 1690650580001.194], [0, 1690650580001.196], [0, 1690650580001.1992], [0, 1690650580001.974], [0, 1690650581001.314], [0, 1690650581001.3162], [0, 1690650581001.3188], [0, 1690650581002.115], [0, 1690650582001.268], [0, 1690650582001.451], [0, 1690650582001.455], [0, 1690650582001.4568], [0, 1690650583001.3928], [0, 1690650583001.573], [0, 1690650583001.574], [0, 1690650583001.58], [0, 1690650584001.555], [0, 1690650584001.717], [0, 1690650584001.7222], [0, 1690650584001.728], [0, 1690650585001.696], [0, 1690650585001.8298], [0, 1690650585001.8381], [0, 1690650585001.843], [0, 1690650586001.8381], [0, 1690650586001.962], [0, 1690650586001.971], [0, 1690650586001.977], [0, 1690650587001.975], [0, 1690650587002.086], [0, 1690650587002.092], [0, 1690650587002.0989], [0, 1690650588001.222], [0, 1690650588001.229], [0, 1690650588001.2349], [0, 1690650588001.239], [0, 1690650589001.3801], [0, 1690650589001.384], [0, 1690650589001.387], [0, 1690650589001.3892], [0, 1690650590001.512], [0, 1690650590001.518], [0, 1690650590001.523], [0, 1690650590001.5261], [0, 1690650591001.654], [0, 1690650591001.654], [0, 1690650591001.654], [0, 1690650591001.656], [0, 1690650592001.3591], [0, 1690650592001.7742], [0, 1690650592001.803], [0, 1690650592001.813], [0, 1690650593001.503], [0, 1690650593001.504], [0, 1690650593001.919], [0, 1690650593001.94], [0, 1690650594001.081], [0, 1690650594001.2039], [0, 1690650594001.28], [0, 1690650594001.4458], [0, 1690650595001.241], [0, 1690650595001.243], [0, 1690650595001.4028], [0, 1690650595001.601], [0, 1690650596001.364], [0, 1690650596001.3818], [0, 1690650596001.385], [0, 1690650596001.718], [0, 1690650597001.43], [0, 1690650597001.4321], [0, 1690650597001.504], [0, 1690650597001.524], [0, 1690650598001.559], [0, 1690650598001.567], [0, 1690650598001.6262], [0, 1690650598001.644], [0, 1690650599001.698], [0, 1690650599001.706], [0, 1690650599001.758], [0, 1690650599001.782], [0, 1690650600001.853], [0, 1690650600001.856], [0, 1690650600001.906], [0, 1690650600001.924], [0, 1690650601001.021], [0, 1690650601002.009], [0, 1690650601002.041], [0, 1690650601002.042], [0, 1690650602001.147], [0, 1690650602001.155], [0, 1690650602001.167], [0, 1690650602001.2632], [0, 1690650603001.29], [0, 1690650603001.291], [0, 1690650603001.2979], [0, 1690650603001.307], [0, 1690650604001.428], [0, 1690650604001.434], [0, 1690650604001.434], [0, 1690650604001.4348], [0, 1690650605001.585], [0, 1690650605001.59], [0, 1690650605001.592], [0, 1690650605001.594], [0, 1690650606001.71], [0, 1690650606001.7112], [0, 1690650606001.7148], [0, 1690650606001.719], [0, 1690650607001.842], [0, 1690650607001.849], [0, 1690650607001.85], [0, 1690650607001.8518], [0, 1690650608001.988], [0, 1690650608001.989], [0, 1690650608001.992], [0, 1690650608001.998], [0, 1690650609001.147], [0, 1690650609002.133], [0, 1690650609002.139], [0, 1690650609002.143], [0, 1690650610000.387], [0, 1690650610000.676], [0, 1690650610001.268], [0, 1690650610001.2732], [0, 1690650611001.4038], [0, 1690650611001.412], [0, 1690650611001.412], [0, 1690650611001.4138], [0, 1690650612001.544], [0, 1690650612001.544], [0, 1690650612001.546], [0, 1690650612001.547], [0, 1690650613000.715], [0, 1690650613001.16], [0, 1690650613001.622], [0, 1690650613001.661], [0, 1690650614001.314], [0, 1690650614001.3162], [0, 1690650614001.7468], [0, 1690650614001.752], [0, 1690650615001.481], [0, 1690650615001.485], [0, 1690650615001.485], [0, 1690650615001.601], [0, 1690650616001.615], [0, 1690650616001.6199], [0, 1690650616001.63], [0, 1690650616001.636], [0, 1690650617001.7559], [0, 1690650617001.758], [0, 1690650617001.773], [0, 1690650617001.773], [0, 1690650618001.8938], [0, 1690650618001.896], [0, 1690650618001.9001], [0, 1690650618001.906], [0, 1690650619002.046], [0, 1690650619002.046], [0, 1690650619002.049], [0, 1690650619002.05], [0, 1690650620001.205], [0, 1690650620001.209], [0, 1690650620001.211], [0, 1690650620001.2139], [0, 1690650621001.333], [0, 1690650621001.335], [0, 1690650621001.3389], [0, 1690650621001.3408], [0, 1690650622001.4631], [0, 1690650622001.482], [0, 1690650622001.4841], [0, 1690650622001.485], [0, 1690650623000.8052], [0, 1690650623000.8699], [0, 1690650623001.583], [0, 1690650623001.597], [0, 1690650624001.7249], [0, 1690650624001.7258], [0, 1690650624001.728], [0, 1690650624001.986], [0, 1690650625001.888], [0, 1690650625001.891], [0, 1690650625001.896], [0, 1690650625002.017], [0, 1690650626001.171], [0, 1690650626001.174], [0, 1690650626002.043], [0, 1690650626002.043], [0, 1690650627001.1929], [0, 1690650627001.195], [0, 1690650627001.311], [0, 1690650627001.3162], [0, 1690650628001.3398], [0, 1690650628001.342], [0, 1690650628001.4421], [0, 1690650628001.454], [0, 1690650629001.492], [0, 1690650629001.493], [0, 1690650629001.572], [0, 1690650629001.5842], [0, 1690650630001.618], [0, 1690650630001.621], [0, 1690650630001.685], [0, 1690650630001.7039], [0, 1690650631001.7742], [0, 1690650631001.7769], [0, 1690650631001.8052], [0, 1690650631001.847], [0, 1690650632001.918], [0, 1690650632001.928], [0, 1690650632001.937], [0, 1690650632001.967], [0, 1690650633002.061], [0, 1690650633002.065], [0, 1690650633002.069], [0, 1690650633002.102], [0, 1690650634001.185], [0, 1690650634001.188], [0, 1690650634001.194], [0, 1690650634001.218], [0, 1690650635001.313], [0, 1690650635001.3152], [0, 1690650635001.3198], [0, 1690650635001.334], [0, 1690650636001.472], [0, 1690650636001.474], [0, 1690650636001.487], [0, 1690650636001.517], [0, 1690650637001.624], [0, 1690650637001.625], [0, 1690650637001.628], [0, 1690650637001.6582], [0, 1690650638001.755], [0, 1690650638001.7568], [0, 1690650638001.759], [0, 1690650638001.7778], [0, 1690650639001.888], [0, 1690650639001.8901], [0, 1690650639001.895], [0, 1690650639001.903], [0, 1690650640002.044], [0, 1690650640002.05], [0, 1690650640002.051], [0, 1690650640002.0532], [0, 1690650641001.207], [0, 1690650641001.208], [0, 1690650641001.209], [0, 1690650641001.211], [0, 1690650642001.335], [0, 1690650642001.3389], [0, 1690650642001.3408], [0, 1690650642001.343], [0, 1690650643001.479], [0, 1690650643001.482], [0, 1690650643001.486], [0, 1690650643001.486], [0, 1690650644000.775], [0, 1690650644001.602], [0, 1690650644001.6052], [0, 1690650644001.608], [0, 1690650645001.7332], [0, 1690650645001.7349], [0, 1690650645001.74], [0, 1690650645001.7432], [0, 1690650646001.891], [0, 1690650646001.891], [0, 1690650646001.896], [0, 1690650646001.898], [0, 1690650647002.035], [0, 1690650647002.039], [0, 1690650647002.04], [0, 1690650647002.043], [0, 1690650648001.185], [0, 1690650648001.185], [0, 1690650648001.185], [0, 1690650648001.187], [0, 1690650649001.334], [0, 1690650649001.3381], [0, 1690650649001.3381], [0, 1690650649001.3408], [0, 1690650650001.4878], [0, 1690650650001.493], [0, 1690650650001.495], [0, 1690650650001.497], [0, 1690650651001.653], [0, 1690650651001.655], [0, 1690650651001.659], [0, 1690650651001.663], [0, 1690650652001.811], [0, 1690650652001.811], [0, 1690650652001.813], [0, 1690650652001.8162], [0, 1690650653001.958], [0, 1690650653001.96], [0, 1690650653001.961], [0, 1690650653001.963], [0, 1690650654002.0852], [0, 1690650654002.09], [0, 1690650654002.094], [0, 1690650654002.094], [0, 1690650655001.24], [0, 1690650655001.241], [0, 1690650655001.244], [0, 1690650655001.244], [0, 1690650656000.864], [0, 1690650656001.3608], [0, 1690650656001.374], [0, 1690650656001.376], [0, 1690650657001.517], [0, 1690650657001.518], [0, 1690650657001.5308], [0, 1690650657001.5308], [0, 1690650658001.572], [0, 1690650658001.6519], [0, 1690650658001.6582], [0, 1690650658001.663], [0, 1690650659001.707], [0, 1690650659001.769], [0, 1690650659001.771], [0, 1690650659001.782], [0, 1690650660001.759], [0, 1690650660001.836], [0, 1690650660001.9001], [0, 1690650660001.903], [0, 1690650661001.893], [0, 1690650661001.955], [0, 1690650661002.02], [0, 1690650661002.025], [0, 1690650662001.168], [0, 1690650662001.176], [0, 1690650662001.9998], [0, 1690650662002.034]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-29T12:11:15-05:00 INFO: query check RC: 0
331971bc5be3bab1684e9f3700ac6c2f48fb8c841b3acf598c291555012f2977
2023-07-29T12:11:47-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2317    715 --:--:-- --:--:-- --:--:--  3050
{"datasource":{"id":1,"uid":"WHOpmB34k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-29T12:11:48-05:00 INFO: Checking grafana data
2023-07-29T12:11:48-05:00 INFO: Grafana data check, rc: 0
2023-07-29T12:11:48-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-29T12:11:53-05:00 INFO: DONE
2023-07-29 12:12:03 INFO: ----------------------------------------------
2023-07-29 12:12:03 INFO: ======== test-maestro-hostmunge ========
2023-07-29 12:12:03 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-hostmunge/test.sh
2023-07-29T12:12:03-05:00 INFO: Checking munge on localhost
2023-07-29T12:12:03-05:00 INFO: munge encode/decode successfully
2023-07-29T12:12:03-05:00 INFO: starting mtest-maestro
a82953c51a168fe5e8eab9454393bc71349fa54c4fb9e656f0e497e649ef24f4
2023-07-29T12:12:05-05:00 INFO: starting mtest-samp-1
26c5c75d6bd0aa0bfcf30cd9d7302dc6f8c43c984d569545fbc41e9b72e15aa1
2023-07-29T12:12:07-05:00 INFO: starting mtest-samp-2
a0a2c5c9b5bc6434409e2a1eebe41a6554fa91af1c519e5fec00e62aa9e8d51f
2023-07-29T12:12:09-05:00 INFO: starting mtest-samp-3
e20e27e909cef39b2080154413b6913574f29f8fa7e49045adc91087e816c9de
2023-07-29T12:12:10-05:00 INFO: starting mtest-samp-4
dbcaec8de045a48e9a531165b066d6813066b5fd53679c62e730e69858053b56
2023-07-29T12:12:12-05:00 INFO: mtest-samp-1 is running
2023-07-29T12:12:12-05:00 INFO: mtest-samp-2 is running
2023-07-29T12:12:12-05:00 INFO: mtest-samp-3 is running
2023-07-29T12:12:12-05:00 INFO: mtest-samp-4 is running
2023-07-29T12:12:12-05:00 INFO: starting mtest-agg-11
afe0d6be892934b145cbf4e26634157bd82e92c1a59e2eb20e6de5af4257c70e
2023-07-29T12:12:13-05:00 INFO: starting mtest-agg-12
3ac405e291cf690ae7a2a58820f3c8fc72ddd8af0a9a63fc13d1c5de20b1809b
2023-07-29T12:12:15-05:00 INFO: mtest-agg-11 is running
2023-07-29T12:12:15-05:00 INFO: mtest-agg-12 is running
2023-07-29T12:12:15-05:00 INFO: starting mtest-agg-2
5a7489c93ca8e66243af89765d12d774ae4253d1f95132a073015f8f27c4ec7e
2023-07-29T12:12:16-05:00 INFO: mtest-agg-2 is running
2023-07-29T12:12:16-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-29T12:14:17-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-29T12:14:19-05:00 INFO: sos check rc: 0
2023-07-29T12:14:20-05:00 INFO: starting mtest-ui
5d72a0f4cf3bea6b9d0b2e236ebdf3162d580239b7e67baf85a0356da7c027c8
2023-07-29T12:14:22-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2611544, 1690650755001.163], [2611544, 1690650755001.164], [2611544, 1690650755001.174], [2611544, 1690650755001.174], [2611916, 1690650756001.325], [2611916, 1690650756001.3262], [2611916, 1690650756001.3262], [2611916, 1690650756001.3281], [2611916, 1690650757001.486], [2611916, 1690650757001.4878], [2611916, 1690650757001.491], [2611916, 1690650757001.492], [2611916, 1690650758001.625], [2611916, 1690650758001.633], [2611916, 1690650758001.636], [2611916, 1690650758001.636], [2611916, 1690650759001.779], [2611916, 1690650759001.783], [2611916, 1690650759001.7869], [2611916, 1690650759001.7878], [2611916, 1690650760001.941], [2611916, 1690650760001.942], [2611916, 1690650760001.944], [2611916, 1690650760001.946], [2611916, 1690650761002.0679], [2611916, 1690650761002.071], [2611916, 1690650761002.072], [2611916, 1690650761002.073], [2611916, 1690650762001.229], [2611916, 1690650762001.232], [2611916, 1690650762001.232], [2611916, 1690650762001.234], [2611916, 1690650763001.377], [2611916, 1690650763001.3792], [2611916, 1690650763001.3828], [2611916, 1690650763001.384], [2611916, 1690650764001.52], [2611916, 1690650764001.521], [2611916, 1690650764001.5261], [2611916, 1690650764001.5261], [2611916, 1690650765001.6792], [2611916, 1690650765001.681], [2611916, 1690650765001.6829], [2611916, 1690650765001.684], [2611916, 1690650766001.8079], [2611916, 1690650766001.814], [2611916, 1690650766001.8162], [2611916, 1690650766001.8188], [2611916, 1690650767001.97], [2611916, 1690650767001.971], [2611916, 1690650767001.973], [2611916, 1690650767001.974], [2611916, 1690650768002.113], [2611916, 1690650768002.1162], [2611916, 1690650768002.1199], [2611916, 1690650768002.1382], [2611916, 1690650769001.2659], [2611916, 1690650769001.271], [2611916, 1690650769001.272], [2611916, 1690650769001.279], [2611916, 1690650770001.3928], [2611916, 1690650770001.4011], [2611916, 1690650770001.4028], [2611916, 1690650770001.412], [2611916, 1690650771001.527], [2611916, 1690650771001.529], [2611916, 1690650771001.53], [2611916, 1690650771001.536], [2611916, 1690650772001.663], [2611916, 1690650772001.67], [2611916, 1690650772001.6719], [2611916, 1690650772001.676], [2611916, 1690650773001.818], [2611916, 1690650773001.822], [2611916, 1690650773001.826], [2611916, 1690650773001.8281], [2611916, 1690650774001.939], [2611916, 1690650774001.939], [2611916, 1690650774001.941], [2611916, 1690650774001.945], [2611916, 1690650775002.069], [2611916, 1690650775002.075], [2611916, 1690650775002.078], [2611916, 1690650775002.078], [2611916, 1690650776000.255], [2611916, 1690650776001.192], [2611916, 1690650776001.208], [2611916, 1690650776001.216], [2611916, 1690650777001.333], [2611916, 1690650777001.335], [2611916, 1690650777001.3381], [2611916, 1690650777001.343], [2611916, 1690650778000.534], [2611916, 1690650778000.8818], [2611916, 1690650778001.458], [2611916, 1690650778001.46], [2611916, 1690650779001.603], [2611916, 1690650779001.603], [2611916, 1690650779001.609], [2611916, 1690650779001.611], [2611916, 1690650780000.975], [2611916, 1690650780001.187], [2611916, 1690650780001.7148], [2611916, 1690650780001.72], [2611916, 1690650781001.325], [2611916, 1690650781001.3262], [2611916, 1690650781001.85], [2611916, 1690650781001.856], [2611916, 1690650782001.2979], [2611916, 1690650782001.4421], [2611916, 1690650782001.4668], [2611916, 1690650782001.536], [2611916, 1690650783001.462], [2611916, 1690650783001.466], [2611916, 1690650783001.602], [2611916, 1690650783001.671], [2611916, 1690650784001.617], [2611916, 1690650784001.624], [2611916, 1690650784001.645], [2611916, 1690650784001.7422], [2611916, 1690650785001.755], [2611916, 1690650785001.7668], [2611916, 1690650785001.78], [2611916, 1690650785001.8728], [2611916, 1690650786001.8828], [2611916, 1690650786001.8938], [2611916, 1690650786001.908], [2611916, 1690650786001.922], [2611916, 1690650787002.061], [2611916, 1690650787002.064], [2611916, 1690650787002.073], [2611916, 1690650787002.077], [2611916, 1690650788001.2239], [2611916, 1690650788001.226], [2611916, 1690650788001.228], [2611916, 1690650788001.229], [2611916, 1690650789001.377], [2611916, 1690650789001.378], [2611916, 1690650789001.3792], [2611916, 1690650789001.384], [2611916, 1690650790001.495], [2611916, 1690650790001.496], [2611916, 1690650790001.502], [2611916, 1690650790001.504], [2611916, 1690650791001.627], [2611916, 1690650791001.628], [2611916, 1690650791001.63], [2611916, 1690650791001.6309], [2611916, 1690650792000.8599], [2611916, 1690650792001.527], [2611916, 1690650792001.752], [2611916, 1690650792001.7668], [2611916, 1690650793001.6938], [2611916, 1690650793001.699], [2611916, 1690650793001.92], [2611916, 1690650793001.968], [2611916, 1690650794001.846], [2611916, 1690650794001.855], [2611916, 1690650794001.857], [2611916, 1690650794002.108], [2611916, 1690650795001.259], [2611916, 1690650795001.999], [2611916, 1690650795001.999], [2611916, 1690650795002.005], [2611916, 1690650796001.143], [2611916, 1690650796001.147], [2611916, 1690650796001.147], [2611916, 1690650796001.405], [2611916, 1690650797001.2942], [2611916, 1690650797001.296], [2611916, 1690650797001.296], [2611916, 1690650797001.554], [2611916, 1690650798001.405], [2611916, 1690650798001.447], [2611916, 1690650798001.479], [2611916, 1690650798001.674], [2611916, 1690650799001.568], [2611916, 1690650799001.573], [2611916, 1690650799001.594], [2611916, 1690650799001.5989], [2611916, 1690650800001.195], [2611916, 1690650800001.697], [2611916, 1690650800001.709], [2611916, 1690650800001.71], [2611916, 1690650801001.3398], [2611916, 1690650801001.342], [2611916, 1690650801001.812], [2611916, 1690650801001.8271], [2611916, 1690650802001.471], [2611916, 1690650802001.4731], [2611916, 1690650802001.58], [2611916, 1690650802001.972], [2611916, 1690650803001.618], [2611916, 1690650803001.6309], [2611916, 1690650803001.635], [2611916, 1690650803002.091], [2611916, 1690650804001.25], [2611916, 1690650804001.2532], [2611916, 1690650804001.7422], [2611916, 1690650804001.7668], [2611916, 1690650805001.4011], [2611916, 1690650805001.416], [2611916, 1690650805001.418], [2611916, 1690650805001.895], [2611916, 1690650806001.469], [2611916, 1690650806001.509], [2611916, 1690650806001.528], [2611916, 1690650806001.529], [2611916, 1690650807001.596], [2611916, 1690650807001.63], [2611916, 1690650807001.645], [2611916, 1690650807001.655], [2611916, 1690650808001.727], [2611916, 1690650808001.759], [2611916, 1690650808001.7651], [2611916, 1690650808001.783], [2611916, 1690650809001.888], [2611916, 1690650809001.906], [2611916, 1690650809001.908], [2611916, 1690650809001.9258], [2611916, 1690650810002.022], [2611916, 1690650810002.023], [2611916, 1690650810002.027], [2611916, 1690650810002.05], [2611916, 1690650811001.167], [2611916, 1690650811001.168], [2611916, 1690650811001.169], [2611916, 1690650811001.179], [2611916, 1690650812001.292], [2611916, 1690650812001.296], [2611916, 1690650812001.2979], [2611916, 1690650812001.302], [2611916, 1690650813000.664], [2611916, 1690650813000.784], [2611916, 1690650813001.413], [2611916, 1690650813001.419], [2611916, 1690650814001.577], [2611916, 1690650814001.579], [2611916, 1690650814001.582], [2611916, 1690650814001.583], [2611916, 1690650815001.723], [2611916, 1690650815001.723], [2611916, 1690650815001.7249], [2611916, 1690650815001.727], [2611916, 1690650816001.878], [2611916, 1690650816001.8792], [2611916, 1690650816001.881], [2611916, 1690650816001.8828], [2611916, 1690650817002.0322], [2611916, 1690650817002.035], [2611916, 1690650817002.035], [2611916, 1690650817002.037], [2611916, 1690650818001.166], [2611916, 1690650818001.1719], [2611916, 1690650818001.1719], [2611916, 1690650818001.173], [2611916, 1690650819001.323], [2611916, 1690650819001.325], [2611916, 1690650819001.3298], [2611916, 1690650819001.332], [2611916, 1690650820001.471], [2611916, 1690650820001.472], [2611916, 1690650820001.475], [2611916, 1690650820001.475], [2611916, 1690650821001.591], [2611916, 1690650821001.5989], [2611916, 1690650821001.602], [2611916, 1690650821001.604], [2611916, 1690650822001.7239], [2611916, 1690650822001.7258], [2611916, 1690650822001.727], [2611916, 1690650822001.728], [2611916, 1690650823001.292], [2611916, 1690650823001.7212], [2611916, 1690650823001.834], [2611916, 1690650823001.8582], [2611916, 1690650824001.461], [2611916, 1690650824001.464], [2611916, 1690650824001.465], [2611916, 1690650824002.009], [2611916, 1690650825000.633], [2611916, 1690650825001.061], [2611916, 1690650825001.585], [2611916, 1690650825001.588], [2611916, 1690650826001.188], [2611916, 1690650826001.192], [2611916, 1690650826001.707], [2611916, 1690650826001.723], [2611916, 1690650827001.333], [2611916, 1690650827001.3381], [2611916, 1690650827001.448], [2611916, 1690650827001.8691], [2611916, 1690650828001.472], [2611916, 1690650828001.482], [2611916, 1690650828001.4878], [2611916, 1690650828001.991], [2611916, 1690650829001.624], [2611916, 1690650829001.638], [2611916, 1690650829001.64], [2611916, 1690650829001.7532], [2611916, 1690650830000.876], [2611916, 1690650830001.7742], [2611916, 1690650830001.7979], [2611916, 1690650830001.891], [2611916, 1690650831001.923], [2611916, 1690650831001.927], [2611916, 1690650831001.927], [2611916, 1690650831001.929], [2611916, 1690650832002.048], [2611916, 1690650832002.049], [2611916, 1690650832002.05], [2611916, 1690650832002.052], [2611916, 1690650833001.1782], [2611916, 1690650833001.179], [2611916, 1690650833001.184], [2611916, 1690650833001.185], [2611916, 1690650834001.3162], [2611916, 1690650834001.321], [2611916, 1690650834001.321], [2611916, 1690650834001.323], [2611916, 1690650835001.48], [2611916, 1690650835001.4841], [2611916, 1690650835001.4841], [2611916, 1690650835001.485], [2611916, 1690650836001.6], [2611916, 1690650836001.608], [2611916, 1690650836001.609], [2611916, 1690650836001.6099], [2611916, 1690650837001.075], [2611916, 1690650837001.7222], [2611916, 1690650837001.731], [2611916, 1690650837001.744], [2611916, 1690650838001.2312], [2611916, 1690650838001.234], [2611916, 1690650838001.876], [2611916, 1690650838001.877], [2611916, 1690650839001.3818], [2611916, 1690650839001.3818], [2611916, 1690650839002.022], [2611916, 1690650839002.024], [2611916, 1690650840001.173], [2611916, 1690650840001.174], [2611916, 1690650840001.536], [2611916, 1690650840001.538], [2611916, 1690650841001.3079], [2611916, 1690650841001.312], [2611916, 1690650841001.664], [2611916, 1690650841001.665], [2611916, 1690650842001.444], [2611916, 1690650842001.449], [2611916, 1690650842001.797], [2611916, 1690650842001.802], [2611916, 1690650843001.591], [2611916, 1690650843001.592], [2611916, 1690650843001.937], [2611916, 1690650843001.94], [2611916, 1690650844001.734], [2611916, 1690650844001.7358], [2611916, 1690650844002.0742], [2611916, 1690650844002.082], [2611916, 1690650845001.223], [2611916, 1690650845001.232], [2611916, 1690650845001.892], [2611916, 1690650845001.8938], [2611916, 1690650846001.3828], [2611916, 1690650846001.384], [2611916, 1690650846002.0469], [2611916, 1690650846002.048], [2611916, 1690650847001.1992], [2611916, 1690650847001.201], [2611916, 1690650847001.536], [2611916, 1690650847001.539], [2611916, 1690650848001.3088], [2611916, 1690650848001.344], [2611916, 1690650848001.384], [2611916, 1690650848001.663], [2611916, 1690650849001.468], [2611916, 1690650849001.474], [2611916, 1690650849001.482], [2611916, 1690650849001.525], [2611916, 1690650850001.633], [2611916, 1690650850001.635], [2611916, 1690650850001.6372], [2611916, 1690650850001.671], [2611916, 1690650851001.7751], [2611916, 1690650851001.7761], [2611916, 1690650851001.7778], [2611916, 1690650851001.794], [2611916, 1690650852001.917], [2611916, 1690650852001.9211], [2611916, 1690650852001.923], [2611916, 1690650852001.925], [2611916, 1690650853002.05], [2611916, 1690650853002.056], [2611916, 1690650853002.063], [2611916, 1690650853002.081], [2611916, 1690650854001.197], [2611916, 1690650854001.203], [2611916, 1690650854001.203], [2611916, 1690650854001.218], [2611916, 1690650855001.335], [2611916, 1690650855001.3572], [2611916, 1690650855001.364], [2611916, 1690650855001.373], [2611916, 1690650856001.469], [2611916, 1690650856001.4778], [2611916, 1690650856001.489], [2611916, 1690650856001.507]]}, {"target": "component_id", "datapoints": [[2, 1690650755001.163], [1, 1690650755001.164], [3, 1690650755001.174], [4, 1690650755001.174], [3, 1690650756001.325], [2, 1690650756001.3262], [4, 1690650756001.3262], [1, 1690650756001.3281], [4, 1690650757001.486], [1, 1690650757001.4878], [3, 1690650757001.491], [2, 1690650757001.492], [2, 1690650758001.625], [1, 1690650758001.633], [3, 1690650758001.636], [4, 1690650758001.636], [2, 1690650759001.779], [1, 1690650759001.783], [4, 1690650759001.7869], [3, 1690650759001.7878], [1, 1690650760001.941], [2, 1690650760001.942], [4, 1690650760001.944], [3, 1690650760001.946], [1, 1690650761002.0679], [3, 1690650761002.071], [4, 1690650761002.072], [2, 1690650761002.073], [1, 1690650762001.229], [3, 1690650762001.232], [4, 1690650762001.232], [2, 1690650762001.234], [1, 1690650763001.377], [3, 1690650763001.3792], [4, 1690650763001.3828], [2, 1690650763001.384], [1, 1690650764001.52], [3, 1690650764001.521], [2, 1690650764001.5261], [4, 1690650764001.5261], [1, 1690650765001.6792], [4, 1690650765001.681], [3, 1690650765001.6829], [2, 1690650765001.684], [2, 1690650766001.8079], [3, 1690650766001.814], [4, 1690650766001.8162], [1, 1690650766001.8188], [2, 1690650767001.97], [1, 1690650767001.971], [3, 1690650767001.973], [4, 1690650767001.974], [1, 1690650768002.113], [2, 1690650768002.1162], [4, 1690650768002.1199], [3, 1690650768002.1382], [4, 1690650769001.2659], [1, 1690650769001.271], [2, 1690650769001.272], [3, 1690650769001.279], [2, 1690650770001.3928], [4, 1690650770001.4011], [3, 1690650770001.4028], [1, 1690650770001.412], [2, 1690650771001.527], [4, 1690650771001.529], [3, 1690650771001.53], [1, 1690650771001.536], [2, 1690650772001.663], [4, 1690650772001.67], [1, 1690650772001.6719], [3, 1690650772001.676], [2, 1690650773001.818], [1, 1690650773001.822], [3, 1690650773001.826], [4, 1690650773001.8281], [1, 1690650774001.939], [4, 1690650774001.939], [2, 1690650774001.941], [3, 1690650774001.945], [2, 1690650775002.069], [1, 1690650775002.075], [3, 1690650775002.078], [4, 1690650775002.078], [4, 1690650776000.255], [3, 1690650776001.192], [2, 1690650776001.208], [1, 1690650776001.216], [2, 1690650777001.333], [4, 1690650777001.335], [1, 1690650777001.3381], [3, 1690650777001.343], [2, 1690650778000.534], [3, 1690650778000.8818], [4, 1690650778001.458], [1, 1690650778001.46], [3, 1690650779001.603], [4, 1690650779001.603], [2, 1690650779001.609], [1, 1690650779001.611], [2, 1690650780000.975], [4, 1690650780001.187], [1, 1690650780001.7148], [3, 1690650780001.72], [4, 1690650781001.325], [3, 1690650781001.3262], [2, 1690650781001.85], [1, 1690650781001.856], [1, 1690650782001.2979], [4, 1690650782001.4421], [3, 1690650782001.4668], [2, 1690650782001.536], [4, 1690650783001.462], [1, 1690650783001.466], [3, 1690650783001.602], [2, 1690650783001.671], [4, 1690650784001.617], [1, 1690650784001.624], [3, 1690650784001.645], [2, 1690650784001.7422], [1, 1690650785001.755], [4, 1690650785001.7668], [3, 1690650785001.78], [2, 1690650785001.8728], [2, 1690650786001.8828], [1, 1690650786001.8938], [4, 1690650786001.908], [3, 1690650786001.922], [1, 1690650787002.061], [2, 1690650787002.064], [3, 1690650787002.073], [4, 1690650787002.077], [3, 1690650788001.2239], [1, 1690650788001.226], [2, 1690650788001.228], [4, 1690650788001.229], [4, 1690650789001.377], [2, 1690650789001.378], [1, 1690650789001.3792], [3, 1690650789001.384], [1, 1690650790001.495], [2, 1690650790001.496], [4, 1690650790001.502], [3, 1690650790001.504], [3, 1690650791001.627], [1, 1690650791001.628], [4, 1690650791001.63], [2, 1690650791001.6309], [1, 1690650792000.8599], [2, 1690650792001.527], [3, 1690650792001.752], [4, 1690650792001.7668], [3, 1690650793001.6938], [2, 1690650793001.699], [4, 1690650793001.92], [1, 1690650793001.968], [2, 1690650794001.846], [3, 1690650794001.855], [1, 1690650794001.857], [4, 1690650794002.108], [4, 1690650795001.259], [1, 1690650795001.999], [3, 1690650795001.999], [2, 1690650795002.005], [1, 1690650796001.143], [2, 1690650796001.147], [3, 1690650796001.147], [4, 1690650796001.405], [1, 1690650797001.2942], [2, 1690650797001.296], [3, 1690650797001.296], [4, 1690650797001.554], [3, 1690650798001.405], [2, 1690650798001.447], [1, 1690650798001.479], [4, 1690650798001.674], [1, 1690650799001.568], [3, 1690650799001.573], [4, 1690650799001.594], [2, 1690650799001.5989], [4, 1690650800001.195], [1, 1690650800001.697], [3, 1690650800001.709], [2, 1690650800001.71], [2, 1690650801001.3398], [4, 1690650801001.342], [3, 1690650801001.812], [1, 1690650801001.8271], [2, 1690650802001.471], [4, 1690650802001.4731], [3, 1690650802001.58], [1, 1690650802001.972], [2, 1690650803001.618], [3, 1690650803001.6309], [4, 1690650803001.635], [1, 1690650803002.091], [3, 1690650804001.25], [1, 1690650804001.2532], [4, 1690650804001.7422], [2, 1690650804001.7668], [1, 1690650805001.4011], [3, 1690650805001.416], [4, 1690650805001.418], [2, 1690650805001.895], [2, 1690650806001.469], [1, 1690650806001.509], [4, 1690650806001.528], [3, 1690650806001.529], [2, 1690650807001.596], [1, 1690650807001.63], [3, 1690650807001.645], [4, 1690650807001.655], [2, 1690650808001.727], [1, 1690650808001.759], [3, 1690650808001.7651], [4, 1690650808001.783], [2, 1690650809001.888], [1, 1690650809001.906], [3, 1690650809001.908], [4, 1690650809001.9258], [3, 1690650810002.022], [1, 1690650810002.023], [2, 1690650810002.027], [4, 1690650810002.05], [3, 1690650811001.167], [1, 1690650811001.168], [2, 1690650811001.169], [4, 1690650811001.179], [4, 1690650812001.292], [1, 1690650812001.296], [3, 1690650812001.2979], [2, 1690650812001.302], [4, 1690650813000.664], [1, 1690650813000.784], [2, 1690650813001.413], [3, 1690650813001.419], [1, 1690650814001.577], [3, 1690650814001.579], [4, 1690650814001.582], [2, 1690650814001.583], [3, 1690650815001.723], [4, 1690650815001.723], [1, 1690650815001.7249], [2, 1690650815001.727], [2, 1690650816001.878], [1, 1690650816001.8792], [4, 1690650816001.881], [3, 1690650816001.8828], [1, 1690650817002.0322], [3, 1690650817002.035], [4, 1690650817002.035], [2, 1690650817002.037], [3, 1690650818001.166], [1, 1690650818001.1719], [4, 1690650818001.1719], [2, 1690650818001.173], [2, 1690650819001.323], [4, 1690650819001.325], [3, 1690650819001.3298], [1, 1690650819001.332], [4, 1690650820001.471], [2, 1690650820001.472], [1, 1690650820001.475], [3, 1690650820001.475], [4, 1690650821001.591], [3, 1690650821001.5989], [1, 1690650821001.602], [2, 1690650821001.604], [4, 1690650822001.7239], [3, 1690650822001.7258], [1, 1690650822001.727], [2, 1690650822001.728], [3, 1690650823001.292], [4, 1690650823001.7212], [2, 1690650823001.834], [1, 1690650823001.8582], [4, 1690650824001.461], [3, 1690650824001.464], [2, 1690650824001.465], [1, 1690650824002.009], [4, 1690650825000.633], [1, 1690650825001.061], [2, 1690650825001.585], [3, 1690650825001.588], [1, 1690650826001.188], [2, 1690650826001.192], [3, 1690650826001.707], [4, 1690650826001.723], [2, 1690650827001.333], [1, 1690650827001.3381], [3, 1690650827001.448], [4, 1690650827001.8691], [2, 1690650828001.472], [3, 1690650828001.482], [1, 1690650828001.4878], [4, 1690650828001.991], [2, 1690650829001.624], [3, 1690650829001.638], [4, 1690650829001.64], [1, 1690650829001.7532], [2, 1690650830000.876], [3, 1690650830001.7742], [4, 1690650830001.7979], [1, 1690650830001.891], [4, 1690650831001.923], [2, 1690650831001.927], [3, 1690650831001.927], [1, 1690650831001.929], [2, 1690650832002.048], [4, 1690650832002.049], [1, 1690650832002.05], [3, 1690650832002.052], [1, 1690650833001.1782], [2, 1690650833001.179], [4, 1690650833001.184], [3, 1690650833001.185], [4, 1690650834001.3162], [1, 1690650834001.321], [3, 1690650834001.321], [2, 1690650834001.323], [3, 1690650835001.48], [2, 1690650835001.4841], [4, 1690650835001.4841], [1, 1690650835001.485], [3, 1690650836001.6], [1, 1690650836001.608], [2, 1690650836001.609], [4, 1690650836001.6099], [3, 1690650837001.075], [1, 1690650837001.7222], [2, 1690650837001.731], [4, 1690650837001.744], [1, 1690650838001.2312], [3, 1690650838001.234], [2, 1690650838001.876], [4, 1690650838001.877], [1, 1690650839001.3818], [3, 1690650839001.3818], [2, 1690650839002.022], [4, 1690650839002.024], [2, 1690650840001.173], [4, 1690650840001.174], [1, 1690650840001.536], [3, 1690650840001.538], [4, 1690650841001.3079], [2, 1690650841001.312], [1, 1690650841001.664], [3, 1690650841001.665], [2, 1690650842001.444], [4, 1690650842001.449], [1, 1690650842001.797], [3, 1690650842001.802], [4, 1690650843001.591], [2, 1690650843001.592], [1, 1690650843001.937], [3, 1690650843001.94], [4, 1690650844001.734], [2, 1690650844001.7358], [3, 1690650844002.0742], [1, 1690650844002.082], [1, 1690650845001.223], [3, 1690650845001.232], [4, 1690650845001.892], [2, 1690650845001.8938], [1, 1690650846001.3828], [3, 1690650846001.384], [4, 1690650846002.0469], [2, 1690650846002.048], [2, 1690650847001.1992], [4, 1690650847001.201], [1, 1690650847001.536], [3, 1690650847001.539], [3, 1690650848001.3088], [2, 1690650848001.344], [4, 1690650848001.384], [1, 1690650848001.663], [1, 1690650849001.468], [3, 1690650849001.474], [2, 1690650849001.482], [4, 1690650849001.525], [2, 1690650850001.633], [1, 1690650850001.635], [3, 1690650850001.6372], [4, 1690650850001.671], [1, 1690650851001.7751], [2, 1690650851001.7761], [3, 1690650851001.7778], [4, 1690650851001.794], [2, 1690650852001.917], [1, 1690650852001.9211], [4, 1690650852001.923], [3, 1690650852001.925], [2, 1690650853002.05], [1, 1690650853002.056], [4, 1690650853002.063], [3, 1690650853002.081], [4, 1690650854001.197], [1, 1690650854001.203], [2, 1690650854001.203], [3, 1690650854001.218], [1, 1690650855001.335], [4, 1690650855001.3572], [2, 1690650855001.364], [3, 1690650855001.373], [1, 1690650856001.469], [4, 1690650856001.4778], [2, 1690650856001.489], [3, 1690650856001.507]]}, {"target": "job_id", "datapoints": [[0, 1690650755001.163], [0, 1690650755001.164], [0, 1690650755001.174], [0, 1690650755001.174], [0, 1690650756001.325], [0, 1690650756001.3262], [0, 1690650756001.3262], [0, 1690650756001.3281], [0, 1690650757001.486], [0, 1690650757001.4878], [0, 1690650757001.491], [0, 1690650757001.492], [0, 1690650758001.625], [0, 1690650758001.633], [0, 1690650758001.636], [0, 1690650758001.636], [0, 1690650759001.779], [0, 1690650759001.783], [0, 1690650759001.7869], [0, 1690650759001.7878], [0, 1690650760001.941], [0, 1690650760001.942], [0, 1690650760001.944], [0, 1690650760001.946], [0, 1690650761002.0679], [0, 1690650761002.071], [0, 1690650761002.072], [0, 1690650761002.073], [0, 1690650762001.229], [0, 1690650762001.232], [0, 1690650762001.232], [0, 1690650762001.234], [0, 1690650763001.377], [0, 1690650763001.3792], [0, 1690650763001.3828], [0, 1690650763001.384], [0, 1690650764001.52], [0, 1690650764001.521], [0, 1690650764001.5261], [0, 1690650764001.5261], [0, 1690650765001.6792], [0, 1690650765001.681], [0, 1690650765001.6829], [0, 1690650765001.684], [0, 1690650766001.8079], [0, 1690650766001.814], [0, 1690650766001.8162], [0, 1690650766001.8188], [0, 1690650767001.97], [0, 1690650767001.971], [0, 1690650767001.973], [0, 1690650767001.974], [0, 1690650768002.113], [0, 1690650768002.1162], [0, 1690650768002.1199], [0, 1690650768002.1382], [0, 1690650769001.2659], [0, 1690650769001.271], [0, 1690650769001.272], [0, 1690650769001.279], [0, 1690650770001.3928], [0, 1690650770001.4011], [0, 1690650770001.4028], [0, 1690650770001.412], [0, 1690650771001.527], [0, 1690650771001.529], [0, 1690650771001.53], [0, 1690650771001.536], [0, 1690650772001.663], [0, 1690650772001.67], [0, 1690650772001.6719], [0, 1690650772001.676], [0, 1690650773001.818], [0, 1690650773001.822], [0, 1690650773001.826], [0, 1690650773001.8281], [0, 1690650774001.939], [0, 1690650774001.939], [0, 1690650774001.941], [0, 1690650774001.945], [0, 1690650775002.069], [0, 1690650775002.075], [0, 1690650775002.078], [0, 1690650775002.078], [0, 1690650776000.255], [0, 1690650776001.192], [0, 1690650776001.208], [0, 1690650776001.216], [0, 1690650777001.333], [0, 1690650777001.335], [0, 1690650777001.3381], [0, 1690650777001.343], [0, 1690650778000.534], [0, 1690650778000.8818], [0, 1690650778001.458], [0, 1690650778001.46], [0, 1690650779001.603], [0, 1690650779001.603], [0, 1690650779001.609], [0, 1690650779001.611], [0, 1690650780000.975], [0, 1690650780001.187], [0, 1690650780001.7148], [0, 1690650780001.72], [0, 1690650781001.325], [0, 1690650781001.3262], [0, 1690650781001.85], [0, 1690650781001.856], [0, 1690650782001.2979], [0, 1690650782001.4421], [0, 1690650782001.4668], [0, 1690650782001.536], [0, 1690650783001.462], [0, 1690650783001.466], [0, 1690650783001.602], [0, 1690650783001.671], [0, 1690650784001.617], [0, 1690650784001.624], [0, 1690650784001.645], [0, 1690650784001.7422], [0, 1690650785001.755], [0, 1690650785001.7668], [0, 1690650785001.78], [0, 1690650785001.8728], [0, 1690650786001.8828], [0, 1690650786001.8938], [0, 1690650786001.908], [0, 1690650786001.922], [0, 1690650787002.061], [0, 1690650787002.064], [0, 1690650787002.073], [0, 1690650787002.077], [0, 1690650788001.2239], [0, 1690650788001.226], [0, 1690650788001.228], [0, 1690650788001.229], [0, 1690650789001.377], [0, 1690650789001.378], [0, 1690650789001.3792], [0, 1690650789001.384], [0, 1690650790001.495], [0, 1690650790001.496], [0, 1690650790001.502], [0, 1690650790001.504], [0, 1690650791001.627], [0, 1690650791001.628], [0, 1690650791001.63], [0, 1690650791001.6309], [0, 1690650792000.8599], [0, 1690650792001.527], [0, 1690650792001.752], [0, 1690650792001.7668], [0, 1690650793001.6938], [0, 1690650793001.699], [0, 1690650793001.92], [0, 1690650793001.968], [0, 1690650794001.846], [0, 1690650794001.855], [0, 1690650794001.857], [0, 1690650794002.108], [0, 1690650795001.259], [0, 1690650795001.999], [0, 1690650795001.999], [0, 1690650795002.005], [0, 1690650796001.143], [0, 1690650796001.147], [0, 1690650796001.147], [0, 1690650796001.405], [0, 1690650797001.2942], [0, 1690650797001.296], [0, 1690650797001.296], [0, 1690650797001.554], [0, 1690650798001.405], [0, 1690650798001.447], [0, 1690650798001.479], [0, 1690650798001.674], [0, 1690650799001.568], [0, 1690650799001.573], [0, 1690650799001.594], [0, 1690650799001.5989], [0, 1690650800001.195], [0, 1690650800001.697], [0, 1690650800001.709], [0, 1690650800001.71], [0, 1690650801001.3398], [0, 1690650801001.342], [0, 1690650801001.812], [0, 1690650801001.8271], [0, 1690650802001.471], [0, 1690650802001.4731], [0, 1690650802001.58], [0, 1690650802001.972], [0, 1690650803001.618], [0, 1690650803001.6309], [0, 1690650803001.635], [0, 1690650803002.091], [0, 1690650804001.25], [0, 1690650804001.2532], [0, 1690650804001.7422], [0, 1690650804001.7668], [0, 1690650805001.4011], [0, 1690650805001.416], [0, 1690650805001.418], [0, 1690650805001.895], [0, 1690650806001.469], [0, 1690650806001.509], [0, 1690650806001.528], [0, 1690650806001.529], [0, 1690650807001.596], [0, 1690650807001.63], [0, 1690650807001.645], [0, 1690650807001.655], [0, 1690650808001.727], [0, 1690650808001.759], [0, 1690650808001.7651], [0, 1690650808001.783], [0, 1690650809001.888], [0, 1690650809001.906], [0, 1690650809001.908], [0, 1690650809001.9258], [0, 1690650810002.022], [0, 1690650810002.023], [0, 1690650810002.027], [0, 1690650810002.05], [0, 1690650811001.167], [0, 1690650811001.168], [0, 1690650811001.169], [0, 1690650811001.179], [0, 1690650812001.292], [0, 1690650812001.296], [0, 1690650812001.2979], [0, 1690650812001.302], [0, 1690650813000.664], [0, 1690650813000.784], [0, 1690650813001.413], [0, 1690650813001.419], [0, 1690650814001.577], [0, 1690650814001.579], [0, 1690650814001.582], [0, 1690650814001.583], [0, 1690650815001.723], [0, 1690650815001.723], [0, 1690650815001.7249], [0, 1690650815001.727], [0, 1690650816001.878], [0, 1690650816001.8792], [0, 1690650816001.881], [0, 1690650816001.8828], [0, 1690650817002.0322], [0, 1690650817002.035], [0, 1690650817002.035], [0, 1690650817002.037], [0, 1690650818001.166], [0, 1690650818001.1719], [0, 1690650818001.1719], [0, 1690650818001.173], [0, 1690650819001.323], [0, 1690650819001.325], [0, 1690650819001.3298], [0, 1690650819001.332], [0, 1690650820001.471], [0, 1690650820001.472], [0, 1690650820001.475], [0, 1690650820001.475], [0, 1690650821001.591], [0, 1690650821001.5989], [0, 1690650821001.602], [0, 1690650821001.604], [0, 1690650822001.7239], [0, 1690650822001.7258], [0, 1690650822001.727], [0, 1690650822001.728], [0, 1690650823001.292], [0, 1690650823001.7212], [0, 1690650823001.834], [0, 1690650823001.8582], [0, 1690650824001.461], [0, 1690650824001.464], [0, 1690650824001.465], [0, 1690650824002.009], [0, 1690650825000.633], [0, 1690650825001.061], [0, 1690650825001.585], [0, 1690650825001.588], [0, 1690650826001.188], [0, 1690650826001.192], [0, 1690650826001.707], [0, 1690650826001.723], [0, 1690650827001.333], [0, 1690650827001.3381], [0, 1690650827001.448], [0, 1690650827001.8691], [0, 1690650828001.472], [0, 1690650828001.482], [0, 1690650828001.4878], [0, 1690650828001.991], [0, 1690650829001.624], [0, 1690650829001.638], [0, 1690650829001.64], [0, 1690650829001.7532], [0, 1690650830000.876], [0, 1690650830001.7742], [0, 1690650830001.7979], [0, 1690650830001.891], [0, 1690650831001.923], [0, 1690650831001.927], [0, 1690650831001.927], [0, 1690650831001.929], [0, 1690650832002.048], [0, 1690650832002.049], [0, 1690650832002.05], [0, 1690650832002.052], [0, 1690650833001.1782], [0, 1690650833001.179], [0, 1690650833001.184], [0, 1690650833001.185], [0, 1690650834001.3162], [0, 1690650834001.321], [0, 1690650834001.321], [0, 1690650834001.323], [0, 1690650835001.48], [0, 1690650835001.4841], [0, 1690650835001.4841], [0, 1690650835001.485], [0, 1690650836001.6], [0, 1690650836001.608], [0, 1690650836001.609], [0, 1690650836001.6099], [0, 1690650837001.075], [0, 1690650837001.7222], [0, 1690650837001.731], [0, 1690650837001.744], [0, 1690650838001.2312], [0, 1690650838001.234], [0, 1690650838001.876], [0, 1690650838001.877], [0, 1690650839001.3818], [0, 1690650839001.3818], [0, 1690650839002.022], [0, 1690650839002.024], [0, 1690650840001.173], [0, 1690650840001.174], [0, 1690650840001.536], [0, 1690650840001.538], [0, 1690650841001.3079], [0, 1690650841001.312], [0, 1690650841001.664], [0, 1690650841001.665], [0, 1690650842001.444], [0, 1690650842001.449], [0, 1690650842001.797], [0, 1690650842001.802], [0, 1690650843001.591], [0, 1690650843001.592], [0, 1690650843001.937], [0, 1690650843001.94], [0, 1690650844001.734], [0, 1690650844001.7358], [0, 1690650844002.0742], [0, 1690650844002.082], [0, 1690650845001.223], [0, 1690650845001.232], [0, 1690650845001.892], [0, 1690650845001.8938], [0, 1690650846001.3828], [0, 1690650846001.384], [0, 1690650846002.0469], [0, 1690650846002.048], [0, 1690650847001.1992], [0, 1690650847001.201], [0, 1690650847001.536], [0, 1690650847001.539], [0, 1690650848001.3088], [0, 1690650848001.344], [0, 1690650848001.384], [0, 1690650848001.663], [0, 1690650849001.468], [0, 1690650849001.474], [0, 1690650849001.482], [0, 1690650849001.525], [0, 1690650850001.633], [0, 1690650850001.635], [0, 1690650850001.6372], [0, 1690650850001.671], [0, 1690650851001.7751], [0, 1690650851001.7761], [0, 1690650851001.7778], [0, 1690650851001.794], [0, 1690650852001.917], [0, 1690650852001.9211], [0, 1690650852001.923], [0, 1690650852001.925], [0, 1690650853002.05], [0, 1690650853002.056], [0, 1690650853002.063], [0, 1690650853002.081], [0, 1690650854001.197], [0, 1690650854001.203], [0, 1690650854001.203], [0, 1690650854001.218], [0, 1690650855001.335], [0, 1690650855001.3572], [0, 1690650855001.364], [0, 1690650855001.373], [0, 1690650856001.469], [0, 1690650856001.4778], [0, 1690650856001.489], [0, 1690650856001.507]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-29T12:14:24-05:00 INFO: query check RC: 0
8c18a4fd0d31244e3b8465617198a3cf098086c74eb0ab6e4400d08afa955323
2023-07-29T12:14:55-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2955    912 --:--:-- --:--:-- --:--:--  3894
{"datasource":{"id":1,"uid":"T1m-ifq4k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-29T12:14:57-05:00 INFO: Checking grafana data
2023-07-29T12:14:57-05:00 INFO: Grafana data check, rc: 0
2023-07-29T12:14:57-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-29T12:15:02-05:00 INFO: DONE
2023-07-29 12:15:12 INFO: ----------------------------------------------
2023-07-29 12:15:12 INFO: ======== test-maestro-munge ========
2023-07-29 12:15:12 INFO: CMD: /home/narate/cron/ldms-test/ldms-containers/test/test-maestro-munge/test.sh
1+0 records in
1+0 records out
4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000290068 s, 14.1 MB/s
2023-07-29T12:15:13-05:00 INFO: starting mtest-maestro
6b61a84abca2bf9907765d5aea25dfcbc6468961a0c3e3cc088c37799e9a5bd2
2023-07-29T12:15:15-05:00 INFO: starting mtest-samp-1
602554a8f846ac136a2ed5443d8f2f0d8719952dea566bee129dc4e759514e8d
2023-07-29T12:15:17-05:00 INFO: starting mtest-samp-2
9e739201678e4566d21a1e9cd9b0fa13bab3ead2184d2fcde09ed766323e0151
2023-07-29T12:15:19-05:00 INFO: starting mtest-samp-3
5be4c2efb02438fbdc56d1b70cbbe67a99bb0fc96b59ff271222f5493ef5243c
2023-07-29T12:15:21-05:00 INFO: starting mtest-samp-4
86f485f8c1e5bf09a80c32ab90455b01d0e935f8b67a9c89d528565bfcae2ce1
2023-07-29T12:15:22-05:00 INFO: mtest-samp-1 is running
2023-07-29T12:15:22-05:00 INFO: mtest-samp-2 is running
2023-07-29T12:15:22-05:00 INFO: mtest-samp-3 is running
2023-07-29T12:15:22-05:00 INFO: mtest-samp-4 is running
2023-07-29T12:15:22-05:00 INFO: starting mtest-agg-11
794cdec89274eaa817f575a2f1368eae13e7b2f48327431c4d77cbcbd483d00a
2023-07-29T12:15:24-05:00 INFO: starting mtest-agg-12
aeb7fdbe8173b9c12471fdcab6c543e297c00f19afd8d69e671f24285c911142
2023-07-29T12:15:25-05:00 INFO: mtest-agg-11 is running
2023-07-29T12:15:25-05:00 INFO: mtest-agg-12 is running
2023-07-29T12:15:25-05:00 INFO: starting mtest-agg-2
1af37e403f72a5d189cc55576eb2bdd2b2c7e602717ed0eb742920512659bccd
2023-07-29T12:15:26-05:00 INFO: mtest-agg-2 is running
2023-07-29T12:15:26-05:00 INFO: Collecting data (into SOS)
mtest-agg-11
mtest-agg-12
2023-07-29T12:17:27-05:00 INFO: Checking SOS data
Component IDs: {1, 2, 3, 4}
2023-07-29T12:17:30-05:00 INFO: sos check rc: 0
2023-07-29T12:17:31-05:00 INFO: starting mtest-ui
a25477398066159b2eb1fee30823854ae41c6393a18d1d1d12bbe56cd3baf0ac
2023-07-29T12:17:32-05:00 INFO: Checking query from mtest-ui: http://mtest-ui/grafana/query
query results: b'[{"target": "Active", "datapoints": [[2611924, 1690650946001.684], [2612296, 1690650947001.85], [2612296, 1690650947001.8518], [2612296, 1690650947001.855], [2612296, 1690650947001.856], [2612296, 1690650948001.998], [2612296, 1690650948001.9998], [2612296, 1690650948002.005], [2612296, 1690650948002.007], [2612296, 1690650949001.149], [2612296, 1690650949001.149], [2612296, 1690650949001.155], [2612296, 1690650949001.156], [2612296, 1690650950001.283], [2612296, 1690650950001.2852], [2612296, 1690650950001.2869], [2612296, 1690650950001.2878], [2612296, 1690650951001.4312], [2612296, 1690650951001.4358], [2612296, 1690650951001.4358], [2612296, 1690650951001.437], [2612296, 1690650952001.582], [2612296, 1690650952001.594], [2612296, 1690650952001.596], [2612296, 1690650952001.6], [2612296, 1690650953001.7239], [2612296, 1690650953001.728], [2612296, 1690650953001.73], [2612296, 1690650953001.7322], [2612296, 1690650954001.874], [2612296, 1690650954001.875], [2612296, 1690650954001.877], [2612296, 1690650954001.878], [2612296, 1690650955002.011], [2612296, 1690650955002.018], [2612296, 1690650955002.019], [2612296, 1690650955002.0212], [2612296, 1690650956001.15], [2612296, 1690650956001.155], [2612296, 1690650956001.156], [2612296, 1690650956001.156], [2612296, 1690650957000.5261], [2612296, 1690650957001.283], [2612296, 1690650957001.3088], [2612296, 1690650957001.311], [2612296, 1690650958001.45], [2612296, 1690650958001.4531], [2612296, 1690650958001.454], [2612296, 1690650958001.46], [2612296, 1690650959001.58], [2612296, 1690650959001.581], [2612296, 1690650959001.583], [2612296, 1690650959001.583], [2612296, 1690650960001.695], [2612296, 1690650960001.7039], [2612296, 1690650960001.706], [2612296, 1690650960001.707], [2612296, 1690650961001.8308], [2612296, 1690650961001.834], [2612296, 1690650961001.839], [2612296, 1690650961001.847], [2612296, 1690650962001.98], [2612296, 1690650962001.982], [2612296, 1690650962001.985], [2612296, 1690650962001.986], [2612296, 1690650963002.1409], [2612296, 1690650963002.144], [2612296, 1690650963002.146], [2612296, 1690650963002.1492], [2612296, 1690650964001.281], [2612296, 1690650964001.282], [2612296, 1690650964001.2852], [2612296, 1690650964001.286], [2612296, 1690650965001.398], [2612296, 1690650965001.407], [2612296, 1690650965001.407], [2612296, 1690650965001.409], [2612296, 1690650966001.525], [2612296, 1690650966001.529], [2612296, 1690650966001.529], [2612296, 1690650966001.532], [2612296, 1690650967001.6682], [2612296, 1690650967001.671], [2612296, 1690650967001.674], [2612296, 1690650967001.674], [2612296, 1690650968001.826], [2612296, 1690650968001.829], [2612296, 1690650968001.832], [2612296, 1690650968001.833], [2612296, 1690650969000.9832], [2612296, 1690650969001.8591], [2612296, 1690650969001.948], [2612296, 1690650969001.977], [2612296, 1690650970001.983], [2612296, 1690650970002.081], [2612296, 1690650970002.098], [2612296, 1690650970002.107], [2612296, 1690650971001.212], [2612296, 1690650971001.215], [2612296, 1690650971001.222], [2612296, 1690650971002.112], [2612296, 1690650972001.2522], [2612296, 1690650972001.343], [2612296, 1690650972001.346], [2612296, 1690650972001.3472], [2612296, 1690650973001.412], [2612296, 1690650973001.486], [2612296, 1690650973001.49], [2612296, 1690650973001.493], [2612296, 1690650974001.556], [2612296, 1690650974001.618], [2612296, 1690650974001.6199], [2612296, 1690650974001.627], [2612296, 1690650975001.686], [2612296, 1690650975001.737], [2612296, 1690650975001.7449], [2612296, 1690650975001.749], [2612296, 1690650976001.834], [2612296, 1690650976001.878], [2612296, 1690650976001.8828], [2612296, 1690650976001.888], [2612296, 1690650977001.99], [2612296, 1690650977002.025], [2612296, 1690650977002.028], [2612296, 1690650977002.033], [2612296, 1690650978000.5708], [2612296, 1690650978000.776], [2612296, 1690650978001.161], [2612296, 1690650978002.143], [2612296, 1690650979001.2852], [2612296, 1690650979001.2979], [2612296, 1690650979001.301], [2612296, 1690650979001.6929], [2612296, 1690650980001.412], [2612296, 1690650980001.412], [2612296, 1690650980001.4148], [2612296, 1690650980001.419], [2612296, 1690650981001.569], [2612296, 1690650981001.572], [2612296, 1690650981001.573], [2612296, 1690650981001.581], [2612296, 1690650982001.708], [2612296, 1690650982001.709], [2612296, 1690650982001.7112], [2612296, 1690650982001.7212], [2612296, 1690650983001.864], [2612296, 1690650983001.8691], [2612296, 1690650983001.8718], [2612296, 1690650983001.874], [2612296, 1690650984001.302], [2612296, 1690650984001.99], [2612296, 1690650984002.014], [2612296, 1690650984002.015], [2612296, 1690650985001.1619], [2612296, 1690650985001.165], [2612296, 1690650985001.4321], [2612296, 1690650985001.458], [2612296, 1690650986001.2952], [2612296, 1690650986001.3052], [2612296, 1690650986001.307], [2612296, 1690650986001.574], [2612296, 1690650987000.633], [2612296, 1690650987000.747], [2612296, 1690650987001.422], [2612296, 1690650987001.4358], [2612296, 1690650988001.52], [2612296, 1690650988001.572], [2612296, 1690650988001.591], [2612296, 1690650988001.8691], [2612296, 1690650989001.482], [2612296, 1690650989001.71], [2612296, 1690650989001.7239], [2612296, 1690650989001.7432], [2612296, 1690650990001.013], [2612296, 1690650990001.63], [2612296, 1690650990001.63], [2612296, 1690650990001.855], [2612296, 1690650991001.147], [2612296, 1690650991001.758], [2612296, 1690650991001.758], [2612296, 1690650991001.77], [2612296, 1690650992001.2869], [2612296, 1690650992001.292], [2612296, 1690650992001.8828], [2612296, 1690650992001.898], [2612296, 1690650993001.4211], [2612296, 1690650993001.422], [2612296, 1690650993001.426], [2612296, 1690650993002.01], [2612296, 1690650994001.166], [2612296, 1690650994001.169], [2612296, 1690650994001.549], [2612296, 1690650994001.568], [2612296, 1690650995001.2952], [2612296, 1690650995001.31], [2612296, 1690650995001.31], [2612296, 1690650995001.6902], [2612296, 1690650996001.423], [2612296, 1690650996001.429], [2612296, 1690650996001.434], [2612296, 1690650996001.4358], [2612296, 1690650997001.538], [2612296, 1690650997001.558], [2612296, 1690650997001.563], [2612296, 1690650997001.565], [2612296, 1690650998001.653], [2612296, 1690650998001.688], [2612296, 1690650998001.6902], [2612296, 1690650998001.6938], [2612296, 1690650999001.811], [2612296, 1690650999001.8281], [2612296, 1690650999001.8381], [2612296, 1690650999001.84], [2612296, 1690651000001.975], [2612296, 1690651000001.9788], [2612296, 1690651000001.984], [2612296, 1690651000001.988], [2612296, 1690651001002.115], [2612296, 1690651001002.126], [2612296, 1690651001002.126], [2612296, 1690651001002.128], [2612296, 1690651002001.239], [2612296, 1690651002001.2522], [2612296, 1690651002001.254], [2612296, 1690651002001.3198], [2612296, 1690651003001.366], [2612296, 1690651003001.3682], [2612296, 1690651003001.3691], [2612296, 1690651003001.4312], [2612296, 1690651004001.52], [2612296, 1690651004001.523], [2612296, 1690651004001.529], [2612296, 1690651004001.575], [2612296, 1690651005001.6619], [2612296, 1690651005001.666], [2612296, 1690651005001.6692], [2612296, 1690651005001.7039], [2612296, 1690651006001.801], [2612296, 1690651006001.8088], [2612296, 1690651006001.8098], [2612296, 1690651006001.8298], [2612296, 1690651007001.955], [2612296, 1690651007001.958], [2612296, 1690651007001.959], [2612296, 1690651007001.964], [2612296, 1690651008002.077], [2612296, 1690651008002.084], [2612296, 1690651008002.0889], [2612296, 1690651008002.09], [2612296, 1690651009001.238], [2612296, 1690651009001.241], [2612296, 1690651009001.2422], [2612296, 1690651009001.2449], [2612296, 1690651010001.367], [2612296, 1690651010001.3708], [2612296, 1690651010001.374], [2612296, 1690651010001.375], [2612296, 1690651011001.502], [2612296, 1690651011001.509], [2612296, 1690651011001.51], [2612296, 1690651011001.511], [2612296, 1690651012001.632], [2612296, 1690651012001.6372], [2612296, 1690651012001.6372], [2612296, 1690651012001.663], [2612296, 1690651013001.7651], [2612296, 1690651013001.771], [2612296, 1690651013001.772], [2612296, 1690651013001.7852], [2612296, 1690651014001.93], [2612296, 1690651014001.933], [2612296, 1690651014001.935], [2612296, 1690651014001.9358], [2612296, 1690651015002.072], [2612296, 1690651015002.0742], [2612296, 1690651015002.075], [2612296, 1690651015002.076], [2612296, 1690651016001.213], [2612296, 1690651016001.222], [2612296, 1690651016001.223], [2612296, 1690651016001.2239], [2612296, 1690651017001.346], [2612296, 1690651017001.3508], [2612296, 1690651017001.3508], [2612296, 1690651017001.355], [2612296, 1690651018001.472], [2612296, 1690651018001.475], [2612296, 1690651018001.4778], [2612296, 1690651018001.481], [2612300, 1690651019001.585], [2612300, 1690651019001.587], [2612300, 1690651019001.615], [2612300, 1690651019001.642], [2612296, 1690651020000.8599], [2612296, 1690651020001.7249], [2612296, 1690651020001.729], [2612296, 1690651020001.7358], [2612296, 1690651021001.8582], [2612296, 1690651021001.8591], [2612296, 1690651021001.868], [2612296, 1690651021001.8691], [2612296, 1690651022001.984], [2612296, 1690651022001.988], [2612296, 1690651022001.989], [2612296, 1690651022001.989], [2612296, 1690651023002.104], [2612296, 1690651023002.114], [2612296, 1690651023002.114], [2612296, 1690651023002.114], [2612296, 1690651024000.761], [2612296, 1690651024001.174], [2612296, 1690651024001.222], [2612296, 1690651024001.247], [2612296, 1690651025001.3262], [2612296, 1690651025001.375], [2612296, 1690651025001.3801], [2612296, 1690651025001.395], [2612296, 1690651026001.462], [2612296, 1690651026001.491], [2612296, 1690651026001.4988], [2612296, 1690651026001.518], [2612296, 1690651027001.597], [2612296, 1690651027001.611], [2612296, 1690651027001.622], [2612296, 1690651027001.635], [2612296, 1690651028001.738], [2612296, 1690651028001.7449], [2612296, 1690651028001.752], [2612296, 1690651028001.7651], [2612296, 1690651029001.887], [2612296, 1690651029001.893], [2612296, 1690651029001.896], [2612296, 1690651029001.91], [2612296, 1690651030002.046], [2612296, 1690651030002.051], [2612296, 1690651030002.051], [2612296, 1690651030002.055], [2612296, 1690651031001.175], [2612296, 1690651031001.177], [2612296, 1690651031001.181], [2612296, 1690651031001.185], [2612296, 1690651032001.2942], [2612296, 1690651032001.312], [2612296, 1690651032001.313], [2612296, 1690651032001.3162], [2612296, 1690651033001.4248], [2612296, 1690651033001.4348], [2612296, 1690651033001.447], [2612296, 1690651033001.447], [2612296, 1690651034001.539], [2612296, 1690651034001.57], [2612296, 1690651034001.575], [2612296, 1690651034001.581], [2612296, 1690651035001.699], [2612296, 1690651035001.7212], [2612296, 1690651035001.727], [2612296, 1690651035001.734], [2612296, 1690651036001.396], [2612296, 1690651036001.8372], [2612296, 1690651036001.849], [2612296, 1690651036001.865], [2612296, 1690651037001.5518], [2612296, 1690651037001.557], [2612296, 1690651037001.969], [2612296, 1690651037002.001], [2612296, 1690651038001.133], [2612296, 1690651038001.677], [2612296, 1690651038001.682], [2612296, 1690651038001.6902], [2612296, 1690651039001.282], [2612296, 1690651039001.2852], [2612296, 1690651039001.8], [2612296, 1690651039001.829], [2612296, 1690651040001.443], [2612296, 1690651040001.445], [2612296, 1690651040001.447], [2612296, 1690651040001.954], [2612296, 1690651041001.593], [2612296, 1690651041001.595], [2612296, 1690651041001.602], [2612296, 1690651041001.606], [2612296, 1690651042001.7239], [2612296, 1690651042001.729], [2612296, 1690651042001.7332], [2612296, 1690651042001.7332], [2612296, 1690651043001.8801], [2612296, 1690651043001.881], [2612296, 1690651043001.8828], [2612296, 1690651043001.885], [2612296, 1690651044002.036], [2612296, 1690651044002.037], [2612296, 1690651044002.041], [2612296, 1690651044002.041], [2612296, 1690651045001.197], [2612296, 1690651045001.198], [2612296, 1690651045001.202], [2612296, 1690651045001.2039], [2612296, 1690651046001.325], [2612296, 1690651046001.3262], [2612296, 1690651046001.3298], [2612296, 1690651046001.3298]]}, {"target": "component_id", "datapoints": [[3, 1690650946001.684], [4, 1690650947001.85], [3, 1690650947001.8518], [2, 1690650947001.855], [1, 1690650947001.856], [4, 1690650948001.998], [1, 1690650948001.9998], [2, 1690650948002.005], [3, 1690650948002.007], [1, 1690650949001.149], [2, 1690650949001.149], [4, 1690650949001.155], [3, 1690650949001.156], [4, 1690650950001.283], [1, 1690650950001.2852], [3, 1690650950001.2869], [2, 1690650950001.2878], [1, 1690650951001.4312], [2, 1690650951001.4358], [4, 1690650951001.4358], [3, 1690650951001.437], [1, 1690650952001.582], [4, 1690650952001.594], [3, 1690650952001.596], [2, 1690650952001.6], [4, 1690650953001.7239], [1, 1690650953001.728], [3, 1690650953001.73], [2, 1690650953001.7322], [4, 1690650954001.874], [3, 1690650954001.875], [2, 1690650954001.877], [1, 1690650954001.878], [3, 1690650955002.011], [2, 1690650955002.018], [1, 1690650955002.019], [4, 1690650955002.0212], [2, 1690650956001.15], [1, 1690650956001.155], [3, 1690650956001.156], [4, 1690650956001.156], [2, 1690650957000.5261], [1, 1690650957001.283], [3, 1690650957001.3088], [4, 1690650957001.311], [2, 1690650958001.45], [1, 1690650958001.4531], [3, 1690650958001.454], [4, 1690650958001.46], [3, 1690650959001.58], [4, 1690650959001.581], [1, 1690650959001.583], [2, 1690650959001.583], [3, 1690650960001.695], [1, 1690650960001.7039], [2, 1690650960001.706], [4, 1690650960001.707], [1, 1690650961001.8308], [3, 1690650961001.834], [4, 1690650961001.839], [2, 1690650961001.847], [4, 1690650962001.98], [1, 1690650962001.982], [2, 1690650962001.985], [3, 1690650962001.986], [1, 1690650963002.1409], [4, 1690650963002.144], [3, 1690650963002.146], [2, 1690650963002.1492], [1, 1690650964001.281], [3, 1690650964001.282], [4, 1690650964001.2852], [2, 1690650964001.286], [3, 1690650965001.398], [1, 1690650965001.407], [2, 1690650965001.407], [4, 1690650965001.409], [3, 1690650966001.525], [1, 1690650966001.529], [2, 1690650966001.529], [4, 1690650966001.532], [3, 1690650967001.6682], [4, 1690650967001.671], [1, 1690650967001.674], [2, 1690650967001.674], [2, 1690650968001.826], [1, 1690650968001.829], [3, 1690650968001.832], [4, 1690650968001.833], [1, 1690650969000.9832], [2, 1690650969001.8591], [3, 1690650969001.948], [4, 1690650969001.977], [2, 1690650970001.983], [3, 1690650970002.081], [1, 1690650970002.098], [4, 1690650970002.107], [1, 1690650971001.212], [3, 1690650971001.215], [4, 1690650971001.222], [2, 1690650971002.112], [2, 1690650972001.2522], [3, 1690650972001.343], [1, 1690650972001.346], [4, 1690650972001.3472], [2, 1690650973001.412], [1, 1690650973001.486], [3, 1690650973001.49], [4, 1690650973001.493], [2, 1690650974001.556], [3, 1690650974001.618], [1, 1690650974001.6199], [4, 1690650974001.627], [2, 1690650975001.686], [3, 1690650975001.737], [1, 1690650975001.7449], [4, 1690650975001.749], [2, 1690650976001.834], [3, 1690650976001.878], [1, 1690650976001.8828], [4, 1690650976001.888], [2, 1690650977001.99], [1, 1690650977002.025], [3, 1690650977002.028], [4, 1690650977002.033], [1, 1690650978000.5708], [3, 1690650978000.776], [4, 1690650978001.161], [2, 1690650978002.143], [2, 1690650979001.2852], [3, 1690650979001.2979], [4, 1690650979001.301], [1, 1690650979001.6929], [2, 1690650980001.412], [3, 1690650980001.412], [4, 1690650980001.4148], [1, 1690650980001.419], [2, 1690650981001.569], [3, 1690650981001.572], [1, 1690650981001.573], [4, 1690650981001.581], [3, 1690650982001.708], [2, 1690650982001.709], [4, 1690650982001.7112], [1, 1690650982001.7212], [2, 1690650983001.864], [4, 1690650983001.8691], [1, 1690650983001.8718], [3, 1690650983001.874], [2, 1690650984001.302], [1, 1690650984001.99], [4, 1690650984002.014], [3, 1690650984002.015], [3, 1690650985001.1619], [4, 1690650985001.165], [1, 1690650985001.4321], [2, 1690650985001.458], [4, 1690650986001.2952], [1, 1690650986001.3052], [3, 1690650986001.307], [2, 1690650986001.574], [1, 1690650987000.633], [2, 1690650987000.747], [3, 1690650987001.422], [4, 1690650987001.4358], [3, 1690650988001.52], [1, 1690650988001.572], [4, 1690650988001.591], [2, 1690650988001.8691], [3, 1690650989001.482], [2, 1690650989001.71], [1, 1690650989001.7239], [4, 1690650989001.7432], [1, 1690650990001.013], [2, 1690650990001.63], [3, 1690650990001.63], [4, 1690650990001.855], [1, 1690650991001.147], [3, 1690650991001.758], [4, 1690650991001.758], [2, 1690650991001.77], [1, 1690650992001.2869], [4, 1690650992001.292], [2, 1690650992001.8828], [3, 1690650992001.898], [1, 1690650993001.4211], [2, 1690650993001.422], [4, 1690650993001.426], [3, 1690650993002.01], [3, 1690650994001.166], [2, 1690650994001.169], [4, 1690650994001.549], [1, 1690650994001.568], [3, 1690650995001.2952], [2, 1690650995001.31], [4, 1690650995001.31], [1, 1690650995001.6902], [3, 1690650996001.423], [2, 1690650996001.429], [4, 1690650996001.434], [1, 1690650996001.4358], [3, 1690650997001.538], [4, 1690650997001.558], [2, 1690650997001.563], [1, 1690650997001.565], [3, 1690650998001.653], [1, 1690650998001.688], [4, 1690650998001.6902], [2, 1690650998001.6938], [3, 1690650999001.811], [1, 1690650999001.8281], [4, 1690650999001.8381], [2, 1690650999001.84], [3, 1690651000001.975], [1, 1690651000001.9788], [4, 1690651000001.984], [2, 1690651000001.988], [4, 1690651001002.115], [1, 1690651001002.126], [3, 1690651001002.126], [2, 1690651001002.128], [1, 1690651002001.239], [2, 1690651002001.2522], [4, 1690651002001.254], [3, 1690651002001.3198], [2, 1690651003001.366], [1, 1690651003001.3682], [4, 1690651003001.3691], [3, 1690651003001.4312], [4, 1690651004001.52], [2, 1690651004001.523], [1, 1690651004001.529], [3, 1690651004001.575], [2, 1690651005001.6619], [1, 1690651005001.666], [4, 1690651005001.6692], [3, 1690651005001.7039], [2, 1690651006001.801], [1, 1690651006001.8088], [4, 1690651006001.8098], [3, 1690651006001.8298], [2, 1690651007001.955], [4, 1690651007001.958], [1, 1690651007001.959], [3, 1690651007001.964], [2, 1690651008002.077], [3, 1690651008002.084], [1, 1690651008002.0889], [4, 1690651008002.09], [3, 1690651009001.238], [1, 1690651009001.241], [4, 1690651009001.2422], [2, 1690651009001.2449], [3, 1690651010001.367], [2, 1690651010001.3708], [4, 1690651010001.374], [1, 1690651010001.375], [4, 1690651011001.502], [3, 1690651011001.509], [2, 1690651011001.51], [1, 1690651011001.511], [2, 1690651012001.632], [1, 1690651012001.6372], [4, 1690651012001.6372], [3, 1690651012001.663], [2, 1690651013001.7651], [4, 1690651013001.771], [1, 1690651013001.772], [3, 1690651013001.7852], [2, 1690651014001.93], [4, 1690651014001.933], [1, 1690651014001.935], [3, 1690651014001.9358], [3, 1690651015002.072], [2, 1690651015002.0742], [4, 1690651015002.075], [1, 1690651015002.076], [2, 1690651016001.213], [4, 1690651016001.222], [3, 1690651016001.223], [1, 1690651016001.2239], [2, 1690651017001.346], [1, 1690651017001.3508], [3, 1690651017001.3508], [4, 1690651017001.355], [1, 1690651018001.472], [2, 1690651018001.475], [4, 1690651018001.4778], [3, 1690651018001.481], [4, 1690651019001.585], [2, 1690651019001.587], [3, 1690651019001.615], [1, 1690651019001.642], [1, 1690651020000.8599], [3, 1690651020001.7249], [2, 1690651020001.729], [4, 1690651020001.7358], [2, 1690651021001.8582], [4, 1690651021001.8591], [1, 1690651021001.868], [3, 1690651021001.8691], [2, 1690651022001.984], [1, 1690651022001.988], [3, 1690651022001.989], [4, 1690651022001.989], [2, 1690651023002.104], [1, 1690651023002.114], [3, 1690651023002.114], [4, 1690651023002.114], [2, 1690651024000.761], [3, 1690651024001.174], [4, 1690651024001.222], [1, 1690651024001.247], [3, 1690651025001.3262], [2, 1690651025001.375], [4, 1690651025001.3801], [1, 1690651025001.395], [3, 1690651026001.462], [2, 1690651026001.491], [4, 1690651026001.4988], [1, 1690651026001.518], [3, 1690651027001.597], [2, 1690651027001.611], [4, 1690651027001.622], [1, 1690651027001.635], [2, 1690651028001.738], [3, 1690651028001.7449], [4, 1690651028001.752], [1, 1690651028001.7651], [2, 1690651029001.887], [3, 1690651029001.893], [4, 1690651029001.896], [1, 1690651029001.91], [2, 1690651030002.046], [3, 1690651030002.051], [4, 1690651030002.051], [1, 1690651030002.055], [1, 1690651031001.175], [2, 1690651031001.177], [3, 1690651031001.181], [4, 1690651031001.185], [1, 1690651032001.2942], [2, 1690651032001.312], [3, 1690651032001.313], [4, 1690651032001.3162], [1, 1690651033001.4248], [2, 1690651033001.4348], [3, 1690651033001.447], [4, 1690651033001.447], [1, 1690651034001.539], [2, 1690651034001.57], [4, 1690651034001.575], [3, 1690651034001.581], [1, 1690651035001.699], [2, 1690651035001.7212], [4, 1690651035001.727], [3, 1690651035001.734], [2, 1690651036001.396], [4, 1690651036001.8372], [1, 1690651036001.849], [3, 1690651036001.865], [4, 1690651037001.5518], [2, 1690651037001.557], [1, 1690651037001.969], [3, 1690651037002.001], [3, 1690651038001.133], [1, 1690651038001.677], [2, 1690651038001.682], [4, 1690651038001.6902], [3, 1690651039001.282], [1, 1690651039001.2852], [4, 1690651039001.8], [2, 1690651039001.829], [3, 1690651040001.443], [1, 1690651040001.445], [4, 1690651040001.447], [2, 1690651040001.954], [3, 1690651041001.593], [1, 1690651041001.595], [2, 1690651041001.602], [4, 1690651041001.606], [4, 1690651042001.7239], [1, 1690651042001.729], [2, 1690651042001.7332], [3, 1690651042001.7332], [4, 1690651043001.8801], [2, 1690651043001.881], [1, 1690651043001.8828], [3, 1690651043001.885], [4, 1690651044002.036], [2, 1690651044002.037], [1, 1690651044002.041], [3, 1690651044002.041], [2, 1690651045001.197], [1, 1690651045001.198], [4, 1690651045001.202], [3, 1690651045001.2039], [4, 1690651046001.325], [3, 1690651046001.3262], [1, 1690651046001.3298], [2, 1690651046001.3298]]}, {"target": "job_id", "datapoints": [[0, 1690650946001.684], [0, 1690650947001.85], [0, 1690650947001.8518], [0, 1690650947001.855], [0, 1690650947001.856], [0, 1690650948001.998], [0, 1690650948001.9998], [0, 1690650948002.005], [0, 1690650948002.007], [0, 1690650949001.149], [0, 1690650949001.149], [0, 1690650949001.155], [0, 1690650949001.156], [0, 1690650950001.283], [0, 1690650950001.2852], [0, 1690650950001.2869], [0, 1690650950001.2878], [0, 1690650951001.4312], [0, 1690650951001.4358], [0, 1690650951001.4358], [0, 1690650951001.437], [0, 1690650952001.582], [0, 1690650952001.594], [0, 1690650952001.596], [0, 1690650952001.6], [0, 1690650953001.7239], [0, 1690650953001.728], [0, 1690650953001.73], [0, 1690650953001.7322], [0, 1690650954001.874], [0, 1690650954001.875], [0, 1690650954001.877], [0, 1690650954001.878], [0, 1690650955002.011], [0, 1690650955002.018], [0, 1690650955002.019], [0, 1690650955002.0212], [0, 1690650956001.15], [0, 1690650956001.155], [0, 1690650956001.156], [0, 1690650956001.156], [0, 1690650957000.5261], [0, 1690650957001.283], [0, 1690650957001.3088], [0, 1690650957001.311], [0, 1690650958001.45], [0, 1690650958001.4531], [0, 1690650958001.454], [0, 1690650958001.46], [0, 1690650959001.58], [0, 1690650959001.581], [0, 1690650959001.583], [0, 1690650959001.583], [0, 1690650960001.695], [0, 1690650960001.7039], [0, 1690650960001.706], [0, 1690650960001.707], [0, 1690650961001.8308], [0, 1690650961001.834], [0, 1690650961001.839], [0, 1690650961001.847], [0, 1690650962001.98], [0, 1690650962001.982], [0, 1690650962001.985], [0, 1690650962001.986], [0, 1690650963002.1409], [0, 1690650963002.144], [0, 1690650963002.146], [0, 1690650963002.1492], [0, 1690650964001.281], [0, 1690650964001.282], [0, 1690650964001.2852], [0, 1690650964001.286], [0, 1690650965001.398], [0, 1690650965001.407], [0, 1690650965001.407], [0, 1690650965001.409], [0, 1690650966001.525], [0, 1690650966001.529], [0, 1690650966001.529], [0, 1690650966001.532], [0, 1690650967001.6682], [0, 1690650967001.671], [0, 1690650967001.674], [0, 1690650967001.674], [0, 1690650968001.826], [0, 1690650968001.829], [0, 1690650968001.832], [0, 1690650968001.833], [0, 1690650969000.9832], [0, 1690650969001.8591], [0, 1690650969001.948], [0, 1690650969001.977], [0, 1690650970001.983], [0, 1690650970002.081], [0, 1690650970002.098], [0, 1690650970002.107], [0, 1690650971001.212], [0, 1690650971001.215], [0, 1690650971001.222], [0, 1690650971002.112], [0, 1690650972001.2522], [0, 1690650972001.343], [0, 1690650972001.346], [0, 1690650972001.3472], [0, 1690650973001.412], [0, 1690650973001.486], [0, 1690650973001.49], [0, 1690650973001.493], [0, 1690650974001.556], [0, 1690650974001.618], [0, 1690650974001.6199], [0, 1690650974001.627], [0, 1690650975001.686], [0, 1690650975001.737], [0, 1690650975001.7449], [0, 1690650975001.749], [0, 1690650976001.834], [0, 1690650976001.878], [0, 1690650976001.8828], [0, 1690650976001.888], [0, 1690650977001.99], [0, 1690650977002.025], [0, 1690650977002.028], [0, 1690650977002.033], [0, 1690650978000.5708], [0, 1690650978000.776], [0, 1690650978001.161], [0, 1690650978002.143], [0, 1690650979001.2852], [0, 1690650979001.2979], [0, 1690650979001.301], [0, 1690650979001.6929], [0, 1690650980001.412], [0, 1690650980001.412], [0, 1690650980001.4148], [0, 1690650980001.419], [0, 1690650981001.569], [0, 1690650981001.572], [0, 1690650981001.573], [0, 1690650981001.581], [0, 1690650982001.708], [0, 1690650982001.709], [0, 1690650982001.7112], [0, 1690650982001.7212], [0, 1690650983001.864], [0, 1690650983001.8691], [0, 1690650983001.8718], [0, 1690650983001.874], [0, 1690650984001.302], [0, 1690650984001.99], [0, 1690650984002.014], [0, 1690650984002.015], [0, 1690650985001.1619], [0, 1690650985001.165], [0, 1690650985001.4321], [0, 1690650985001.458], [0, 1690650986001.2952], [0, 1690650986001.3052], [0, 1690650986001.307], [0, 1690650986001.574], [0, 1690650987000.633], [0, 1690650987000.747], [0, 1690650987001.422], [0, 1690650987001.4358], [0, 1690650988001.52], [0, 1690650988001.572], [0, 1690650988001.591], [0, 1690650988001.8691], [0, 1690650989001.482], [0, 1690650989001.71], [0, 1690650989001.7239], [0, 1690650989001.7432], [0, 1690650990001.013], [0, 1690650990001.63], [0, 1690650990001.63], [0, 1690650990001.855], [0, 1690650991001.147], [0, 1690650991001.758], [0, 1690650991001.758], [0, 1690650991001.77], [0, 1690650992001.2869], [0, 1690650992001.292], [0, 1690650992001.8828], [0, 1690650992001.898], [0, 1690650993001.4211], [0, 1690650993001.422], [0, 1690650993001.426], [0, 1690650993002.01], [0, 1690650994001.166], [0, 1690650994001.169], [0, 1690650994001.549], [0, 1690650994001.568], [0, 1690650995001.2952], [0, 1690650995001.31], [0, 1690650995001.31], [0, 1690650995001.6902], [0, 1690650996001.423], [0, 1690650996001.429], [0, 1690650996001.434], [0, 1690650996001.4358], [0, 1690650997001.538], [0, 1690650997001.558], [0, 1690650997001.563], [0, 1690650997001.565], [0, 1690650998001.653], [0, 1690650998001.688], [0, 1690650998001.6902], [0, 1690650998001.6938], [0, 1690650999001.811], [0, 1690650999001.8281], [0, 1690650999001.8381], [0, 1690650999001.84], [0, 1690651000001.975], [0, 1690651000001.9788], [0, 1690651000001.984], [0, 1690651000001.988], [0, 1690651001002.115], [0, 1690651001002.126], [0, 1690651001002.126], [0, 1690651001002.128], [0, 1690651002001.239], [0, 1690651002001.2522], [0, 1690651002001.254], [0, 1690651002001.3198], [0, 1690651003001.366], [0, 1690651003001.3682], [0, 1690651003001.3691], [0, 1690651003001.4312], [0, 1690651004001.52], [0, 1690651004001.523], [0, 1690651004001.529], [0, 1690651004001.575], [0, 1690651005001.6619], [0, 1690651005001.666], [0, 1690651005001.6692], [0, 1690651005001.7039], [0, 1690651006001.801], [0, 1690651006001.8088], [0, 1690651006001.8098], [0, 1690651006001.8298], [0, 1690651007001.955], [0, 1690651007001.958], [0, 1690651007001.959], [0, 1690651007001.964], [0, 1690651008002.077], [0, 1690651008002.084], [0, 1690651008002.0889], [0, 1690651008002.09], [0, 1690651009001.238], [0, 1690651009001.241], [0, 1690651009001.2422], [0, 1690651009001.2449], [0, 1690651010001.367], [0, 1690651010001.3708], [0, 1690651010001.374], [0, 1690651010001.375], [0, 1690651011001.502], [0, 1690651011001.509], [0, 1690651011001.51], [0, 1690651011001.511], [0, 1690651012001.632], [0, 1690651012001.6372], [0, 1690651012001.6372], [0, 1690651012001.663], [0, 1690651013001.7651], [0, 1690651013001.771], [0, 1690651013001.772], [0, 1690651013001.7852], [0, 1690651014001.93], [0, 1690651014001.933], [0, 1690651014001.935], [0, 1690651014001.9358], [0, 1690651015002.072], [0, 1690651015002.0742], [0, 1690651015002.075], [0, 1690651015002.076], [0, 1690651016001.213], [0, 1690651016001.222], [0, 1690651016001.223], [0, 1690651016001.2239], [0, 1690651017001.346], [0, 1690651017001.3508], [0, 1690651017001.3508], [0, 1690651017001.355], [0, 1690651018001.472], [0, 1690651018001.475], [0, 1690651018001.4778], [0, 1690651018001.481], [0, 1690651019001.585], [0, 1690651019001.587], [0, 1690651019001.615], [0, 1690651019001.642], [0, 1690651020000.8599], [0, 1690651020001.7249], [0, 1690651020001.729], [0, 1690651020001.7358], [0, 1690651021001.8582], [0, 1690651021001.8591], [0, 1690651021001.868], [0, 1690651021001.8691], [0, 1690651022001.984], [0, 1690651022001.988], [0, 1690651022001.989], [0, 1690651022001.989], [0, 1690651023002.104], [0, 1690651023002.114], [0, 1690651023002.114], [0, 1690651023002.114], [0, 1690651024000.761], [0, 1690651024001.174], [0, 1690651024001.222], [0, 1690651024001.247], [0, 1690651025001.3262], [0, 1690651025001.375], [0, 1690651025001.3801], [0, 1690651025001.395], [0, 1690651026001.462], [0, 1690651026001.491], [0, 1690651026001.4988], [0, 1690651026001.518], [0, 1690651027001.597], [0, 1690651027001.611], [0, 1690651027001.622], [0, 1690651027001.635], [0, 1690651028001.738], [0, 1690651028001.7449], [0, 1690651028001.752], [0, 1690651028001.7651], [0, 1690651029001.887], [0, 1690651029001.893], [0, 1690651029001.896], [0, 1690651029001.91], [0, 1690651030002.046], [0, 1690651030002.051], [0, 1690651030002.051], [0, 1690651030002.055], [0, 1690651031001.175], [0, 1690651031001.177], [0, 1690651031001.181], [0, 1690651031001.185], [0, 1690651032001.2942], [0, 1690651032001.312], [0, 1690651032001.313], [0, 1690651032001.3162], [0, 1690651033001.4248], [0, 1690651033001.4348], [0, 1690651033001.447], [0, 1690651033001.447], [0, 1690651034001.539], [0, 1690651034001.57], [0, 1690651034001.575], [0, 1690651034001.581], [0, 1690651035001.699], [0, 1690651035001.7212], [0, 1690651035001.727], [0, 1690651035001.734], [0, 1690651036001.396], [0, 1690651036001.8372], [0, 1690651036001.849], [0, 1690651036001.865], [0, 1690651037001.5518], [0, 1690651037001.557], [0, 1690651037001.969], [0, 1690651037002.001], [0, 1690651038001.133], [0, 1690651038001.677], [0, 1690651038001.682], [0, 1690651038001.6902], [0, 1690651039001.282], [0, 1690651039001.2852], [0, 1690651039001.8], [0, 1690651039001.829], [0, 1690651040001.443], [0, 1690651040001.445], [0, 1690651040001.447], [0, 1690651040001.954], [0, 1690651041001.593], [0, 1690651041001.595], [0, 1690651041001.602], [0, 1690651041001.606], [0, 1690651042001.7239], [0, 1690651042001.729], [0, 1690651042001.7332], [0, 1690651042001.7332], [0, 1690651043001.8801], [0, 1690651043001.881], [0, 1690651043001.8828], [0, 1690651043001.885], [0, 1690651044002.036], [0, 1690651044002.037], [0, 1690651044002.041], [0, 1690651044002.041], [0, 1690651045001.197], [0, 1690651045001.198], [0, 1690651045001.202], [0, 1690651045001.2039], [0, 1690651046001.325], [0, 1690651046001.3262], [0, 1690651046001.3298], [0, 1690651046001.3298]]}]'
comp_ids:{1, 2, 3, 4}
2023-07-29T12:17:34-05:00 INFO: query check RC: 0
ac8036e337e65a081fdeea142519d04372a5f96723c49f461d256b3bed90315d
2023-07-29T12:18:06-05:00 INFO: Adding DSOS data source in Grafana
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   479  100   366  100   113   2407    743 --:--:-- --:--:-- --:--:--  3172
{"datasource":{"id":1,"uid":"s098mBq4k","orgId":1,"name":"SOS-2","type":"dsosds","typeLogoUrl":"","access":"proxy","url":"http://mtest-ui/grafana","user":"","database":"","basicAuth":false,"basicAuthUser":"","withCredentials":false,"isDefault":true,"jsonData":{},"secureJsonFields":{},"version":1,"readOnly":false},"id":1,"message":"Datasource added","name":"SOS-2"}
2023-07-29T12:18:07-05:00 INFO: Checking grafana data
2023-07-29T12:18:07-05:00 INFO: Grafana data check, rc: 0
2023-07-29T12:18:07-05:00 INFO: Cleaning up ...
mtest-samp-1
mtest-samp-2
mtest-samp-3
mtest-samp-4
mtest-agg-11
mtest-agg-12
mtest-agg-2
mtest-maestro
mtest-ui
mtest-grafana
2023-07-29T12:18:12-05:00 INFO: DONE
2023-07-29 12:18:22 INFO: ----------------------------------------------
2023-07-29 12:18:22 INFO: ==== Summary ====
ldmsd_ctrl_test: [01;32mPASSED[0m
papi_store_test: [01;32mPASSED[0m
updtr_status_test: [01;32mPASSED[0m
ldms_stream_test: [01;32mPASSED[0m
ldmsd_auth_ovis_test: [01;32mPASSED[0m
store_app_test: [01;32mPASSED[0m
store_list_record_test: [01;32mPASSED[0m
set_array_test: [01;32mPASSED[0m
ovis_ev_test: [01;32mPASSED[0m
setgroup_test: [01;32mPASSED[0m
ldms_list_test: [01;32mPASSED[0m
slurm_stream_test: [01;32mPASSED[0m
maestro_cfg_test: [01;32mPASSED[0m
ldms_set_info_test: [01;32mPASSED[0m
spank_notifier_test: [01;31mFAILED[0m
agg_slurm_test: [01;32mPASSED[0m
updtr_match_add_test: [01;32mPASSED[0m
updtr_match_del_test: [01;32mPASSED[0m
direct_prdcr_subscribe_test: [01;32mPASSED[0m
ldmsd_auth_test: [01;32mPASSED[0m
cont-test-maestro-munge: [01;32mPASSED[0m
cont-test-ldms: [01;32mPASSED[0m
run_inside_cont_test.py: [01;32mPASSED[0m
prdcr_subscribe_test: [01;32mPASSED[0m
cont-test-maestro-hostmunge: [01;32mPASSED[0m
ldmsd_long_config_test: [01;32mPASSED[0m
maestro_raft_test: [01;32mPASSED[0m
ldmsd_stream_test2: [01;32mPASSED[0m
updtr_start_test: [01;32mPASSED[0m
failover_test: [01;32mPASSED[0m
ldms_record_test: [01;32mPASSED[0m
libovis_log_test: [01;32mPASSED[0m
cont-test-maestro: [01;32mPASSED[0m
ldmsd_decomp_test: [01;32mPASSED[0m
ldmsd_flex_decomp_test: [01;32mPASSED[0m
quick_set_add_rm_test: [01;32mPASSED[0m
updtr_prdcr_del_test: [01;32mPASSED[0m
updtr_prdcr_add_test: [01;32mPASSED[0m
mt-slurm-test: [01;32mPASSED[0m
ovis_json_test: [01;32mPASSED[0m
papi_sampler_test: [01;32mPASSED[0m
syspapi_test: [01;32mPASSED[0m
updtr_del_test: [01;32mPASSED[0m
ldmsd_autointerval_test: [01;32mPASSED[0m
ldms_rail_test: [01;32mPASSED[0m
updtr_add_test: [01;32mPASSED[0m
direct_ldms_ls_conn_test: [01;32mPASSED[0m
set_array_hang_test: [01;32mPASSED[0m
slurm_sampler2_test: [01;31mFAILED[0m
ldms_schema_digest_test: [01;32mPASSED[0m
agg_test: [01;32mPASSED[0m
------------------------------------------
Total tests passed: 49/51
------------------------------------------
